\documentclass[fr,license=none]{../../../eplsummary}

%\usepackage[hideerrors]{xcolor}
\usepackage{array}
\usepackage{fancybox}
\usepackage{float}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{qtree}
\usepackage{tensor}
\usepackage{multirow}

\DeclareMathOperator{\Sur}{Sur}
\DeclareMathOperator{\In}{In}
\DeclareMathOperator{\newnull}{null}
\DeclareMathOperator{\newim}{Im}
\DeclareMathOperator{\newker}{Ker}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\newrang}{rang}
\DeclareMathOperator{\sev}{sev}

\newcommand{\fle}{\tensor*[_f]{(L)}{_e}}

\hypertitle{Math\'ematique}{1}{FSAB}{1101}
{Guillaume Fran\c{c}ois\and Beno\^it Legat}
{Kouider Ben-Naoum, Olivier Pereira et Michel Verleysen}

\part{Analyse}
% y^
%  |    LYSE
%  | ANA
%  +--------->x
\section{Fondements}
\subsection{Démonstrations}
\begin{itemize}
  \item Implication
  \item Contraposition
  \item Equivalence
  \item Contradiction (absurde)
  \item Récurrence (induction)
\end{itemize}
\subsection{Relations}
\begin{itemize}
  \item Réflexive ($xRx$)
  \item Symétrique ($xRy \Rightarrow yRx$)
  \item Transitive
    \footnote{$\land$ est le ``et'' logique}
    ($xRy \land yRz \Rightarrow xRz$)
  \item Antisymétrique ($xRy \land yRx \Rightarrow x = y$)
\end{itemize}
$ $\\
\begin{itemize}
  \item Equivalence $\Leftrightarrow$ Symétrique, réflexive, transitive
  \item Ordre partiel $\Leftrightarrow$ Antisymétrique, réflexive, transitive
  \item Ordre total $\Leftrightarrow$ Ordre partiel et $\forall x, y \in A$
    on a $xRy$ OU $yRx$
\end{itemize}

\section{Limite et continuité}
\subsection{Limite $L$ au point $a$}
Soit une fonction $f$ définie sur un intervalle centré en $a$.
\[ \lim_{x \to a} f(x) = L \]
si et seulement si
$\forall \varepsilon > 0, \exists \delta > 0$ tel que
$\forall x \in A$, si $|x - a| \leq \delta$ alors
$|f(x) - L| \leq \varepsilon$.

\subsection{Continuité au point $a$}
Une fonction $f$ est continue au point $a$ si et seulement si
\[ \lim_{x \to a} f(x) = f(a) \]

\subsection{Dérivabilité au point $a$}
$f : I \to \mathbb{R}$ est dérivable en $a \in I$ si et seulement si
\[ \lim_{x \to a} \frac{f(x) - f(a)}{x - a} \]
existe.

\subsection{Théorème des valeurs intermédiaires}
Soit $f : I \to \mathbb{R}$ une fonction continue sur $[a,b] \subseteq I$.

Si $f(a) \leq y \leq f(b)$ ou $f(b) \leq y \leq f(a)$
$\exists c \in [a,b]$ tel que $y = f(c)$.

\subsection{Théorème des bornes atteintes}
Une fonction continue sur un intervalle fermé, bornée atteint ses bornes.
C'est à dire qu'il existe $q, p \in [a,b]$ tel que
$f(q) = \sup f$ sur $[a,b]$
et que $f(p) = \inf f$ sur $[a,b]$.

\subsection{Théorème de Rolle}
Soit $f:I \to \mathbb{R}$ une fonction continue sur $[a,b] \subseteq I$ et dérivable sur $]a,b[$.

Si $f(a) = f(b)$ alors il existe un $c \in ]a,b[$ tel que $f'(c) = 0$.

\subsection{Théorème des accroissements finis}
Soit $f$ continue sur $ [a,b] \rightarrow \mathbb{R}$ et dérivable sur $]a,b[$.

Alors il existe un $c \in ]a,b[$ tel que $f'(c) = \frac{f(b) - f(a)}{b - a}$.

\subsection{Théorème de la valeur constante}
Soit $f$ continue sur $ [a,b] \rightarrow \mathbb{R}$.

Si $f$ est dérivable sur $]a,b[$ et que
$f'(c) = 0$ $\forall x \in ]a,b[$ alors $f$ est constante.

\section{Polynôme de Taylor}
\subsection{Définition}
Etant donné une fonction $f : A \rightarrow \mathbb{R}$,
un naturel $n \geq 1$ et un point $a$ appartenant à \emph{l'intérieur}
de $A$ en lequel $f$ est $n$ fois dérivable,
le polynôme de Taylor d'ordre $n$ de $f$ autour du point $a$ est:
\[ T_n^{f,a}(x) = \sum_{k = 0}^n \frac{f^{(k)}(a)}{k!}(x - a)^k \]

\subsection{Théorème de Taylor}
Soit une fonction $f : A \rightarrow \mathbb{R}$,
un point $a$ appartenant à \emph{l'intérieur} de $A$,
et un naturel $n \geq 1$.

Si $f$ est $n$ fois dérivable en $a$, alors:
\[ \lim_{\substack{x \rightarrow a}}
\frac{f(x) - T_n^{f,a}(x)}{(x - a)^n} = 0 \]

\subsection{Réciproque de Taylor}
Soit une fonction $f : A \rightarrow \mathbb{R}$, un point $a$ appartenant
à \emph{l'intérieur} de $A$, et un naturel $n \geq 1$.

Si $f$ est $n$ fois dérivable en $a$, et si
$P_n(x)$ est un polynôme de degré inférieur ou égal à $n$, vérifiant
\[ \lim_{\substack{x \rightarrow a}} \frac{f(x) - P_n(x)}{(x - a)^n} = 0 \]
alors
\[ P_n(x) = T_n^{f,a}(x) \]

\subsection{Théorème du reste}
Soit une fonction $f : A \rightarrow \mathbb{R}$,
un point $a$ appartenant à \emph{l'intérieur} de $A$,
un intervalle ouvert $I$ tel que $a \in I$ et $I\subset A$
et un naturel $n \geq 1$.

Si $f$ est $n + 1$ fois dérivable sur $I$,
alors $\forall x \in I \backslash \{a\}$,
il existe un point $c$ compris strictement entre $a$ et $x$ tel que
\[ f(x) = T_n^{f,a}(x) + \frac{f^{n+1}(c)(x - a)^{n+1}}{(n + 1)!} \]

\subsection{Dérivée de Taylor}
\[ \left(T_n^{f,a}\right)'(x) =
\sum_{k = 0}^{n-1} \frac{f^{(k+1)}(a)(x - a)^{k}}{k!} \]

\section{Intégration}
\subsection{Implications}
\begin{itemize}
  \item Si $f$ est intégrable alors $f$ est bornée;
  \item Si $f$ est continue alors elle est intégrable.
\end{itemize}

\subsection{Théorème de la moyenne}
Soit $f$ une fonction continue donc intégrable de $[a,b] \rightarrow \mathbb{R}$.
Alors $\exists c \in [a,b]$ tel que $f(c) = \mu{(f)}$,
la moyenne de $f$ sur $[a,b]$,
c'est-à-dire
\[ \int_a^b f(x) \dif x = (b - a)f(c). \]

\subsection{Théorème fondamental (1)}
Soit $f:[a,b] \to \mathbb{R}$ une fonction continue et $p \in [a,b]$,
alors la fonction $F : [a,b] \to \mathbb{R} : x \mapsto F(x)$
telle que
\[ F(x) = \int_p^x f(t) dt \]
est une primitive de $f$.

\subsection{Théorème fondamental (2)}
Soit $f : I \to \mathbb{R}$ une fonction continue,
si $F$ est une primitive de $f$ sur $I$, alors
\[ \int_p^q f(t) dt = F(q) - F(p) \]
avec $p, q \in I$.

\subsection{Corollaire du théorème fondamental}
Soient $I$ et $J$, 2 intervalles,
$f$ une fonction continue de $I \to \mathbb{R}$ et
2 fonctions $U : J \to \mathbb{R}$ et
$V : J \to \mathbb{R}$ dérivables,
telles que $U(J) \subset I$ et $V(J) \subset I$.

La fonction
\[ H : J \to \mathbb{R} :
x \mapsto H(x) = \int_{U(x)}^{V(x)} f(t) dt \]
est dérivable et sa dérivée est donnée par
\[ H'(x) = f(V(x)).V'(x) - f(U(x)).U'(x) \]

En effet, par le théorème fondamental,
\[ H(x) = F(V(x)) - F(U(x)) \]
où $F$ est une primitive de $f$.

\section{Suites et séries}
\subsection{Convergence d'une série}
On définit la somme partielle $S_n$ des $a_n$ jusque $n$ comme suit
\[ S_n \eqdef \sum_{k = 0}^n a_k \]

On remarque que
\[ \sum_{n = 0}^{\infty} a_n = \lim_{n \to \infty} S_n \]
Dès lors, on dit que
\begin{itemize}
  \item Si la suite $S_n$ diverge, alors la série $\sum a_n$ diverge;
  \item Si la suite $S_n$ converge vers une valeur,
    alors la série $\sum a_n$ converge vers cette même valeur.
\end{itemize}

Seulement, pour appliquer cette définition, il nous faut calculer $S_n$.
Et c'est seulement possible dans des cas particuliers comme les
séries géométriques et les séries téléscopantes.

C'est pourquoi on a d'autres tests qui nous permettent de déterminer si
$\sum a_n$ converge ou pas même s'ils ne nous permettent pas de savoir
vers quelle valeur elle converge.

\subsection{Séries téléscopantes}
Une série est dite \emph{téléscopante} lorsque
ses sommes partielles se simplifient entre elles.

Par exemple, si on arrive à trouver une suite $b_n$ telle que
$a_n = b_n - b_{n+1}$, on a
$S_n = b_1 - b_{n+1}$.

Exemple à retenir:
\[ \sum_{n = 1}^{\infty} \frac{1}{n(n + 1)} =
\sum_{n =1}^{\infty} \left(\frac{1}{n} - \frac{1}{n + 1}\right) \]

\subsection{Séries géométriques}
Une suite géométrique est une suite telle que $\exists a, r \in \mathbb{R}$
tels que
\[ a_n = ar^{n-1} \]
$r$ est appelé la \emph{raison} de la suite et $a = a_1$.
L'avantage est qu'on connait la somme d'une suite géométrique, elle vaut
\[ S_n = a \frac{r^n - 1}{r - 1} \]

Dès lors, soit la série suivante
\[ \sum_{n = 1}^{\infty} ar^{n-1} \]
\begin{itemize}
  \item Si $a = 0$,
    \begin{itemize}
      \item La série converge vers 0.
    \end{itemize}
  \item Sinon,
    \begin{itemize}
      \item Si $|r| < 1 \rightarrow$ La série converge vers $\frac{a}{1 - r}$;
      \item Si $r \geq 1 \rightarrow$ La série diverge vers $\pm \infty$
        en fonction du signe de $a$;
      \item Si $r \leq -1 \rightarrow$ La série diverge.
    \end{itemize}
\end{itemize}

\subsection{Combinaisons linéaires}
Si deux séries sont convergentes alors
toutes leurs combinaisons linéaires sont convergentes.
\[ \sum_{n=0}^{\infty} (\alpha a_n + \beta b_n)
= \alpha\sum_{n=0}^{\infty}a_n + \beta\sum_{n=0}^{\infty}b_n \]

\subsection{Convergence absolue}
\begin{itemize}
  \item
    La série $\sum a_n$ est absolument convergente
    lorsque $\sum |a_n|$ converge;
  \item
    $\sum a_n$ est absolument convergente
    $\Rightarrow$ $\sum a_n$ est convergente;
  \item
    La réciproque n'est pas vraie !
\end{itemize}

\subsection{Test de divergence}
Si $\lim_{n \to \infty}a_n \neq 0$ où n'existe pas,
alors la série $\sum_{n = 1}^{\infty}a_n$ diverge.

\subsection{Test de la série alternante}
Si $\exists N_0 \in \mathbb{R}_{> 0}$ tel que
ces trois conditions soient respectées
$\forall n \geq N_0$
\begin{itemize}
  \item $a_na_{n+1} < 0$
  \item $|a_{n+1}| \leq |a_{n}|$
  \item $\lim_{n\to\infty}a_n = 0$
\end{itemize}
alors $\sum a_n$ converge.

\subsection{Test de l'intégrale}
Soit $(a_n)$ une suite à termes positifs.
On suppose que $(a_n) = f(n)$, où $f$ est une fonction continue,
positive, décroissante $\forall x > N_0$.

Alors la série $\sum_{n =N_0}^{\infty} a_n$ et
$\int_{N_0}^{\infty} f(x) \dif x$ converge ou diverge en même temps.

\subsection{Test de comparaison}
Si $\exists K, N_0 \in \mathbb{R}_{> 0}$ tels que
$0 \leq a_n \leq Kb_n$ $\forall n \geq N_0$.
\begin{center}
  \begin{tabular}{ll}
    Si $\sum a_n$ diverge & alors $\sum b_n$ diverge\\
    Si $\sum b_n$ converge & alors $\sum a_n$ converge
  \end{tabular}
\end{center}

\subsection{Test de comparaison de la limite}
Soit
\[ L \eqdef \lim_{n \to \infty} \frac{a_n}{b_n} \]
\begin{tabular}{ll}
  Si $L < \infty$ et $\sum b_n$ converge &
  alors $\sum a_n$ converge\\
  Si $L > 0$ et $\sum b_n$ diverge à l'infini &
  alors $\sum a_n$ diverge à l'infini
\end{tabular}

\subsection{Test du quotient}
Soit
\[ \rho \eqdef \lim_{n \to \infty} \frac{|a_{n+1}|}{|a_n|} \]
\begin{itemize}
  \item Si $0 \leq \rho < 1$, alors la série converge;
  \item Si $1 < \rho \leq \infty$, la série diverge.
\end{itemize}

\subsection{Test de la racine}
Soit
\[ \sigma \eqdef \lim_{n \to \infty} \sqrt[n]{|a_n|} \]
\begin{itemize}
  \item Si $0 \leq \sigma < 1$, alors la série converge;
  \item Si $1 < \sigma \leq \infty$, la série diverge.
\end{itemize}

\subsection{Série harmonique}
\[ \sum_{n = 1}^{\infty} \frac{1}{n} \]
\begin{itemize}
  \item La série harmonique diverge;
  \item C'est un cas particulier des P-séries pour $p = 1$;
  \item L'Annexe~\ref{ann:prime_harm} présente
    une démonstration assez intéressante utilisant
    le fait que la série harmonique diverge.
\end{itemize}

\subsection{P-séries}
\[ \sum_{n = 1}^{\infty} \frac{1}{n^p} \]
\begin{itemize}
  \item La série converge pour $p > 1$;
  \item La série diverge pour $p \leq 1$.
\end{itemize}

\subsection{Séries entières (Power series)}
\[ \sum_{n =0}^{\infty}  a_n(x - c)^n \]
\begin{itemize}
  \item $a_n$ est une suite de réels appelés coefficients de la série entière;
  \item $c$ est un réel appelé centre de convergence de la série.
\end{itemize}

L'ensemble des valeurs de $x$ pour lesquelles la série converge est
un intervalle centré en $x = c$ appelé intervalle de convergence,
 et est égal à $[c - R, c + R]$, avec
\[ \frac{1}{R} = \lim_{n \to \infty} \left| \frac{a_{n + 1}}{a_n} \right|
  = \lim_{n \to \infty} \sqrt[n]{|a_n|} \]
$R$ peut être soit $[$, soit $]$,
les cas $x = c - R$ et $x = c + R$ sont à traiter séparément.


\section{Equations différentielles}
\subsection{Classification}
\resizebox{\textwidth}{!}{
  \Tree [.{Equations différentielles}
    [.{Ordre > 1} {\color{violet}...} ]
    [.{Ordre 1}
      [.Linéaire
        [.Homogène {\color{violet}$y' + a(x)y = 0$} ]
        [.Non-homogène {\color{violet}$y' + a(x)y = b(x)$} ]
      ]
      [.Non-linéaire
        [.{Variables\\séparables} {\color{violet}$y' = a(x)b(y)$} ]
        [.{Variables\\non-séparables} {\color{violet}...} ]
      ]
    ]
  ]
}

\subsection{Définition}
Une équation différentielle est une équation qui a pour inconnue une
\emph{fonction} dont une ou plusieurs
\emph{dérivées} apparaissent dans l'équation.
\subsection[Linéaire homogène de premier ordre]
{Equation différentielle linéaire homogène de premier ordre}
\begin{eqnarray*}
  y' +a(x)y &=& 0\\
  \frac{1}{y}\frac{\dif y}{\dif x} &=& -a(x)\\
  \int{ \frac{1}{y} \dif y}&=& - \int{a(x) \dif x}\\
  \ln{|y|} &=& - \int_k^x{a(x) \dif x} + C\\
  y(x) &=& ke^{-\int{a(x) \dif x}}
\end{eqnarray*}
Avec $k$ une valeur réelle.

\subsection[Linéaire non-homogène de premier ordre]
{Equation différentielle linéaire non-homogène de premier ordre}
\subsubsection{Première méthode}
La solution générale de ce genre d'équation est égale à la somme de la solution
générale de l'équation homogène associée ($y_h$)
et d'une solution particulière de l'équation non-homogène ($y_p$).
\[ y = y_h + y_p \]
\begin{center}
  \begin{tabular}{p{6cm}|p{6cm}}
    \strong{Forme de $b(x)$}&\strong{Forme de $y_p$}\\
    \hline\\
    Polynôme de degré $n$ & Polynôme de degré $n$ si $a(x) \neq 0 \qquad$
    Polynôme de degré $n+1$ si $a(x) = 0$\\
    &\\
    $k_1\cos{\theta}x + k_2\sin{\theta}x$ &
    $l_1\cos{\theta}x + l_2\cos{\theta}x$\\
    &\\
    $e^{\lambda{x}}P(x)$&$e^{\lambda{x}}Q(x)$\\
    $P(x)$ un polynôme de degré $n$ et $\lambda$ un réel ou un complexe. &
    $Q(x)$ un polynôme de degré $n$ ou $n + 1$\\
    &\\
    Constante, de même que $a(x)$ & $B/A$
  \end{tabular}
\end{center}

\subsubsection{Deuxième méthode}
Résoudre l'équation homogène associée.
\[ y_h = ke^{-\int{a(x) \dif x}} \]
Supposer que si k est une fonction de $x$,
$y_h$ est une solution de l'équation $y' +a(x)y =b(x)$.
\[ (k(x)e^{-\int{a(x) \dif x}})' + a(x) k(x)e^{-\int{a(x) \dif x}} = b(x) \]
L'expression se simplifie alors pour donner une expression de $k(x)$.
\subsection[Non-linéaire de premier ordre à variables séparables]
{Equation différentielle non-linéaire de premier ordre à variables séparables}
Se résoud de la même manière que les équations différentielles
linéaires homogènes de premier ordre.
\begin{align*}
  y' & = a(x)b(y)\\
  \frac{\dif y}{\dif x} & = a(x) b(y)\\
  \int{ \frac{1}{b(y)} \dif y} & = \int{a(x) \dif x}
\end{align*}
\subsection{Problème de Cauchy}
Le problème formé par l'équation différentielle et
la condition initiale est appelée \emph{Problème de Cauchy}.
\[
\left\{
  \begin{array}{r c l}
    y'(x) &=& f(x,y)\\
    y(x_0) &=& y_0
  \end{array}
\right.
 \]

\part{Algèbre}
%     _________
% \  / Algebra |
%  \/
\section{Espaces vectoriels}
\subsection{Définition}
$E$ est un espace vectoriel sur $K$ si
$\forall x,y \in E$ et $\forall \alpha,\beta \in K$
\begin{itemize}
  \item $x + y = y + x$;
  \item $(\alpha + \beta)x = \alpha{x} + \beta{x}$;
  \item $\alpha{(x + y)} = \alpha{x} + \alpha{y}$;
  \item $x(\alpha{\beta}) = (x\alpha{})\beta{}$;
  \item $x\cdot1 = x$.
\end{itemize}

\subsection{Sous-espaces vectoriels}
La partie $V$ de l'espace vectoriel $E$ sur un corps $K$ est
un sous espace vectoriel si,
elle est une partie non vide de $E$ stable par combinaison linéaire.
\[ \forall{x, y} \in V, \alpha, \beta \in K \qquad \alpha x + \beta y \in V \]

\subsection{Somme directe}
Tout vecteur $x$ de $V_1 \oplus V_2 \oplus \cdots \oplus V_n$ s'écrit de manière
unique comme une somme de vecteurs appartenant à $V_1, V_2, \cdots , V_n$.
Ce qui est équivalent à, $\forall v_1 \in V_1, \ldots, v_n \in V_n$
\[ \sum_{i = 1}^{n} v_i = 0\quad\Rightarrow\quad v_i =
0\quad\forall i\in\{1, \ldots, n\} \]

\subsection{sev engendré}
C'est le plus petit sous-espace vectoriel contenant $v_1, v_2, \cdots ,v_n$.
On le note
\[ \sev< v_1, v_2, \cdots, v_n> \]

\subsection{Libre, Génératrice, Base}
$(e_1, e_2, \cdots , e_n)$ est une
\begin{description}
  \item[suite génératrice] de $E$ si $sev<v_1, v_2, \cdots , v_n> = E$;
  \item[suite libre] si $\sum_{i = 1}^{n}\alpha_ie_i = 0
    \Rightarrow \alpha_1 = \alpha_2 = \cdots = \alpha_n = 0$;
  \item[base] de $E$ si elle est à la fois libre et génératrice de $E$.
\end{description}
Si $e = (e_1, e_2, \cdots , e_n)$ est une base de $E$, tout vecteur de $E$ s'écrit
de manière unique comme combinaison linéaire de cette suite.

Les coefficients de cette combinaison linéaire sont appelés les coordonnées
du vecteur $x$ en question dans la base $e$.
On met ces coordonnées en colonnes et on les notes $\tensor*[_e]{x}{}$.

\subsubsection{Suffisance de 2 critères pour être une base}
Soit $e$ une famille de $k$ vecteurs \emph{appartenant à} un
sous espace vectoriel $V$ de dimension $n$.
Pour que cette famille soit une base,
il \emph{suffit} que 2 des 3 conditions suivantes soit vérifiées
\begin{itemize}
  \item $k = n$;
  \item $e$ est une famille libre;
  \item $e$ est une famille génératrice.
\end{itemize}

\subsection{Changement de base}
\label{sec:changementdebase}
Soit un sous espace vectoriel $V$ de dimension $n$ et $e, f$ deux de ces bases.
$\exists P \in K^{n \times n}$ tel que $\forall x \in V$
\[ \tensor*[_f]{(x)}{} = P\tensor*[_e]{(x)}{} \]
On a d'ailleurs $P = \tensor*[_f]{(I)}{_e}$.
La matrice $P$ est régulière (possède une inverse).

\subsection{Dimension}
Toutes les bases d'un espace vectoriel finement engendré ont le même nombre
d'éléments. Ce nombre est appelé \emph{dimension de l'espace vectoriel}.
\[ \dim(V + W)  + \dim(V \cap W) = \dim V + \dim W \]

\subsection{Rang d'une matrice}
$A \in K^{m\times{n}}$\\
\begin{itemize}
  \item $\mathcal{L}(A)$ = SEV des lignes $\subset K^n$;
  \item $\mathcal{C}(A)$ = SEV des colonnes $\subset K^m$.
\end{itemize}
Théorème:
\[ \dim\mathcal{L}(A) = \dim\mathcal{C}(A) = \newrang(A) \]
Si A = BC alors:
\begin{itemize}
  \item $\mathcal{L}(A) \subset \mathcal{L}(C)$
  \item $\mathcal{C}(A) \subset \mathcal{C}(B)$
\end{itemize}
D'où
\[ \dim A \leq \min(\dim B, \dim C) \]

\subsection{Théorème de Rouché}
$Ax = b$ admet une solution \emph{ssi} $\newrang(A) = \newrang(A | b)$.

\section{Systèmes linéaires \& calcul matriciel}

\subsection{Linéarité}
$\forall x,y \in{E}, \alpha, \beta \in{K}$,
\[ A(\alpha{x} + \beta{y}) = \alpha{(Ax)} + \beta{(Ay)} \]

\subsection{Opérations élémentaires}
\begin{description}
  \item[Type I] $L_i \rightarrow L_i + \lambda{L_j}$
  \item[Type II] $L_i \leftrightarrow L_j$
  \item[Type II] $L_i \rightarrow \lambda{L_i} \qquad{(\lambda \neq 0)}$
\end{description}
\subsection{Opérations par blocs}
\[
  \left(
  \begin{array}{cc|c}
    a_{11} & a_{12} & a_{13} \\
    \hline
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33}
  \end{array}
  \right)
  = \begin{pmatrix}
    A&B \\
    C&D
  \end{pmatrix}
\]
Attention à ce que les blocs soient compatibles lors d'opérations.

\subsection{Transposée}
\[ (a_{ij})^t = (a_{ji}) \]

\subsection{Inverse}
Soit $A\in K^{m \times n}$:
\begin{center}
  \begin{tabular}{ll}
    Inverse à gauche $BA = I$ & $\Leftrightarrow \newrang A  = n$\\
    Inverse à droite $AC = I$ & $\Leftrightarrow \newrang A  = m$
  \end{tabular}
\end{center}

Une matrice est inversible, régulière, non-singulière
si elle possède un inverse à gauche et une inverse à droite.
\[ BA = AB = I \]
La matrice inverse est unique et $\newrang A = n = m$.
\[ (A|I) \xrightarrow[\textrm{élémentaires}]{\textrm{opérations}} (I|A^{-1}) \]

\subsubsection{Propriétés}
Si $A$ est régulière,
\begin{itemize}
  \item $(AB)^{-1} = B^{-1}A^{-1}$;
  \item $(AB)^t = B^tA^t$;
  \item $(A^{-1})^t = (A^t)^{-1}$;
  \item $\det(A^{-1}) = \frac{1}{\det(A)}$;
  \item $\det(A) \neq 0$;
  \item $\newrang(A) = n$.
\end{itemize}

\subsection{Déterminant}
\begin{itemize}
  \item Si une matrice est \emph{singulière} son déterminant est nul.
  \item $\det(AB) = \det(A) \det(B)$
  \item $\det(A^t) = \det(A)$
  \item \[ \det(A) = \sum_{l = 1}^{n}(-1)^{l+k}a_{lk}\det(A_{lk}) \]
\end{itemize}

\subsection{Matrice des cofacteurs}
\[ \cof(A) = \left((-1)^{i + j}\det(A_{i,j})\right)_{i,j} \]
\[ A^{-1} = (\det(A))^{-1} . (\cof(A))^t \]

\subsection{Résolution de systèmes linéaires}
\label{sec:solvelinsys}
Soit un système linéaire
\[ A x = b \]
où $A \in \mathbb{R}^{m \times n}$.

Pour le résoudre, commencez par mettre la matrice $(A|b)$
sous sa forme de Gauss-Jordan $(A'|b')$.
$A'$ devrait ressemble à ceci
\[ \begin{pmatrix}
    1 & 0 & \cdots & 0 & a'_{1;r+1} & a'_{1;r+2} & \cdots & a'_{1;n}\\
    0 & 1 & \cdots & 0 & a'_{2;r+1} & a'_{2;r+2} & \cdots & a'_{2;n}\\
    \vdots & \vdots & \vdots & \vdots &
    \vdots & \vdots & \vdots & \vdots\\
    0 & 0 & \cdots & 1 & a'_{r;r+1} & a'_{r;r+2} & \cdots & a'_{r;n}\\
    \vdots & \vdots & \vdots & \vdots &
    \vdots & \vdots & \vdots & \vdots\\
    0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0
\end{pmatrix}. \]
On peut ici vérifier s'il existe une solution en regardant si les lignes
nulles de $A'$ sont bien suivies par un 0 dans $b'$.
Si ce n'est pas le cas, il n'y a aucune solution.

Si c'est le cas, il y a une ensemble solution de
dimension $n - r = n - \newrang A$.
Si $b = 0$, c'est un sous espace vectoriel, sa base est simplement
\[ \begin{pmatrix}
    \begin{pmatrix}
      -a'_{1;r+1}\\
      -a'_{2;r+1}\\
      \vdots\\
      -a'_{r;r+1}\\
      1\\
      0\\
      \vdots\\
      0
    \end{pmatrix},
    \begin{pmatrix}
      -a'_{1;r+2}\\
      -a'_{2;r+2}\\
      \vdots\\
      -a'_{r;r+2}\\
      0\\
      1\\
      \vdots\\
      0
    \end{pmatrix},
    \cdots,
    \begin{pmatrix}
      -a'_{1;n}\\
      -a'_{2;n}\\
      \vdots\\
      -a'_{r;n}\\
      0\\
      0\\
      \vdots\\
      1
    \end{pmatrix}
\end{pmatrix}. \]

Si $b \neq 0$, la solution est constitué de tous les éléments de
la solution homogène (en prenant $b = 0$) chacun additionné avec la solution
particulière qui vaut
\[
  \begin{pmatrix}
    b'_1\\
    b'_2\\
    \vdots\\
    b'_r\\
    0\\
    0\\
    \vdots\\
    0
  \end{pmatrix}.
\]

\section{Applications linéaires}

\subsection{Notion d'application linéaire}
Soient E et F, des espaces vectoriels sur K.
Une application $A : E \rightarrow F$ est dite linéaire
si la condition suivante est vérifiée:
\[ \forall a,y \in E, \alpha, \beta \in\mathbb{K} :
A(\alpha{x} + \beta{y}) = \alpha{A(x)} + \beta{A(y)} \]
Une application linéaire bijective est appelées isomorphisme.

\subsection{Noyau et image}
Le noyau de l'application linéaire
$A: E \rightarrow F$ est un sev de $E$ tel que:
\[ \newker A = \{x \in E| A(x) = \vec{0}\}. \]
L'espace image de A est un sev de $F$ tel que:
\[ \newim A = \{y \in F | \exists x \in E, A(x) = y\}. \]
On note
\[ \newnull A \eqdef \dim \newker A \]
et
\[ \newrang A \eqdef \dim \newim A. \]

\subsection{Propriétés}
\begin{itemize}
  \item L'ensemble des solutions pour l'équation linéaire de type $A(x) = b$
    est égal à la somme d'une solution particulière et du noyau de $A$:
    \[ u + \newker A = \{u + v | v \in \newker A\} \]
  \item $A$ est inversible à gauche si il existe $B : F \rightarrow E$ tel que:
    \[ B\circ A = I_E \]
  \item $A$ est inversible à droite si il existe $B : F \rightarrow E$ tel que:
    \[ A\circ B = I_F \]
\end{itemize}

\subsubsection{Théorème de la nullité et du rang}
Soit $A : E \rightarrow F$.
\[ \newnull A + \newrang A = \dim E \]

En corollaire de ce théorème, on peut établir que
\begin{center}
  \begin{tabular}{|c c|}
    \hline
    &\\
    A injective & A surjective\\
    $\Updownarrow$ & $\Updownarrow$\\
    $\newker A = \{0\}$ & $\newim A = F$\\
    $\Updownarrow$ & $\Updownarrow$\\
    $\newnull A = 0$ & $\newnull A = \dim E - \dim F$\\
    $\Updownarrow$ & $\Updownarrow$\\
    $\newrang A = \dim E$ & $\newrang A = \dim F$\\
    $\Updownarrow$ & $\Updownarrow$\\
    A inversible à gauche&A inversible à droite\\
    &\\
    \hline
  \end{tabular}
\end{center}

\subsection{Représentation matricielle}
Une application linéaire $A : E \rightarrow F$ peut être représentée par
une matrice $\tensor*[_f]{(A)}{_e}$ dont chaque colonne
est formée par l'image d'un vecteur
de la base $e = (e_1, \ldots, e_n)$ de $E$,
exprimé dans la base $f = (f_1, \ldots, f_m)$ de $F$.

C'est à dire
\[ \tensor*[_f]{(A)}{_e} =
\begin{pmatrix} \tensor*[_f]{L(e_1)}{} \tensor*[_f]{L(e_2)}{}
\cdots \tensor*[_f]{L(e_n)}{} \end{pmatrix} \]

On a alors
\[ \tensor*[_f]{L(x)}{} = \tensor*[_f]{(A)}{_e} \cdot \tensor*[_e]{x}{}. \]

Remarquons tout de même que la définition de $\tensor*[_f]{(A)}{_e}$
découle directement de la linéarité de $L$.
En effet,
soit $a_1, \ldots, a_n$ les coordonnées de $x$ dans $e$, on a
\begin{align*}
  L(x) & = L(a_1 e_1 + \ldots + a_n e_n)\\
  & =
  a_1L(e_1) + \ldots + a_nL(e_n)\\
  & =
  \begin{pmatrix}
    L(e_1) & \cdot L(e_n)
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    a_1\\
    \vdots\\
    a_n
  \end{pmatrix}\\
  & =
  \begin{pmatrix}
    L(e_1) & \cdot L(e_n)
  \end{pmatrix}
  \cdot
  \tensor*[_e]{x}{}.
\end{align*}
Le fait de demander les $L(e_i)$ dans la base $f$ permet d'avoir
une matrice de scalaires et non un matrice ligne de vecteur,
ce qui est plus simple à manipuler.

\subsubsection{Propriétés}
On a
\begin{align*}
  \newrang L & = \newrang \fle\\
  \newnull L & = n - \newrang \fle.
\end{align*}
Notez que la deuxième identité est obtenue directement
de la première par le théorème de la nullité et du rang
mais elle permet de faire un lien avec le fait que
le $\ker L$ est l'ensemble de solutions de l'équation
\[ \fle \cdot \tensor*[_e]{x}{} = 0. \]

\subsubsection{Changement de base}
Comme vu à la section~\ref{sec:changementdebase},
il existe un matrice régulière dite de changement de base.
Soit $e$ et $g$ deux bases de $E$, pour calculer
la matrice pour passer de la base $g$ à la base $e$,
il suffit de calculer
$\tensor*[_e]{(I)}{_g}$
où $I$ est l'application linéaire identité, i.e.
$I : E \to E : x \mapsto x$.

Elle est très pratique car on a la propriété suivante:
\[ \tensor*[_f]{(L)}{_g} = \tensor*[_f]{(L \circ I)}{_g}
= \fle \cdot \tensor*[_e]{(I)}{_g}. \]

\subsubsection{Résolution automique avec $\fle$}
Il existe un moyen automatique pour trouver $\newker L$ et $\newim L$
à l'aide de la matrice $\fle$.

\paragraph{Trouver le kernel}
Pour trouver $\newker L$, il suffit de résoudre le système linéaire
\[ \fle \cdot \tensor*[_e]{x}{} = 0. \]

$\newker L$ est l'ensemble des solutions de ce système.
Voir la section~\ref{sec:solvelinsys} pour savoir comment résoudre
un système linéaire.

\paragraph{Trouver l'image}
On sait que $\newim L = \mathcal{C} \left(\fle\right)$.
Autrement dit, les colonnes de $\fle$, i.e. les
$\tensor*[_f]{(L(e_i))}{}$, forment une famille génératrice de $\newim L$.

Pour en trouver un base, il suffit de retirer les vecteurs qui sont
combinaisons linéaires des autres.
Si vous connaissez déjà le kernel, vous savez, par le théorème
de la nullité et du rang, que vous devez
retirer des vecteurs jusqu'à en avoir $n - \newnull L$.

De façon plus automatique, il suffit d'échelonner les colonnes de
$\fle$ et les colonnes non nulles après échelonnement constituent
une base de $\newim L$.

\part{Maths discrètes}
% _________________
% \ Maths discrètes \
%  \_________________\

\section{Ensembles}

\subsection{Définitions}
\begin{itemize}
  \item \emph{Equipotence}: A et B sont équipotents, noté $A\approx B$,
    si il existe une bijection de A vers B.
  \item \emph{Ensemble fini}:
    Si $A \approx \{1, \cdots , n\}$ pour $n \in \mathbb{N}$.
    $n$ est le \emph{cardinal} de A, noté $|A|$.
  \item \emph{Ensemble infini}:
    $A$ est infini dénombrable si $A \approx \mathbb{N}$.
\end{itemize}

\subsection{\emph{Power set}}
\paragraph{Définition}
Pour tout ensemble non-vide A,
l'ensemble $P(A)$ est \emph{équipotents} à l'ensemble
$\{0, 1\}^{A}$ des fonctions de A vers $\{0, 1\}^{A}$.
\begin{proof}
  À tout sous-ensemble $B$ de $A$,
  on associe la fonction $f_B : A \rightarrow \{0, 1\}$ définie comme suit:
  \[
    \left \{
      \begin{array}{r c l}
        f_B(x) &=& 1 \qquad{\qquad} \text{si } x \in B\\
        f_B(x) &=& 0 \qquad{\qquad} \text{si } x \in A \backslash B
      \end{array}
    \right.
   \]
\end{proof}
La fonction $f_B$ est appelée la fonction caractéristique
de $B$ comme sous-ensemble de $A$.

\subsection{Principe des tiroirs}
\paragraph{Informel}
Si $m$ objets sont rangés dans $n$ tiroirs et si $m > n$,
alors il y au moins un tiroir qui contient plus d'un objet.

\paragraph{Formel}
Soient $A$ et $B$, des ensembles finis non-vides tels que $|A| > |B|$.
Alors il n'existe pas de fonction injective de $A$ dans $B$.

\subsection{Principe d'induction}
Conditions:
\begin{description}
  \item[(1)] $P(n_0)$ est vrai;
  \item[(2)] Avec $k \in \mathbb{N}, \forall k \geq n_0$,
    si $P(k)$ est vrai, alors $P(k+1)$ est vrai.
\end{description}
Alors $P(n)$ est vrai pour tout naturel $n \geq n_0$.

\subsection{Principe d'inclusion et d'exclusion}
\paragraph{Formules}
\begin{itemize}
\item $|A \cup B| = |A| + |B| - |A \cap B|$;
\item $|A\backslash B| \geq |A| - |B|$;
\item $|A \Delta B| = |A| + |B| - 2|A \cap B|$.
\end{itemize}

\paragraph{Principe}
Soit $S$ un ensemble fini non-vide, et soient $S_1, S_2, \cdots, S_N$
des sous-ensembles de $S$ avec $n \geq 1$.
On s'intéresse au nombre d'éléments de $S$ qui n'appartiennent
à aucun des $S_i$, c'est-à-dire au nombre
\[ \sigma =|S| - \left | \bigcup_{i = 1}^n S_i \right | \]

\paragraph{Généralisation}
\begin{itemize}
  \item $S_I = \bigcap_{i \in I} S_i$;
  \item $S_{\emptyset} = S$;
  \item $R_n = \{1, \cdots , n\}$.
\end{itemize}
\[ \sigma = \sum_{r = 0}^n\left ( (-1)^r \sum_{|I| = r}|S_I| \right ) \]

\subsection{Règle de la somme}
Si $A \cap B = \emptyset$, alors $|A \cup B| = |A| + |B|$.

\subsection{Règle du produit}
\[ |A \times B| = |A||B| \]

\section{Dénombrement}
\subsection{Binôme de Newton}
\[ (x + y)^n = \sum_{k = 0}^n \begin{pmatrix}n\\ k \end{pmatrix} x^ky^{n-k} \]

\subsection{Les fonctions}
Une fonction de $A$ vers $B$ est un triple $(A,B,R)$
tel que $\forall a \in A$ il existe un unique $b \in B : aRb$.

On note $B^{A} = \{f | f : A \rightarrow B\}$.

\begin{itemize}
  \item Rangement des objets de $A$ dans les tiroirs de $B$;
  \item Mot de longueur $|A|$ pris dans l'alphabet $B$.
\end{itemize}
Soient A et B finis, $|A| = n , |B| = k$, le nombre de fonctions de A vers B est
\[ k^n \]

\subsection{Injections}
Une fonction $A \to B$ est injective si et seulement si
le mot qui la représente ne contient pas deux fois la même lettre.

Soient A et B finis, $|A| = k, |B| = n$,
\[ \In(n \leftarrow k) = [n]_k \]
\paragraph{Récurrence}
\[ \In(n \leftarrow k) = k\In(n-1 \leftarrow k-1) + \In(n-1 \leftarrow k) \]

\subsection{Surjections}
Une fonction $A \to B$ est surjective si et seulement si le mot
qui la représente contient au moins une fois fois chaque lettres de de $B$.
Si $|A| = n, |B| = k$,
\[ \Sur(n \rightarrow k) = \sum_{r = 0}^k (-1)^r
  \begin{pmatrix} k\\ r \end{pmatrix} (k - r)^n \]
On trouve ce résultat à l'aide du principe d'inclusion et d'exclusion.
\paragraph{Récurrence}
\[ \Sur(n \to k) = k\Sur(n-1 \to k-1) + k\Sur(n-1 \to k) \]

\subsection{Bijections}
Une fonction bijective ou bijection,
est une fonction à la fois injective et surjective.

Le nombre de bijections de $A$ vers $A$ est $n!$.

\subsection{Les dérangements}
Un dérangement sur $A$ est une bijection $f$ sur $A$ telle que
\[ \forall a \in A : f(a) \neq a \]
Le nombre de dérangements d'un n-ensembles est
\[ d_n = n! \sum_{r = 0}^{n} \frac{(-1)^r}{r!} \]

\subsection{Combinaisons}
\begin{center}
  \begin{tabular}{p{0.19\textwidth}|p{0.4\textwidth}|p{0.4\textwidth}}
    &\strong{Sans ordre}&\strong{Avec ordre}\\
    \hline
    &&\\
    \strong{Sans répétitions} &
    $B(n,k) =
    \begin{pmatrix} n\\ k \end{pmatrix} = C^k_n$ \[ \frac{n!}{k!(n - k)!} \] &
    $[n]_k = A^k_n = \In(n \leftarrow k)$\[ \frac{n!}{(n - k)!} \]\\
    \hline
    &&\\
    \strong{Avec répétions} &
    $B^*(n, k) = {n + k - 1 \choose k}$
    \[ |\bullet | \bullet \bullet | \, |\bullet | \bullet \bullet | \] &
    \[ n^k \]\\
  \end{tabular}
\end{center}
\paragraph{Récurrence}
\[ {n \choose k} = {n-1 \choose k-1} + {n-1 \choose k} \]

\subsection{Stirling}
Le nombre de \emph{k-partitions} d'un ensemble de cardinal $n$.
\[ S(n, k) = \frac{1}{k!}\Sur( n \rightarrow k) \]
\paragraph{Récurrence}
\[ S(n, k) = S(n - 1, k - 1) + kS(n - 1, k) \]

\subsection{Bell}
Le nombre total de \emph{partitions} d'un n-ensemble.
\[ b_n = \sum_{i = 0}^{i = n}S(n, i) \]
\paragraph{Récurrence}
\[ b_n = \sum_{i = 0}^{n - 1}\begin{pmatrix}  n - 1  \\ i \end{pmatrix}b_i \]

\subsection{Parallélisme}
\begin{center}
  \begin{tabular}{p{6cm}|p{6cm}}
    \strong{Pascal-Ensembles}&\strong{Stirling-Partitions}\\
    \hline
    &\\
    \[ B(n,k) = \frac{1}{k!}\In(n \leftarrow k) \] &
    \[ S(n,k) = \frac{1}{k!}\Sur(n \rightarrow k) \]\\
    \hline
    &\\
    $B(n,k) =$\[ B(n - 1, k - 1) +  B(n - 1, k) \] &
    $S(n, k) =$\[ S(n - 1, k - 1) + kS(n - 1, k) \]\\
    \hline
    &\\
    $\In(n \leftarrow k)=$\[ k\In(n-1 \leftarrow k-1) + \In(n-1 \leftarrow k) \]
    & $\Sur(n \to k) =$\[ k\Sur(n-1 \to k-1) + k\Sur(n-1 \to k) \]\\
    \hline
    &\\
    \[ \sum_{k = 0}^{n}\begin{pmatrix}  n \\ k \end{pmatrix} = 2^n \] &
    \[ \sum_{k = 0}^{n}S(n,k) = b_n \]\\
  \end{tabular}
\end{center}

\section{Equations de récurrence}

\subsection{Définition}
Soit $k$ un entier naturel. Une récurrence linéaire d'ordre $k$,
à coefficients constants, en la suite \emph{inconnue} $(v_n)_{n = 0}^{\infty}$,
est une équation de la forme
\[ a_0v_{n+k} + a_1v_{n+k-1} + \cdots + a_{k-1}v_{n+1} + a_kv_n =
b_n \]
avec $n \in \mathbb{N}$.
\begin{itemize}
  \item $(v_n)^{\infty}_{n = 0}$ est la suite \emph{inconnue};
  \item $a_0, \cdots , a_k$ sont des coefficients réels constants;
  \item $(b_n)_{n = 0}^{\infty}$ est une suite de réels donnée.
\end{itemize}
\begin{itemize}
  \item \emph{Homogène}: $\forall n \in \mathbb{N} : b_n = 0$;
  \item \emph{Affine}: cas général.
\end{itemize}
\subsection{Récurrences homogènes de degré 0 et 1}
\begin{itemize}
  \item \emph{Degré 0}: $v_n = 0 \qquad \forall n$;
  \item \emph{Degré 1}: $v_n = v_0\left (\frac{-a_1}{a_0} \right )^n$.
\end{itemize}

\subsection{Récurrences homogène de degré 2}
Équation générale:
\[ v_{n+2} + a_1v_{n+1} + a_2v_n = 0 \]
Équation caractéristique:
\[ r^2 + a_1r + a_2 = 0 \]

\subsection{Cas avec 2 racines réels distinctes}
Solution générale:
\[ v_n = c_1r_1^n + c_2r_2^n \]
Conditions initiales:
\[ c_1 + c_2 = v_0 \qquad{\qquad{\qquad{\qquad}}} c_1r_1 + c_2r_2 = v1 \]

\subsection{Cas avec 2 racines très proches}
Si les racines sont très proches l'une de l'autre,
$r_2 = r_1(1 + \delta )$, avec $|\delta | \ll 1$.

La solution générale sera de la forme approchée
\begin{align*}
  v_n & \approx c_1r_1^n + c_2r_1^n(1 + n\delta)\\
      & = (c_1 + c_2)r_1^n + (c_2\delta )nr_1^n\\
      & = (c_1' + c_2')r_1^n + (c_2'\delta )nr_1^{n-1}.
\end{align*}
Pour avoir la dernière équation on définit $c_2' = c_2r_1$ et $c_1' = c_1 + c_2 - c_2r_1$.
La raison d'écrire la solution sous la forme $nr_1^{n-1}$ est purement esthétique.
En effet, comme $nr_1^{n-1} = (r_1^n)'$, on préfère avoir $nr_1^{n-1}$ que $nr_1^n$.

\subsection{Cas avec 2 racines confondues}
Solution générale:
\[ v_n = d_0r_1^n + d_1nr_1^{n-1} \]
Conditions initiales:
\[ d_0 = v_0 \qquad{\qquad{\qquad{\qquad}}} d_0r_1 + d_1 = v_1 \]

\subsection{Cas avec 2 racines complexes conjuguées}
Solution générale:
\[ v_n = c_1\rho{^n}\cos{(n\theta)} + c_2\rho{^n}\sin{(n\theta)} \]
Conditions initiales:
\[ c_1 = v_0 \qquad{\qquad{\qquad{\qquad}}}
c_1\rho\cos{(\theta)} + c_2\rho\sin{(\theta)} = v_1 \]

\subsection{Cas général}
Soit la suite $(v_n)_{n \in \mathbb{N}}$ respectant
\[ \sum_{i = 0}^{k} a_i v_{n + i} = 0 \]
Soient $r_1, ..., r_s$, respectivement de multiplicité,
$m_1, ..., m_s$, les racines de l'équation caractéristique
\[ \sum_{i = 0}^{k} a_i x^{i} \]
On a donc bien évidemment $\sum_{i = 0}^{s} m_i = k$.
Et la solution générale de $v_n$ est
$$\sum_{i=0}^{s} P_i(n)r_{i}^{n}$$
Où $P_i$ est un polynôme avec
$\deg(P_i) \leq m_i$ $\forall i \in \{1, ..., s\}$.

\subsection{Certaines récurrences non homogènes}
Equation générale:
\[ a_0v_{n+k} + a_1v_{n+k-1} + \cdots + a_{k-1}v_{n+1} + a_kv_n = b_n \]
avec $n \in \mathbb{N}$
Supposons $b_n = bs^n$.
Résolution:
\begin{itemize}
  \item Somme d'une solution particulière et des solutions homogènes;
  \item \emph{Degré 0}: $v_n = bs^n \qquad \forall n$.
\end{itemize}

\subsection{Récurrences non homogènes de degré 1}
Equation générale:
\[ v_{n+1} - av_n = bs^n \]
Résolution, en cherchant $v_n = cs^n$:
\begin{itemize}
  \item $s \neq a$
    \begin{align*}
      v_n & = \frac{b}{s - a}s^n + da^n\\
      v_0 & = \frac{b}{s - a} + d
    \end{align*}
  \item $s = a$ \[ v_n = bna^{n-1} + v_0a^n \]
\end{itemize}


\section{Graphes}

\subsection{Définition}
Soit un $N$ ensemble fini non vide,
dont les éléments sont appelés des noeuds.
Soit un $R$ ensemble fini,
dont les éléments sont appelés des arêtes.
Soit $I$ une relation entre noeuds et arêtes,
c'est-à-dire un sous-ensemble de $N \times R$,
appelé relation d'\emph{incidence},
telle que le nombre de noeuds incidents à une arête soit égal à 1 ou à 2.
On dit alors que le triplet $(N, R, I)$ est un \emph{graphe} (non orienté).

\subsection{Vocabulaire}
\begin{itemize}
  \item $\alpha \in \mathbb{R}$ est une boucle si $|\{i|iI\alpha\}| = 1$;
  \item $|N|$ est l'\emph{ordre} du graphe;
  \item Le \emph{degré} de $n$, noté $\deg(n)$,
    est le nombre d'arêtes incidentes au noeud $n$, les boucles comptant double;
  \item Un graphe est \emph{simple} si il n'a ni boucle,
    ni noeuds reliés par des arêtes multiples;
  \item $\alpha = \{i, j\}$ identifie l'unique arête
    telle que $iI\alpha$ et $jI\alpha$.
\end{itemize}

\subsection{Isomorphisme}
Deux graphes simples $G = (N,R)$ et $G' = (N',R')$ sont \emph{isomorphes} si:
\begin{itemize}
  \item Il existe une bijection $f : N \rightarrow N'$;
  \item $\{i,j\} \in R \Leftrightarrow \{f(i), f(j)\} \in R'$.
\end{itemize}

\subsection{Matrice d'incidence}
La matrice d'incidence M est de genre $|N|\times |R|$
\begin{itemize}
  \item $m_{i,\alpha} := 2 \Leftrightarrow \alpha$ est boucle sur $i$;
  \item $m_{i,\alpha} := 1 \Leftrightarrow \alpha$
    est une arête ordinaire incidente à $i$;
  \item $m_{i,\alpha} := 0 \Leftrightarrow \alpha$ n'est pas incidente à $i$.
\end{itemize}
Propriétés:
\begin{itemize}
  \item $\sum_{i \in N}m_{i,\alpha} = 2$;
  \item $\sum_{\alpha \in R}m_{i,\alpha} = \deg(i)$;
  \item $\sum_{i \in N}\deg(i) = 2|R|$;
  \item Le nombre de noeuds de degré impair d'un graphe est pair.
\end{itemize}

\subsection{Matrice d'adjacence}
La matrice d'adjacence $A$ est de genre $|N| \times |N|$
\begin{itemize}
  \item $a_{i,j} :=$ nombre d'arêtes reliant $i$ et $j$ si $i \neq j$;
  \item $a_{i,i} :=$ deux fois le nombre de boucles sur $i$.
\end{itemize}
Propriétés:
\begin{itemize}
  \item $\sum_{j \in N} a_{i,j} = \sum_{i \in N} a_{i,j} = \deg(i)$;
  \item $\sum_{(i,j) \in N^2}a_{i,j} = 2|R|$;
  \item $MM^t = A + D$ où D est la matrice diagonale des degrés.
\end{itemize}

\subsection{Voyages dans un graphe}
\begin{center}
  \begin{tabular}{p{4cm}|p{3.5cm}|p{3.5cm}}
    &\strong{Noeuds distincts}&\strong{Arêtes distinctes}\\
    \hline
    &&\\
    \strong{Parcours ouvert $\: i_0 \neq i_k$}&chemin&piste ouverte\\
    &&\\
    \hline
    &&\\
    \strong{Parcours fermé}&cycle&circuit (piste fermée)\\
    &&\\
  \end{tabular}
\end{center}
Si $G$ est un graphe simple, le nombre de parcours de longueur $k$
entre ses noeuds $i$ et $j$ est donné par $(A^k)_{i,j}$.

\subsection{Graphes bipartis}
Le graphe simple $G = (N, R)$ est \emph{biparti}
si il existe une partition $\{N_0, N_1\}$ de $N$ telle que
\[ \{i, j\} \in R \Rightarrow i \in N_0\: et \:
j \in N_1\:(ou\:i \in N_1\: et \: j \in N_0) \]
Un graphe simple est biparti si et seulement si
il ne possède aucun cycle de longueur impaire.

\subsection{Connexité}
Un graphe est \emph{connexe}
si il existe un chemin reliant toute paire de noeuds.

Soit un graphe $G = (N, R)$.
On considère la partition $\{N_1, \cdots , N_m\}$ de $N$
et l'ensemble $\{R_1, \cdots R_m\}$ des sous-ensembles $R_l$ de $R$,
disjoints deux à deux, qui satisfont aux deux conditions suivantes:
\begin{itemize}
  \item $R_l$ est l'ensemble des arêtes incidentes aux noeuds dans $N_l$;
  \item Le graphe $G_l := (N_l, R_l)$ est connexe,
\end{itemize}
pour $l = 1, \cdots , m$. Alors les graphes $G_1, \cdots , G_m$
sont appelés les composantes connexes du graphe G.

Si il existe $N' \subset N$ non vide tel que aucune arête ne relie
un noeud de $N'$ et un noeud de $N\backslash N'$, alors $G$ n'est pas connexe.

\paragraph{Test de connexité}
Soient:
\begin{itemize}
  \item $i_0 \in N$ un noeud quelconque de $G = (N,R)$;
  \item $N' = \{i_0\}$ et $R' = \emptyset$;
  \item $R_{reste} = R$.
\end{itemize}
Tant que $\exists \alpha \in R_{reste}$ et $i \in N'$ tels que $iI\alpha$
\begin{itemize}
  \item $R_{reste} := R_{reste}\backslash \{\alpha\},
    R' = R' \cup \{\alpha\}$;
  \item $N' := N' \cup \{j | jI\alpha\}$.
\end{itemize}
$G$ est connexe si et seulement si $N' = N$.

\paragraph{Corollaires}
\begin{itemize}
  \item Si $G = (N,R)$ est connexe alors $|R| \geq |N| - 1$;
  \item Si $G = (N,R)$ est connexe alors $|R| = |N| - 1$
    si et seulement si G est sans cycle.
\end{itemize}

\subsection{Arbres}
\begin{itemize}
  \item $G$ est connexe et sans cycle;
  \item $G$ est connexe et $|R| = |N| - 1$;
  \item $G$ est sans cycle et $|R| = |N| - 1$;
  \item $G$ est sans cycle et lui ajouter une arête crée un et un seul cycle;
  \item $G$ est connexe et supprimer une arête quelconque le déconnecte;
  \item Deux noeuds distincts de $G$ sont reliés par un et un seul chemin.
\end{itemize}

\subsection{Arbres sous-tendants}
L'arbre $G' = (N', R')$ est un arbre sous-tendant
$G = (N,R)$ si $N = N'$ et $R' \subseteq R$.

\begin{itemize}
  \item $G$ est connexe $\Leftrightarrow$ $G$ possède un arbre sous-tendant.
\end{itemize}

\subsection{Arbres sous-tendants de poids minimum}
Soient
\begin{itemize}
  \item $c : R \rightarrow \mathbb{R}$ associant un poids à chaque arête;
  \item $c(G) := \sum_{r \in R}c(r)$ est le poids du graphe $G$.
\end{itemize}
$A$ est un \emph{arbre sous-tendant de poids minimum} de $G$
si et seulement si $A$ sous-tend $G$ et tout arbre
$A'$ sous-tendant $G$ est tel que $c(A) \leq c(A')$.

\paragraph{Kruskal}
Soit pour un graphe connexe $G = (N,R)$:
\begin{itemize}
  \item $R_{ord} := trier(R)$;
  \item $R' := \emptyset$.
\end{itemize}
Tant que $|R'| < |N| - 1$:
\begin{itemize}
  \item $\alpha = Premier(R_{ord})$;
  \item $R_{ord} = R_{ord}\backslash\{\alpha\}$;
  \item Si $(N, R' \cup \{\alpha\})$ est sans cycle,
    alors $R' = R' \cup \{\alpha\}$.
\end{itemize}
$(N, R')$ est un arbre sous-tendant de poids minimum de $(N, R)$.

\subsection{Euler}
\begin{itemize}
  \item Une \emph{piste eulérienne} est une piste
    qui passe par toutes les arêtes du graphe;
  \item Un graphe connexe $G$ est eulérien $\Leftrightarrow$ $G$ possède un
    circuit eulérien $\Leftrightarrow$ Tous les noeuds de $G$ sont de degré pair;
  \item Le graphe $G$ possède une piste eulérienne $\Leftrightarrow$ $G$ est
    connexe, et contient au maximum deux noeuds de degré impair.
\end{itemize}

\subsection{Hamilton}
\begin{itemize}
  \item un graphe simple possède un \emph{chemin hamiltonien} si il possède
    un chemin passant par chacun de ses noeuds une et une seule fois;
  \item Un graphe simple possède un \emph{cycle hamiltonien} si il possède
    un cycle passant par chacun de ses noeuds une et une seule fois.
    Un \emph{graphe hamiltonien} est
    un graphe simple possédant un cycle hamiltonien.
\end{itemize}

\subsection{Voyages complets dans un graphe}
\begin{center}
  \begin{tabular}{p{4cm}|p{3.5cm}|p{3.5cm}}
    & \strong{Par tous les noeuds une et une seule fois}
    & \strong{Par toutes les arêtes une et une seule fois}\\
    \hline
    \strong{Parcours ouvert $\: i_0 \neq i_k$}
    & chemin hamiltonien & piste eulérienne\\
    \hline
    \multirow{2}{*}{\strong{Parcours fermé}} & cycle hamiltonien
    & circuit eulérien\\
    & graphe hamiltonien & graphe eulérien\\
  \end{tabular}
\end{center}

\annexe
\section{Série harmonique et nombres premiers}
\label{ann:prime_harm}
Il est amusant, pour commencer, de remarquer que la fonction
\[ \zeta : \mathbb{C} \to \mathbb{C} :
s \mapsto \sum_{n=1}^\infty \frac{1}{n^s} \]
dont la recherche des racines est le problème le plus
important du millénaire pour les mathématiques (voir ``Hypothèse de Riemann''),
n'est autre qu'une p-série dans laquelle on a mis des complexes.

Le cas particulier $\sum \frac{1}{n}$ est la série harmonique, $\zeta(1)$.
Elle apparait dans une démonstration de $|\mathbb{P}| = \infty$
où $\mathbb{P}$ est l'ensemble des nombres premiers.
\begin{proof}
  Procédons par l'absurde.
  Si $|\mathbb{P}|$ est fini,
  comme $\sum_{k = 0}^{\infty}\frac{1}{p^k}$ est convergente,
  $$\sum_{k = 1}^{\infty}\frac{1}{k} = \prod_{p\in\mathbb{P}}\sum_{k = 0}^{\infty}\frac{1}{p^k}$$
  serait convergente aussi, car ça serait un produit fini de facteurs finis.
  Ce qui est absurde car c'est une série harmonique.
\end{proof}

\end{document}
