\documentclass[paper=a4,fontsize=8pt,pagesize,DIV=calc]{scrartcl}
\usepackage[utf8]{inputenc}

\usepackage{titlesec}
\usepackage{fixmath,mathtools}
\usepackage{listings}
\usepackage{color}
\usepackage{array}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{esdiff}  %pour la commande \diff{~}{~}
\usepackage{multirow}
\usepackage{tabu,enumerate,tabularx}
\newcounter{row}
\usepackage{txfonts}
\def\Perp{\perp\!\!\!\perp}

% Colonne
\usepackage{multicol}
\setlength{\columnsep}{4mm}
\setlength{\columnseprule}{1pt}

% Titre: taille
\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\definecolor{applegreen}{rgb}{0.55, 0.71, 0.0}
\usepackage{sectsty}
\sectionfont{\color{ao(english)}\fontsize{9}{9}\selectfont}
\subsectionfont{\fontsize{9}{9}\selectfont}
\subsubsectionfont{\fontsize{8}{8}\selectfont}
\paragraphfont{\color{applegreen}\fontsize{8}{8}\selectfont}

% Titre: espacements
\titlespacing\section{0pt}{-10pt plus 4pt minus 2pt}{-10pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{-10pt plus 4pt minus 2pt}{-10pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{-10pt plus 4pt minus 2pt}{-10pt plus 2pt minus 2pt}
\titlespacing\paragraph{0pt}{-13pt plus 4pt minus 2pt}{4pt plus 2pt minus 2pt}

% General document
\renewcommand{\familydefault}{\sfdefault}
\usepackage[nohead,nofoot]{geometry}
\setlength{\parindent}{0cm}
\setlength{\parskip}{4ex plus 0.3ex minus 0.1ex}
\renewcommand{\thesubsection}{\alph{subsection}.}

% Remove page number
\usepackage{nopageno}

% Itemize
\usepackage[shortlabels]{enumitem}
\setlist{nolistsep,leftmargin=*,topsep=-12pt}
\renewcommand\textbullet{\ensuremath{\bullet}}

\begin{document}

\begin{titlepage}\newgeometry{left=3cm,right=3cm,bottom=2cm,top=5cm}
\center

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\Huge Université catholique de Louvain}\\[1.5cm]
\textsc{\Huge Ecole polytechnique de Louvain}\\[0.5cm]
\textsc{\Large LFSAB1105: Probabilité et Statistiques}\\[5cm]

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries FORM 2016}\\[0.4cm]
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Cynthia \textsc{Laureys}
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Professor:} \\
Anour \textsc{El Ghouch}
\end{flushright}
\end{minipage}\\[5cm]


\vfill
%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[3cm]

\end{titlepage}

\newgeometry{left=0.75cm, right=0.75cm, top=1.0cm, bottom=1.0cm}

\begin{center}
{\Large LFSAB1105 Probability and Statistics - FORM}
\end{center}

\begin{multicols}{3}

\section{Introduction}
\paragraph{Sample mean} $\bar{x}=n^{-1}\sum^n_{i=1} x_i$
\paragraph{Samples quantiles} $q_p=x_{(1+(n-1)p)}$ for $p \in [0,1]$
\paragraph{Sample median} $q_{0.5}$
\paragraph{Sample mode} Most frequent $x_i$ value
\paragraph{Range} $q_1-q_0$
\paragraph{Inter-quartile range} $IQR=q_{0.75}-q_{0.25}$
\paragraph{Outliers} $x<q_{0.25}-1.5 \cdot IQR$ or \\$ x>q_{0.75}+1.5 \cdot IQR$
\paragraph{Sample variance} $s^2=(n-1)^{-1} \sum_i (x_i-\bar{x})^2$\\$[\bar{x}-as;\bar{x}+as]: 68-95-99.7
\%$ for $a=1,2,3$
\paragraph{Sample standard-deviation (SD)} $s=\sqrt{s^2}$
\paragraph{Sample coefficient of variation} $cv=\frac{s}{\bar{x}}$


\section{Probabilities}
\paragraph{Axiomatic definition of probability}
\begin{itemize}
\item $P(S)=1$ with $S=$ sample space
\item For every event A, $P(A)\geq 0$
\item If $\{A\}_i$ are incompatibles events, $P(\cup_iA_i)=\sum_iP(A_i)$
\end{itemize}
$P(A)=\lim\limits_{N \rightarrow \infty}\frac{n(A)}{N}$ (relative frequency)
\paragraph{Properties}
\begin{itemize}
\item $0\leq P(A)\leq 1 \textrm{, } P(\emptyset)=0 \textrm{, } P(\bar{A})=1-P(A)$
\item $P(A)=P(A\cap B)+P(A\cap \bar{B})$
\item Additive law: $P(A\cup B)=P(A)+P(B)-P(A \cap B)$
\item $P(\overline{A\cap B})=P(\bar{A} \cup \bar{B}) $ \& $ P(\overline{A\cup B})=P(\bar{A} \cap \bar{B})$
\item If $A\subset B$, then $P(A) < P(B)$
\item $P(\cup^n_{i=1} A_i)\leq \sum^n_{i=1} P(A_i)$ \& $P(\cap^n_{i=1} A_i)\geq 1-\sum^n_{i=1} P(\bar{A}_i)$
\item Conditional proba.: $P(A|B)=\frac{P(A\cap B)}{P(B)}=1-P(\bar{A}|B)$
\item Multipl. law: $P(A\cap B)=P(A|B)P(B)=P(B|A)P(A)$
\end{itemize}
\paragraph{Independance} $A/\bar{A}$ and $B/\bar{B}$ ind. $ \Leftrightarrow P(A|B)=P(A) \Leftrightarrow P(B|A)=P(B) \Leftrightarrow P(A\cap B)=P(A)P(B)$
\paragraph{Bayes' Theorem} $P(B|A)=\frac{P(A|B)P(B)}{P(A)}$ or \\$P(B_i|A)=\frac{P(A|B_i)P(B_i)}{\sum^l_{i=1}P(A|B_i)P(B_i)}$ ($\cup_iB_i=S$ and $B_i\cap B_j=\emptyset$)
\paragraph{Permutation} (= with order)
\begin{itemize}
\item Without replacement $A^r_n=\frac{n!}{(n-r)!}$
\item With replacement $n^r$
\end{itemize}
\paragraph{Combination} (= without order)
\begin{itemize}
\item Without replacement $C^r_n=\frac{A^r_n}{r!}=\frac{n!}{r!(n-r)!}$
\item With replacement $C^r_{n+r-1}$
\end{itemize}

\section{Random Variables}
$X: S \rightarrow \mathbb{R}$ \& $X(S)=\{x:x=X(w), w\in S\}\subset\mathbb{R}$

\paragraph{Discrete: Probability Mass Function (pmf)} ~~\\ $p(x)=P(X=x)$ \& $P(X\in I)=\sum_{i:x_i\in I} p(x_i)$
\paragraph{Continuous: Probability Density Function (pdf)}~~\\  $P(X=x)=0$ \& $P(X\in I)=\int_I f(x)dx$\\
Existence: $f(x)\geq0$ \& $\int^{+\infty}_{-\infty} f(x)dx=1$
\paragraph{Cumulative Distribution Funct. (cdf)}$F:\mathbb{R}\rightarrow [0,1]$\\
$F(x)=P(X\leq x)=\sum_{i:x_i\leq x} p(x_i)$ or $\int^x_{-\infty}f(t)dt$\\
Properties: (1-2-3 $\leftrightarrow$ existence F)
\begin{enumerate}
\item F is non decreasing: $x<y \Rightarrow F(x) \leq F(y)$
\item F is normalized: $F(-\infty)=0 \ \& \ F(+\infty)=1$
\item F is right-continuous: $F(x+)=F(x)$
\item $P(x-)=P(X<x)$ ($=F(x)$ for a continuous r.v.)
\item $P(X=x)=F(X)-F(x-)$
\item $P(a < X \leq b)=F(b)-F(a)$
\end{enumerate}
\paragraph{Expected Value} $\mathbb{E}(X)\equiv\mu_X$\\
$=\sum_x xp(x)$ or $\int^{+\infty}_{-\infty}xf(x)dx$
\begin{itemize}
\item $\mathbb{E}(h(X))=\sum_x h(x)p(x)$ or $\int^{+\infty}_{-\infty}h(x)f(x)dx$
\item Integer $k$: $k^{th}$ moment of X = $\mathbb{E}(X^k)$
\item $\mathbb{E}(aX+b)=a\mathbb{E}(X)+b$ \& $\mathbb{E}(\sum_i X_i)=\sum_i \mathbb{E}(X_i)$
\end{itemize}

\paragraph{Variance} $\mathbb{V}(X)\equiv\sigma^2_X$
$=\mathbb{E}[(X-\mathbb{E}(X))^2]=\mathbb{E}(X^2)-\mu_X^2$
\begin{itemize}
\item $\mathbb{V}(a+bX)=b^2\mathbb{V(X)}$
\end{itemize}

\paragraph{Markov Inequality} $\forall a,r>0$: $P(|X|\geq a)\leq   \dfrac{\mathbb{E}|X|^r}{a^r}$ 
\paragraph{Chebyshev Inequality}~~\\$\forall k>0$: $P(|X-\mu_X|\geq k\sigma_X)\leq   \frac{1}{k^2}$ 
\paragraph{Quantiles of order $p$ of $X$} $q_p: P(X<q_p)\leq p \leq P(X\leq q_p)$ and if continuous: $q_p=F^{-1}(p)$
\paragraph{Moment Generating Function (mgf)}
~~\\$m_X(t)=\mathbb{E}(\exp(tX))$
\begin{itemize}
\item $\left. \frac{d^k m_X(t)}{dt^k}\right|_{t=0}=\mathbb{E}(X^k)$
\item $m_X(t)=m_Y(t) \textrm{, }\forall t \in (-\epsilon ,\epsilon) \Rightarrow X=_\textbf{d}Y$
\item $m_{a+bX}(t)=e^{at}m_X(bt)$
\item $m_{\sum^n_{i=1} Y_i}(t)=\prod^n_{i=1} m_{Y_i}(t)$ with $Y_i$ mutually indep.
\end{itemize}
\paragraph{Important discrete distributions}
\begin{enumerate}
\item \textbf{Uniform distribution  $X \sim U$} (P=cst)
\begin{itemize}
\item $P(X=x_i)=n^{-1}$
\item $\mu_X=(x_1+...+x_n)\cdot n^{-1}$
\item $\sigma^2_X=(x_1^2+...+x_n^2)\cdot n^{-1}-\mu_X^2$
\end{itemize}
\item \textbf{Bernoulli trials  $X \sim Be(p)$} with $p=P(X=1)$
\begin{itemize}
\item $P(X=x)=p^x(1-p)^{(1-x)}$  ($x =0,1$)
\item $\mu_X=p$ and $\sigma^2_X=pq$ with $q=1-p$
\end{itemize}
\item \textbf{Binomial experiment $X \sim Bi(n,p)$}
~~\\$X=X_1+X_2+...+X_n$ with $X_i\sim_{iid} Be(p)$
\begin{itemize}
\item $P(X=x)=C^x_np^xq^{(n-x)}$ ($x =0,1,...,n$)
\item $m_X(t)=(pe^t+q)^n$, $\mu_X=np$ and $\sigma^2_X=npq$
\end{itemize}
\item \textbf{Geometric distribution $X\sim Ge(p)$}
\begin{itemize}
\item $P(X=x)=q^{x-1}p$ ($x =1,2,...$)
\item $m_X(t)=\frac{pe^t}{1-qe^t}$ with $qe^t<1$, $\mu_X=\frac{1}{p}$ and $\sigma^2_X=\frac{q}{p^{2}}$
\end{itemize}
\item \textbf{Poisson distribution $X \sim Po(\lambda)$}
\begin{itemize}
\item $P(X=x)=e^{-\lambda} \frac{\lambda^x}{x!}$  ($x =0,1,...$)
\item $m_X(t)=\exp{[\lambda (e^t-1)]}$, $\mu_X=\lambda$ and $\sigma^2_X=\lambda$
\item $Bi(n,\lambda/n) \xrightarrow{n \rightarrow \infty} Po(\lambda)$ or $\approx$ for $\lambda=np<7$
\end{itemize}
\item \textbf{Negative binomial distribution $X \sim NB(r,p)$}
\begin{itemize}
\item $P(X=x)=C^{r-1}_{x-1}p^rq^{x-r}$ ($r\geq1, \ x=r,r+1,...,n$)
\item $m_X(t)=\left(\frac{pe^t}{1-qe^t}\right)^r$ with $qe^t<1$
\item $\mu_X=\frac{r}{p}$ and $\sigma^2_X=\frac{rq}{p^2}$
\end{itemize}
\item \textbf{Hypergeometric distribution $X \sim Hyp(n,r,N)$}
\begin{itemize}
\item $P(X=x)=\frac{C^{x}_{r}\cdot C^{n-x}_{N-r}}{C^n_N}$ ($x =0,1,...,min(n,r)$)
\end{itemize}
\end{enumerate}
\paragraph{Important continuous distributions}
\begin{enumerate}
\item \textbf{Uniform distribution $X \sim U(a,b)$} (P=cst)
\begin{itemize}
\item $f(x)= \left\{ \begin{array}{cl}
\frac{1}{b-a} & \textrm{if } x\in [a,b]\\
0 & \textrm{otherwise }\\
\end{array} \right. $
\item $F(x)= \left\{ \begin{array}{cl}
0 & \textrm{if } x<a\\
\frac{x-a}{b-a} & \textrm{if } x\in [a,b]\\
1 & \textrm{if } x>b\\
\end{array} \right. $
\item $m_X(t)=\frac{e^{tb}-e^{ta}}{t(b-a)}$, $\mu_X=\frac{a+b}{2}$ and $\sigma^2_X=\frac{(b-a)^2}{12}$
\end{itemize}
\item \textbf{Normal distribution $X \sim \mathcal{N}(\mu,\sigma^2)$}
\begin{itemize}
\item $f(x)= \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$ ($\sigma \in \mathbb{R}^+$, $\mu \in \mathbb{R}, x\in \mathbb{R}$)
\item $m_X(t)=\exp(\mu t+\sigma^2t^2/2)$, $\mu_X=\mu$ and $\sigma^2_X=\sigma^2$
\item $a+bX\sim \mathcal{N}(a+b\mu ,b^2 \sigma^2)$ (linear transformation)
\end{itemize}
\item \textbf{Standard normal distribution $X \sim \mathcal{N}(0,1)$}
\begin{itemize}
\item $P(Z\leq x)=1-P(Z\geq x)$
\item $P(Z\geq -x)=P(Z\leq x)$ \& $P(Z\leq -x)=P(Z\geq x)$
\item "Percentage point" $z_\alpha$ with $\alpha \in (0,1)$: \\$P(Z\geq z_\alpha)=\alpha$, $z_\alpha=q_{1-\alpha}=-z_{1-\alpha}$
\item If $X\sim \mathcal{N}(\mu,\sigma^2)$ then $Z=\frac{X-\mu}{\sigma} \sim \mathcal{N}(0,1)$ =stand.
\end{itemize}
\item \textbf{Gamma distribution $X \sim Gamma(\alpha,\beta)$}
\begin{itemize}
\item $f(x)= \dfrac{x^{\alpha-1}e^{-x/\beta}}{\beta^\alpha \Gamma(\alpha)}$ ($x\geq 0$)
\begin{itemize}
\item $\alpha,\beta >0$ and $\Gamma(\alpha)=\int^{\infty}_0 t^{\alpha-1}e^{-t}dt$
\item $\Gamma(\alpha+1)=\alpha \Gamma(\alpha)$, $\Gamma(1)=1$, $\Gamma(1/2)\approx\sqrt{\pi}$
\item $\Gamma(n+1)=n!$ for $n\in \mathbb{N}$
\end{itemize} 
\item $m_X(t)=(1-\beta t)^{-\alpha}$ with $\beta t<1$
\item $\mu_X=\alpha\beta$ and $\sigma^2_X=\alpha\beta^2$
\end{itemize}
\item \textbf{Exponential distribution $X \sim Expo(\beta)$} ~~\\= $Gamma(1,\beta)$
\begin{itemize}
\item $P(X\leq t+x|X>t)=P(X\leq x)$:\\ memory-less
\end{itemize}
\item \textbf{Chi-square distribution $X \sim \mathcal{X}_n^2$} ($n\in \mathbb{N}$)
~~\\= $Gamma(n/2,2)$
\end{enumerate}

\section{Multivariate Probability Distributions}
2 of more concurrently defined real-valued functions on the elements of a given sample space 
\paragraph{Discrete Joint Distribution}
\begin{itemize}
\item $p(x,y)=P(X=x,Y=y)$
\item $P((X,Y)\in A)=\sum_{(x,y)\in A} p(x,y)$ with $A\subset\mathbb{R}^2$
\item $F(x,y)=\sum_{s\leq x, t\leq y} p(s,t)$
\end{itemize}
\paragraph{Continuous Joint Distribution}
\begin{itemize}
\item $P((X,Y)\in A)=\iint_Af(x,y)dxdy$
\item Existence: $f\geq 0$ \& $\int^{+\infty}_{-\infty}\int^{+\infty}_{-\infty} f(x,y)dxdy=1$
\item $F(x,y)=\int^{y}_{-\infty}\int^{x}_{-\infty} f(s,t)dsdt \Leftrightarrow$ $f(s,t)=\frac{\partial^2F(s,t)}{\partial x\partial y}$
\end{itemize}
\paragraph{Marginal Distributions (C\&D)}
\begin{itemize}
\item $p_X(x,y)=P(X=x)=$\\
$\sum_y P(X=x,Y=y)$ or $f_X(x)=\int^{+\infty}_{-\infty}f(x,y)dy$
\item $F_X(x,y)=P(X\leq x)=F(x,\infty)$
\end{itemize}
\paragraph{Conditional Distribution (C\&D)}
\begin{itemize}
\item $P(Y\in B|X\in A)=\frac{\sum_{y\in B} \sum_{x\in A}p(x,y)}{\sum_{x\in A}p_X(x)}$ or $ \frac{\int_B \int_A f(x,y)dx dy}{\int_A f_X(x)dx}$
\item $p_{Y|X}(y|x)=p(y|x)=P(Y=y|X=x)=\frac{p(x,y)}{p_X(x)}$\\
or $f_{Y|X}(y|x)=f(y|x)=\frac{f(x,y)}{f_X(x)}$
\item $P(Y\in B|X=x)=$\\ 
$ \sum_{y\in B} p(y|x)$ or $\int_B f(x|y)dy$
\item $F(y|x)=P(Y\leq y|X=x)= \sum_{t\leq y} p(t|x)$\\ or $\int_{-\infty}^y f(t|x)dt$ (Remark: $\frac{\partial F}{\partial y}(y|x)=f(y|x)$)
\end{itemize}
\paragraph{Independent Random Variables} $X \Perp Y \Leftrightarrow$
\begin{itemize}
\item Definition: $P(X\in A, Y \in B)=P(X\in A)P(Y\in B)$
\item $F(x,y)=F_X(x)F_Y(y)\Leftrightarrow f(x,y)=f_X(x)f_Y(y)\Leftrightarrow f(y|x)=f_Y(y)\Leftrightarrow f(x|y)=f_X(x)$
\item $F(x,y)=h(x)g(y)\Leftrightarrow f(x,y)=h(x)g(y)\Leftrightarrow$ for any functions $g,h: h(X)\Perp g(Y)$
\end{itemize}
\paragraph{Conditional Expectation}
\begin{itemize}
\item $\mathbb{E}(g(Y)|X=x)=\left\{ 
\begin{array}{l} \sum_y g(y) p(y|x) \\ 
\int g(y) f(y|x)dy \end{array} \right.$
\item $\mu_Y(x)=\mathbb{E}(Y|X=x)$ and $\sigma^2_Y(x)=\mathbb{V}(Y|X=x)$
\end{itemize}
\paragraph{Joint Distribution Expectation and Variance}
~~\\$\mathbb{E}(g(X,Y))=\left\{ 
\begin{array}{l} \sum_{x,y} g(x,y) p(y,x) \\ 
\iint g(x,y) f(x,y)dxdy \end{array} \right.$
\begin{itemize}
\item $\mathbb{E}(X+Y)=\mathbb{E}(X)+\mathbb{E}(Y)$
\item $\mathbb{E}(XY)=\mathbb{E}(X)\mathbb{E}(Y)$, if $X \Perp Y$
\item $\mathbb{V}(X+Y)=\mathbb{V}(X)+\mathbb{V}(Y)+2(\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y))$
\item $\mathbb{V}(X+Y)=\mathbb{V}(X)+\mathbb{V}(Y)$, if $X \Perp Y$
\end{itemize}
\paragraph{Moment Generating Function}
~~\\$m_{X,Y}(s,t)=m(s,t)=\mathbb{E}(\exp(sX+tY))$
\begin{itemize}
\item $\dfrac{\partial^{k+n}m}{\partial s^k \partial t^n}(0,0)= \mathbb{E}(X^kY^n)$
\item $m(s,0)=\mathbb{E}(e^{sX})=m_X(s)$ \& $m(0,t)=\mathbb{E}(e^{tY})=m_Y(t)$
\item $X \Perp Y$ $\Leftrightarrow$ $m_{X,Y}(s,t)=m_{X}(s) \cdot m_{Y}(t)$
\end{itemize}
\paragraph{Covariance} (Linear relationship) $\sigma_{XY}\equiv \mathbb{C}ov(X,Y)=$\\$\mathbb{E}[(X-\mathbb{E}(X))(Y-\mathbb{E}(Y))]=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)$
\begin{itemize}
\item $\mathbb{C}ov(X,a+bX)=b\mathbb{V}(X)$
\item $\mathbb{C}ov(X,Y)=\mathbb{C}ov(Y,X)$
\item $X \Perp Y \Rightarrow \mathbb{C}ov(X,Y)=0$ ($X$ and $Y$ uncorrelated)
\item $\mathbb{C}ov(aX+bY,Z)=a\mathbb{C}ov(X,Z)+b\mathbb{C}ov(Y,Z)$
\item $\mathbb{V}(aX+bY)=a^2\mathbb{V}(X)+b^2\mathbb{V}(Y)+2ab\mathbb{C}ov(X,Y)$
\item $\mathbb{V}(\sum^d_{i=1}a_iX_i)=\sum^d_{i=1}a_i^2\mathbb{V}(X_i)+2\sum_{1\leq i<j\leq d} a_ia_j\mathbb{C}ov(X_i,X_j)$
\end{itemize}
\paragraph{Correlation} (Interdep.) $\rho_{XY}\equiv \rho(X,Y)=\frac{\sigma_{XY}}{\sigma_X \sigma_Y}$
\begin{itemize}
\item Cauchy-Schwarz inequality $|\rho_{XY}|\leq 1$
\item $\rho_{XY}=\pm 1 \Leftrightarrow Y=a+bX$ for $b\neq 0$
\item Best linear predictor: $\hat{Y}=a+bX$ that minimizes $\mathbb{E}[(\hat{Y}-Y)^2] \Rightarrow$ $b=\rho_{XY}/\sigma_{X}^2$ and $a=\mathbb{E}(Y)-b\mathbb{E}(X)$ $\Rightarrow \sigma_Y^2=\mathbb{V}(\hat{Y})+\mathbb{V}(\hat{Y}-Y)=\sigma_Y^2\rho_{XY}^2+\sigma_Y^2(1-\rho_{XY}^2)$
\end{itemize}
\paragraph{Multinomial Distribution} Multiple outcomes ($d\geq 2$)
$P(X_1=x_1,...,X_d=x_d)=\frac{n!}{x_1!...x_d!}p_1^{x_1}...p_d^{x_d}$
\begin{itemize}
\item $\sum_i p_i=1$, $0\leq x_i\leq n$ and $\sum_i x_i=n$ 
\item $X_i \sim Bi(n,p_i)$ and $\mathbb{C}ov(X_j,X_i)=-np_ip_j$ for $i\neq j$
\end{itemize}
\paragraph{Bivariate Normal} $(X_1,X_2) \sim N_2$\\
$f(x_1,x_2)=\frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}}\exp\left(-\frac{Q(x_1,x_2)}{2(1-\rho^2)}\right)$ \\
($-\infty <x_1,x_2< \infty$)\\
$Q(x_1,x_2)=\left(\frac{x_1-\mu_1}{\sigma_1}\right)^2 +\left(\frac{x_2-\mu_2}{\sigma_2}\right)^2-2\rho\left(\frac{x_1-\mu_1}{\sigma_1}\right)\left(\frac{x_2-\mu_2}{\sigma_2}\right)$\\
Consequences:
\begin{itemize}
\item $X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2)$ and $X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)$
\item $\rho_{X_1,X_2}=\rho$
\item $X_1 \Perp X_2$ iff $\rho=0$
\item $(X_1,X_2) \sim N_2 \Leftrightarrow aX_1+bX_2+c \sim \mathcal{N}$ ($\forall a,b,c : \neq 0$)
\end{itemize}
If $X_1,X_2 \sim \mathcal{N}(\mu_i, \sigma_i^2)$ and $X_1 \Perp X_2$ then $(X_1,X_2) \sim N_2$ with $\rho=0$

\section{Functions of Random Variables}
\paragraph{Order Statistics}
\begin{itemize}
\item $F_{Y_{(n)}}=P(Y_{(n)}\leq y)=P(Y_1\leq y)...P(Y_n\leq y)=[F(y)]^n$ \& $g_{(n)}=n[F(y)]^{n-1}f(y)$
\item $F_{Y_{(1)}}=1-P(Y_{(1)}> y)=1-[1-F(y)]^n$\\
\& $g_{(1)}=n[1-F(y)]^{n-1}f(y)$
\end{itemize}
\paragraph{Discrete} ~~\\ $P(Y=y)=\sum_{x:g(x)=y}P(X=x)$
\\ $P(g(X)\in A)=\sum_{x:g(x)\in A} P(X=x)$ \\
$P(Y\leq y)=\sum_{x:g(x)\leq y}P(X=x)$
\paragraph{Continuous}~~\\ $P(g(X)\in A)=\int_{x:g(x)\in A} f_X(x)dx$\\
\paragraph{Strictly monotonic transformation}
\begin{itemize}
\item $X$ contin. r.v. with domain $I$ \& density $f_X(x)$
\item $Y=g(X)$ with $g$ differentiable  \& strict. monot. on $I$
\end{itemize}
$f_Y(y)=f_X(g^{-1}(y))|\diff{g^{-1}(y)}{y}| $, for $ y \in g(I)$\\
Linear transf.: If $Y=a+bX$, then $f_Y(y)=\frac{1}{|b|}f_X \left( \frac{y-a}{b}\right)$
\paragraph{Not monotonic Transformation} (piecewise inv.) ~~\\
$f_Y(y)=\sum_{x\in g^{-1}(y)} f_X(g^{-1}(y))|\diff{g^{-1}(y)}{y}| $,\\
where $ g^{-1}(y)= \{x:g(x)=y\}$
\paragraph{One-to-one transformations} $Y_i=g_i(X_1,X_2)$\\
$f_\textbf{Y}(\textbf{y})=f_\textbf{X}(g^{-1}(\textbf{y}))|Jac(g^{-1}(\textbf{y}))|$
\paragraph{Two-to-one transformations} $Z=g(X,Y)$\\
$F_Z(z)=P(Z\leq z)= \iint I(g(x,y)\leq z)f(x,y)dxdy$
\begin{enumerate}
\item Sum of R.V.
\begin{itemize}
\item $F_{X+Y}(z)=\int^{+\infty}_{-\infty}(\int^{z-y}_{-\infty}(f(x,y)dx)dy$
\item $f_{X+Y}(z)=\int^{+\infty}_{-\infty}f(z-y,y)dy$\\$=_{ind}\int^{+\infty}_{-\infty}f_X(z-y)f_Y(y)dx$ (convol. of $f_X$ \& $f_Y$)
\end{itemize}
\item Product/Ration of R.V.
\begin{itemize}
\item $F_{XY}(z)=\int^{0}_{-\infty}(\int^{+\infty}_{z/y}(f(x,y)dx)dy+\int^{+\infty}_{0}(\int^{z/y}_{-\infty}(f(x,y)dx)dy$
\item $f_{XY}(z)=\int^{+\infty}_{-\infty}\frac{1}{|y|}f(z/y,y)dy$
\end{itemize}
\end{enumerate}
\paragraph{Some distributions}
\begin{enumerate}
\item \textbf{Log-Normal distribution $Y \sim LN(\mu,\sigma^2)$ :}
\begin{itemize}
\item $X \sim \mathcal{N}(\mu,\sigma^2)$ and $Y=e^X $ ($y>0$)
\item $\mathbb{E}(Y)=\exp(\mu+\sigma^2/2)$ \& $\mathbb{V}(Y)=(e^{\sigma^2}-1)e^{2\mu+\sigma^2}$
\end{itemize}
\item \textbf{$\mathcal{X}^2$ Distribution $Y \sim \mathcal{X}^2_1$ :} $X \sim \mathcal{N}(0,1)$ \& $Y=X^2$ 
\item \textbf{Standard Cauchy distribution $Z\sim t_1$ :}
\begin{itemize}
\item $X,Y$ indep. \& $\sim \mathcal{N}(0,1)$ and $Z=Y/X$ 
\item $\mathbb{E}(Z)$ \& $\mathbb{V}(Z)$ undefined, $z\in ]-\infty; +\infty[$
\end{itemize}
\item \textbf{T-distribution $Z \sim t_n$ :}
\begin{itemize}
\item $X \sim \mathcal{N}(0,1)$ and $Y \sim \mathcal{X}^2_n$ indep., $Z=\frac{X}{\sqrt{Y/n}}$
\item $f(z)=\frac{\Gamma \left( \frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma \left( \frac{n}{2}\right)} \left(1+\frac{z^2}{n} \right)^{-\frac{n+1}{2}}$, $-\infty<z<\infty$
\item Symmetric around 0, $t_n\sim \mathcal{N}(0,1)$ for $n\geq 30 $
\end{itemize}
\item \textbf{F-distribution $Z\sim \mathcal{F}_{n_1,n_2}$ :}
\begin{itemize}
\item $X \sim \mathcal{X}^2_{n_1}$ and $X \sim \mathcal{X}^2_{n_2}$ indep., $Z=\frac{X/n_1}{Y/n_2}$
\item $f(z)=\frac{\Gamma((n_1+n_2)/2)(n_1/n_2)^{n_1/2}z^{(n_1/2-1)}}{\Gamma(n_1/2)\Gamma(n_2/2)(1+n_1z/n_2)^{(n_1+n_2/2}}$, $z>0$
\item If $F \sim \mathcal{F}_{n_1,n_2}$, then $1/F\sim \mathcal{F}_{n_2,n_1}$. $\mathcal{F}_{1,n}\equiv t^2_n$
\item $1/F_{a,b,\alpha}=F_{b,a,1-\alpha}$
\end{itemize}
\end{enumerate}
\paragraph{Some applications of mgf}~~\\
$X_i$ mutually ind. r.v. and $Y_n=\sum^n_{i=1}X_i$
\begin{enumerate}
\item If $X_i\sim Be(p)$, then $Y_n\sim Bi(n,p)$
\item If $X_i\sim Bi(n_i,p)$, then $Y_n\sim Bi(\sum^n_{i=1} n_i,p)$
\item If $X_i\sim Po(\lambda_i)$, then $Y_n\sim Po(\sum^n_{i=1} \lambda_i)$
\item If $X_i\sim NB(n_i,p)$, then $Y_n\sim NB(\sum^n_{i=1} n_i,p)$
\item If $X_i\sim Gamma(\alpha_i,\beta)$, then$Y_n\sim Gamma(\sum^n_{i=1} \alpha_i,\beta)$
\item If $X_i\sim Expo(\beta)$, then $Y_n\sim Gamma(n,\beta)$
\item If $X_i \sim \mathcal{X}^2_{n_i}$, then $Y_n \sim  \mathcal{X}^2_{\sum^n_{i=1} n_i}$
\item If $X_i\sim \mathcal{N}(\mu_i, \sigma_i^2)$, then $Y_n\sim \mathcal{N}(\sum^n_{i=1} \mu_i,\sum^n_{i=1} \sigma^2_i)$
\end{enumerate}
\begin{itemize}
\item (5) $\Rightarrow$ (6) and (7)
\item (7) $\Rightarrow$ If $Z_i\sim_{ind} \mathcal{N}(0,1)$, then  $\sum_{i=1}^n Z^2_i \sim  \mathcal{X}^2_n$ 
\end{itemize}

\section{Sampling Distributions and CLT}
\begin{itemize}
\item Random sample: collection of \textbf{iid}= variables
\begin{itemize}
\item $X_1,...,X_n$ mutually independent
\item The marginal pdf or pmf or each $X_i$ is $f$
\end{itemize}
\item Parameter $\theta=\theta(f)\in \Theta \subset \mathbb{R}^p$
\item Statistic: r.v., function of the sample ($h(X_1,...,X_n)$)
\end{itemize}
\paragraph{Common statistics}
\begin{itemize}
\item \textbf{Sample mean} $\hat{\mu}_n=\bar{X}_n$, $\mathbb{E}(\hat{\mu}_n)=\mu$,  $\mathbb{V}(\hat{\mu}_n)=\sigma^2n^{-1}$
\item \textbf{Sample moments} $\hat{\mu}_n^{(k)}=n^{-1}\sum^n_{i=1} X^k_i$
\item \textbf{Sample variance} $\hat{\sigma}^2_n=S^2$, $\mathbb{E}(S^2)=\sigma^2$,  $\mathbb{V}(S^2)=\frac{1}{n}\left(\eta_4-\frac{n-3}{n-1}\sigma^4\right)$ with $\eta_4=\mathbb{E}((X_i-\mu)^4)$
\item \textbf{Sample median} ~~\\$\hat{M}_n=\left\{ \begin{array}{l} X_{[(n+1)/2]}\textrm{, if $n$ is odd}\\
(X_{[n/2]}+X_{[n/2+1]})/2\textrm{, if $n$ is even}\end{array} \right. $
\end{itemize}

\paragraph{Normal population} $X_i \sim_{iid} \mathcal{N}(\mu,\sigma^2)$\\
\setcounter{row}{0}
\begin{tabularx}{\linewidth}[t]{*{2}{>{\stepcounter{row}\makebox[1.8em][l]{(\therow)\hfill}}X}}
$\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1)$ & $\bar{X} \Perp S^2$ \\
$\frac{(n-1)S^2}{\sigma^2} \sim \mathcal{X}^2_{n-1}$& $\frac{\bar{X}-\mu}{S/\sqrt{n}} \sim t_{n-1}$\\
\end{tabularx}
\paragraph{Law of large numbers LLN} $X_i$, $i=1,...,n$, uncorrelated r.v.'s with same $\mu$ and $\sigma^2$ then $\bar{X}_n \xrightarrow{p}\mu$\\ $\forall \epsilon >0 \lim\limits_{n \rightarrow \infty} P(|\bar{X}_n-\mu|>\epsilon)=0$
\paragraph{Central Limit Theorem CLT} $X_i$ iid r.v's, $\mathbb{E}(X_i)=\mu$, $\mathbb{V}(Y_i)=\sigma^2$. As $n \rightarrow \infty$: $Z_n=\sqrt{n}\frac{\bar{X}_n-\mu}{\sigma}\xrightarrow{d} Z \sim \mathcal{N}(0,1)$
\begin{enumerate}
\item $Bi(n,p)\approx\mathcal{N}(np,np(1-p))$ $\left(n>9\frac{max(p,q)}{min(p,q)}\right)$
\begin{itemize}
\item $Y_n\sim Bi(n,p)=\sum^n_i X_i$, $X_i \sim Be(p)$, $\hat{p}_n=Y_n/n$
\item $\sqrt{n}\frac{\hat{p}_n-p}{\sqrt{p(1-p)}}=\frac{Y_n-np}{\sqrt{np(1-p)}}\xrightarrow{d} Z \sim \mathcal{N}(0,1)$
\item Continuity corr.: $P(Y\leq y+0.5)$, $P(Y\geq y-0.5)$
\end{itemize}
\item $Po(\lambda)\approx \mathcal{N}(\lambda,\lambda)$ for $\lambda\rightarrow \infty$
\item $Gamma(\alpha,\beta)\approx \mathcal{N}(\alpha\beta,\alpha\beta^2)$ for $\alpha\rightarrow \infty$
\item $\mathcal{X}^2_n\approx \mathcal{N}(n,2n)$ for $n\rightarrow \infty$
\end{enumerate}

\section{Point Estimation}
\paragraph{Joint pdf/pmf}~~\\ $f_n(x;\theta)=\prod^n_{i=1}f(x;\theta)$ under iid assumption
\paragraph{Estimator} Random variable/vector $\hat{\theta}= \theta(\textbf{X})$ 
\paragraph{Bias} $Bias(\hat{\theta}=\mathbb{E}(\hat{\theta})-\theta$ $\rightarrow$ unbiased $\Leftrightarrow$ $Bias(\hat{\theta}=0$
\paragraph{Mean square error}~~\\ $MSE(\hat{\theta})=\mathbb{E}[(\hat{\theta}-\theta)^2]=Bias^2(\hat{\theta})+\mathbb{V}(\hat{\theta})$
\paragraph{Consistency of $\hat{\theta}$}
\begin{itemize}
\item If for any $\epsilon >0$, $P(|\hat{\theta}-\theta|>\epsilon)\rightarrow 0$, as $n \rightarrow \infty$
\item If $MSE(\hat{\theta})\rightarrow 0$ $\left(P(|\hat{\theta}-\theta|>\epsilon) \leq \frac{\mathbb{E}[(\hat{\theta}-\theta)^2]}{\epsilon^2}=\frac{MSE(\hat{\theta})}{\epsilon^2}\right)$
\item If g is a continuous function in $\theta$, $g(\hat{\theta})$ is also a consistent estimator of $g(\theta)$
\end{itemize}
Ex.: Estimator of $\sigma^2$ of $X_i\sim_{iid}\mathcal{N}(\mu,\sigma^2)$ with known $\mu$
\begin{itemize}
\item $T_n=\frac{1}{n}\sum^n_{i=1}(X_i-\mu)^2$: unbiased
\item $\sqrt{T_n}$: biased \& asymptotically unbiased
\item $\sqrt{\frac{\pi}{2}}\frac{1}{n}\sum^n_{i=1}|X_i-\mu|$: unbiased estimator of $\sigma$
\end{itemize}
\paragraph{Method of Moments MoM}~~\\
Solving the equations $\hat{\mu}_k(\theta)=\mu_k(\theta)$, $k=1,...$ in $\theta$ $\rightarrow \hat{\theta}$ 
\paragraph{Maximum Likelihood MLE}
\begin{itemize}
\item $L_n(\theta)=f_n(x_1,...,x_n;\theta)=_{iid}\prod^n_{i=1}f(x_i;\theta)$
\item $LL_n(\theta)=_{iid}\sum^n_{i=1}\ln(f(x_i;\theta))$
\item Score function: $S_n(\theta)=(\frac{\partial LL_n}{\partial \theta_1},...,\frac{\partial LL_n}{\partial \theta_n})=0$
\item Condition of maximum: $H_{LL_n}(\hat{\theta})$ defined negative\\
$det(H_{LL}(\hat{\theta}))=\frac{\partial^2 LL}{\partial\theta_1^2}(\hat{\theta})\frac{\partial^2 LL}{\partial\theta_2^2}(\hat{\theta})-\left(\frac{\partial^2 LL}{\partial\theta_1\partial\theta_2}(\hat{\theta})\right)^2$
\item Equivariance property: If $\hat{\theta}$ is the MLE of $\theta$, then $g(\hat{\theta })$ is the MLE of $g(\theta )$.
\end{itemize}
\paragraph{Asymptotic properties MLE}
\begin{itemize}
\item Consistent: $\hat{\theta} \xrightarrow{p} \theta$
\item Asymptotically normal: $\hat{\theta} \approx \mathcal{N}(\theta,I^{-1}_n(\theta))$
\item $I^{-1}_n(\theta)$ = Cramer-Rao Lower Bound: $I_n(\theta)=-\mathbb{E}(H_{LL}(\theta))$ and if $\tilde{\theta}_n$ unbiased $\mathbb{V}(\tilde{\theta}_n)\geq I^{-1}_n(\theta)$
\item $g(\hat{\theta})$ consistent \& $g(\hat{\theta})\approx \mathcal{N}(g(\theta),(g'(\theta))^2I^{-1}_n(\theta))$
\end{itemize}
\section{Confidence Interval}
\begin{itemize}
\item Two-sided:$[\hat{\theta}_L,\hat{\theta}_U]$ such that $P(\hat{\theta}_L\leq \theta\leq \hat{\theta}_U)=1-\alpha$
\item Lower one-sided: $[\hat{\theta}_L,\infty[$ such that $P(\hat{\theta}_L\leq \theta < \infty)=1-\alpha$ (resp. Upper one-sided)
\item $P(\theta \in ic)= 0/1$ $(\neq 1- \alpha)$ :$100\times(1-\alpha)\%$ confident 
\item $\{\hat{\theta}_L$, $\hat{\theta}_U$, $l=|\hat{\theta}_U-\hat{\theta}_L|\}=$ r.v.'s
\item $Z(\textbf{X},\theta)$ = pivotal statistic ($\Perp$ any unknown variable)
\item $g$ strictly $\uparrow: [g(L),g(U)]$ or  $\downarrow: [g(U),g(L)]$
\end{itemize} 
\paragraph{(Asymptotically valid) CI for $\mu$}
\begin{itemize}
\item $X_i\sim_{iid}\mathcal{N}(\mu,\sigma^2)$ or $\approx$ by CLT/MLE \\ \& known $\sigma$ or $S_n( n>30)$ \\
$Z=\sqrt{n}\frac{\bar{X}-\mu}{\sigma}\sim \mathcal{N}(0,1)$\\ 
$CI=\bar{X}\pm z_{a/2}\frac{\sigma}{\sqrt{n}}$ $\rightarrow l\uparrow$ with $\alpha \downarrow, \sigma^2 \uparrow, n \downarrow$
\item $X_i\sim_{iid}\mathcal{N}(\mu,\sigma^2)$ \& unknown $\sigma$ ($n<30$): \\
$Z=\sqrt{n}\frac{\bar{X}-\mu}{S_n}\sim t_{n-1}$\hspace{0.5cm}$CI^t=\bar{X}\pm t_{n-1,a/2}\frac{S_n}{\sqrt{n}}$
\end{itemize}
\paragraph{CI for $\sigma^2$}
\begin{itemize}
\item $X_i\sim_{iid}\mathcal{N}(\mu,\sigma^2)$: $Z=(n-1)\frac{S^2}{\sigma^2}\sim \mathcal{X}^2_{n-1}$\\
$CI^t=\left[\frac{(n-1)S^2}{\mathcal{X}^2_{n-1,\alpha/2}},\frac{(n-1)S^2}{\mathcal{X}^2_{n-1,1-\alpha/2}}\right]$
\end{itemize}
\paragraph{(Asymptotically valid) CI for $\mu_1-\mu_2$}
\begin{itemize}
\item $X_{ji}\sim_{iid}\mathcal{N}(\mu_j,\sigma_j^2)$ or $\approx$ by CLT/MLE \\ \& known $\sigma_i$ or $S_{n,i} (n_i>30)$ \\
$Z=\frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim \mathcal{N}(0,1)$\\ 
$CI=(\bar{X}_1-\bar{X}_2)\pm z_{a/2}\sqrt{\frac{\sigma_1}{n_1}+\frac{\sigma_2}{n_2}}$
\item $X_{ji}\sim_{iid}\mathcal{N}(\mu_j,\sigma^2)$ \& same unknown $\sigma_j$ ($n<30$): \\
$Z=\frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)}{S_p\sqrt{1/n_1+1/n_2}}\sim t_{n_1+n_2-2}$\\ 
$CI=(\bar{X}_1-\bar{X}_2)\pm t_{n_1+n_2-2,\alpha/2}S_p\sqrt{1/n_1+1/n_2}$\\
$S_p^2=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}$
\end{itemize}
\section{Hypothesis Testing}
\begin{itemize}
\item $H_0$: $\theta\in\Theta_0$ vs $H_1$: $\theta\in\Theta_1$
\item Test statistic $T_n(\textbf{X})$: $H_0$ or $RH_0$ based on some observations 
\item $H_0: \mu \geq(\leq) \mu_0$ vs $H_1: \mu <(>) \mu_0 \\ \Leftrightarrow H_0: \mu=\mu_0$ vs $H_1: \mu <(>) \mu_0$
\item $\alpha + \beta \downarrow$ with sample size and distance ($H_0 \leftrightarrow H_1$)
\end{itemize} 
\paragraph{Error types}
\begin{itemize}
\item $\alpha=P($type I error$)=P(RH_0|H_0)$ = significant level
\item $\beta=P($type II error$)=P(AH_0|H_1)$
\item Rejection region $R_{\alpha,n}$: $P(T_n\in R_{\alpha,n}|H_0)=\alpha$\item Decision:
\begin{itemize}
\item $t \in R: RH_0$ (significant test statistic)
\item $t \notin R: \backslash $ (non significant), $AH_0$ if very small  $\beta$
\end{itemize}
\end{itemize}
\paragraph{Duality CI \& Hypothesis testing}~~\\
$t\in R_{\alpha,n}$ (reject $H_0$) $\Leftrightarrow \mu_0\notin CI^t(\mu)$
\paragraph{P-Value Method} 
\begin{itemize}
\item p-value = probability of obtaining the observed data ($t_0$) or worse when the null hypothesis is true
\item $T_n(\textbf{X})$ test statistic such that small values give evidence that $H_0$ is wrong
\item $p_n(x)=P(T_n(\textbf{X})\leq T_n(\textbf{x})|H_0)$
\end{itemize}

\begin{tabular}{|c|l|}
\hline
RR&p-value\\
\hline
$\{t\geq c\}$ & $P(T\geq t_0|H_0)$\\
$\{t\leq c\}$ & $P(T\leq t_0|H_0)$\\
$\{|t|>c\}$ & $2P(T\geq |t_0||H_0)$\\
\hline
\end{tabular}
\vspace{0.1cm}
\paragraph{P-value and critical region} 
\begin{itemize}
\item p-value: $P(T\leq t|H_0)=P(t_{n-1}\leq t)$
\item critical region:\\
$t=\sqrt{n}\frac{\bar{x}-\mu_0}{s}<-t_{n-1,\alpha}$\\
$\Leftrightarrow P(t_{n-1}\leq t)<P(t_{n-1}\leq -t_{n-1,\alpha})$
$\Leftrightarrow$ p-value$<\alpha$
\end{itemize}
Rejecting a null at level $\alpha$ using the critical region is equivalent to rejecting the null when p-value$<\alpha$.

\paragraph{Test on $\mathcal{N}$ distribution under $H_0$} ~~\\
$X_i\sim_{iid}\mathcal{N}(\mu,\sigma^2)$ and unknown $\mu\backslash\sigma$
\begin{itemize}
\item For $\mu$, we use: $T=\sqrt{n}\frac{\bar{X}-\mu_0}{S}\sim t_{n-1}$\\
\end{itemize}
\vspace*{-.7\baselineskip}
\begin{tabular}{|c|l|l|}
\hline
$H_1$&Finding c &Critical region\\
\hline
$\mu>\mu_0$ & $P(T>c|H_0)=\alpha$&$t>t_{n-1,\alpha}$\\
$\mu<\mu_0$ & $P(T<c|H_0)=\alpha$&$t<-t_{n-1,\alpha}$\\
$\mu\neq\mu_0$ & $P(|T|>c|H_0)=\alpha$&$|t|>t_{n-1,\alpha/2}$\\
\hline
\end{tabular}
\begin{itemize}
\item For $\sigma^2$, we use: $X^2=(n-1)\frac{S^2}{\sigma^2_0}\sim \mathcal{X}^2_{n-1}$\\
\end{itemize}
\vspace*{-.3\baselineskip}
\begin{tabular}{|c|l|l|}
\hline
$H_1$&Finding c &Critical region\\
\hline
$\sigma^2>\sigma^2_0$ & $P(X^2>c|H_0)=\alpha$& $x^2>\mathcal{X}^2_{n-1,\alpha}$\\
$\sigma^2<\sigma^2_0$ & $P(X^2<c|H_0)=\alpha$& $x^2<\mathcal{X}^2_{n-1,1-\alpha}$\\
$\sigma^2\neq\sigma^2_0$ & $P(|X^2|>c|H_0)=\alpha$& $x^2<\mathcal{X}^2_{n-1,1-\alpha/2}$ \\
&&or $>\mathcal{X}^2_{n-1,\alpha/2}$\\
\hline
\end{tabular}
\vspace{0.1cm}
$X_{ji}\sim_{iid}\mathcal{N}(\mu_j,\sigma^2)$ \& same unknown $\sigma$
\begin{itemize}
\item Comparing $\mu_1-\mu_2$ (T-Test), \\we use $T=\frac{(\bar{X}_1-\bar{X}_2)-\delta_0}{S_p\sqrt{1/n_1+1/n_2}}\sim t_{n_1+n_2-2}$
\end{itemize}
$X_{ji}\sim_{iid}\mathcal{N}(\mu_j,\sigma_j^2)$ \& unknown $\sigma_j$: 
\begin{itemize}
\item Comparing $\sigma^2_1-\sigma^2_2$ (F-Test), \\we use $F=\frac{S^2_1}{S^2_2}\sim \mathcal{F}_{n_1-1,n_2-1}$
\end{itemize}
\vspace*{-.05\baselineskip}
\begin{tabular}{|c|c|}
\hline
$H_1$&Critical region\\
\hline
$\sigma^2>\sigma^2_0$& $f>\mathcal{F}_{n_1-1,n_2-1,\alpha}$\\
$\sigma^2<\sigma^2_0$ & $f<\mathcal{F}_{n_1-1,n_2-1,1-\alpha}$ \\
$\sigma^2\neq\sigma^2_0$ & $f<\mathcal{F}_{n_1-1,n_2-1,1-\alpha/2}$ or $f>\mathcal{F}_{n_1-1,n_2-1,\alpha/2}$\\
\hline
\end{tabular}
\vspace{0.02cm}

\paragraph{Other distributions under $H_0$}
~~\\
$X_i\sim_{iid}Expo(\lambda)$ \& unknown $\lambda$ \& $\hat{\lambda}=\bar{X}_n$
\begin{itemize}
\item For $\lambda$, we use $Y=\frac{2n}{\lambda_0}\hat{\lambda}\sim\mathcal{X}^2_{2n}$
\end{itemize}
$X_i\sim_{iid} Bi(n,p)$ \& unknown $p$ \& given $x$ 
\begin{itemize}
\item For $p$, we use $P(Bin(n,p_0)\leq x)=\alpha$ if $H_1:\mu<\mu_0$
\end{itemize}

\paragraph{Approximate (Large Sample) Tests}~~\\
Consistent estimator $\hat{\theta}_n$, such that for $n \uparrow$, $\hat{\theta}_n\approx\mathcal{N}(\theta,\sigma^2_n(\theta))$ asymptotically we have: \\$Z=\frac{\hat{\theta}_n-\theta_0}{\sigma_n(\theta_0)}\approx \mathcal{N}(0,1)$ (Example: $\hat{p}$ of $Bi(n,p)$)\\
\begin{tabular}{|c|c|c|}
\hline
$H_1$&Asymp. crit. region& Asymp. p-value\\
\hline
$\theta>\theta_0$& $z>z_{\alpha}$&$P(\mathcal{N}(0,1)\geq z)$\\
$\theta<\theta_0$ & $z<-z_{\alpha}$&$P(\mathcal{N}(0,1)\leq z)$\\
$\theta\neq\theta_0$ & $|z|>z_{\alpha/2}$&$2P(\mathcal{N}(0,1)\geq |z|)$\\
\hline
\end{tabular}
\vspace{0.02cm}

\section{Remarks}
\paragraph{Binomial identity} $(a+b)^n=\sum^n_{i=0}C^i_na^ib^{n-i}$
\paragraph{Devel. Taylor Serie} $e^y=\sum^{+\infty}_{n=0}\frac{y^n}{n!}=\lim\limits_{n \rightarrow \infty} (1+\frac{x}{n})^n$
\paragraph{Leibniz Integral Rule} $\frac{d}{dy}\left( \int^{b(y)}_{a(y)} f(x,y) dx \right)=\int^{b(y)}_{a(y)} \frac{\partial f}{\partial y}(x,y) dx+f(b(y),y)b'(y)-f(a(y),y)a'(y)$
\paragraph{Sum formula} $\sum^n_{i=1} i^2=n(n+1)(2n+1)/6$

\end{multicols}
\end{document}
