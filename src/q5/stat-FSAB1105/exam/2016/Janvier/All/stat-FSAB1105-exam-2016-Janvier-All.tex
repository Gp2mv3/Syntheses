\documentclass[en]{../../../../../../eplexam}

\usepackage{../../../../../../eplunits}
\usepackage{../../../../stat-FSAB1105}

\hypertitle{Probabilité et Statistiques}{5}{FSAB}{1105}{2016}{Janvier}{All}
{Simon Demaré\and Cynthia Laureys \and Martin Braquet}
{Anouar El Ghouch et Rainer von Sachs}

L'examen était en anglais et consistait en 5 questions dont la répartition des points était, respectivement: /3 , /7, /4, /2, /4, le tout /20.

%%%%%%%%%%
% QUESTION 1
%%%%%%%%%%
\section{(/3)}
Soient deux variables aléatoires indépendantes $X$ et $Y$ qui suivent respectivement une loi
\begin{itemize}
    \item $X \sim \gammad(\alpha_1,\beta)$
    \item $Y \sim \gammad(\alpha_2,\beta)$
\end{itemize}
\begin{enumerate}
    \item \begin{enumerate}
    \item Quelle est la joint density function de $(U,V)=(X+Y,\dfrac{X}{X+Y})$? Est-ce que $U$ et $V$ sont indépendants? Justifier.
    \item Quelle est la distribution (laquelle?) de $U$ ? Quel est son nom? Quels sont ses paramètres? Justifier.
\end{enumerate}
\item Si $X$ est continue avec comme cumulative distribution function $F$, quelle est la distribution de $Y=-\ln{F(X)}$? Quel est son nom? Quels sont ses paramètres? Justifier.
\end{enumerate}

\begin{solution}
\begin{enumerate}
\item \begin{enumerate}
\item On sait que $X, Y \simind \gammad(\alpha_i,\beta)$ avec $i$ valant respectivement 1 et 2. En outre, on connait $\textbf{U}=g(\textbf{X})=\begin{cases} U=X+Y \\V=\dfrac{X}{X+Y}\end{cases}$. Pour déterminer la distribution de $\textbf{U}=(U,V)$, il s'agit donc ici d'appliquer une \textit{2-to-2 transformation} définie comme suit:
$$  f_\textbf{U}(\textbf{u})=\left.\dfrac{f_\textbf{X}(\textbf{x})}{|Jac_g(\textbf{x})|} \right|_{\textbf{x}=g^{-1}(\textbf{u})} \textrm { avec } U_i=g_i(X,Y) $$
\begin{itemize}
\item Déterminons la \textit{Continuous Joint Distribution} $f_\textbf{X}(\textbf{x})$ de $\textbf{X}=(X,Y)$. Puisque $X$ et $Y$ sont indépendantes, cette distribution peut s'écrire comme le produit de leur distribution respective:
$$X \Perp Y \Leftrightarrow f(x,y)=f_X(x)f_Y(y)$$
\begin{align*}
	f_\textbf{X}(\textbf{x})&=f_X(x)f_Y(y)\\
	&= \dfrac{x^{\alpha_1-1}e^{-x/\beta}}{\beta^{\alpha_1} \Gamma(\alpha_1)}\dfrac{y^{\alpha_2-1}e^{-y/\beta}}{\beta^{\alpha_2} \Gamma(\alpha_2)} && \textrm{ pour } x,y \geq 0\\
	&= \dfrac{x^{\alpha_1-1}y^{\alpha_2-1}e^{-(x+y)/\beta}}{\beta^{\alpha_1}\beta^{\alpha_2} \Gamma(\alpha_1)\Gamma(\alpha_2)}&& \textrm{ pour } x,y \geq 0\\
\end{align*}
\item Évaluons la valeur absolue du Jacobien $|Jac_g(\textbf{x})|$:
\begin{align*}
    |Jac_g(\textbf{x})|&= \begin{vmatrix}
    \dfrac{\partial g_1}{\partial x} &\dfrac{\partial g_1}{\partial y} \\ \dfrac{\partial g_2}{\partial x} & \dfrac{\partial g_2}{\partial y} \end{vmatrix} =\begin{vmatrix} 1 & 1 \\ \dfrac{y}{(x+y)^2} & -\dfrac{x}{(x+y)^2} \end{vmatrix}\\
    &= \left| \dfrac{\partial g_1}{\partial x}\dfrac{\partial g_2}{\partial y}-\dfrac{\partial g_1}{\partial y}\dfrac{\partial g_2}{\partial x} \right|= \left| - \dfrac{x+y}{(x+y)^2}\right|= \left|- \dfrac{1}{x+y}\right|= \dfrac{1}{x+y} && \textrm{ car } x,y \geq 0
\end{align*}
\item Calculons $g^{-1}(\textbf{U})$:
$$g(\textbf{X})=\begin{cases} U=X+Y \\V=\dfrac{X}{X+Y}\end{cases}\Leftrightarrow g^{-1}(\textbf{U})=\begin{cases} X=UV \\Y=U-UV\end{cases} $$
\end{itemize}
On obtient par conséquent:
\begin{align*}
    f_\textbf{U}(\textbf{u})&=\left.\dfrac{f_\textbf{X}(\textbf{x})}{|Jac_g(\textbf{x})|} \right|_{\textbf{x}=g^{-1}(\textbf{u})} && \textrm { pour } u,v \geq 0\\
    &=\left. (x+y) \dfrac{x^{\alpha_1-1}y^{\alpha_2-1}e^{-(x+y)/\beta}}{\beta^{\alpha_1}\beta^{\alpha_2} \Gamma(\alpha_1)\Gamma(\alpha_2)} \right|_{\textbf{x}=g^{-1}(\textbf{u})} && \textrm { pour } u,v \geq 0\\
    &= u \dfrac{(uv)^{\alpha_1-1}(u-uv)^{\alpha_2-1}e^{-u/\beta}}{\beta^{\alpha_1}\beta^{\alpha_2} \Gamma(\alpha_1)\Gamma(\alpha_2)} && \textrm { pour } u,v \geq 0\\\end{align*}
Pour prouver l'indépendance de $U$ et $V$, on peut utiliser le fait que si la \textit{Continuous Joint Distribution} peut s'écrire comme le produit de deux fonctions, l'une dépendant uniquement de $U$ et l'autre de $V$ alors elles seront indépendantes. Ce ne sera le cas que si les domaines de définition de $U$ et $V$ sont indépendants ce qui est bien le cas ici. On montre donc:
$$U \Perp V \Leftrightarrow f(u,v)=f_U(u)f_V(v)$$ où $f(u,v)=\dfrac{1}{\beta^{\alpha_1}\beta^{\alpha_2} \Gamma(\alpha_1)\Gamma(\alpha_2)} \left[u^{\alpha_1+\alpha_2-1}e^{-u/\beta}\right]\left[v^{\alpha_1-1}(1-v)^{\alpha_2-1} \right]$
\item On sait que $U=X+Y$ avec $X, Y \simind \gammad(\alpha_i,\beta)$ pour $i$ valant respectivement 1 et 2. Il s'agit donc d'une somme de deux distributions Gamma indépendantes. On va donc se servir de la propriété de la M.G.F. suivante:
$$m_{\sum^n_{i=1} Y_i}(t)=\prod^n_{i=1} m_{Y_i}(t) \textrm{ avec }  Y_i \textrm{ mutuellement indépendants}$$ On connait les M.G.F. de $X$ et $Y$: $m_{X_i}(t)=(1-\beta t)^{-\alpha_i}$. On peut donc déterminer la M.G.F. de $U$:
\begin{align*}
	m_U(t)&=m_{\sum^2_{i=1} X_i}(t)=\prod^2_{i=1} m_{X_i}(t)\\
	&= m_{X}(t)m_{Y}(t)=(1-\beta t)^{-\alpha_1}(1-\beta t)^{-\alpha_2}=(1-\beta t)^{-(\alpha_1+\alpha_2)}
\end{align*}
L'expression de la M.G.F. nous permet d'identifier la distribution de $U$, en effet on reconnait la M.G.F. d'une distribution Gamma. En conclusion, $U\sim \gammad(\alpha_1+\alpha_2, \beta)$.
\end{enumerate}
\item Déterminer la distribution de $Y$ se fera en deux étapes. Il faut trouver la distribution de $U=F(X)$ et puis lui appliquer une \textit{Strictly Monotonic Transformation}.
\begin{itemize}
\item Commençons par déterminer la distribution de $U$. Puisque $F(x)$ est une \textit{Continuous Cumulative Distribution Function}, il s'agit d'une fonction strictement croissante. On peut donc appliquer une \textit{Strictly Monotonic Transformation} sur $X$:
\begin{align*}
	f_U(u)&=\left.\dfrac{f_X(x)}{|g'(x)|} \right|_{x=g^{-1}(u)} && \textrm{ pour } u \in g(I)\\
	\intertext{Dans ce cas-ci, la transformation appliquée est $g(x)=F(x)$. Par les propriétés d'une \textit{Continuous Cumulative Distribution Function}, on sait que $g'(x)=F'(x)=f_X(x)$ et que $g(I)=F(I)\in [0,1]$.}\\
	f_U(u)&=\left.\dfrac{f_X(x)}{|F'(x)|} \right|_{x=F^{-1}(u)} && \textrm{ pour } u \in F(I)\\
	&=\left.\dfrac{f_X(x)}{|f_X(x)|} \right|_{x=F^{-1}(u)} && \textrm{ pour } u \in [0,1]\\
	\intertext{Une \textit{Continuous Probability Density Function} étant toujours définie positive:}
	&=\left.\dfrac{f_X(x)}{f_X(x)} \right|_{x=F^{-1}(u)}=1 && \textrm{ pour } u \in [0,1]\\
\end{align*}
On reconnait une distribution Uniforme: $U \sim U(0,1)$.
\item Maintenant que nous connaissons la distribution de $U$, nous pouvons lui appliquer à nouveau une \textit{Strictly Monotonic Transformation} puisque la fonction $g(U)=-\ln{U}$ est strictement décroissante:
\begin{align*}
	f_Y(y)&=\left.\dfrac{f_U(u)}{|g'(u)|} \right|_{u=g^{-1}(y)} && \textrm{ pour } y \in g(I)\\
	\intertext{Nous pouvons calculer que $g'(u)=-\dfrac{1}{u}$. De plus, nous savons que $f_U(u)=1$ $\forall u \in [0,1]$.}
	&=\left. u \right|_{u=g^{-1}(y)} && \textrm{ pour } y \geq 0\\
	\intertext{Si $g(u)=-\ln{u}$, alors $g^{-1}(y)=\exp{(-y)}$.}
	&=\exp{(-y)} && \textrm{ pour } y \geq 0
\end{align*}
\end{itemize}
On reconnait une distribution Gamma telle que $Y\sim \gammad(1,1)$ ou plus spécifiquement une distribution Exponentielle: $Y \sim \expo(1)$.
\end{enumerate}
\end{solution}

%%%%%%%%%%
% QUESTION 2
%%%%%%%%%%
\section{(/7)}
Soient $X_1,\ldots,X_n$, des valeurs aléatoires \textit{iid} dont la probability density function est
\begin{equation}
  f(x)=\dfrac{\alpha}{\beta}x^{\alpha-1}\exp{(\dfrac{-x^\alpha}{\beta})} \text{   avec }x>0
\end{equation}
$\alpha,\beta>0$ et $\alpha$ connu et $\beta$ inconnu
\begin{enumerate}
    \item Soit $\pi \in (0,1)$ donné, trouver $q_\pi$, le $\pi$-quantile de $X$ en fonction de $\alpha, \beta$
    \item Quelle est la distribution de $X^\alpha$ ? Quel est son nom, ses paramètres? Justifier?
    \item Quel est $\widehat{\beta}$, l'estimateur de $\beta$ ? Que vaut $\mse(\widehat{\beta})$? Est-ce que $\widehat{\beta}$ est ``consistent''? En déduire $\widehat{q}_\pi$, un estimateur ``consistent'' pour $q_\pi$. Justifier.
    \item Utiliser la \textit{mgf} pour montrer que $n\widehat{\beta}\sim \gammad(n,\beta)$. En déduire la distribution de $\dfrac{2n\widehat{\beta}}{\beta}$. Justifier
    \item On veut tester $H_0$  : $\beta\leq\beta_0$ VS $H_1$ : $\beta>\beta_0$. Suggérez un test statistique \footnote{``Suggest a statistical test''} et dérivez-en la $p$-valeur. Justifiez chaque étape
    \item Donnez un intervalle de confiance de \SI{95}{\%} pour $q_\pi$.
    \item Quelle est la distribution asymptotique de $\widehat{q}_\pi$ ? Donnez l'intervalle de confiance asymptotique de $q_\pi$.
\end{enumerate}

\begin{solution}
  \begin{enumerate}
    \item
      On a
      \begin{align*}
        F(x)
        & = \int_0^x \dfrac{\alpha}{\beta}x^{\alpha-1}\exp{(\dfrac{-x^\alpha}{\beta})} \dif s\\
        & = \left[-\exp{(\dfrac{-x^\alpha}{\beta})}\right]_0^x\\
        & = 1-\exp{(\dfrac{-x^\alpha}{\beta})}.
      \end{align*}
      On sait que $q_\pi$ est tel que $\pi = F(q_\pi)$ donc
      \begin{align*}
        F(q_\pi) & = \pi\\
        1-\exp{(\dfrac{-q_\pi^\alpha}{\beta})} & = \pi\\
        q_\pi & = \sqrt[\alpha]{-\beta\ln(1-\pi)}
      \end{align*}
    \item Soit $Y = X^\alpha$. On a
      \begin{align*}
        f_Y(y)
        & = \left.\frac{f_X(x)}{\alpha x^{\alpha-1}}\right|_{x = \sqrt[\alpha]{y}}\\
        & = \left.\frac{\frac{\alpha}{\beta}x^{\alpha-1}\exp{\frac{-x^\alpha}{\beta}}}{\alpha x^{\alpha-1}}\right|_{x = \sqrt[\alpha]{y}}\\
        & = \left.\frac{\exp{\frac{-x^\alpha}{\beta}}}{\beta}\right|_{x = \sqrt[\alpha]{y}}\\
        & = \frac{1}{\beta}\exp{\frac{-y}{\beta}}.
      \end{align*}
      On en déduit que $Y \sim \expo(\beta)$.
    \item
      Comme $\alpha$ est connu, à partir de $X_1, \ldots, X_n$, on peut calculer $Y_1 = X_1^\alpha, \ldots, Y_n = X_n^\alpha$.
      On peut voir dans les tables de la fonction exponentielle que $\mu_Y = \beta$.

      Il est à savoir que pour une distribution $Z$ quelconque, $\bar{Z}$ est un estimateur de $\mu_Z$
      avec
      \[ \mse(\bar{Z}_n) = \frac{\sigma_Z^2}{n}. \]

      On sait donc que $\hat{\beta} = \bar{Y}$ est un estimateur ``consistent'' de $\beta$ avec
      \[ \mse(\hat{\beta}) = \frac{\sigma_Y^2}{n} = \frac{\beta^2}{n}. \]

      On voit que $\lim_{n\to\infty} \mse(\hat{\beta}) = 0$ ce qui implique que notre estimateur est ``consistent''.

      On sait aussi que si $\hat{\theta}$ est un estimateur consistent de $\theta$ et que $g$ est une fonction continue, alors $g(\hat{\theta})$ est un estimateur consistent de $g(\theta)$.

      Comme la fonction $g(\beta) = \sqrt[\alpha]{-\ln(1-\pi)\beta}$ est continue, $\sqrt[\alpha]{-\ln(1-\pi)\overline{X^\alpha}}$ est un estimateur consistent de $q_\pi$.
    \item
      On a
      \[ n\hat{\beta} = \sum_{i=1}^n X_i^\alpha = \sum_{i=1}^n Y_i. \]
      $n\hat{\beta}$ est donc une somme de $n$ exponentielle indépendente.
      On a donc
      \[ m_{n\hat{\beta}} = m_{\sum_{i=1}^n Y_i} = \prod_{i=1}^n m_{Y_i} = (1-\beta t)^{-n}. \]
      On voit alors que $n\hat{\beta} \sim \gammad(n,\beta)$.

      Soit $V = n\hat{\beta}$ et $W = \frac{2}{\beta}n\hat{\beta}$.
      Comme la fonction $x \mapsto \frac{2}{\beta}x$ est strictement monotonique, on a
      \begin{align*}
        f_W(w)
        & = \left.\frac{f_V(v)}{\frac{2}{\beta}}\right|_{v = \frac{\beta}{2}{w}}\\
        & = \left.\frac{1}{2\Gamma(n)\beta^{n-1}}v^{n-1}e^{-v/\beta}\right|_{v = \frac{\beta}{2}{w}}\\
        & = \frac{1}{2^n\Gamma(n)}w^{n-1}e^{-w/2}.
      \end{align*}
      On en déduit que $W$ a une distribution Chi-square avec $\nu=2n$.
    \item
      On suggère $\bar{Y}$ comme test statistique. Trouver la région de rejet pour ces hypothèses revient à trouver la région de rejet en modifiant l'hypothèse $H_0:\beta \leq \beta_0$ par $H_0:\beta = \beta_0$. Sous $H_0$, on a $W=\frac{2n\overline{Y}}{\beta_0}\sim \chi^2_{2n} $. On sait ainsi que
      \[
      		P\left(\frac{2n\overline{Y}}{\beta_0}\geq \chi^2_{2n,\alpha} \right)=\alpha
      \]
      On rejette donc $H_0$ si $\overline{Y}$ est dans la région de rejet:
      \[ 
      		R_{\alpha,n}=\left[\frac{\beta_0\chi^2_{2n,\alpha}}{2n};+\infty\right]
      \]
      La p-valeur vaut $P\left(\frac{2n\overline{Y}}{\beta_0}\leq W \right)=\chi^2_{2n,\frac{2n\overline{Y}}{\beta_0}}$.
    \item
     On a comme intervalle exact
   	 \begin{align*}
    	0.95
    	& = P\left(\chi^2_{2n,0.975} \leq \frac{2n\bar{Y}}{\beta} \leq \chi^2_{2n,0.025}\right)\\
    	& = P\left(\frac{2n\bar{Y}}{\chi^2_{2n,0.025}} \leq \beta \leq P(\frac{2n\bar{Y}}{\chi^2_{2n,0.975}}\right)\\
    	& = \Pr\left(\sqrt[\alpha]{\frac{-2n\bar{Y}\ln(1-\pi)}{\chi^2_{2n,0.025}}} \leq q_\pi \leq \sqrt[\alpha]{\frac{-2n\bar{Y}\ln(1-\pi)}{\chi^2_{2n,0975}}}\right).
   	 \end{align*}

    \item
      Soit $T = \bar{Y}$ et soit $U$ la variable aléatoire $q_\pi$.
      Par le \textit{CLT}, asymptotiquement on a $T \sim \N(\beta,\beta^2/n)$.
      Dès lors, comme la transformation $t \mapsto \sqrt[\alpha]{-\ln(1-\pi)t}$ est strictement monotone, (on renomme $\pi$ en $\pi_q$ pour ne pas le confondre avec le nombre irrationnel)
      \begin{align*}
        f_{U}(u)
        & = \left.\frac{f_T(t)}{g'(t)}\right|_{t = -u^\alpha/\ln(1-\pi)}\\
        & = \left.\frac{\frac{\sqrt{n}}{\beta\sqrt{2\pi}}\exp\left[-\left(\frac{n}{2\beta^2}(t-\beta)^2\right)\right]}{\frac{1}{\alpha}(-\ln(1-\pi_q)t)^{1/\alpha-1}(-\ln(1-\pi_q))}\right|_{t = -u^\alpha/\ln(1-\pi)}\\
        & = \frac{\alpha\sqrt{n}}{-\ln(1-\pi_q)\beta\sqrt{2\pi}}u^{\alpha-1}\exp\left[\frac{-n}{2\beta^2}\left(\frac{u^\alpha}{\ln(1-\pi_q)}+\beta\right)^2\right].
      \end{align*}
  
  	      On a comme intervalle asymptotique
  	\begin{align*}
  		0.95
  		& = \Pr(-1.96 \leq \sqrt{n}\frac{\bar{Y}-\beta}{\beta} \leq 1.96)\\
  		& = \Pr(\frac{\bar{Y}}{1+\frac{1.96}{\sqrt{n}}} \leq \beta \leq \frac{\bar{Y}}{1-\frac{1.96}{\sqrt{n}}})\\
  		& = \Pr\left(\sqrt[\alpha]{\frac{-\ln(1-\pi)\bar{Y}}{1+\frac{1.96}{\sqrt{n}}}} \leq q_\pi \leq \sqrt[\alpha]{\frac{-\ln(1-\pi)\bar{Y}}{1-\frac{1.96}{\sqrt{n}}}}\right).
  	\end{align*}
  
  \end{enumerate}
\end{solution}

%%%%%%%%%%
% QUESTION 3
%%%%%%%%%%
\section{(/4)}
Soit une distribution jointe:
\begin{equation}
    f(x,y)=\dfrac{3y}{4} \text{ ($0\leq y \leq x \leq 2$)}
\end{equation}
\begin{enumerate}
    \item $\Pr(Y|X=x)$ ?
    \item $P\left(Y>\dfrac{1}{2}|X=\dfrac{3}{2}\right)$ et $E\left(Y|X=\dfrac{3}{2}\right)$ ?
    \item $\Pr(X+Y<2)$ ?
    \item $V(X-2Y)$ ?
\end{enumerate}

\begin{solution}
Commençons par dessiner le domaine de définition de la loi de distribution $f(x,y)$. Il s'agit du triangle de sommets (0,0), (2,0) et (2,2) délimité par les droites $y=0$, $x=2$ et $x=y$.
\begin{enumerate}
    \item $\Pr(Y|X=x)=\dfrac{f(x,y)}{f_X(x)}$ \\
    Nous connaissons la \textit{Joint Probability Density Function} $f(x,y)$. Il reste donc à déterminer la \textit{Marginal Probability Density Function} $f_X(x)$ :
    $$f_X(x)=\int_0^x f(x,y) \dif y= \int_0^x \dfrac{3y}{4} \dif y= \dfrac{3}{4}\left[\dfrac{y^2}{2}\right]_0^x=\dfrac{3 x^2}{8}$$
    On obtient dès lors: $\Pr(Y|X=x)=\dfrac{3y}{4} \dfrac{8}{3x^2}=\dfrac{2y}{x^2}$.
    \item Puisqu'on connait $\Pr(Y|X=x)$, on sait que $P\left(Y|X=\dfrac{3}{2}\right)=\dfrac{8y}{9}$:
    \begin{itemize}
    \item $P\left(Y>\dfrac{1}{2}|X=\dfrac{3}{2}\right)=\int_{1/2}^{3/2} P\left(Y|X=\dfrac{3}{2}\right) \dif y=\dfrac{8}{9}\int_{1/2}^{3/2} y \dif y=\dfrac{8}{9}\left[\dfrac{y^2}{2} \right]_{1/2}^{3/2}=\dfrac{8}{9} \left(\dfrac{9}{8}-\dfrac{1}{8} \right)=\dfrac{8}{9}$
    \item $E\left(Y|X=\dfrac{3}{2}\right)=\int_0^{3/2}yP\left(Y|X=\dfrac{3}{2}\right) \dif y=\dfrac{8}{9}\int_0^{3/2}y^2 \dif y = \dfrac{8}{9} \left[\dfrac{y^3}{3} \right]_0^{3/2}=\dfrac{2^3}{3^2}\dfrac{3^3}{2^3 \cdot 3}=1$
    \end{itemize}
    \item Pour calculer cette intégrale, il faut intégrer $f(x,y)$ où $x+y<2$. Il s'agit du triangle de sommets (0,0), (1,1) et  (2,0) délimité par les droites $y=0$, $x=y$ et $x=2-y$. \\
    $\Pr(X+Y<2)=\int_0^1 \int_y^{2-y} f(x,y) \dif x \dif y=\dfrac{3}{4}\int_0^1 \int_y^{2-y} y \dif x \dif y=\dfrac{3}{4}\int_0^1 y [x]_y^{2-y} \dif y=\dfrac{3}{2}\int_0^1 y-y^2\dif y=\dfrac{3}{2}\left[\dfrac{y^2}{2}-\dfrac{y^3}{3} \right]_0^1=\dfrac{3}{2}\left( \dfrac{1}{2}-\dfrac{1}{3}\right)=\dfrac{1}{4}$
    \item Par la propriété de la variance: $V(aX+bY)=a^2V(X)+b^2V(Y)+2ab\cov(X,Y)$, nous avons ici:
$$V(X-2Y)=V(X)+4V(Y)-4\cov(X,Y)$$
où $V(X)=\E(X^2)-\E(X)^2$, $V(Y)=\E(Y^2)-\E(Y)^2$ et $\cov(X,Y)=\E(XY)-\E(X)\E(Y)$. Il nous reste donc à calculer les différentes espérances. Mais avant, déterminons la \textit{Marginal Probability Density Function} de $Y$:
$$f_Y(y)=\int_y^2 f(x,y) \dif x= \int_y^2 \dfrac{3y}{4} \dif x= \dfrac{3y}{4}[x]_y^2=\dfrac{3y}{4} (2-y)=\dfrac{3}{2}y-\dfrac{3}{4}y^2$$
\begin{itemize}
\item $\E(X)=\int_0^2 xf_X(x) \dif x=\dfrac{3}{8}\int_0^2 x^3 \dif x=\dfrac{3}{8}\left[ \dfrac{x^4}{4}\right]_0^2=\dfrac{3}{2}$
\item $\E(X^2)=\int_0^2 x^2f_X(x) \dif x=\dfrac{3}{8}\int_0^2 x^4 \dif x=\dfrac{3}{8}\left[ \dfrac{x^5}{5}\right]_0^2=\dfrac{12}{5}$
\item $\E(Y)=\int_0^2 yf_Y(y) \dif y=\int_0^2 \dfrac{3}{2}y^2-\dfrac{3}{4}y^3 \dif y=\left[ \dfrac{3}{2}\dfrac{y^3}{3}-\dfrac{3}{4}\dfrac{y^4}{4}\right]_0^2=4-3=1$
\item $\E(Y^2)=\int_0^2 y^2f_Y(y) \dif y=\int_0^2 \dfrac{3}{2}y^3-\dfrac{3}{4}y^4 \dif y=\left[ \dfrac{3}{2}\dfrac{y^4}{4}-\dfrac{3}{4}\dfrac{y^5}{5}\right]_0^2=6-\dfrac{24}{5}=\dfrac{6}{5}$
\item $\E(XY)=\int_0^2\int_0^x xy f(x,y)\dif y \dif x=\dfrac{3}{4}\int_0^2 x\int_0^x y^2\dif y \dif x=\dfrac{3}{4}\int_0^2 x \left[\dfrac{y^3}{3} \right]_0^x \dif x=\dfrac{1}{4}\int_0^2 x^4 \dif x=\dfrac{1}{4} \left[\dfrac{x^5}{5}\right]_0^2=\dfrac{1}{4} \dfrac{2^5}{5}=\dfrac{8}{5}$
\end{itemize}
    \begin{align*}
    V(X-2Y)&=[\E(X^2)-\E(X)^2]+4[\E(Y^2)-\E(Y)^2]-4[\E(XY)-\E(X)\E(Y)]\\
    &= \left[\dfrac{12}{5}-\dfrac{9}{4}\right]+4\left[\frac{6}{5}-1\right]-4\left[\dfrac{8}{5}-\dfrac{3}{2}\cdot 1\right]\\
    &= \dfrac{3}{20}+4 \cdot \dfrac{1}{5}-4\cdot\dfrac{1}{10}\\
    &=\dfrac{11}{20}
    \end{align*}
\end{enumerate}
\end{solution}

%%%%%%%%%%
% QUESTION 4
%%%%%%%%%%

\section{(/2)}
Des composants électroniques sont placés en parallèle. Il y en a $n_1$ qui suivent une distribution exponentielle (en nombre d'années) $\sim \expo(\beta_1)$ et $n_2$ dont la distribution est une exponentielle (en nombre d'années) $\sim \expo(\beta_2)$ avec $n_1+n_2=n$ et $n>2$. Sachant que le système ne fonctionne plus si TOUS les composants ne fonctionnent plus, quelle est la probabilité que le système dans sa globalité tiendra plus de 10 ans? Exprimer votre réponse en terme $\beta_1$, $\beta_2$, $n_1$ et $n_2$.

\begin{solution}
Le système ne fonctionnant plus si tous les composants ne fonctionnent plus, le temps de fonctionnement sera déterminé par le temps de fonctionnement de la composante, parmi les $n$ composantes, qui fonctionnera le plus longtemps.

Soient $X_i \sim \expo(\beta_1)$ pour $i \in {1, ..., n_1}$, $Y_j \sim \expo(\beta_2)$ pour $j \in {1, ..., n_2}$ et $U=X_1, ..., X_{n_1},Y_1, ..., Y_{n_2}$. Nous voulons connaitre la distribution de $U_{(n)}$ telle que $U_{(1)}\leq ... \leq U_{(n)}$.
\begin{align*}
F_{U_{(n)}}(u)&=\Pr(U_{(n)}\leq u)=\Pr(U_{(1)}\leq u) \cdots \Pr(U_{(n)}\leq u)\\
&= [F_X(u)]^{n_1}[F_Y(u)]^{n_2}\\
\intertext{Il nous reste à obtenir les expressions des \textit{Cumulative Distribution Function} de $X_i$ et $Y_j$ par intégration de leur \textit{Probability Density Function}:}
F_{X_i}(x_i)&=\begin{cases} 0 & \textrm{if } x_i<0 \\
1-\exp{\left(-\dfrac{x_i}{\beta_1}\right)} & \textrm{if } x_i\geq 0 \end{cases}\\
F_{Y_i}(y_i)&=\begin{cases} 0 & \textrm{if } y_i<0 \\
1-\exp{\left(-\dfrac{y_i}{\beta_2}\right)} & \textrm{if } y_i\geq 0 \end{cases}\\
\intertext{Nous obtenons dès lors:}
\Pr(U_{(n)}>10)&=1-\Pr(U_{(n)}\leq10)= 1- F_{U_{(n)}}(10)=1-[F_X(10)]^{n_1}[F_Y(10)]^{n_2}\\
&=1-\left[1-\exp{\left(-\dfrac{10}{\beta_1}\right)} \right]^{n_1}\left[1-\exp{\left(-\dfrac{10}{\beta_2}\right)} \right]^{n_2}\\
\intertext{En dérivant, nous obtiendrons la \textit{Probability Density Function} de $U_{(n)}$:}
f_{U{(n)}}(u)&=n_1[F_X(u)]^{n_1-1}f_X(u)[F_Y(u)]^{n_2}+n_2[F_Y(u)]^{n_2-1}f_Y(u)[F_X(u)]^{n_1}
\end{align*}
\end{solution}

%%%%%%%%%%
% QUESTION 5
%%%%%%%%%%
\section{(/4)}
Une population de composants a une probabilité de dysfonctionnement de \SI{8}{\%}.
\begin{enumerate}
    \item Si $n=12$, quelle est la probabilité que plus de 2 composants aient un défaut?
    \item Si $n=200$, quelle est la probabilité que moins de 20 composants aient un défaut?
    \item On teste les composants l'un à la suite de l'autre. Quelle est la probabilité que l'on doive en tester plus de 3 avant de trouver le premier dysfonctionnant?
    \item On teste les composants l'un à la suite de l'autre. Quelle est la probabilité que l'on doive en tester plus de 5 avant d'en trouver 3 dysfonctionnant?
\end{enumerate}

\begin{solution}
\begin{enumerate}
    \item Soit $X \sim \bin(12,0.08)$ avec $p=0.08$ et $n=12$. On sait que
    $$\Pr(X=x)=C^x_np^x(1-p)^{(n-x)} \textrm{ pour } x =0,1,...,n$$
    \begin{align*}
    \Pr(X>2)&=1-\Pr(X=0)-\Pr(X=1)-\Pr(X=2)\\
    &=1-C_{12}^0p^0(1-p)^{(12-0)}-C^1_{12}p^1(1-p)^{(12-1)}-C^2_{12}p^2(1-p)^{(12-2)}\\
    &=1-0.92^{12}-12 \cdot 0.08 \cdot 0.92^{11}-\frac{12!}{10!2!}0.08^2\cdot 0.92^{10}=0.065196
    \end{align*}
    \item Soit $X \sim \bin(200,0.08)$ avec $p=0.08$ et $n=200$.    On cherche $\Pr(X<20)$. Comme le calcul est beaucoup plus long, on va l'approximer par le \textit{Central Limit Theorem} qui nous dit:
    $$\frac{X_n-np}{\sqrt{np(1-p)}}\xrightarrow{d} Z \sim \mathcal{N}(0,1)$$
    et d'une \textit{Continuity Correction}:
    \begin{align*}
    \Pr(X< 20)&\approx P\left(\frac{X-np}{\sqrt{np(1-p)}} < \frac{19.5-np}{\sqrt{np(1-p)}} \right)\\
    &= P\left(\frac{X-16}{\sqrt{16\cdot 0.92}} < \frac{19.5-16}{\sqrt{16 \cdot 0.92}} \right)\\
    &= 1- P\left(Z \geq 0.91 \right)\\
    &=0.8186
    \end{align*}
    \item Soit $X \sim Ge(p)$. On sait que
      $$\Pr(X=x)=(1-p)^{x-1}p \textrm{ pour } x =0,1,\ldots$$
    \begin{align*}
      \Pr(X>3)&=1-\Pr(X=1)-\Pr(X=2)-\Pr(X=3)\\
      &= 1 - p - (1-p)p - (1-p)^2p\\
      &= 1 - 0.08 - 0.0736 - 0.0677\\
      &= 0.7787.
    \end{align*}

    \item Soit $X \sim NB(3,0.08)$ avec $p=0.08$ et $r=3$. On sait que
    $$\Pr(X=x)=C^{r-1}_{x-1}p^rq^{x-r} \textrm{ pour } x=3,4+1,...,n$$
    \begin{align*}
    \Pr(X>5)&= 1-\Pr(X=3)-\Pr(X=4)-\Pr(X=5)\\
    &=1-C^{3-1}_{3-1}p^3q^{3-3}-C^{3-1}_{4-1}p^3q^{4-3} -C^{3-1}_{5-1}p^3q^{5-3} \\
    &=1-0.08^3-C^{2}_{3}0.08^30.92 -C^{2}_{4}0.08^30.92^2 \\
    &=1-0.08^3-\dfrac{3!}{2!}0.08^30.92 -\dfrac{4!}{2!2!}0.08^30.92^2 \\
    &=0.99677481.
    \end{align*}
\end{enumerate}
\end{solution}

\end{document}
