\documentclass[en]{../../../../../../eplexam}

\usepackage{../../../../../../eplunits}
\usepackage{../../../../stat-FSAB1105}

\hypertitle{Probabilité et Statistiques}{5}{FSAB}{1105}{2016}{Janvier}
{Cynthia Laureys}
{Anouar El Ghouch et Rainer von Sachs}

L'examen était en anglais et consistait en 5 questions dont la répartition des points était, respectivement: /3 , /7, /4, /2, /4, le tout /20.

%%%%%%%%%%
% QUESTION 1
%%%%%%%%%% 
\section{(/3)}
Soient deux variables aléatoires indépendantes X et Y qui suivent respectivement une loi
\begin{itemize}
    \item $X \sim \gammad(\alpha_1,\beta)$
    \item $Y \sim \gammad(\alpha_2,\beta)$
\end{itemize}
\begin{enumerate}
    \item \begin{enumerate}
    \item Quelle est la joint density function de $(U,V)=(X+Y,\dfrac{X}{X+Y})$? Est-ce que $U$ et $V$ sont indépendants? Justifier.
    \item Quelle est la distribution (laquelle?) de $U$ ? Quel est son nom? Quels sont ses paramètres? Justifier.
\end{enumerate}
\item Si $X$ est continue avec comme cumulative distribution function $F$, quelle est la distribution de $Y=-\ln{F(x)}$? Quel est son nom? Quels sont ses paramètres? Justifier.
\end{enumerate}

\begin{solution}
\begin{enumerate}
\item \begin{enumerate}
\item On sait que $X, Y \sim_{ind} Gamma(\alpha_i,\beta)$ avec $i$ valant respectivement 1 et 2. En outre, on connait $\textbf{U}=g(\textbf{X})=\begin{cases} U=X+Y \\V=\dfrac{X}{X+Y}\end{cases}$. Pour déterminer la distribution de $\textbf{U}=(U,V)$, il s'agit donc ici d'appliquer une \textit{One-to-one transformation} définie comme suit:
$$  f_\textbf{U}(\textbf{u})=\left.\dfrac{f_\textbf{X}(\textbf{x})}{|Jac_g(\textbf{x})|} \right|_{\textbf{x}=g^{-1}(\textbf{u})} \textrm { avec } U_i=g_i(X,Y) $$
\begin{itemize}
\item Déterminons la \textit{Continuous Joint Distribution} $f_\textbf{X}(\textbf{x})$ de $\textbf{X}=(X,Y)$. Puisque $X$ et $Y$ sont indépendantes, cette distribution peut s'écrire comme le produit de leur distribution respective:
$$X \Perp Y \Leftrightarrow f(x,y)=f_X(x)f_Y(y)$$
\begin{align*}
	f_\textbf{X}(\textbf{x})&=f_X(x)f_Y(y)\\
	&= \dfrac{x^{\alpha_1-1}e^{-x/\beta}}{\beta^{\alpha_1} \Gamma(\alpha_1)}\dfrac{y^{\alpha_2-1}e^{-y/\beta}}{\beta^{\alpha_2} \Gamma(\alpha_2)} && \textrm{ pour } x,y \geq 0\\
	&= \dfrac{x^{\alpha_1-1}y^{\alpha_2-1}e^{-(x+y)/\beta}}{\beta^{\alpha_1}\beta^{\alpha_2} \Gamma(\alpha_1)\Gamma(\alpha_2)}&& \textrm{ pour } x,y \geq 0\\
\end{align*}
\item Évaluons la valeur absolue du Jacobien $|Jac_g(\textbf{x})|$:
\begin{align*}
    |Jac_g(\textbf{x})|&= \begin{vmatrix}
    \dfrac{\partial g_1}{\partial x} &\dfrac{\partial g_1}{\partial y} \\ \dfrac{\partial g_2}{\partial x} & \dfrac{\partial g_2}{\partial y} \end{vmatrix} =\begin{vmatrix} 1 & 1 \\ \dfrac{y}{(x+y)^2} & -\dfrac{x}{(x+y)^2} \end{vmatrix}\\
    &= \left| \dfrac{\partial g_1}{\partial x}\dfrac{\partial g_2}{\partial y}-\dfrac{\partial g_1}{\partial y}\dfrac{\partial g_2}{\partial x} \right|= \left| - \dfrac{x+y}{(x+y)^2}\right|= \left|- \dfrac{1}{x+y}\right|= \dfrac{1}{x+y} && \textrm{ car } x,y \geq 0
\end{align*}
\item Calculons $g^{-1}(\textbf{U})$:
$$g(\textbf{X})=\begin{cases} U=X+Y \\V=\dfrac{X}{X+Y}\end{cases}\Leftrightarrow g^{-1}(\textbf{U})=\begin{cases} X=UV \\Y=U-UV\end{cases} $$
\end{itemize}   
On obtient par conséquent:
\begin{align*}
    f_\textbf{U}(\textbf{u})&=\left.\dfrac{f_\textbf{X}(\textbf{x})}{|Jac_g(\textbf{x})|} \right|_{\textbf{x}=g^{-1}(\textbf{u})} && \textrm { pour } u,v \geq 0\\
    &=\left. (x+y) \dfrac{x^{\alpha_1-1}y^{\alpha_2-1}e^{-(x+y)/\beta}}{\beta^{\alpha_1}\beta^{\alpha_2} \Gamma(\alpha_1)\Gamma(\alpha_2)} \right|_{\textbf{x}=g^{-1}(\textbf{u})} && \textrm { pour } u,v \geq 0\\
    &= u \dfrac{(uv)^{\alpha_1-1}(u-uv)^{\alpha_2-1}e^{-u/\beta}}{\beta^{\alpha_1}\beta^{\alpha_2} \Gamma(\alpha_1)\Gamma(\alpha_2)} && \textrm { pour } u,v \geq 0\\\end{align*}  
Pour prouver l'indépendance de $U$ et $V$, on peut utiliser le fait que si la \textit{Continuous Joint Distribution} peut s'écrire comme le produit de deux fonctions, l'une dépendant uniquement de $U$ et l'autre de $V$ alors elles seront indépendantes. Ce ne sera le cas que si les domaines de définition de $U$ et $V$ sont indépendants ce qui est bien le cas ici. On montre donc:
$$U \Perp V \Leftrightarrow f(u,v)=f_U(u)f_V(v)$$ où $f(u,v)=\dfrac{1}{\beta^{\alpha_1}\beta^{\alpha_2} \Gamma(\alpha_1)\Gamma(\alpha_2)} \left[u^{\alpha_1-\alpha_2-1}e^{-u/\beta}\right]\left[v^{\alpha_1-1}(1-v)^{\alpha_2-1} \right]$
\item On sait que $U=X+Y$ avec $X, Y \sim_{ind} Gamma(\alpha_i,\beta)$ pour $i$ valant respectivement 1 et 2. Il s'agit donc d'une somme de deux distributions Gamma indépendantes. On va donc se servir de la propriété de la M.G.F. suivante:
$$m_{\sum^n_{i=1} Y_i}(t)=\prod^n_{i=1} m_{Y_i}(t) \textrm{ avec }  Y_i \textrm{ mutuellement indépendants}$$ On connait les M.G.F. de $X$ et $Y$: $m_{X_i}(t)=(1-\beta t)^{-\alpha_i}$. On peut donc déterminer la M.G.F. de $U$:
\begin{align*}
	m_U(t)&=m_{\sum^2_{i=1} X_i}(t)=\prod^2_{i=1} m_{X_i}(t)\\
	&= m_{X}(t)m_{Y}(t)=(1-\beta t)^{-\alpha_1}(1-\beta t)^{-\alpha_2}=(1-\beta t)^{-(\alpha_1+\alpha_2)}
\end{align*}
L'expression de la M.G.F. nous permet d'identifier la distribution de $U$, en effet on reconnait la M.G.F. d'une distribution Gamma. En conclusion, $U\sim Gamma(\alpha_1+\alpha_2, \beta)$.
\end{enumerate}
\item Déterminer la distribution de $Y$ se fera en deux étapes. Il faut trouver la distribution de $U=F(X)$ et puis lui appliquer une \textit{Strictly Monotonic Transformation}.
\begin{itemize}
\item Commençons par déterminer la distribution de $U$. Puisque $F(x)$ est une \textit{Continuous Cumulative Distribution Function}, il s'agit d'une fonction strictement croissante. On peut donc appliquer une \textit{Strictly Monotonic Transformation} sur $X$:
\begin{align*}
	f_U(u)&=\left.\dfrac{f_X(x)}{|g'(x)|} \right|_{x=g^{-1}(u)} && \textrm{ pour } u \in g(I)\\
	\intertext{Dans ce cas-ci, la transformation appliquée est $g(x)=F(x)$. Par les propriétés d'une \textit{Continuous Cumulative Distribution Function}, on sait que $g'(x)=F'(x)=f_X(x)$ et que $g(I)=F(I)\in [0,1]$.}\\
	f_U(u)&=\left.\dfrac{f_X(x)}{|F'(x)|} \right|_{x=F^{-1}(u)} && \textrm{ pour } u \in F(I)\\
	&=\left.\dfrac{f_X(x)}{|f_X(x)|} \right|_{x=F^{-1}(u)} && \textrm{ pour } u \in [0,1]\\
	\intertext{Une \textit{Continuous Probability Density Function} étant toujours définie positive:}
	&=\left.\dfrac{f_X(x)}{f_X(x)} \right|_{x=F^{-1}(u)}=1 && \textrm{ pour } u \in [0,1]\\
\end{align*}
On reconnait une distribution Uniforme: $U \sim U(0,1)$. 
\item Maintenant que nous connaissons la distribution de $U$, nous pouvons lui appliquer à nouveau une \textit{Strictly Monotonic Transformation} puisque la fonction $g(U)=-\ln{U}$ est strictement décroissante:
\begin{align*}
	f_Y(y)&=\left.\dfrac{f_U(u)}{|g'(u)|} \right|_{u=g^{-1}(y)} && \textrm{ pour } y \in g(I)\\
	\intertext{Nous pouvons calculer que $g'(u)=-\dfrac{1}{u}$. De plus, nous savons que $f_U(u)=1$ $\forall u \in [0,1]$.}
	&=\left. u \right|_{u=g^{-1}(y)} && \textrm{ pour } y \geq 0\\
	\intertext{Si $g(u)=-\ln{u}$, alors $g^{-1}(y)=\exp{(-y)}$.}
	&=\exp{(-y)} && \textrm{ pour } y \geq 0
\end{align*}
\end{itemize}
On reconnait une distribution Gamma telle que $Y\sim Gamma(1,1)$ ou plus spécifiquement une distribution Exponentielle: $Y \sim Expo(1)$.
\end{enumerate}
\end{solution}

%%%%%%%%%%
% QUESTION 2
%%%%%%%%%% 
\section{(/7)}
Soient $X_1,\ldots,X_n$, des valeurs aléatoires dont la probability density function est
\begin{equation}
  f(x)=\dfrac{\alpha}{\beta}x^{\alpha-1}\exp{(\dfrac{-x^\alpha}{\beta})} \text{   avec }x>0
\end{equation}
$\alpha,\beta>0$ et $\alpha$ connu et $\beta$ inconnu
\begin{enumerate}
    \item Soit $\pi \in (0,1)$ donné, trouver $q_\pi$, le $\pi$-quantile de $X$ en fonction de $\alpha, \beta$
    \item Quelle est la distribution de $X^\alpha$ ? Quel est son nom, ses paramètres? Justifier?
    \item Quel est $\widehat{\beta}$, l'estimateur de $\beta$ ? Que vaut $\mse(\widehat{\beta})$? Est que $\widehat{\beta}$ est ``consistent''? En déduire $\widehat{q}_\pi$, un estimateur ``consistent'' pour $q_\pi$. Justifier
    \item Utiliser la mgf pour montrer que $n\widehat{\beta}\sim \gammad(n,\beta)$. En déduire la distribution de $\dfrac{2n\widehat{\beta}}{\beta}$. Justifier
    \item On veut tester $H_0$  : $\beta\leq\beta_0$ VS $H_1$ : $\beta>\beta_0$. Suggérez un test statistic \footnote{``Suggest a statistical test''} and dérivez-en la $p$-value. Justifiez chaque étape
    \item Donnez un intervalle de confiance de \SI{95}{\%} pour $q_\pi$
    \item Quelle est la distribution asymptotique de $\widehat{q}_\pi$ ? Donnez l'intervalle de confiance asymptotique de $q_\pi$
\end{enumerate}
\nosolution

%%%%%%%%%%
% QUESTION 3
%%%%%%%%%% 
\section{(/4)}
Soit une distribution jointe:
\begin{equation}
    f(x,y)=\dfrac{3y}{4} \text{ ($0\leq y \leq x \leq 2$)}
\end{equation}
\begin{enumerate}
    \item $P(Y|X=x)$ ?
    \item $P\left(Y>\dfrac{1}{2}|X=\dfrac{3}{2}\right)$ et $E\left(Y|X=\dfrac{3}{2}\right)$ ?
    \item $P(X+Y<2)$ ?
    \item $V(X-2Y)$ ?
\end{enumerate}

\begin{solution}
Commençons par dessiner le domaine de définition de la loi de distribution $f(x,y)$. Il s'agit du triangle de sommets (0,0), (2,0) et (2,2) délimité par les droites $y=0$, $x=2$ et $x=y$.
\begin{enumerate}
    \item $P(Y|X=x)=\dfrac{f(x,y)}{f_X(x)}$ \\
    Nous connaissons la \textit{Joint Probability Density Function} $f(x,y)$. Il reste donc à déterminer la \textit{Marginal Probability Density Function} $f_X(x)$ :
    $$f_X(x)=\int_0^x f(x,y) dy= \int_0^x \dfrac{3y}{4} dy= \dfrac{3}{4}\left[\dfrac{y^2}{2}\right]_0^x=\dfrac{3 x^2}{8}$$
    On obtient dès lors: $P(Y|X=x)=\dfrac{3y}{4} \dfrac{8}{3x^2}=\dfrac{2y}{x^2}$.
    \item Puisqu'on connait $P(Y|X=x)$, on sait que $P\left(Y|X=\dfrac{3}{2}\right)=\dfrac{8y}{9}$:
    \begin{itemize}
    \item $P\left(Y>\dfrac{1}{2}|X=\dfrac{3}{2}\right)=\int_{1/2}^{3/2} P\left(Y|X=\dfrac{3}{2}\right) dy=\dfrac{8}{9}\int_{1/2}^{3/2} y dy=\dfrac{8}{9}\left[\dfrac{y^2}{2} \right]_{1/2}^{3/2}=\dfrac{8}{9} \left(\dfrac{9}{8}-\dfrac{1}{8} \right)=\dfrac{8}{9}$ 
    \item $E\left(Y|X=\dfrac{3}{2}\right)=\int_0^{3/2}yP\left(Y|X=\dfrac{3}{2}\right) dy=\dfrac{8}{9}\int_0^{3/2}y^2 dy = \dfrac{8}{9} \left[\dfrac{y^3}{3} \right]_0^{3/2}=\dfrac{2^3}{3^2}\dfrac{3^3}{2^3 \cdot 3}=1$ 
    \end{itemize}
    \item Pour calculer cette intégrale, il faut intégrer $f(x,y)$ où $x+y<2$. Il s'agit du triangle de sommets (0,0), (1,1) et  (2,0) délimité par les droites $y=0$, $x=y$ et $x=2-y$. \\
    $P(X+Y<2)=\int_0^1 \int_y^{2-y} f(x,y) dx dy=\dfrac{3}{4}\int_0^1 \int_y^{2-y} y dx dy=\dfrac{3}{4}\int_0^1 y [x]_y^{2-y} dy=\dfrac{3}{2}\int_0^1 y-y^2dy=\dfrac{3}{2}\left[\dfrac{y^2}{2}-\dfrac{y^3}{3} \right]_0^1=\dfrac{3}{2}\left( \dfrac{1}{2}-\dfrac{1}{3}\right)=\dfrac{1}{4}$
    \item Par la propriété de la variance: $V(aX+bY)=a^2V(X)+b^2V(Y)+2abCov(X,Y)$, nous avons ici:
$$V(X-2Y)=V(X)+4V(Y)-4Cov(X,Y)$$
où $V(X)=E(X^2)-E(X)^2$, $V(Y)=E(Y^2)-E(Y)^2$ et $Cov(X,Y)=E(XY)-E(X)E(Y)$. Il nous reste donc à calculer les différentes espérances. Mais avant, déterminons la \textit{Marginal Probability Density Function} de $Y$:
$$f_Y(y)=\int_y^2 f(x,y) dx= \int_y^2 \dfrac{3y}{4} dx= \dfrac{3y}{4}[x]_y^2=\dfrac{3y}{4} (2-y)=\dfrac{3}{2}y-\dfrac{3}{4}y^2$$
\begin{itemize}
\item $E(X)=\int_0^2 xf_X(x) dx=\dfrac{3}{8}\int_0^2 x^3 dx=\dfrac{3}{8}\left[ \dfrac{x^4}{4}\right]_0^2=\dfrac{3}{2}$
\item $E(X^2)=\int_0^2 x^2f_X(x) dx=\dfrac{3}{8}\int_0^2 x^4 dx=\dfrac{3}{8}\left[ \dfrac{x^5}{5}\right]_0^2=\dfrac{12}{5}$
\item $E(Y)=\int_0^2 yf_Y(y) dy=\int_0^2 \dfrac{3}{2}y^2-\dfrac{3}{4}y^3 dy=\left[ \dfrac{3}{2}\dfrac{y^3}{3}-\dfrac{3}{4}\dfrac{y^4}{4}\right]_0^2=4-3=1$
\item $E(X^2)=\int_0^2 y^2f_Y(y) dy=\int_0^2 \dfrac{3}{2}y^3-\dfrac{3}{4}y^4 dy=\left[ \dfrac{3}{2}\dfrac{y^4}{4}-\dfrac{3}{4}\dfrac{y^5}{5}\right]_0^2=6-\dfrac{24}{5}=\dfrac{6}{5}$
\item $E(XY)=\int_0^2\int_0^x xy f(x,y)dy dx=\dfrac{3}{4}\int_0^2 x\int_0^x y^2dy dx=\dfrac{3}{4}\int_0^2 x \left[\dfrac{y^3}{3} \right]_0^x dx=\dfrac{1}{4}\int_0^2 x^4 dx=\dfrac{1}{4} \left[\dfrac{x^5}{5}\right]_0^2=\dfrac{1}{4} \dfrac{2^5}{5}=\dfrac{8}{5}$
\end{itemize}
    \begin{align*}
    V(X-2Y)&=[E(X^2)-E(X)^2]+4[E(Y^2)-E(Y)^2]-4[E(XY)-E(X)E(Y)]\\
    &= \left[\dfrac{12}{5}-\dfrac{9}{4}\right]+4\left[\frac{6}{5}-1\right]-4\left[\dfrac{8}{5}-\dfrac{3}{2}\cdot 1\right]\\
    &= \dfrac{3}{20}+4 \cdot \dfrac{1}{5}-4\cdot\dfrac{1}{10}\\
    &=\dfrac{11}{20}
    \end{align*}
\end{enumerate}
\end{solution}

%%%%%%%%%%
% QUESTION 4
%%%%%%%%%% 

\section{(/2)}
Des composants électroniques sont placés en parallèle. Il y en a $n_1$ qui suivent une distribution exponentielle (en nombre d'années) $\sim \expo(\beta_1)$ et $n_2$ dont la distribution est une exponentielle (en nombre d'années) $\sim \expo(\beta_2)$ avec $n_1+n_2=n$ et $n>2$. Sachant que le système ne fonctionne plus si TOUS les composants ne fonctionnent plus, quelle est la probabilité que le système dans sa globalité tiendra plus de 10 ans? Exprimer votre réponse en terme $\beta_1$, $\beta_2$, $n_1$ et $n_2$.

\begin{solution}
Le système ne fonctionnant plus si tous les composants ne fonctionnent plus, le temps de fonctionnement sera déterminé par le temps de fonctionnement de la composante, parmi les n composantes, qui fonctionnera le plus longtemps.

Soient $X_i \sim Expo(\beta_1)$ pour $i \in {1, ..., n_1}$, $Y_j \sim Expo(\beta_2)$ pour $j \in {1, ..., n_2}$ et $U=X_1, ..., X_{n_1},Y_1, ..., Y_{n_2}$. Nous voulons connaitre la distribution de $U_{(n)}$ telle que $U_{(1)}\leq ... \leq U_{(n)}$.
\begin{align*}
F_{U_{(n)}}(u)&=P(U_{(n)}\leq u)=P(U_{(1)}\leq u)...P(U_{(n)}\leq u)\\
&= [F_X(u)]^{n_1}[F_Y(u)]^{n_2}\\
\intertext{Il nous reste à obtenir les expressions des \textit{Cumulative Distribution Function} de $X_i$ et $Y_j$ par intégration de leur \textit{Probability Density Function}:}
F_{X_i}(x_i)&=\begin{cases} 0 & \textrm{if } x_i<0 \\
1-\exp{\left(-\dfrac{x_i}{\beta_1}\right)} & \textrm{if } x_i\geq 0 \end{cases}\\
F_{Y_i}(y_i)&=\begin{cases} 0 & \textrm{if } y_i<0 \\
1-\exp{\left(-\dfrac{y_i}{\beta_2}\right)} & \textrm{if } y_i\geq 0 \end{cases}\\
\intertext{Nous obtenons dès lors:}
P(U_{(n)}>10)&=1-P(U_{(n)}\leq10)= 1- F_{U_{(n)}}(10)=1-[F_X(10)]^{n_1}[F_Y(10)]^{n_2}\\
&=1-\left[1-\exp{\left(-\dfrac{10}{\beta_1}\right)} \right]^{n_1}\left[1-\exp{\left(-\dfrac{10}{\beta_2}\right)} \right]^{n_2}\\
\intertext{En dérivant, nous obtiendrons la \textit{Probability Density Function} de $U_{(n)}$:}
f_{U{(n)}}(u)&=n_1[F_X(u)]^{n_1-1}f_X(u)[F_Y(u)]^{n_2}+n_2[F_Y(u)]^{n_2-1}f_Y(u)[F_X(u)]^{n_1}
\end{align*}
\end{solution}

%%%%%%%%%%
% QUESTION 5
%%%%%%%%%% 
\section{(/4)}
Une population de composants a une probabilité de dysfonctionnement de \SI{8}{\%}.
\begin{enumerate}
    \item Si $n=12$, quelle est la probabilité que plus de 2 composants aient un défaut?
    \item Si $n=200$, quelle est la probabilité que moins de 20 composants aient un défaut?
    \item On teste les composants l'un à la suite de l'autre. Quelle est la probabilité que l'on doive en tester plus de 3 avant de trouver le premier dysfonctionnant?
    \item On teste les composants l'un à la suite de l'autre. Quelle est la probabilité que l'on doive en tester plus de 5 avant d'en trouver 3 dysfonctionnant?
\end{enumerate}

\begin{solution}
\begin{enumerate}
    \item Soit $X \sim Bi(12,0.8)$ avec $p=0.08$ et $n=12$. On sait que 
    $$P(X=x)=C^x_np^x(1-p)^{(n-x)} \textrm{ pour } x =0,1,...,n$$ 
    \begin{align*}
    P(X>2)&=1-P(X=0)-P(X=1)-P(X=2)\\
    &=1-C_{12}^0p^0(1-p)^{(12-0)}-C^1_{12}p^1(1-p)^{(12-1)}-C^2_{12}p^2(1-p)^{(12-2)}\\
    &=1-0.92^{12}-12 \cdot 0.08 \cdot 0.92^{11}-\frac{12!}{10!2!}0.08^2\cdot 0.92^{10}=0.081876563
    \end{align*}
    \item Soit $X \sim Bi(200,0.08)$ avec $p=0.08$ et $n=200$.    On cherche $P(X<20)$. Comme le calcul est beaucoup plus long, on va l'approximer par le \textit{Central Limit Theorem} qui nous dit:
    $$\frac{X_n-np}{\sqrt{np(1-p)}}\xrightarrow{d} Z \sim \mathcal{N}(0,1)$$
    et d'une \textit{Continuity Correction}:
    \begin{align*}
    P(X< 20)&\approx P\left(\frac{X-np}{\sqrt{np(1-p)}} \leq \frac{20.5-np}{\sqrt{np(1-p)}} \right)\\
    &= P\left(\frac{X-16}{\sqrt{16\cdot 0.92}} \leq \frac{20.5-16}{\sqrt{16 \cdot 0.92}} \right)\\
    &= P\left(Z \geq 1.17 \right)\\
    &=0.121
    \end{align*}
    \item Soit $X \sim Hyp(n,r,N)$ avec $n=?$, $r=?$ et $N=?$. On sait que
    $$P(X=x)=\frac{C^{x}_{r}\cdot C^{n-x}_{N-r}}{C^n_N} \textrm{ pour } x =0,1,...,min(n,r)$$
    \begin{align*}
    P(X>3)&=1-P(X=0)-P(X=1)-P(X=2)-P(X=3)\\
    &= ...
    \end{align*}
    
    \item Soit $X \sim NB(3,0.08)$ avec $p=0.08$ et $r=3$. On sait que
    $$P(X=x)=C^{r-1}_{x-1}p^rq^{x-r} \textrm{ pour } x=3,4+1,...,n$$
    \begin{align*}
    P(X>5)&= 1-P(X=3)-P(X=4)-P(X=5)\\
    &=1-C^{3-1}_{3-1}p^3q^{3-3}-C^{3-1}_{4-1}p^3q^{4-3} -C^{3-1}_{5-1}p^3q^{5-3} \\
    &=1-0.08^3-C^{2}_{3}0.08^30.92 -C^{2}_{4}0.08^30.92^2 \\
    &=1-0.08^3-\dfrac{3!}{2!}0.08^30.92 -\dfrac{4!}{2!2!}0.08^30.92^2 \\
    &=0.99677481
    \end{align*}
\end{enumerate}
\end{solution}

\end{document}