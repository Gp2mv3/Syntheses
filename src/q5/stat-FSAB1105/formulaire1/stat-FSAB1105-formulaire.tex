\documentclass[10pt,landscape,en]{../../../eplformulaire}

\usepackage{../stat-FSAB1105}

\usepackage[left=0.1cm,right=0.1cm,top=0.1cm,bottom=0.2cm]{geometry}
\usepackage{empheq}

\setlength{\columnseprule}{1pt}
\def\columnseprulecolor{\color{black}}

\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\hypertitle{Probabilit√© et Statistiques}{5}{FSAB}{1105}
{Charles Momin}
{Anour El Ghouch et Rainer von Sachs}

\begin{multicols*}{4}
\scriptsize

\textbf{\footnotesize Data set properties}

\begin{itemize}
\item{\textbf{Mean}: $\bar{X} = \dfrac{1}{n} \displaystyle{\sum_{i=1}^{n} X_i }$}
\item{\textbf{Variance}: \\ \textbullet $s^2 = \dfrac{1}{n-1} \displaystyle{\sum_{i=1}^n \left( X_i - \overline{X}\right) ^2}$ }
\item{\textbf{Standard deviation}: $s = \sqrt{s^2}$}
\item{\textbf{Quantile}: \\ for a sample of $n$ data \\ $q_p = X \left( 1 + (n-1)\cdot p\right)$}
\end{itemize}

\textbf{\footnotesize Probability properties}

\begin{itemize}
\item[\textbullet]{\textbf{$P(A) = 1 - P \left( \overline{A} \right)$}}
\item[\textbullet]{\textbf{$P \left( A\cap B\right) = P(A) \cdot P(B)$ (iff indep.)}}
\item[\textbullet]{\textbf{$P(A) = P \left( A \cap B \right)+ P \left( A \cap \overline{B} \right) $}}
\item[\textbullet]{\textbf{$P(A\mid B) = \dfrac{P(A \cap B)}{P(B)}$}}
\item[\textbullet]{\textbf{$P(A_i\mid B) = \dfrac{P(B \mid A_i)P(A_i)}{\sum P(B\mid A_i)P(A_i)}$}}
\item[\textbullet]{\textbf{$P(A\mid B) = P(A)$ (iff indep.)} }
\item[\textbullet]{\textbf{$P(B\mid A) = \dfrac{P(A\mid B) \cdot P(B)}{P(A)}$}}
\item[\textbullet]{$P(A \cap B) = P(B)P(A|B) = P(B)P(B|A)$}
\item[\textbullet]{$P(A \cup B) = P(A) + P(B) - P(A \cap B)$}
\item[\textbullet]{$P(\overline{A \cup B}) = P(\overline{A} \cap \overline{B})$}
\item[\textbullet]{$P(\overline{A \cap B}) = P(\overline{A} \cup \overline{B})$}
\end{itemize}


\textbf{\footnotesize Enumerative combinations}
$r$ objects taken from $n$ distinct objects
\begin{itemize}
\item Order, replacement: $n^k$
\item{\textbf{Permutation:} order, no replacement, $k = n$: $n!$ }
\item{\textbf{Partial permutation/Arrangement:} \\ order, no replacement\\ $A_n^r = P_n^r = \dfrac{n!}{(n-r)!}$ \\ if replacement possible permutations = $n^r$}
\item{\textbf{Combination:} \\ no order, no replacement \\ $C_n^r = \dfrac{n!}{r!(n-r)!}$}
\end{itemize}

\textbf{\footnotesize Random var. properties}

\begin{itemize}
\item{\textbf{Probability:} \\ $P(X\in I) = \displaystyle{\int_I f(x) ~\textrm{d}x}$}
\item{\textbf{Expectation}:
    probability density function $f(x)$ : $0 \leq f(x)$, $\E(1) = \int_{-\infty}^\infty f(x) = 1$\\
    \textbullet $\E(g(X)) = \displaystyle{\int_{-\infty}^{\infty}} g(x) \cdot f(x) \dif x$ \\
    discrete probability function $p(x)$ : $0 \leq p(x) \leq 1$, $\E(1) = \sum_{x} p(x) = 1$\\
    \textbullet $\E(g(X)) = \sum_{x} g(x) \cdot  p(x)$ \\
    $\mu_X = \E(X)$ \\
    Cumulative distribution function: Probability that $X$ is smaller than $a$:\\
    $F(a) = P(X \leq a) = \int_{-\infty}^a f(x) \dif x$\\
  \textit{Property:} \\ \textbullet $\E(aX + bY + cZ + d) = a \E(X) + b\E(Y) + c\E(z) + d$ \\
  \textbullet $E \left(\sum_i X_i\right) = \sum_i \E(X_i)$ \\
  \textbullet $\E(XY) = \E(X)\E(Y)$ \textbf{if ind}}
\item{\textbf{Variance:} \\ $\sigma^2_X = \E(X-\mu_X)^2 = \E(X^2) - (\E(X))^2$ \\ standard deviation$=\sqrt{\sigma^2_X}$ \\ \textit{Property} \\ \textbullet $\var(a) =0$ \\
  \textbullet $\var(a+bX)=b^2 \var(X)$}
\item{\textbf{Momentum:} \\ $m_X(t) = \E(\exp(tX))$ \\ (k-om) $\displaystyle{\left.\dfrac{d^k m_X(t)}{dt^k}\right|_{t=0} = \E(X^k)  }$}
\end{itemize}


\textbf{\footnotesize Discrete distributions}

\begin{itemize}
\item{\textbf{Uniform:} \\ \textbullet $P(X=x) = \dfrac{1}{n}$ \\ \textbullet $\mu_X = \dfrac{X_1 + ... + X_n}{n}$ \\ \textbullet $\sigma_X^2 = \dfrac{X_1^2 + ... + X_n^2}{n} - \mu_X^2$}
\item{\textbf{Bernouilli (nbr succ. in $n$ trials):} \\ if $X\sim Be(p)$ \\ \textbullet $P(X=x)=p^x(1-p)^{1-x}$ \\ \textbullet $\mu_X = p$ \\ \textbullet $\sigma_X^2 = pq = p(1-p)$ \\ if $X\sim Bi(n,p)$ \\ \textbullet $P(X=x)=C_n^xp^xq^{n-x}$ \\ \textbullet $\mu_X = np$ and $\sigma_X^2=npq$ \\ \textbullet $m_X(t)=(pe^t+q)^n$ \\ \textbullet $m'_X(t) = npe^t(pe^t+q)^{n-1}$ \\ \textbullet $npe^t(npe^t+q)(pe^t+q)^{n-2}$}
\item{\textbf{Geometric (nbr. rep. until $1^{st}$ succ.}}
\item{\textbf{Poisson (events occured in speficied boundaries):} $X\sim Po(\lambda)$ \\ \textbullet Homogen($\lambda u = cst$), Indep., Rare($\lim_{n\Rightarrow \infty} p =0$) \\ \textbullet if $n\gg$, $p\ll$ and $\lambda=np<7$ then $Bi(n,p)\approx Po(np)$ \\ \textbullet $P(X=x)=e^{-\lambda}\dfrac{\lambda^x}{x!}$ \\ \textbullet $Bi(n,\dfrac{\lambda}{n}) \Rightarrow Po(\lambda)$ (for $n \Rightarrow \infty$) \\ \textbullet $\mu_X=\lambda$ and $\sigma_X^2=\lambda$}
\end{itemize}


\textbf{Continuous distributions}
\begin{itemize}
\item{\textbf{Normal:} if $X\sim N(\mu, \sigma^2)$ \\ \textbullet $P(Z\geq a)=1-P(Z\geq -a)$}
\item{\textbf{Standard Normal:} $Z\sim N(0,1)$\\ \textbullet Normal with $\mu=0$ and $\sigma=1$ \\ for $Z\sim N(0,1)$ and $\alpha \in (0,1)$: \\ \textbullet $z_{\alpha} = q_{1-\alpha}\mid P(Z\geq z_{\alpha})=\alpha$ \\ \textbullet $z_{\alpha}=-z_{1-\alpha}$ \\ \textbullet if $X\sim N(\mu, \sigma^2)$ then $a+bX\sim N(a+b\mu, b^2\sigma^2)$ and : \\  if $X\sim N(\mu, \sigma^2)$ then $Z=\dfrac{X-\mu}{\sigma}\sim N(0,1)$ \\ \textbullet if $X \sim N(\mu, \sigma^2)$: \\  $P(X\leq x) = P(\dfrac{X-\mu}{\sigma} \leq \dfrac{x-\mu}{\sigma}) = P(Z\leq \dfrac{x-\mu}{\sigma})$ \\ \textbullet $\%\alpha$ of $N(\mu,\sigma^2) = \sigma z_{\alpha} + \mu$ \\ \textbullet $P(\mid X - \mu \mid \leq k\sigma) \geq 1 -\dfrac{1}{k^2}$ \\ in partucal if $k=4.47$ then $P(X \in \left[ \mu - k \sigma , \mu + k \sigma \right] ) \geq 0.95 \approx 1$}
\item{\textbf{Exponential:} \\ \textbullet Defining the lifetime of a event or the waiting time  for an occurence of a event}
\item{\textbf{Gamma:} \\ \textbullet $\gammad (\alpha) = \displaystyle{\int_0^{\infty} t^{\alpha-1} e^{-t} dt}$ \\ \textbullet $\gammad (\alpha+1)=\alpha \gammad (\alpha)$ \\
\textbullet $\gammad (1)=1$ \\ \textbullet $\gammad (n+1)=n!$}
\end{itemize}

\textbf{\footnotesize Multivariable dist.}

\begin{itemize}
\item{\textbf{Joint distribution:} for the vector $(X,Y)$: \\ \textbullet $p(x,y)=P(X=x, Y=y)$ \\ \textbullet for $A = \left[ a,b \right] \times \left[ c,d \right] \subset R^2$: \\ $P((X,Y)\in A) = \displaystyle{\sum_{(x,y) \in A} p(x,y)}$ \\ \textbullet for $f(x,y) \mid f\geq0$ and \\ $\displaystyle{\int_{\infty}^{\infty} \int_{\infty}^{\infty} f(x,y) dxdy =1}$, then: \\ $P((X,Y) \in A) = \displaystyle{\int \int _{A} f(x,y) dxdy}$ \\ \textbullet $F(x,y)=P(X\leq x, Y\leq y)$: \\ \textbf{(dis.)} $F(x,y)= \displaystyle{\sum_{s\leq x, t\leq y} p(s,t)}$ \\ \textbf{(cont.)} $F(x,y)=\displaystyle{\int_{\infty}^y \int_{\infty}^x f(s,t) dsdt}$ \\ \textbullet $f(s,t)= \dfrac{\partial ^2 F}{\partial x \partial y}$}
\item{\textbf{Marginal}: \\ \textbullet if $X,Y$ have joint pdf: \\
$f_X(x) = P(X=x)=$ \\ (dis.) $\displaystyle{\sum_y P(X=x,Y=y)}$ \\ (con.) $\displaystyle{\int_{\infty}^{\infty} f(x,y)dy}$ \\ \textbullet if $X,Y$ have joint cdf: \\ $F_X(x) = P(X\leq x)= F(x, \infty)$}
\item{\textbf{Conditionnal}: if $P(X\in A)>0$: \\ $P(Y \in B \mid X\in A) = $ \\ \textbf{(disc.)} $\displaystyle{\sum_{y\in B} \dfrac{\sum_{x\in A} p(x,y)}{\sum_{x \in A} p_X(x)}}$ \\
\textbf{(cont.)} $\displaystyle{\int_B \dfrac{\int_A f(x,y)dx}{\int_A f_X(x)dx} dy}$ \\ \textbf{discrete} \\ \textbullet  $p(y\mid x)=P(Y=y \mid X=x) = \dfrac{p(x,y)}{p_X(x)}$ \\ \textbullet $P(Y\in B \mid X=x)= \displaystyle{\sum_{y\in B} p(y\mid x)}$ \\ \textbullet $F(y\mid x)=P(Y\leq y \mid X=x)= \displaystyle{\sum_{t\leq y} p(t\mid x)}$ \\ \textbf{conti.} \\ \textbullet $P(Y \in B, X=x)= \displaystyle{ \int_B f(y\mid x)dy}$ \\ \textbullet $F(y\mid x)=P(Y\leq u \mid X=x)= \displaystyle{\int_{\infty}^y f(t \mid x) dt}$ \\ \textbullet $\dfrac{\partial F}{\partial y}(y\mid x)=f(y\mid x)$}
\item{\textbf{Momentum:} \\ \textbullet $m_{X,Y}(s,t)=m(s,t)=\E(e^{sX+tY})$ \\ \textbullet $\dfrac{\partial ^{k+n}m}{\partial s^k \partial t^n}(0,0) = \E(X^kY^n)$ \\ \textbullet $m(s,0)=$ marg. mgf of X \\ \textbullet $m(0,s)=$ marg. mgf of Y}
\item{\textbf{Covariance} \\ \textbullet $\sigma_{XY}\equiv \cov(X,Y) = \E((X-\E(X))(Y-\E(Y))) = \E(XY)-\E(X)\E(Y)$ \\ \textbullet $\cov(X,Y)=\cov(Y,X)$ \\ \textbullet $\cov(X,X)=V(X)$ \\ \textbullet $\cov(a,X)=0$}
\item{\textbf{Correlation}: \\ \textbullet $\rho_{XY}\equiv \rho(X,Y)=\dfrac{\sigma_{XY}}{\sigma_X \sigma_Y}$ \\ \textbullet $\mid \rho_{XY} \mid \leq 1$ (Cauchy-swartz) \\ \textbullet $\rho_{XY}=\pm 1 \Leftrightarrow Y = a+bX$}
\item{\textbf{Mutlinomial dist}: \\ \textbullet $P(Y_1=y_1, ..., Y_n=y_n) = \dfrac{n!}{n_1!...n_n!}p_1^{n_1}...p_n^{n_n}$}
\end{itemize}


\textbf{Independant r.v.}
\begin{itemize}
\item[\textbullet]{$P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$}
\item[\textbullet]{\textbf{Condition neces.}: \\ \textbullet $F(x,y) = F_X(x)F_Y(y)$ \\ \textbullet $f(x,y)=f_X(x)f_Y(y)$ \\ \textbullet $f(y\mid x)=f_Y(y)$ \\ \textbullet $f(x\mid y)=f_X(x)$)}
\item{\textbf{Conditionnal expect.}: \\ \textbullet \begin{empheq}[left={$f(x)$= \empheqlbrace}]{align}
& \sum g(y)p(y\mid x)  \\ & \int^y g(y)f(y\mid x) dy
\end{empheq} \\ \textbullet $\mu_Y = \E(Y\mid X=x)$ \\ \textbullet $\sigma^2_Y(x)\equiv \var(Y\mid X=x) = \E(Y^2\mid X=x) - \mu^2_Y(x)$}
\end{itemize}


\textbf{Function of rv and transformation}

\begin{itemize}
\item{\textbf{Monotomic function}: \\ \textbullet $f_Y(y)=\dfrac{f_X(x)}{|g'(x)|} |_{x=g^{-1}(y)}$}
\item{\textbf{Lognormale dist}: if $X\sim N(\mu, \sigma^2)$,  $Y=e^X \sim LN(\mu, \sigma^2)$ and fdp. of $Y$= \\
$f(y)=\dfrac{1}{y\sqrt{2\pi \sigma^2}}e^{-\dfrac{1}{2}\left( \dfrac{\ln y - \mu }{\sigma} \right)^2}$}
\item{\textbf{Chi squarred dist} if $X\sim N(0,1)$ and $Y=X^2$ then $Y\sim \chi^2_1$ (v=2)}
\item{\textbf{Bivarate transf}: \\ \textbullet $f_Y(y) = \dfrac{f_X(x)}{\mid J_g(x)\mid} |_{x=g^-1(y)'}$ \\ \textbullet $|J_g(x)|=$abs$\left[ \dfrac{\partial g_1}{\partial x_1}\dfrac{\partial g_2}{\partial x_2}-\dfrac{\partial g_1}{\partial x_2}\dfrac{\partial g_2}{\partial x_1}\right]$}
\end{itemize}
\begin{align*}
F_{X+Y}(z) &= \int_{-\infty}^\infty \int_{-\infty}^{z-y} f(x,y) \dif x \dif y\\
           &= \int_{-\infty}^\infty \int_{-\infty}^{z-x} f(x,y) \dif y \dif x \\
f_{X+Y}(z) &= \int_{-\infty}^\infty f(z-y,y) \dif y\\
  &= \int_{-\infty}^\infty f(x,z-x) \dif x \\
F_{XY}(z) &= \int_{-\infty}^0 \int_{z/y}^{\infty} f(x,y) \dif x \dif y\\
  &\quad+ \int_{0}^\infty \int_{-\infty}^{z/y} f(x,y) \dif x \dif y \\
f_{XY}(z) &= \int_{-\infty}^\infty \frac{1}{|y|}f(z/y,y) \dif y \\
F_{X/Y}(z) &= \int_{-\infty}^0 \int_{yz}^{\infty} f(x,y) \dif x \dif y\\
           &\quad+ \int_{0}^\infty \int_{-\infty}^{yz} f(x,y) \dif x \dif y \\
f_{X/Y}(z) &= \int_{-\infty}^\infty |y|f(yz,y) \dif y
\end{align*}
$m_{X+Y} = m_X(t)m_Y(t)$ si $X,Y$ ind.\\
$m_{\sum X_i} = \prod m_{X_i}(t)$ si $X_1, ...$ ind.\\
if $X \sim \mathcal{N}(0,1)$,$Y \sim \mathcal{N}(0,1)$ (normales standards), $X,Y$ ind. and $Z = Y/X$,
\begin{align*}
  f_Z(z) & = \frac{1}{\pi}\int_0^\infty ye^{-y^2(z^2+1)/2} \dif y\\
         & = \frac{1}{2\pi}\int_0^\infty e^{-t(z^2+1)/2} \dif t\\
         & = \frac{1}{\pi(z^2+1)}, -\infty < z < \infty.
\end{align*}
This is called standard Cauchy density.
$\E(Z)$ and $\var(Z)$ are undefined.

\textbf{Sampling distributions}


\begin{itemize}
\item{\textbf{Mean \& Variance:} For $X_i$,i=1...n \\ \textbullet $\mu=\E(X_i)$, $\sigma^2=\var(X_i)$ \\ \textbullet $\eta_4 = \E((X_i-\mu)^4)$ \\ \textbullet $\E(\overline{X})=\mu$ and $\var(\overline{X})=\dfrac{\sigma^2}{n}$ \\ \textbullet $\E(S^2)=\sigma^2$ and $\var(S^2)=\dfrac{1}{n}\left( \eta_4 - \dfrac{n-3}{n-1}\sigma^4 \right)$ \\ \textbullet if $X\sim Be(p)|\hat{p_n} = n^{-1}\displaystyle{\sum_{i=1}^n X_i}$, then: \\ \textbullet $\E(\hat{p_n})=p$, $\var(\hat{p_n})=\dfrac{p(1-p)}{n}$}
\item{\textbf{Normal population} if $X\sim N(\mu, \sigma^2)$ then: \\ \textbullet $\dfrac{\overline{X}-\mu }{\sigma /\sqrt{n}} \sim N(0,1)$ \\ \textbullet $\overline{X} \indep S^2$ \\ \textbullet $(n-1)S^2/\sigma^2 \sim \chi^2_{n-1}$ \\ \textbullet $\dfrac{\overline{X}-\mu }{S /\sqrt{n}} \sim t_{n-1}$}
\item{\textbf{Central Limit th.} if $n\Rightarrow \infty$\\ \textbullet $Z_n = \sqrt{n}\dfrac{\overline{X_n}-\mu}{\sigma} \Rightarrow Z \sim N(0,1)$ \\ for $n\gg$ \\ \textbullet $\displaystyle{\sum^n_{i=1} X_i \approx N(\sqrt{n}\mu , \sqrt{n}\sigma)}$}
\item{\textbf{Normal approx } \\ if $n>9\dfrac{\max(p,q)}{\min(p,q)}$ \\ \textbullet $\bin(n,p)\approx N(np,np(1-p))$ \\ \textbullet $\po(\lambda) \approx N(\lambda , \lambda)$, ($\lambda\gg$) \\ \textbullet $Gamma(\alpha, \beta) \approx N(\alpha \beta , \alpha \beta ^2)$,($\alpha \gg$) \\ \textbullet $\chi^2_n \approx N(n,2n)$,($n\gg$)}
\end{itemize}

\textbf{Point estimations}
\begin{itemize}
\item{\textbf{Joint pdf.} for \textbf{X}($X_1$,...)\\ \textbullet $f_n(x;\theta) = \displaystyle{\prod_{i=1}^n f(x_i; \theta)}$}
\item{\textbf{Bias and MSE} \\ \textbullet Unbiased if $\E(\hat{\theta}) = \theta$ \\ \textbullet $\bias(\hat{\theta}) = \E(\hat{\theta})-\theta$ \\ \textbullet $\mse(\hat{\theta})=E\left[ \left( \hat{\theta} - \theta \right) ^2 \right]$ \\ $=\bias^2(\hat{\theta}) + \var(\hat{\theta})$}
\item{\textbf{Consestency} $\hat{\theta}$ cnst. if : \\ \textbullet $P(\mid \hat{\theta} - \theta \mid > \epsilon) \Rightarrow 0$, $n\Rightarrow \infty$ \\ \textbullet $\mse(\hat{\theta}) \Rightarrow 0$ \\ \textbullet if $\hat{\theta}$ cnst. for $\theta$,$g(\hat{\theta})$ cnst. for $g(\theta)$}
\item{\textbf{Method of Moments} \\ \textbullet $\mu_k = \E(X^k)$ \\ \textbullet $\hat{\mu_k}=n^{-1} \sum_{i=1}^n X_i^k$ \\ Solve $\Rightarrow \hat{\mu_k} = \mu_k(\theta)$ in $\theta$}
\item{\textbf{Maximum Likelihood} \\ \textbullet $L_n(\theta )=f_n(x_1,...,x_n;\theta ) =_{iid} \displaystyle{\prod_{i=1}^n f(x_i;\theta )}$ \\ \textbullet $LL_n(\theta ) = \displaystyle{\sum_{i=1}^n \ln(f(x_i;\theta ))}$ \\ S\textbullet Solve $S_n(\theta) = \left( \dfrac{\partial LL}{\partial \theta_1} (\theta), \dfrac{\partial LL}{\partial \theta_2} (\theta) \right)^T =0$ \\ Check $2^{nd}$ to be sure of the maximum \\ \textbullet $\hat{\theta}$ equivariant and consistant}
\end{itemize}

\textbf{Expect. and varia. properties}

\begin{itemize}
\item{\textbf{General}: \\ \textbullet $\E(a+bX+cY)=a+b\E(X)+c\E(Y)$ \\ \textbullet $\var(a+bX+cY)=b^2\var(X)+c^2\var(Y)+2bc\cov(X,Y)$ \\ \textbullet $\cov(a+bX,c+dY)=bd\cov(X,Y)$ \\ \textbullet $\cov(X+Y,Z)=\cov(X,Z)+\cov(Y,Z)$ \\ \textbullet $\E(h(X))\neq h(\E(X))$ \\ \textbullet $\E(XY)\neq \E(X)\E(Y)$ \\ \textbullet $\var(X+Y)\neq \var(X)+\var(Y)$}
\item{\textbf{if $X \indep Y$}: \\ \textbullet $\E(g(X)h(Y))=\E(g(X))\E(h(Y))$ \\ \textbullet $m_{X+Y}(t) = m_X(t)m_Y(t)$ \\ \textbullet $\sigma_{XY}=0=\rho(X,Y)$ \\ \textbullet $\var(a+bX+cY)=b^2\var(X)+c^2\var(Y)$}
\end{itemize}

\textbf{Confidence interval}
\begin{itemize}
\item{\textbf{Goal}: \\ \textbullet $P(\hat{\theta_L} \leq \theta \leq \hat{\theta_U})=1-\alpha$}
\item{\textbf{Example}: \\ \textbf{Mean} \\ \textbullet $CI=\left[ \overline{X}-z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}}, \overline{X}+z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}} \right]$ \\ and $l=2z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}}$ (not random!)}
\item{\textbf{Deriving a CI} for $Z_n(\theta )\equiv Z(\textbf{X},\theta )$\\ \textbullet $P(a\leq Z_n(\theta ) \leq b)=1-\alpha$ \\ $\Rightarrow P(L\leq \theta \leq U)=1-\alpha$}
\end{itemize}

\textbf{Additionnal informations}

\begin{itemize}
\item{\textbf{Geometric serie sum:} \\ \textbullet $\displaystyle{\sum_{i=0}^n q^i} = \dfrac{1-q^n}{1-q}$}
\item{\textbf{Tchebyshev theorem:} \\ \textbullet $P( | X- \mu |\geq \sigma k) \leq \dfrac{1}{k^2}$}
\item{\textbf{Markov theorem:} \\ \textbullet $P(|X|\geq a)\leq \dfrac{E|X|}{a}$}
\end{itemize}

\end{multicols*}

\end{document}
