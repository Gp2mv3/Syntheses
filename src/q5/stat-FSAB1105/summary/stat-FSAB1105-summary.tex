\documentclass[fr,license=none]{../../../eplsummary}

\usepackage{../stat-FSAB1105}
\newcommand{\englishit}[1]{\textit{#1}} % english words in french need to be in
% italics.

\hypertitle{Probabilité et Statistiques}{5}{FSAB}{1105}
{Benoît Legat\and Antoine Paris}
{Anouar El Ghouch et Rainer von Sachs}

\section{Probabilités}
\begin{mydef}
Une expérience aléatoire est une opération pour laquelle
le résultat n'est pas connu a priori avec certitude même
en la répétant plusieurs fois dans les mêmes conditions.
\end{mydef}

\begin{mydef}
Le \englishit{sample space} d'une expérience aléatoire est
l'ensemble, noté $S$, des résultats possibles de cette expérience.
\end{mydef}

\begin{mydef}
Un évènement est une combinaison de résultats possibles
d'une expérience. Formellement, il s'agit d'un sous-ensemble
$A$ de $S$.
\end{mydef}

\begin{mydef}
Un évènement simple est un évènement qui ne contient qu'un seul
point du \englishit{sample space}.
\end{mydef}

\begin{mydef}[Probabilité]
À chaque évènement $A$ de $S$ on associe un nombre
$P(A)$, appelée la probabilité de $A$, tel que
\begin{itemize}
	\item $P(A) \geq 0$ ;
	\item $P(S) = 1$ ;
	\item Si ${A}_i$ sont incompatibles (mutuellement exclusifs)
	\[ P(\cup_iA_i) = \sum_i P(A_i). \]
\end{itemize}
\end{mydef}

\subsection{Propriétés élémentaires des probabilités}
\[ P(\widebar{A}) = 1 - P(A) \]
\[ P(A) = P(A \cap B) + P(A \cap \widebar{B})\]
\[ P(\widebar{A \cup B}) = P(\widebar{A} \cap
\widebar{B}) \]
\[ P(\widebar{A \cap B}) = P(\widebar{A} \cup
\widebar{B}) \]

\subsection{Dénombrement}
\begin{mytheo}
Avec $m$ éléments $a_1, a_2, \dots, a_m$ et $n$
éléments $b_1, b_2, \dots, b_n$, il est possible
de créer $m \times n$ paires différentes.
\end{mytheo}

\begin{mytheo}
Un arrangement ordonné de $r$ objets distincts est
appelé une \emph{permutation}. Le nombre de façons
de remplir $r$ positions avec $n$ objets distincts
est noté $P^n_r$ et est donné par

\[ P^n_r = \frac{n!}{(n-r)!}.\]
\end{mytheo}

\begin{mytheo}
Le nombre de façon de partitionner $n$ objets distincts
en $k$ groupes distincts contenant $n_1, n_2, \dots, n_k$
objets respectivement est donné par

\[ N = \frac{n!}{n1!\,n2!\dots n_k!} \]

si l'ordre des objets dans chaque groupe n'a pas
d'importance.
\end{mytheo}

\begin{mytheo}
Le nombre de sous-ensembles non ordonnés de taille $r$
choisis (sans remise) parmis $n$ objets est

\[ C^n_r = \frac{P^n_r}{r!} = \frac{n!}{r!\,(n-r)!}. \]
\end{mytheo}

\subsection{Probabilité conditionelle et indépendance}
\begin{mydef}
La probabilité conditionnelle d'un évènement $A$, étant
donné qu'un évènement $B$ s'est produit est donnée
par

\[ P(A \mid B) = \frac{P(A \cap B)}{P(B)}. \]
\end{mydef}

\begin{mydef}
Deux évènement $A$ et $B$ sont \emph{indépendants} si
une de ces conditions est satisfaites
\[ P(A \mid B) = P(A) \]
\[ P(B \mid A) = P(B) \]
\[ P(A \cap B) = P(A)P(B). \]

Sinon, $A$ et $B$ sont dépendants.
\end{mydef}

\subsection{Loi multiplicative}
\begin{mytheo}
La probabilité de $A \cap B$ est donnée par
\[ P(A \cap B) = P(A)P(B\mid A) = P(B)P(A\mid B). \]
Si $A$ et $B$ sont indépendants, alors
\[ P(A \cap B) = P(A)P(B). \]
\end{mytheo}

\subsection{Loi additive}
\begin{mytheo}
La probabilité de $A \cup B$ est donnée par
\[ P(A \cup B) = P(A) + P(B) - P(A \cap B). \]
Si $A$ et $B$ sont mutuellement exclusifs, alors
\[ P(A \cup B) = P(A) + P(B). \]
\end{mytheo}

\subsection{Loi des probabilités totales}
\begin{mydef}
Pour un entier positif $k$, soient $B_1, B_2 \dots B_k$
tels que
\[ S = B_1 \cup B_2 \cup \dots \cup B_k \]
\[ B_i \cap B_j = \varnothing, \forall i \neq j, \]
alors ${B_1, B_2, \dots, B_k}$ est une \emph{partition}
de $S$.
\end{mydef}

\begin{mytheo}
Si ${B_1, B_2 \dots B_k}$ est une partition de $S$ tel
que $P(B_i) > 0$ $\forall i$, alors pour chaque évènement $A$
\[ P(A) = \sum_{i=1}^k P(A \cap B_i) =
\sum_{i=1}^k P(A\mid B_i)P(B_i). \]
\end{mytheo}

\subsection{Loi de Bayes}
\begin{mytheo}[Général]
Si ${B_1, B_2 \dots B_k}$ est une partition de $S$ tel
que $P(B_i) > 0$ $\forall i$, alors
\[ P(B_j\mid A) = \frac{P(A\mid B_j)P(B_j)}{\sum_{i=1}^k P(A\mid B_i)P(B_i)}.\]
\end{mytheo}

En utilisant la loi des probabilités totales, la loi de Bayes
devient
\[ P(B\mid A) = \frac{P(A\mid B)P(B)}{P(A)}. \]
Elle nous permet donc d'obtenir $P(B\mid A)$ à partir de $P(A\mid B)$.

\section{Variables aléatoires et Distributions}
\subsection{Variable aléatoire}
Une variable aléatoire est un nombre réel tiré d'une expérience aléatoire.
L'ensemble des valeurs qu'elle peut prendre est appelé son domaine.
Une variable aléatoire peut être
\begin{description}
  \item[discrète]
    c'est-à-dire que son domaine est un ensemble comptable.
  \item[continue]
    c'est-à-dire que son domaine est un ensemble incomptable.
\end{description}

\subsection{Distribution}
Une distribution d'une variable aléatoire \emph{discrète} $X$ est une fonction $p(x)$ qui associe pour
chaque valeur $x$ de $X$ la probabilité
\[ p(x) = P(X = x) \]
qu'on tire $x$ de l'expérience aléatoire.

Une distribution d'une variable aléatoire \emph{continue} $X$ n'a pas de fonction $p$ par $P(X = x) = 0$
$\forall x \in \R$.
On définit plutôt $f$, sa fonction de densité, telle que
\[ P(a \leq X \leq b) = \int_a^b f(x) \dif x \]
$\forall a, b \in \R$ tels que $a < b$.
$f$ est donc la probabilité par unité.
Comme $P(X = a) = 0$, $\leq, <$ et $\geq, >$ s'utilisent sans distinction pour une distribution \emph{continue}.

La fonction de répartition $F$ est définie comme suit
\[ F(x) \eqdef P(X \leq x) \]
ce qui signifie pour une distribution respectivement discrète et continue
\begin{align*}
  F(x) & = \sum_{k = -\infty}^x p(k) & F(x) & = \int_{-\infty}^x f(t) \dif t.
\end{align*}

Le quantile $ x_{\textnormal{p}} $ d'une distribution où $0 \leq p \leq 1$ est le nombre tel que
$F( x_{\textnormal{p}} ) = p$, c'est-à-dire $ x_{\textnormal{p}}  = F^{-1}(p)$.
$x_{0.5}$ est appelé le \emph{médian}.

\subsubsection{Espérance, variance et moments}
Soit la fonction $\E$ telle que pour toute fonction $h$,
\begin{align*}
  \E(h(X)) & = \sum_{x = -\infty}^{+\infty} h(x)p(x) & \E(h(x)) & = \int_{-\infty}^{+\infty} h(x)f(x) \dif x.
\end{align*}

L'\emph{espérance} d'une variable aléatoire est $\mu \equiv \E(X)$.

La \emph{variance} d'une variable aléatoire $\sigma^2 \equiv \var(X)$ est définie comme
\begin{align*}
  \var(X) & = \E((X-\mu)^2)\\
          & = \E(X^2) - (\E(X))^2.
\end{align*}

Le $k$\ieme{} moment d'une variable aléatoire est $\E(X^k)$.
On remarque que $\mu$ est le premier moment et que $\sigma^2$ est
la différence entre le deuxième moment et le carré du premier moment.
On sait donc calculer $\mu$ et $\sigma^2$ rien qu'avec les moments.

On peut calculer les moments facilement à l'aide de
la \emph{fonction génératrice des moments} $m_X(t)$ définie comme suit
\[ m_X(t) = \E(\e^{tX}). \]
On a donc
\begin{align*}
  m_X(t) & = \sum_{x=-\infty}^{+\infty} \e^{tx}p(x) & m_X(t) & = \int_{-\infty}^{+\infty} \e^{tx} f(x) \dif x.
\end{align*}
En effet, on remarque que
\begin{align*}
  \E(X^k) & = \left.\E(X^k \e^{tX})\right|_{t=0}\\
          & = \left.\fdif{^km_X(t)}{t^k}\right|_{t = 0}.
\end{align*}

Soit deux distributions $X, Y$, si $\forall t \in \R$, $m_X(t) = m_Y(t)$,
alors $X \sim Y$.
La fonction génératrice des moments détermine donc entièrement la distribution.

\subsection{Vecteurs aléatoires}
Un vecteur aléatoire de dimension $k$ est donné par
$(X_1, \ldots, X_k)$ où chaque $X_i$ est une variable aléatoire discrète ou continue.
Traitons ici le cas $k = 2$.

$p(x)$ devient $p(x, y) = P(X = x, Y = y)$.
On définit aussi la distribution marginale $p_X(x) = P(X = x)$,
c'est-à-dire la probabilité que $X = x$ mais que $Y$ vaut ce qu'on veut
donc
\begin{align*}
  p_X(x) & = \sum_{y = -\infty}^{+\infty} p(x, y) & f_X(x) & = \int_{-\infty}^{+\infty} f(x, y) \dif y.
\end{align*}

$F$ devient
\begin{align*}
  F(x, y) & = \sum_{r = -\infty}^{x} \sum_{s = -\infty}^{y} p(r, s) & F(x, y) & = \int_{r=-\infty}^{x} \int_{s=-\infty}^{y} f(r, s) \dif r \dif s.
\end{align*}

\subsubsection{Distributions contitionnelles}
On peut définir également des distributions conditionnelles
\begin{align*}
  p_{X\mid Y}(x\mid y) & = \frac{p(x, y)}{p(y)} & f_{X\mid Y}(x\mid y) & = \frac{f(x,y)}{f(y)}
\end{align*}
et
\begin{align*}
  F_{X\mid Y}(x\mid y) & = \sum_{k \leq x} p_{X\mid Y}(k\mid y) & F_{X\mid Y}(x\mid y) & = \int_{t \leq x} f_{X\mid Y}(t\mid y).
\end{align*}

On définit aussi l'espérance conditionnelle de façon évidente ($\forall \textrm{ fonction } g$)
\begin{align*}
  \E(g(Y)\mid X=x) & = \sum_{-\infty}^{+\infty} g(y) p(y\mid x) & \E(g(Y)\mid X=x) & = \int_{-\infty}^{+\infty} g(y) f(y\mid x)
\end{align*}
On peut alors définir la moyenne de $X$ ou de $Y$, avec l'autre variable fixée. Par exemple,
\[ \mu_Y(x) = \E(Y \mid  X = x). \]
De façon équivalente, la variance est
\begin{align*}
  \var(Y\mid X=x) & = \E((Y - \mu_Y(x))^2 \mid  X = x)\\
              & = \E(Y^2\mid X = x) - \mu_Y(x)^2.
\end{align*}

On définit aussi la fonction jointe génératrice de moments
de $X$ et $Y$
\[ m_{X,Y}(s,t) \equiv m(s,t) = \E(\e^{sX+tY}).\]

\subsubsection{Indépendance}
Si $\forall x, y \in \R$,
\[ F(x, y) = F_X(x) F_Y(y), \]
on dit que $x$ et $y$ sont indépendants, on l'écrit $X \perp Y$.

De façon équivalente, $X \perp Y$ si et seulement si
\begin{align*}
  p(x, y) & = p_X(x) p_Y(y) & f(x, y) & = f_X(x) f_Y(y).
\end{align*}

En dimension $k$, on a que $X_1 \perp X_2 \perp \ldots \perp X_k$
si et seulement si pour tout $x_1, \ldots, x_k \in \R$,
\begin{align*}
  F(x_1, x_2, \ldots, x_k) & = F_1(x_1) \cdot F_2(x_2) \cdot \ldots \cdot F_k(x_k).
\end{align*}
Attention que $X_1 \perp X_2 \perp \ldots \perp X_k \overset{\centernot\impliedby }{\implies} X_i \perp X_j$ $\forall i \neq j$.

\subsubsection{Covariance}
La covariance $\sigma_{XY} = \cov(X, Y)$ est définie comme
\begin{align*}
  \cov(X, Y) & = \E((X-\mu_X)(Y-\mu_Y))\\
             & = \E(XY) - \mu_X \mu_Y.
\end{align*}
On retrouve $\sigma_{XX} = \sigma_{X}^2$ ou encore $\cov(X,X) = \var(X)$.
\begin{itemize}
  \item $\sigma_{XY} > 0$ signifie que lorsque $X$ augmente, $Y$ augmente.
  \item $\sigma_{XY} < 0$ signifie que lorsque $X$ augmente, $Y$ diminue.
  \item $X \perp Y \overset{\centernot\impliedby}{\implies} \sigma_{XY} = 0$.
  \item Si $X$ et $Y$ sont normales, $X \perp Y \iff \sigma_{XY} = 0$.
\end{itemize}

La covariance est une mesure de l'intensité de la relation
\emph{linéaire} existant entre $X$ et $Y$. C'est pour ça que
$\sigma_{XY} = 0 \centernot\implies X \perp Y$. Si $\sigma_{XY} = 0$,
alors il n'y a pas de relation linéaire entre $X$ et $Y$, ce n'est
pas pour autant que $X$ et $Y$ ne sont pas correlées d'une autre manière.

Le problème de $\sigma_{XY}$, c'est qu'il est sensible aux changements d'échelle.
Définissons alors le coefficient de correlation
\begin{align*}
  \rho(X, Y) & = \frac{\cov(X,Y)}{\sqrt{\var(X)\var(Y)}}\\
             & = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}.
\end{align*}
On a d'ailleurs
\[ -1 \leq \rho \leq 1. \]

\subsection{Transformation}
% TODO : méthode de la fonction de répartition

Si $h$ est continu et strictement monotone et que $X,Y$ sont des variables aléatoires telles que $Y = h(X)$,
on a
\[ f_Y(y) = \abs{\fdif{h^{-1}(y)}{y}}f_X(h^{-1}(y)). \]
Ça se généralise avec une variable aléatoire de dimension $k$ $X, Y$ tels que $Y = h(X)$,
\[ f_Y(y) = \abs{J_{h^{-1}}(y)}f_X(h^{-1}(y)). \]

On peut aussi obtenir la distribution à partir de la fonction génératrice des moments car
elle détermine entièrement la distribution en utilisant par exemple \eqref{eq:sum_mgf}.

\subsection{Minimum et maximum de variables aléatoires indépendantes}
Soient $Y_1, Y_2, \dots, Y_n$ des variables aléatoires indépendantes de fonction
de répartition $F(y)$ et de densité de probabilité $f(y)$.
Soient également $Y_{(1)} = \min(Y_1, Y_2, \dots, Y_n)$ et
$Y_{(n)} = \max(Y_1, Y_2, \dots, Y_n)$. En utilisant la méthode de la
fonction de répartition, on trouve
\[ g_{(n)}(y) = nF^{n-1}(y)f(y) \]
et
\[ g_{(1)}(y) = n(1 - F(y))^{n-1}f(y). \]

\subsection{Propriétés}
On a, pour des constantes $a, b, c, d \in \R$, des variables aléatoires $X, Y, Z$ et des fonctions $g, h$
\begin{align*}
  \E(a + bX + cY) & = a + b\E(X) + c\E(Y)\\
  \var(a + bX + cY) & = b^2\var(X) + c^2\var(Y) + 2bc\cov(X,Y)\\
  \cov(a + bX, c + dY) & = bd \cov(X, Y)\\
  \cov(X + Y, Z) & = \cov(X, Z) + \cov(Y, Z).
\end{align*}
$\E$ est d'ailleurs une application linéaire.
En général,
\begin{align*}
  \E(h(X)) & \neq h(\E(X))\\
  \E(XY) & \neq \E(X) \E(Y)\\
  \var(X + Y) & \neq \var(X) + \var(Y).
\end{align*}
Par contre, si $X \perp Y$, on peut dire bien plus.
$X \perp Y \overset{\centernot\impliedby }{\implies}$
\begin{align}
  g(X) & \perp h(Y)\\
  \E(g(X)h(Y)) & = \E(g(X)) \E(h(Y))\\
  \label{eq:sum_mgf}
  m_{X + Y}(t) & = m_X(t) m_Y(t)\\
  \sigma_{XY} & = 0 = \rho(X, Y)\\
  \var(a + bX + cY) & = b^2\var(X) + c^2\var(Y)\\
  p(x \mid  y) & = p_X(x) & f(x \mid  y) & = f_X(x).
\end{align}

On a pour toutes variables aléatoires $X, Y$
\begin{align*}
  \fpart{F}{y}(x,y) & = f_Y(y) F_{X\mid Y}(x\mid y) & \fpart{F_{X\mid Y}}{y}(x\mid y) & = f_{X\mid Y}(x\mid y).
\end{align*}

\begin{myineg}[Markov]
  $\forall a, k > 0$,
  \[ P(\abs{X} \geq a) \leq \frac{\E(\abs{X}^k)}{a^k}. \]
\end{myineg}

\begin{myineg}[Tchebychev]
  $\forall a > 0$,
  \[ P(\abs{X - \mu_X} \geq a\sigma_X) \leq \frac{1}{a^2}. \]
\end{myineg}

Pour une variable aléatoire continue
\begin{align*}
  0 \leq F(x) & \leq 1\\
  \int_{-\infty}^{+\infty} f(x) \dif x & = 1\\
  f(x) & = F'(x).
\end{align*}

\section{Échantillon}
Un échantillon (\emph{sample}) est une suite de $n$ valeurs $X_1, \ldots, X_n$ qui sont tirées d'une variable aléatoire $X$.

On définit la moyenne $\widebar{x}$ et la variation d'un échantillon $s^2$ comme suit
\begin{align*}
  \widebar{x} & = \frac{\sum_{i=1}^n x_i}{n}\\
  s^2 & = \frac{\sum_{i=1}^n (x_i - \widebar{x})^2}{n-1}
\end{align*}
où il faut remarquer qu'on utilise $n-1$ pour $s$ au lieu du $n$ utilisé pour $\sigma$.
C'est parce qu'on utilise $\widebar{x}$ au lieu de $\mu$ supposé inconnu.
Si on calcule le biais de $s^2$, on voit alors bien pourquoi on divise par $n-1$, en divisant par $n$ on aurait un estimateur biaisé!
Si on connaissait $\mu$, on aurait simplement
\[ s^2 = \frac{\sum_{i=1}^n (x_i - \mu)^2}{n} \]
comme estimateur non biaisé.

Pour $\sigma$ et $\rho$, si on veut une mesure indépendante de l'échelle,
il faut utiliser le coefficient de variation d'un échantillon $cv$ défini par
\[ cv = \frac{s}{\widebar{x}}. \]

On définit aussi
\begin{align*}
  \mathrm{Range} & = q_1 - q_0\\
  \mathrm{IQR} & = q_{0.75} - q_{0.25}
\end{align*}
et une observation est considérée comme un \englishit{outlier} si elle est
$\leq q_{0.25} - 1.5\cdot\mathrm{IQR}$ ou $\geq q_{0.75} + 1.5\cdot\mathrm{IQR}$.

\subsection{Estimateur}
Supposons qu'on ait un échantillon $X_1, \ldots, X_n$ dont on connaît
la distribution mais pas les paramètres.
On sait par exemple que c'est une exponentielle $\expo(\beta)$ mais on ne connaît pas $\beta$.

La variable estimée à partir de l'échantillon est appelée un estimateur et est notée $\hat{\beta}$.
On l'estimera en fonction des variables $X_1, \ldots, X_n$ avec une certaine fonction $h$,
$\hat{\beta} = h(X_1, \ldots, X_n)$.
Comme $X_1, \ldots, X_n$ sont des variables aléatoires,
$\hat{\beta}$ sera une transformation de variables aléatoires et donc aussi une variable aléatoire.
Seulement, sa distribution ne sera pas spécialement la même que les $X_i$.
Sa distribution est appelée \emph{sample distribution}.

On définit le biais d'un estimateur $\bias(\hat{\theta}) = \E(\hat{\theta}) - \theta$.
Si $\bias(\hat{\theta}) = 0$, on dira qu'il est non biaisé.
Le MSE (\englishit{Mean Squared Error}) de $\hat{\theta}$ est défini comme suit:
\begin{align*}
  \mse(\hat{\theta}) & = \E((\hat{\theta} - \theta)^2)\\
                    & = \bias^2(\hat{\theta}) + \var(\hat{\theta}).
\end{align*}

On dira que $\hat{\theta}$ est pertinent si $\forall \epsilon > 0$,
si $n \to \infty$,
\[ P(\abs{\hat{\theta} - \theta} > \epsilon) = 0. \]
On a que
\begin{itemize}
  \item Si $\mse(\hat{\theta}) \to 0$, alors $\hat\theta$ est pertinent.
  \item Si $g$ est continu et que $\hat\theta$ est pertinent pour $\theta$,
    $g(\hat{\theta})$ est pertinent pour $g(\theta)$.
\end{itemize}

Soit $\theta$, cette variable qu'on veut estimer,
si c'est $\mu$ ou $\sigma^2$, on va simplement l'approximer par $\widebar{x}$ ou $s^2$ mais
si c'est plus compliqué comme pour $\beta$, on a besoin d'un moyen plus sophistiqué.
Il y a deux techniques

\subsubsection{Méthode des moments}
Une première méthode est de trouver $\theta$ en imposant que les moments théoriques
de la distribution de $X$ donnée par $\theta$ doivent égaler les moments qu'on obtient
avec l'échantillon.
Ce qui donne l'équation suivante
\[ \frac{\sum_{i = 1}^n X_i^k}{n} = \E(X^k) \]
où le terme de droite dépend de $\theta$.
On fait ça jusqu'au moment $k$ si on a $k$ inconnues pour avoir $k$ équations.

\subsubsection{Méthode du Maximum de vraisemblance}
Bien qu'il y ait des facteurs inconnus,
on connaît notre distribution,
donc on a $p$ ou $f$ en fonction de $\theta$.
La probabilité d'avoir $X_i$ est donc $p(X_i)$.
Pour une distribution discrète, la probabilité est nulle mais prenons $f(X_i)$.
Comme les $X_i$ sont indépendants,
la probabilité d'obtenir l'échantillon qu'on a eu avec un certain $\theta$ est
\begin{align*}
  L & = \prod_{i=1}^n p(X_i) & L & = \prod_{i=1}^n f(X_i).
\end{align*}

Il n'est pas déraisonnable de penser ici que $\theta$ maximiserait $L$.
On a donc nécessairement
\[ \fpart{L}{\theta} = 0 \]
pour chaque $\theta$ inconnu ce qui fait une équation par inconnue.
Bien entendu, $\grad L = 0$ nous assure d'avoir un point critique mais ça peut tout
autant être un maximum de $L$ qu'un point selle ou un minimum.

Comme la dérivée d'un produit n'est pas très ragoutante,
utilisons le fait que la fonction logarithme est croissante et remarquons
donc que maximiser $L$ revient à maximiser $LL = \ln(L)$. C'est à dire
\begin{align*}
  LL & = \sum_{i=1}^n \ln(p(X_i)) & LL & = \sum_{i=1}^n \ln(f(X_i)).
\end{align*}

On va donc plutôt résoudre
\[ \fpart{LL}{\theta} = 0. \]

Cette méthode est \emph{pertinente} et \emph{équivariante}.
Équivariante signifie que
``Si $\hat{\theta}$ est l'estimateur par maximum de vraisemblance de $\theta$
et $g$ une fonction bijective, alors $g(\hat{\theta})$ est l'estimateur par maximum
de vraisemblance de $g(\theta)$.''

\subsubsection{Intervalles de confiance}
Pour estimer l'intervalle de confiance d'un estimateur,
on se rappelle que c'est une variable aléatoire et on
effectue une transformation de cette variable aléatoire
pour arriver à une distribution connue.
Il n'est pas facile de trouver quelle transformation faire.

Pour la moyenne $\widebar{X}$ (en majuscule car c'est une distribution)
et la variation $S^2$ d'un échantillon d'espérance $\mu$ et de variance $\sigma$, on a
\begin{align*}
  \E(\widebar{X}) & = \mu & \var(\widebar{X}) & = \frac{\sigma^2}{n}\\
  \E(S^2) & = \sigma^2 & \var(S^2) & = \frac{1}{n}\left(\E((X_i-\mu)^4)-\frac{n-3}{n-1}\sigma^4\right)
\end{align*}
et si $X$ suit une distribution normale,
\begin{align*}
  \frac{\widebar{X} - \mu}{\sqrt{\frac{\sigma^2}{n}}} & \sim \N(0, 1)\\
  \frac{\widebar{X} - \mu}{\sqrt{\frac{S^2}{n}}} & \sim t_{n-1}\\
  (n-1)\frac{S^2}{\sigma^2} & \sim \chi_{n-1}^2\\
  \widebar{X} & \perp S^2\\
\end{align*}

Seulement, le \englishit{Central Limit Theorem} stipule que pour n'importe quelle distribution
d'espérance $\mu$ et de variance $\sigma$,
\[
  Z_n = \frac{\widebar{X} - \mu}{\sqrt{\frac{\sigma^2}{n}}} \stackrel{d}{\to} Z \sim \N(0, 1).
\]
On a aussi que
\begin{align*}
  \bin(n,p) & \stackrel{n\to\infty}{\to} \N(np,np(1-p))\\
  \po(\lambda) & \stackrel{\lambda\to\infty}{\to} \N(\lambda,\lambda)\\
  \gammad(\alpha,\beta) & \stackrel{\alpha\to\infty}{\to} \N(\alpha\beta,\alpha\beta^2)\\
  \chi_n^2 & \stackrel{n\to\infty}{\to} \N(n, 2n).
\end{align*}

Au final, les transformations courantes sont les suivantes, prises de~\cite[pp.~397,430,435]{wackerly2008mathematical}.
Il suffit alors d'utiliser les tables pour la distribution en question pour arriver à l'intervalle de confiance sur
l'estimateur.
On a dans la première colonne la variable qu'on veut estimer et
dans la deuxième des conditions qui doivent être vérifiées.
On suppose que les expériences $A$ et $B$ soient indépendantes.
\begin{align}
  \label{eq:musigma}
  \mu & & Z & = \frac{\widebar{X} - \mu}{\sqrt{\frac{\sigma^2}{n}}} \sim \N(0, 1)\\
  \label{eq:mus}
  \mu & & T & = \frac{\widebar{X} - \mu}{\sqrt{\frac{S^2}{n}}} \sim t_{n-1} \stackrel{n \to \infty}{\to} \N(0,1)\\
  \label{eq:muabsigma}
  \mu_A - \mu_B & \quad \sigma^2 = \sigma_A^2 = \sigma_B^2 & Z & = \frac{(\widebar{X_A} - \widebar{X_B}) - (\mu_A - \mu_B)}{\sigma\sqrt{\frac{1}{n_A} + \frac{1}{n_B}}} \sim \N(0,1)\\
  \label{eq:muabs}
  \mu_A - \mu_B & \quad \sigma_A^2 = \sigma_B^2 & Z & = \frac{(\widebar{X_A} - \widebar{X_B}) - (\mu_A - \mu_B)}{S_p\sqrt{\frac{1}{n_A} + \frac{1}{n_B}}} \sim t_{n_A+n_B-2} \stackrel{n_A + n_B \to \infty}{\to} \N(0,1)\\
  \sigma^2 & & K & = \frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2\\
  \label{eq:sigmaab}
  \frac{\sigma_A^2}{\sigma_B^2} & \quad \sigma_A^2 = \sigma_B^2 & F & = \frac{S_A^2}{S_B^2} \sim F_{n_A-1,n_B-1}\\ % FIXME is sigma_A^2 = sigma_B^2 mandatory ??
  \label{eq:p}
  p & & Z & = \frac{\hat{p} - p}{\sqrt{\frac{p(1-p)}{n}}} \sim \N(0, 1)\\
  \label{eq:pab}
  p_A - p_B & \quad p_A = p_B & Z & = \frac{(\hat{p_A} - \hat{p_B}) - (p_A - p_B)}{\sqrt{\frac{p_A(1-p_A)}{n_A} + \frac{p_B(1-p_B)}{n_B}}}
  \sim \N(0, 1)
\end{align}
où
\[ S_p^2 = \frac{(n_A-1)S_A^2 + (n_B-1)S_B^2}{n_A+n_B-2}. \]
On utilise \eqref{eq:musigma} et \eqref{eq:muabsigma} lorsqu'on connait $\sigma^2$,
sinon on utilise \eqref{eq:mus} et \eqref{eq:muabs}.

Dans \eqref{eq:p} et \eqref{eq:pab}, on utilisera au dénominateur ce qu'on connait
entre $\hat{p}$, $p$, $p_A$, $p_B$, $\hat{p_A}$ et $\hat{p_B}$ pour nous simplifier la vie.
Dans le calcul d'intervalle de confiance, on connait $\hat{p}$ donc on met $\hat{p}$
au dénominateur.
Dans le test d'hypothèse, il suppose que $H_0$ est vrai donc on connait $p$,
on met alors $p$ au dénominateur.
Ici on a mis $p$ pour \eqref{eq:p} et $p_A,p_B$ pour \eqref{eq:pab} mais
on a le choix.

Dans \eqref{eq:pab}, le $p$ ou $\hat{p}$ au dénominateur est calculé sur tout
l'échantillon rassemblant $A$ et $B$.
Dans \eqref{eq:sigmaab}, il faut mettre le plus grand $\sigma$ au numérateur donc
si $H_1: \sigma_A < \sigma_B$, il faut prendre $\sigma_B^2/\sigma_A^2$ et $S_B^2/S_A^2$.

\begin{myexem}
  Intervalle de confiance $100(1-\alpha)\:\%$ autour de $\mu$ avec $\sigma$ inconnu.
  \begin{align*}
    P\left(-z_{\alpha/2} \leq \frac{\widebar{X}-\mu}{\sqrt{\frac{\sigma^2}{n}}} \leq z_{\alpha/2}\right) & = 1 - \alpha\\
    P\left(\widebar{X}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}} \leq \mu \leq \widebar{X}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right) & = 1 - \alpha.
  \end{align*}
\end{myexem}

\subsubsection{Test d'hypothèse}
Supposons qu'on ait une hypothèse $H_0$ et une hypothèse contradictoire $H_1$.
On aimerait sur base d'un échantillon décider si on rejette $H_0$ ($RH_0$) ou si on l'accepte ($AH_0$).
On va alors observer une statistique $t$ et le but du jeu est de définir
un ensemble $R$ et puis rejeter $H_0$ si $t \in R$ et de l'accepter si $t \notin R$.
\begin{center}
  \begin{tabular}{lll}
    Vérité & $H_0$ & $H_1$\\
    $AH_0$ & Correct & Erreur de type II ($\beta$)\\
    $RH_0$ & Erreur de type I ($\alpha$) & Correct
  \end{tabular}
\end{center}
La probabilité de se tromper est alors
$P(RH_0\mid H_0)P(H_0) + P(AH_0\mid H_1)P(H_1) = \alpha P(H_0) + \beta P(H_1)$.
On va alors essayer de trouver un $R$ qui minimise $\alpha$ et $\beta$.
Malheureusement, en diminuant $R$, $\beta$ augmente et en l'augmentant, $\alpha$ augmente,
il faut trouver un compromis.

Si on veut savoir s'il faut accepter ou rejeter $H_0$,
on fait alors comme suit :
\begin{itemize}
  \item On choisit $R$ pour que $\alpha = 0.05$ ou autre chose (pas trop bas pour que $\beta$ ne soit pas trop grand).
  \item On calcule $t$ puis si $t \in R$ alors $RH_0$, sinon $AH_0$.
\end{itemize}
On peut le plus souvent se ramener à $H_0: t = c$ pour une certaine constante $c$.
Si on a $H_0: t \leq c$ et $H_1: t > c$ (resp. $H_0: t \geq c$ et $H_1: t < c$),
on peut montrer que ça revient au même de travailler
avec $H_0: t = c$ et $H_1: t > c$ (resp. $H_1: t < c$).

Ensuite, il nous suffit de calculer un intervalle de confiance de $1-\alpha$ en utilisant le fait que $H_0$ est vrai,
c'est-à-dire que $t = c$, $R$ est le complémentaire de cet intervalle de confiance.
\begin{itemize}
  \item Si $H_1: t < c$ (resp. $t > c$), on fait un intervalle de confiance asymétrique en laissant $\alpha$ à gauche (resp. à droite).
  \item Si $H_1: t \neq c$, on fait un intervalle de confiance symétrique en laissant $\alpha/2$ de chaque côté.
\end{itemize}

\paragraph{$p$-values}
Une technique équivalente est la technique des $p$-values.
Prenons un test d'hypothèse où $H_0: t = c$ et $H_1: t < c$, on a donc un $R = \{\hat{t} \leq C\}$
où $C$ est une constante telle que $P(\hat{t} \in R \mid  H_0) = P(\hat{t} \leq C \mid  t = c) = \alpha$.
Avec les $p$-values, on ne va pas utiliser de $R$, on va plutôt dire qu'on rejette $H_0$ si
$$P(T \leq \hat{t}\mid H_0) < \alpha$$
ce qui revient au même. On appelle le membre de gauche une $p$-value.
La $p$-value correspond en fait à la probabilité d'obtenir les données
observées ou pire quand $H_0$ vrai.

\subsubsection{Régression linéaire}
Supposons qu'on veuille approximer un nuage de points par une droite en minimisant la somme des distances
\emph{verticales} entre les points et la droite.
On cherche donc l'équation $\beta_0 + \beta_1x$ approximant les points $(X_i, Y_i)$.
Il y aura une erreur $\varepsilon$ telle que $\beta_0 + \beta_1 X_i + \varepsilon_i = Y_i$
qui est une variable aléatoire de distribution $\N(0, \sigma^2)$.
On peut trouver un estimateur $S^2$ pour $\sigma^2$ et des estimateurs $\hat{\beta_1}$, $\hat{\beta_2}$ comme suit
\begin{align*}
  \hat{\beta_0} & = \widebar{Y} - \hat{\beta_1}\widebar{X}\\
  \hat{\beta_1} & = \frac{S_{XY}}{S_{XX}}\\
  S^2 & = \frac{S_{YY} - \hat{\beta_1}S_{XY}}{n-2}\\
      & = \frac{\sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2}{n-2}
\end{align*}
où
\begin{align*}
  S_{XX} & = \sum_{i=1}^n (X_i - \widebar{X})^2\\
  S_{XY} & = \sum_{i=1}^n (Y_i - \widebar{Y})(X_i - \widebar{X})\\
  S_{YY} & = \sum_{i=1}^n (Y_i - \widebar{Y})^2
\end{align*}
qui sont à $\sigma_{XY}$ ce que $S$ est à $\sigma$, la variation en dimension 2.

\biblio

\end{document}
