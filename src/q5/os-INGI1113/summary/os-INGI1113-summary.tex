\documentclass[fr,license=none]{../../../eplsummary}

\usepackage{../../../eplcode}

\hypertitle{Systèmes informatiques 2}{5}{INGI}{1113}
{Benoît Legat\and Yannick Tivisse\and Antoine Walsdorff\and Mélanie Sedda}
{Marc Lobelle}

%------------------------------------------------------------------------------
%				Chapitre 1
%------------------------------------------------------------------------------


\part{Background}
\section{Chapter 1 : Computer System Overview}

\subsection{Basic Elements}
Explication des 4  éléments structuraux principaux d'un ordinateur: Processor (CPU), main memory, I/O modules, System bus.
Explication des memory address register (MAR), memory buffer register (MBR), I/OAR et I/OBR.


\subsection{Evolution of the Microprocessor}
Petit brin d'histoire sur le passage de microprocesseur sur un seul chip à multiprocesseur: Chaque chip (appellée socket) contient plusieurs processeurs (appelés cores).
Explication des Graphical Processing Units (GPUs), qui calculent sur un tableau de données respectant le Single-Instruction Multiple Data (SIMD).
Explication des Digital SIgnal Procesors (DSPs), qui gère les flux de signaux (audio, video, modems, ..).
Le multiprocesseur classique est en train de laisser place à un System on a Chip (SoC),
où non seulement les CPUs et les caches sont sur le même chip,
mais aussi beaucoup d'autres composants du systeme (DSPs, GPUs, I/O devices (codecs, radios) et main memory).


\subsection{Instruction Execution}
Rien de nouveau, explication d'une Instruction cycle.
Consiste en 2  étapes: ``Fetch stage'' et ``Execute stage''.
Le Program Counter (PC) garde la prochaine instruction à lire en mémoire.
L'instruction fetchée est chargée dans le Instruction Register (IR).
Les instructions peuvent êtres de 4 types : Processor-Memory, Processor-I/O,
Data processing (arithmetic or logical operations) ou Control
(par ex: la prochaine instruction à exécuter n'est pas à l'adresse suivante mais autre part).
Explication d'un format d'instruction sur une machine hypothétique + explication de l'accumulateur (AC) et exemple de 3 cycles d'instruction.

\subsection{Interrupts}
Pendant une interruption, le processeur peut exécuter d'autres instructions pendant que le dispositif I/O est occupé.
Quand le dispositif externe est prêt à recevoir plus de données du processeur,
son module I/O envoie un Interrupt Request Signal au proccesseur.
Le processeur suspend l'exécution du programme courant (si il le peut),
lance une routine appellée Interrupt Handler,
et reviens à l'exécution du programme courant quand le dispositif I/O est servi.
NB: l'interrupt handler fait partie de l'OS,
et son bon fonctionement n'est pas de la responsabilité de l'utilisateur.
Si le tempsd'attente I/O est court,
l'utilisation d'interruption est efficace.
Si le temps d'attente est long, il ne va pas y avoir d'interruption mais c'est quand même efficace magré le temps d'attente,
puisque le processeur aura eu le temps d'exécueter d'autres instruction?


\subsubsection{Interrupt Processing}
Lorsque un appareil I/O termine une opération, il y a neuf étapes: 1-5 : HARDWARE et 6-9 : SOFTWARE
\begin{enumerate}
  \item Le dispositif I/O lance un Interrupt signal au processeur.
  \item Le processeur achève l'éxécution de l'instruction courante avant de répondre à l'interruption.
  \item Le processeur regarde si il y a un Interrupt Request en attente,
    et envoie un acknowledgment signal au dispositif qui a lancé l'interruption,
    cela l'autorise à retirer son Interrupt Signal.
  \item Le processeur push le Program Status Word (PSW) et PC sur la control stack.
  \item Le processeur charge le nouveau PC basé sur l'interruption.
  \item Sauvegarde du reste des Process State Information.
  \item Process Interrupt.
  \item Restore process state information.
  \item Restore old PSW et PC.
\end{enumerate}

\subsubsection{Multiple Interrupts}
Il faut considérer le cas où plusieurs interruptions peuvent arriver en même temps,
un programme peut lire les données dans une ligne de communication et imprimer les résultats en même temps.
Il existe deux approches:

\begin{itemize}
  \item Désactiver les interruptions quand une interruption est en cours.
    Problème: Ne tiens pas compte des priorités d'exécutions et des besoin en temps critiques.
    Ex: Il se peut que des données doivent êtres traitées rapidement pour laisser place à d'autres données.
    Imaginons que le prmeier paquet de données n'ai pas été absorbé quand le deuxième arrive,
    alors il se peut que l'on perde des données barce que le buffer est en overflow.
    Sequential Interrupt Processing.
  \item Définir les priorités d'interruption et autoriser une interruption de plus haute priorité
    à interrompre le Interruption Handler d'une interruption à plus faible priorité.
\end{itemize}

\subsection{The Memory Hierarchy}
On a une pyramide qui représente la hierarchie de la mémoire.
Plus on descend dans la pyramide:
\begin{itemize}
  \item Plus le cout par bit décroit
  \item Plus la capacité augmente
  \item Plus le temps d'accès augmente
  \item Plus la fréquence d'accès du processeur à la mémoire diminue
\end{itemize}
Un ordi contient en moyenne une douzaine de registre, puis deux rang plus bas,
il y a la main memory et entre les deux la cache qui gère les mouvements de données entre registres
et main memory pour améliorer les performances.
Ces 3 types de mémoires sont volatiles.
Il existe aussi les mémoires auxiliaires ou on va stocker les programmes et les fichiers,
et ou on va aussi exploiter la mémoire virtuelle.

\subsection{Cache Memory}
\subsubsection{Motivation}
Un gros problème d'antan était que la vitesse du processeur augmentait plus rapidement que la vitesse d'accès à la mémoire.
La solution est la cache: Une mémoire petite et rapide d'accès entre le processeur et la main memory.

\subsubsection{Cache Principles}
Si le processeur essaye de lire un mot en mémoire,
il check d'abord si il est pas dans la cache.
Si oui, il le charge (rapidement).
Si non, il le charge de la mémoire dans la cache.
Il peut y avoir plusieurs niveau de cache.
De plus en plus grands et de moins en moins rapides.
Voir figure~1.17 pour structure de la cache.

\subsubsection{Cache Design}
Plusieurs facteurs sont développés:
\begin{itemize}
	\item Cache Size
	\item Block Size
	\item Mapping function
	\item Replacement algorithm
	\item Writ policy
	\item Number of cache levels.
\end{itemize}

\subsection{Direct Memory Access}
Trois techniques sont possibles pour les opérations I/O:
\begin{itemize}
	\item Programmed I/O: Le dispositif I/O prend les mesures nécessaires pour se préparer à recevoir des données mais n'interrompt pas le processeur.
      C'est donc au processeur de vérifier régulièrement si le module I/O est prêt.
      Les performances sont largement dégradées
	\item Interrupter-driver I/O: Le module I/O interrompt le processeur quand il est prêt,
      avant le processeur fait autre chose.
      Ça demande quand même une intervention active du processeur et donc c'est pas top.
      \begin{enumerate}
        \item Le transfert I/O est limité par la vitesse à laquelle le processeur peut tester et servir le matériel et
        \item Le processeur est mainetnu par la gestion du transfert I/O.
      \end{enumerate}
	\item DIRECT MEMORY ACCESS: réalisé dans un module séparé sur le bus système ou incorporé dans le dispositif I/O.
      Le processeur délègue tout le travail au module DMA et poursuit son boulot.
      Quand le transfert est fini, le module DMA envoie une interruption au processeur.
\end{itemize}

\subsection{Multiprocessor and Multicore Organization}
\label{sec:smp}
Il y a trois types de parralélisme: SMP, Multicores computes, et clusters.
Cluster est vu plus loin dans le livre.

\subsubsection{Symmetric Multiprocessors}
\begin{enumerate}
	\item Il y a deux ou plus processeurs semblables de capacités comparables.
	\item Ces processeurs partagent la même mémoire et les I/O et sont interconnectés par un bus => Accès mémoire identique.
	\item Tous les deux ont accès aux I/O.
	\item Ils savent faire la même chose.
	\item L'OS fait interagir les deux procs.
\end{enumerate}
Voir \cite[p.~53-54]{stallings} pour la définition et les avantages.

\subsubsection{Mutlicore Computers}
Plusieurs processeurs (cores) sur un seul die.
Voir exemple \cite[p.~56]{stallings}.

\subsection{Appendix 1A}
Discussions des performances.

\newpage


%------------------------------------------------------------------------------
%				Chapitre 2
%------------------------------------------------------------------------------
\section{Chapter 2 : Operating System Overview}

\subsection{OS Objectives and Functions}
Définition d'un OS avec ses 3 objectifs: Convenience, Efficiency, Abillity to evolve.

\subsubsection{The OS as a User/Computer Interface}

Explique en quoi l'OS constitue une interface entre l'utilisateur et l'ordinateur dans les différentes régions:
\begin{itemize}
\item Program development
\item Program execution
\item Access to I/O devices
\item Controlled access to files
\item System access
\item Error detection and response
\item Accounting
\item Instruction Set Architecture (ISA)
\item Application Binary Interface (ABI)
\item Application Programming Interface (API).
\end{itemize}

\subsubsection{The OS as Ressource Manager}
Explique en quoi est ce que l'OS est responsable de gérer les ressources,
tout en faisant partie de celle-ci (Contrairement à un thermomètre qui controle un système de chauffage,
tout en ne faisant pas partie du système de chauffage).

\subsubsection{Ease of Evolution of an OS}
L'OS évolue à cause de plusieurs raisons:
\begin{itemize}
  \item Hardwares upgrades plus new types of hardware
  \item New services
  \item Fixes.
\end{itemize}

\subsection{The Evolution of OS}

\subsubsection{Serial Processing}
Dans le temps, les gens venaient avec leur progrmamme sur cartes perforées et venaient executer leur programme sur une grosse machine commune.
Cela présentait 2 gros problèmes:
\begin{itemize}
  \item Scheduling: Temps perdu pour rien car plage d'exécution fixée.
  \item Setup time: Pas de linking en début d'exécution,...
Prend bcp de temps rien que
    pour charger le programme sur l'ordi.
\end{itemize}

\subsubsection{Simple Batch Systems}
L'idée centrale est d'utiliser un moniteur.
Avec ce type d'OS, l'utilisateur n'a plus accès au processeur.
Les jobs sont placés en files, disponibles pour le monitor.
En gros, le monitor est un programme qui tourne et dont le but est de faire tourner les jobs en attente,
dans le but de minimiser le temps entre les exécutions (Scheduling function).
Voir la \cite[p.~74]{stallings} pour plus d'info sur la structure d'un job.
Pour que ça marche, il faut certaines caractéristiques hardware (sinon ben caca):
\begin{itemize}
  \item Memory Protection
  \item Timer
  \item Priviledged instructions
  \item Interrupts.
\end{itemize}

On voit arriver le concept de p-mode d'opération: User Mode et Kernel Mode


\subsubsection{Multiprogrammed Batch Systems}
Quand un job attent pour I/O, le processeur peut passer à un autre job qui préférentiellement n'attend pas d'I/O.
Cela s'appelle le multiprogramming ou multitasking.
Voir exemple \cite[p.~77]{stallings}.

\subsubsection{Time-Sharing Systems}
But: Minimiser le temps de réponse, avec des commandes entrées dans le terminal.
Voir ex \cite[p.~79]{stallings}.
NB le CTSS est primitif par rapport au time-sharing actuel, mais à l'époque ils n'avaient pas les même facilités d'interface que nous.

\subsection{Major Achievements}

\subsubsection{The Process}
Définition de Processus.
Trois lignes de développement ont poussé à la création du concept de processus:
\begin{enumerate}
  \item Multiprogramming batch operation
  \item Time-sharing : Le but est d'être capable de supporter plusieurs utilisateurs à la fois
  \item Real-time transaction System
\end{enumerate}

Un des premiers outils disponibles fut les interruptions.
Mais il s'avère que l'implémentation est incroyabelement sophistiquée dans ce contexte là.
Si il arrive une erreur d'exécution, on ne sais pas dire d'ou elle viens car il n'y a pas de structure (Il faut faire la distinction entre erreur matérielle, erreur logicielle, ou erreur d'exécution due à un ordre bien particulier non compatible d'exécution).
En général, ces erreurs viennent de 4 causes:
\begin{itemize}
  \item Improper synchronization
  \item Failed mutual exclusion
  \item Nondeterminate program operation
  \item Deadlocks
\end{itemize}

Pour éviter ces erreurs, il faut une manière systématique de monitorer et de controler les différents programmes s'exécutant sur le processeur, et la définition de processus fournit la base de cette solution.
Processus : 3 composants:
\begin{enumerate}
  \item Un programme exécutable
  \item Les données associées nécessaires au programme (variables, espace de traval, buffer,...)
  \item Le execution context (ou process state) du programme: SUPER IMPORTANT.
    Ce sont les données internes avec lesquelles l'OS est capable de controler le processus.
\end{enumerate}


\subsubsection{Memory Management}
L'OS a 5 principales responsabilités par rappot à la gestion de la mémoire:
\begin{enumerate}
  \item Process isolation
  \item Automatic allocation and management
  \item Support of modular programming
  \item Protection and access control
  \item Long-term storage.
\end{enumerate}
Ces 5 responsabilités sont assurées grace à la mémoire virtuelle et aux système de fichiers.
Explication du système de pages, et d'adresse virtuelle/réelle \cite[p.~87]{stallings} +
explication de l'adressage en mémoire virtuelle \cite[p.~88]{stallings}.

\subsubsection{Information Protection and Security}
Regroupe 4 catégories:
\begin{itemize}
  \item Availability
  \item Confidentiality
  \item Data Integrity
  \item Authenticity.
\end{itemize}

\subsubsection{Scheduling and Ressource Management}
Toutes les allocations de ressources et la scheduling policy doit considérer 3 facteurs:
\begin{enumerate}
  \item Fairness
  \item Differential responsiveness
  \item Efficiency.
\end{enumerate}
Explication de la technique de Round-Robin (queues circulaires) + différentes queues d'attentes (long, short, I/O).

\subsection{Developments Leading to Modern OS}
A cause de différents facteurs comme l'arrivée d'internet, un certain nombre de catégories ont été fort travaillées ces dernières années:
\begin{itemize}
  \item Microkernel Architecture
  \item Multithreading
  \item Symmetric multiprocessing
  \item Distributed OS
  \item Object-Oriented design.
\end{itemize}

\subsection{Virtual Machines}

\subsubsection{VM and Virtualizing}
Virtualization technology autorise un PC ou un serveur à pouvoir lancer simultanément plusieurs OS ou plusieurs sessions sur le même OS.
Un OS hôte peut supporter un certain nombre de Virtual Machines (VM)
Le Virtual Machine Monitor (VMM) gère toutes les communications des OS avec le processeur, le storage medium et le réseau.

\subsubsection{VM Architecture}
L'ABI définit la machine comme vue par le processus.
L'API définit la machine comme vue par l'application.
Pour l'OS, c'est l'ISA qui définit l'interface entre le système et la machine.

\paragraph{Process VM}
Un process VM est une plateforme virtuelle qui execute un seul programme.
Le process VM est créé quand le processus est créé et est terminée quand le processus est terminé.
(ex: JAVA, .NET FRAMEWORK)

\paragraph{System VM}
Lire \cite[p.~96]{stallings}

\subsection{OS Design Considerations for Multiprocessor and Multicore}

\subsubsection{Symmetric Multiprocessor OS Considerations}
En gros, le kernel peut s'exécuter sur le processeur qu'il veut,
et il peut même exécuter différentes parties indépendantes de lui-même sur plusieurs processeurs.
Le problème est que toutes ces ressources sont partagées et donc il faut pas que ça couille.
Donc un OS multiprocesseurs doit fournir toutes les fonctionnalités d'un multiprogramming
system avec des caractéristiques en plus pour permettre l'utilisation de plusieurs processeurs.
Ces caractéristiques sont:
\begin{itemize}
  \item Simultaneous concurrent processes or threads
  \item Scheduling
  \item Synchronization
  \item Memory management
  \item Reliability and fault tolerance.
\end{itemize}

\subsubsection{Multicore OS Considerations}
Les considérations sont les même que pour les systèmes SMP (voir section~\ref{sec:smp}).
Mais il vient se rajouter plusieurs choses.
Le problème est de bien exploiter les ressources potentielles offertes par le multicores.
La question est de savoir comment bien exploiter le parralélisme qui existe à 3 niveau dans les systèmes multicoeurs actuels:
\begin{enumerate}
  \item
    Hardware parallelism within each core processor.
    (Instruction Level Parallelism)
  \item Multiprogramming and multithreaded execution within each processor
  \item A single application executing in concurrent processes or threads accross multiple cores.
\end{enumerate}

\subsection{Microsoft Windows Overview}
Il faut retenir que windows a presque pas changé depuis NT :)

\subsection{Traditional Unix Systems}
Au début le kernel était pas modulaire, c'est pour cela qu'ils ont ajouté plein de code pour connaitre ce qu'on a aujourd'hui.

\subsection{Modern Unix Systems}
UNIX c'est les meilleurs.

\subsection{Linux}
La plupart des kernel UNIX sont monolithiques, et ben linux a fait pareil.
Il a l'avantage des monolithiques qu'il y a pas d'overhead à passer d'un serveur à l'autre mais
il n'a pas les inconvénients grâce à ses loadable modules.
Si un modules doit être modifié, on est pas obligé de recharger tout le kernel et de relinker tout le schmilblick.
Les modules peuvent avoir des dépendances entre eux.
Tout est géré par une structure chainée, les dépendances sont automatiquement loaded en cas de besoin et
unloaded si elles ne sont plus nécessaires pour gagner de la place en RAM.
Le kernel est composés des composantes suivantes, voir \cite[p.~117]{stallings} pour les relations entre eux.
\begin{itemize}
  \item System calls;
  \item Virtual memory;
  \item Signals;
  \item Traps \& faults;
  \item Physical memory;
  \item Processes \& scheduler;
  \item Char device drivers;
  \item File systems;
  \item Block device drivers;
  \item Interrupts;
  \item Network protocols;
  \item Network device drivers;
\end{itemize}
Un processus en userspace peut accéder directement au System calls et Virtual memory et est
accédé directement par Signals.

\subsection{Linux Vserver VM Architecture}
Si on veut $n$ machines virtuelle qui sont toutes Linux
et que le host est aussi Linux.
Linux Vserver permet de ne pas avoir $n+1$ kernels.
Il y en a qu'un seul et les machines virtuelles n'ont pas leur propre kernel.
Ils utililisent tous le même.
C'est par exemple ce que fait OpenVZ.

\newpage


%------------------------------------------------------------------------------
%				Chapitre 3
%------------------------------------------------------------------------------

\part{Processes}

\section{Chapter 3 : Process Description and Control}

\subsection{What is a Process?}
\subsubsection{Background}
D'abord il rappelle les concepts vus au 2 premiers chapitres par rapport aux applications, system software et ressources.
\subsubsection{Processes and Process Control Blocks}
Il définit un processus.
Deux éléments important: Le code du programme et un ensemble de données associées au programme.
TRES IMPORTANT: Le process control block, il contient à n'importe quel moment, un nombre de caractéristiques qui permettent de définir uniquement le processus:
\begin{enumerate}
  \item Identifier
  \item State
  \item Priority
  \item Program Counter
  \item Memory Pointers
  \item Context Data
  \item I/O Status Information
  \item Accounting information.
\end{enumerate}
Toutes ces données sont suffisante pour que le processeur puisse gérér tous les processus.

\subsection{Process State}
\begin{description}
  \item[Trace d'un processus] séquence d'instructions à executer dans le processus
  \item[Dispatcher] permet de passer d'un processus à l'autre pendant l'exécution.
    Change de processus toutes les 6 instructions par exemple (version simplifiée de la réalité)
\end{description}

\subsubsection{A Two State Process Model}
Un processus peut avoir 2 états: Running ou Not Running.
L'OS doit pouvoir garder trace de chaque processus, cela se fait avec le PCB.
Les processus qui sont Not Running sont placés dans une sorte de queue (Ou plutot leur PCB est dans une queue), attendant patiemment leur tour.
Ce modèle est trop simple.

\subsubsection{The Creation and Termination of Processes}
Process Creation
Un processus peut être créé par 4 types d'évènements:
\begin{itemize}
  \item New Batch Job
  \item Interactive log-on
  \item Created by OS to provide a service
  \item Spawned by existing process
\end{itemize}
Process Termination
Un processus peut s'arrêter pour plusieurs raisons:
\begin{itemize}
  \item Normal completion
  \item Time limit exceeded
  \item Memory unavailable
  \item Bounds violation
  \item Protection error
  \item Arithmetic error
  \item Time overrun
  \item I/O failre
  \item Invalid instruction
  \item Priviledged instruction
  \item Data misuse
  \item Operator or OS intervention
  \item Parent termination
  \item Parent request
\end{itemize}

\subsubsection{A Five-State Model}
L'idée, c'est que tous les processus dans la file d'attente ne sont pas forcément prêts, il peuvent par exemple attendre un dispositif I/O.
Le dispatcher devrait alors chercher dans le processus celui qui est prêt et qui attend depuis le plus longtemps.
Voila pourquoi on va séparer l'état Not running en 2 états: Ready et Blocked.
On ajoute deux états et on a notre Five-State Model:
\begin{enumerate}
  \item Running
  \item Ready
  \item Blocked/Waiting
  \item New
  \item Exit
\end{enumerate}
Les différents états ainsi que les différentes transitions entre états sont très bien expliquées dans le livre.
La structure des files d'attentes est expliquée \cite[p.~140]{stallings}.

\subsubsection{Suspended Processes}
The Need for Swapping
Si aucun processus dans la main memory n'est dans l'état ready, on va en déplacer un sur le disque(swap) et charger un processus ready du disque vers la main memory, ou lancer un nouveau processus.
Néanmoins, ceci est une opération I/O, mais comme les disk I/O sont les opérations I/O les plus rapide, ceci va améliorer les performances.
On ajoute alors un nouvel état : SUSPENDED.
Il faut faire attention: Si on a juste un état suspended, on ne fait pas la différence entre un processus qui était ready ou blocked avant d'être suspendu.
Donc on besoin de 4 états:
\begin{itemize}
  \item Ready
  \item Blocked
  \item Blocked/Suspended
  \item Ready/Suspended
\end{itemize}
Il faut aussi considérer que l'on a supposé qu'il n'y avait pas de virtual memory.
On pourrait penser que la virtual memory peut eliminer le besoin de swap explicite, mais en fait ça peut pas faire de tort, voir \cite[p.~142]{stallings}.
Toutes les nouvelles transitions sont aussi très bien expliquées :)
Other Uses of Suspension
\begin{itemize}
  \item Swapping
  \item Other OS reason
  \item Interactive user request
  \item Timing
  \item Parent process request
\end{itemize}

\subsection{Process Description}
On peut voir l'OS comme une entité qui gère l'utilisation des ressources du système par les processus.

\subsubsection{OS Control Structures}
L'OS doit garder des informations sur l'état actuel de toutes les entités qu'il gère.
Il construit et maintient des tables d'information:
\begin{enumerate}
  \item
    Memory Tables: Garde la trace de la main et virtual memory.
    (Certaines parties de la main memory sont réservées à l'OS).
    Les informations retenues sont:
    \begin{itemize}
      \item L'allocation de la main memory aux processus
      \item L'allocation de la virtual memory aux processus
      \item Des attributs de protection de certains block dans la main ou virtual memory, comme par exemple si une partie de la mémoire est partagée entre plusieurs processus
      \item Toute information nécessaire pour gérer la Virtual Memory
    \end{itemize}
  \item I/O tables: Si une opération I/O est en cours, l'OS doit savoir le statut de l'opération I/O et la partie dans la main memory utilisée comme source ou destination du transfert I/O
  \item File tables: Garde des infos sur l'existence de fichiers, leur endroit dans la virtual memory; leur statut actuel et d'autres attributs.
  \item Process tables: Sera expliqué dans la prochaine section.
    D'abord deux considérations:
    \begin{enumerate}
      \item Les tables doivent être liées entre-elles.
      \item L'OS doit avant toutes choses connaitre son environnement basique (ex: qu'y a-t-il comme dispositif I/O), cela se fait par assistance humuaine ou autoconfiguration d'un programme.
    \end{enumerate}
\end{enumerate}

\subsubsection{Process Control Structures}
L'OS doit savoir 2 choses pour gérer les processus:
Process Location
Un processus inclus un programme ou un ensemble de programme à executer, associés à un ensemble de données pour les variables locales et globales, ajoutons à ça une stack, et un ensemble d'attributs pour que l'OS puisse gérer le processus.
Cette collection d'information est appellée Process Image.
N'oublions pas qu'a n'importe quel moment, la portion de la process image peut être dans la main memmory, le reste étant dans la secondary memory.
Donc les process tables doivent garder les positions de chaque pages de chaque process images.
À cet effet, il y a une primary process table avec une entrée pour chaque processus qui contienne autant de pointeurs qu'il y a de block à la process image.

\paragraph{Process Attributes}
Toutes les informations requises rentrent dans 3 catégories.
Voir table \cite[p.~150]{stallings} pour plus de détails.
\begin{itemize}
  \item Process identification: Définit d'une manière unique le processus.
  \item Processor state information: Comprend tous les contenus des registres du processeur.
    (Ex: EFLAGS register in Intel x86)
  \item Process control information: Tout ce qu'il faut à l'OS pour controler les processus.
\end{itemize}
Voir figures 3.13 et 3.14 \cite[p.~153-154]{stallings} pour les strucures des process images et des files d'attentes de processus.
The Role of Process Cotrol Block
C'est la structure la plus important dans un OS.
Mais il y a néanmoins un soucis de protection.

\subsection{Process Control}
\subsubsection{Modes of Execution}
Un bug dans une simple routine, comme un interrupt handler, peut endommager un process control block.
Un changement dans la structure du process control block va affecter nombre de modules de l'OS.
Process Control Modes of Execution
Tous les OS ont au moins 2 modes d'exécution: Usermode et Kernelmode.
La raison est simple: protéger l'OS et les tables clés de l'OS, comme le process control block.
Le processeur sait dans quel mode on est grace à un bit dans le PSW.
Voir la table~3.7 pour les fonctions typiques d'un OS \cite[p.~135]{stallings}.

\subsubsection{Process Creation}
On a déjà parlé des raisons qui pouvaient pousser à la création d'un processus.
Cela se fait dans cet ordre.
\begin{enumerate}
  \item Assign a unique process identifier to the new process
  \item Allocate space for the process
  \item Initialize the process control block
  \item Set the appropriate linkages
  \item Create or expand other data structures.
\end{enumerate}

\subsubsection{Process Switching}
Ca l'air simple, mais ça amène quelques difficultés d'implémentations.
When to Switch Processes
Ca peut arriver chaque fois que l'OS obtient le controle des processus.
Cela peut provenir de 3 façons:
\begin{itemize}
  \item Interrupt : Evènement externe et indépendant du processus courant.
Le controle est alors donné à une routine (interrupt handler).
Cela peut provenir de plusieurs sources (Clock interrupt, I/O interrupt, Memory fault,...)
  \item Trap : Du a une erreur ou une exception du processus courant.
Soit l'erreur est fatale, le processus courant passe à l'état exit => Process switch.
Soit elle ne l'est pas, et en fonction de l'erreur =>process switch ou resume process.
  \item Supervisor call: ex: un user process demande une I/O operation (file open).
L'appel est transmis à une routine de l'OS.
\end{itemize}
\paragraph{Mode Switching}
Lors d'une instruction cycle, si une interruption est en attente, le processeur fait 2 choses:
\begin{enumerate}
  \item Place le PC à l'adresse de départ de l'interrupt handler.
  \item Switch de usermode à kernelmode pour que l'interruption puisse bénéficier des instructions privilégiées.
\end{enumerate}
\paragraph{Change of Pocess State}
Les différentes étapes d'un process switch sont les suivantes:
\begin{enumerate}
  \item Sauver le contexte du processeur (PC, et autre registres)
  \item Update le Process control block du processus qui est dans le running state.
  \item Déplacer le process control block dans une file d'attente appropriée
  \item Sélectioner un nouveau processus à exécuter.
  \item Update du process control block du nouveau processus à exécuter.
  \item Update des structures de gestion des données en mémoire (memory management data structures)
  \item Restaurer le contexte du processeur à celui qui existait au dernier moment ou le processus fraichement rappelé s'était exécuté.
\end{enumerate}

\subsection{Execution of the OS}
Nous avons vu 2 faits intérressants à propos de l'OS
\begin{itemize}
  \item L'OS fonctionne comme un simple programme étant donné que c'est un ensemble de programmes exécutés par le processeur
  \item L'OS donne fréquemment le contrôle au processeur et dépend du processeur pour retrouver le controle.
\end{itemize}
On peut alors se demander si l'OS est un processus, et si oui, comment est-il controllé.
On va voir 3 façons de faire das les OS contemporains.

\subsubsection{Nonprocess Kernel}
Approche traditionnelle commune à de nombreux OS plus anciens.
Le kernel s'exécute en dehors de tout processus.
L'OS a sa propre région de mémoire et sa propre stack.
Le concept de processus ne s'applique alors qu'au programmes utilisateurs.
L'OS s'exécute comme une entité séparée dans un mode privilégié.

\subsubsection{Execution withing User Processes}
\label{sec:uproc}
Alternative très commune avec les OS sur de petits ordinateurs (PC,..), on execute virtuellement toutes les fonctionnalités de l'OS dans le contexte d'un processus utilisateur.
Chaque process image contient également les programmes, données et stack pour les kernel programs.
C'est-à-dire qu'une kernel stack séparée est utilisée pour gérer les appels-retours de fonctions quand le processus est en kernelmode.
Toutes les données et codes de l'OS sont en partage avec tous les user processes.

\subsubsection{Process-Based Operating System}
Dernière alternative, implémenter l'OS comme une collection de processus système.
Avantages:
\begin{itemize}
  \item Impose une discipline de program design qui encourage l'utilisation d'un OS modulaire avec un minimum d'interfaces propres entre modules.
  \item Des parties non critiques de l'OS sont implémentées dans les processus séparés.
  \item C'est très utilse pour un environnement multiprocesseur ou multicomputer, car on eut dédier certains processeurs à l'exécution de différents morceaux de l'OS, augmenttation des performances.
\end{itemize}

\subsection{Security Issues}
Le clé de sécurité dans le design de tout OS qui se repecte est de détecter, voire prévenir, les essais d'un utilisateur ou d'un malware d'accéder au privilèges nonautorisés du système, plus particulièrement aux droits de superutilisateur (root access).

\subsubsection{System Access Threats}
Il y a deux types de System Access Threats:
\begin{itemize}
  \item Intruders (hackers, crackers)
    \begin{itemize}
      \item Masquerader : outsider
      \item Misfeasor : insider
      \item Clandestine user : (in/out)sider
    \end{itemize}
  \item Malicious Softwares (Malwares)
    \begin{itemize}
      \item Il y a ceux qui ont besoin d'un programme hôte = parasitic (bomb, virus,backdoors) et ceux qui sont indépendant (worms, bot)
      \item Il y a ceux qui ne se réplique pas (logic bomb, backdoors et bot) et ceux qui se dispersent (virus, worms)
    \end{itemize}
\end{itemize}

\subsubsection{Countermeasures}
Il y a l'intrusion detection.
Voir \cite[p.~165]{stallings} les différent type de détections.
Il y a l'authentification.
Première ligne de défense, voir livre Il y a le controle d'acces.
voir livre Il y a le pare-feu.
spécialisé dans le reseau, surveille les fichiers suspects.

\subsection{UNIX SVR4 Process Management}
UNIX SVR4 utilise ce qui est présenté à la section~\ref{sec:uproc}:
``Execution within User Processes''.
Il y a deux mode, usermode et kernelmode

\subsubsection{Process States}
Un total de 9 états:
\begin{itemize}
  \item user running
  \item kernel running
  \item ready to run, in memory
  \item asleep in memory
  \item ready to run, swapped
  \item sleeping, swapped
  \item preempted
  \item created
  \item zombie
\end{itemize}

\subsubsection{Process Description}
Voir \cite[pp.~169-171]{stallings}.
explication d'une process image, de la process table entry et U area.

\subsubsection{Process Control}
Explication des étapes de création d'un processus avec fork(), et des différentes décisions d'exécution qui peuvent êtres prises à la fin de la création.

\newpage

%------------------------------------------------------------------------------
%				Chapitre 4
%------------------------------------------------------------------------------

\section{Chapter 4 : Threads}

\subsection{Processes and Threads }
\subsubsection{Multithreading}
Multithreading: abilité d'un OS à supporter de multiples chemins d'exécution concurrents à l'intérieur d'un processus.
Dans un environnement multithreadé, un processus est une unité d'allocation de ressources et une unité de protection.
Cela comprend:
\begin{itemize}
  \item Une espace d'adresse virtuelle qui contient la process image
  \item Un accès protégé au processeurs, autres processus, fichiers et dispositifs I/O.
\end{itemize}
Dans un processus, il peut y avoir plusieurs thread, chacun possédant:
\begin{itemize}
  \item Un état d'exécution (Running,...)
  \item Un thread context sauvé si not running (Un thread est un Program Counter
  \item indépendant dans un processus.)
  \item Une stack
  \item Un espace de stockage de variable local par thread
  \item Un accès à la mémoire et aux ressources du processus.
\end{itemize}
Dans le processus, il y a toujours un seul process control block et un user adress space.
Le gros bénéfice des thread provient des performances:
\begin{itemize}
  \item Faut moins de temps pour créer un thread qu'un nouveau processus (10 fois moins)
  \item Faut moins de temps pour terminer un thread qu'un processus
  \item Faut moins de temps pour switcher entre deux thread dans le même processus que de switcher entre 2 processus.
  \item 2 thread peuvent communiquer entre eux sans passer par le kernel, contrairement aux processus.
\end{itemize}
Les thread sont super intéressants sur un pc multiprocesseur.
Mais également sur un mono processeur.
En voici quelques exemples sur un multiprocesseur (\cite[p.~181]{stallings})
\begin{itemize}
  \item Foreground and background work
  \item Asynchronous processing
  \item Speed of execution
  \item Modular program structure
\end{itemize}

\subsubsection{Thread Functionality}
\paragraph{Thread States}
Les états clés sont running, ready et blocked.
Suspended n'a pas beaucoup de sens car il est propre aux processus.
Il y a 4 opérations qui peuvent changer l'état d'un thread: Spawn, Block, Unblock et Finish.

\paragraph{Thread Synchronization}
Comme les thread partagent les mêmes ressources du processus, il est nécessaire de synchroniser les activités des différents threads pour qu'ils n'interfèrent pas en eux ou ne corrompent les structures de données.
C'est en général le même problème que la synchronisation des processus, voir chapitre 5 et 6.

\subsection{Types of Threads}
\subsubsection{User-Level and Kernel-Level Threads}
\paragraph{User-Level Threads}
Tout le travail de gestion des threads est fait par l'application et le kernel n'est pas au courant de l'existence de threads.
Cela se fait en utilisant des librairies de threads (genre POSIX), qui sont des packages pour la gestion ULT.
Ces librairies contiennent les codes, pour créer, détruire, scheduler,...
les threads.
Le kernel continue de scheduler le processus comme une unité et lui assigne un seul état d'exécution.
Voir ex \cite[p.~186]{stallings} Avantage de ULT
\begin{enumerate}
  \item Le thread switching ne demande pas de privilèges en mode kernel
  \item Le scheduler peut être spécifique à l'application (round robin, priority-based,...)
  \item Les ULTs peuvent tourner sur tous les OS.
\end{enumerate}
Désavantages de ULT
\begin{enumerate}
  \item Les appels systèmes sont souvent bloquants, si un ULT fait un appel système bloquant, tous les autres threads sont également bloqués
  \item Dans une stratégie purement ULT, les application multithreadées ne savent pas profiter des avantages du multiprocessing.
En effet, un seul processus peut s'exécuter à la fois, donc un seul thread aussi.
\end{enumerate}
\paragraph{Kernel-Level Threads}
Tout le travail de gestion des threads est fait par le kernel.
Il y a un API dans les installations thread du kernel.
(Ex:Windows)
Les KTL n'ont pas les deux désavantages des UTL, de plus les routines de gestion des threads du kernel peuvent être multithreadées.
Le principal désavantage est que le transfert de controle d'un thread à l'autre dans le même processus demande un mode switch du kernel, ce qui prend plus de temps.
\paragraph{Combined Approach}
Mélange les deux pour profiter des avantages des 2 si c'est bien réalisé.
Solaris fait ça, en forçant la relation UTL/KTL à être one-to-one.

\subsubsection{Other Arrangements}
On a vu le One thread: One process (Traditional Unix implementation)
Le Many threads: One process (Window NT, Solaris, Linux,...)
Le One thread:Many Process (Le thread saute de processus en processus, et donc peut migrer de système) Voir \cite[p.~189]{stallings} Le Many to Many a été exploré dans un OS expérimental, TRIX.
voir \cite[p.~188]{stallings}

\subsection{Multicores and Multithreading}
\subsubsection{Performance of Software on Multicore}
En gros, on nous explique fonctionner avec plusieurs processeurs augmente l'éfficacité d'exécution si on sait bien paralléliser sont code, maintenant à partir d'un certain niveau, les performances se dégradent.
Mais pour certaines applications il est possible d'exploiter le multiprocessing de manière optimale (voir exemples \cite[p.~193]{stallings})

\subsubsection{Application Example: Valve Game Software}
Gordon Freeman serait fier de lire cette section.
Cette section explique en quoi le moteur physique de Valve utilise le multithreading à bon escient.

\subsection{Windows 7 Threads ans SMP Management}
Explication de la structure d'un thread ou d'un processus.
L'un comme l'autre, ce sont des objets.

\subsubsection{Process and Thread Objects}
Explication du process object et thread object et leurs attributs.

\subsubsection{Multithreading}
Windows supporte la concurrence des processus car des threads dans différens processus peuvent s'exécuter concurrentiellement.
Un processus orienté-objet multithreadé est une idée efficace pour implémenter une application de serveurs, car elle peut gérer un certain nombre de clients concurrentiellement.

\subsubsection{Thread States}
Il y a 6 états:
\begin{itemize}
  \item Ready
  \item Standby
  \item Running
  \item Waiting
  \item Transition
  \item Terminated
\end{itemize}

\subsubsection{Support for OS Subsystems}
Explique en gros comment un sous-système doit faire pour créer un nouveau processus ou thread...
Pas vraiment développé.
\subsubsection{Symmetric Multiprocessing Support}
Windows supporte le SMP.
En effet, un thread peut s'exécuter sur n'importe quel processeur (généralement le premier disponible).
Le dispatcher peut aussi essayer de réassigner un processus avec le même processeur que sa dernière exécution pour pouvoir réutiliser la mémoire cache de sa dernière exécution.
Il est ausi possible q'une application force l'exécution d'un thread sur certains processeurs.
\subsection{Solaris Thread and SMP Management }
\subsubsection{Multithreaded Architecture}
Explication des 4 types de thread-related concepts: Process, User-level Threads, Lightweight processes, Kernel Threads.
Voir \cite[p.~202]{stallings}.
\subsubsection{Motivation}
Le but d'une implémentation des thread à 3 niveau (ULT, KLT, LWP) est de faciliter la gestion des threas par l'OS.
\subsubsection{Process Structure}
Explication de la structure d'un processus dans Solaris par rapport à celle du Unix traditionnel.
\subsubsection{Thread execution}
Voir le diagramme d'état.
il y a:
\begin{itemize}
  \item RUN
  \item ONPROC
  \item SLEEP
  \item STOP
  \item ZOMBIE
  \item FREE
\end{itemize}
\subsubsection{Interrupts as Threads}
Solaris a implémenté les interruptions comme des kernel Threads.
Voir \cite[p.~206]{stallings}

\subsection{Linux Tasks}
Un processus est une structure de donnée .
Voir \cite[p.~207]{stallings}

\subsubsection{Linux Threads}
Linux généralise les processus.
On peut soit créé un processus avec \lstinline|fork| (POSIX),
soit créé un processus ``généralisé'' avec \lstinline|clone| où on peut par exemple dire
qu'on partage la mémoire (le heap) avec le parent où qu'on partage sont process id.
Il gère donc ces processus ``généralisés'' sans faire de distinction.
\lstinline|pthread| est donc juste une librairie en user space (comme \lstinline|printf|)
et pas un appel système (comme \lstinline|read|).
\lstinline|pthread_create| par exemple appelle l'appel système \lstinline|clone|
en demandant de partager la mémoire, le process id et tout ce qu'il faut
pour avoir en fait un thread.

\subsubsection{Mac OS X Grand Central Dispatch (GCD)}
La flemme ...

\newpage

%------------------------------------------------------------------------------
%				Chapitre 5
%------------------------------------------------------------------------------


\section{Chapter 5 : Mutual Exclusion and Synchronization}
Explique les situations où on peut trouver de la concurrence et définit les mots clés (deadlock, critical section,...)

\subsection{Principles of Concurrency}
Interleaving et overlapping apporte plusieurs problèmes: Le partages des ressources globales est mis en péril;
l'allocation des ressources ne peut plus être facilement gérée par l'OS, on peut avoir des deadlock;
les erreurs sont difficiles à trouver car elles sont non-déterministes et non-reproductibles.

\subsubsection{A Simple Example}
Un exemple simple avec la fonction echo montre plusieurs scénarios d'erreurs.
Conclusion, il faut controller l'accès aux ressources partagées.

\subsubsection{Race Condition}
Cela arrive quand plusieurs processus ou thread lisent et écrivent des données et
que le résultat final dépend de l'ordre d'exécution des instructions.
Exemple à la clé.

\subsubsection{OS Concerns}
Explique les problèmes de gestion et de design qu'apporte la concurrence du point de vue de l'OS.
Voir \cite[p.~224]{stallings}.
Le dernier point est important dans ce chapitre,
il dit que le fonctionnement d'un processus doit être indépendant de la vitesse à laquelle son exécution est menée
par rapport à la vitesse des autres processus concurrents.

\subsubsection{Process Interaction}
L'interaction d'un processus avec les autres dépend de son degré de conscience des autres.
Il y en a 3:
\begin{itemize}
  \item Les processus sont inconscients des autres:
    Les processus ne sont pas censés travailler ensemble.
    Il y a donc une relation de compétition pour les ressources.
    Cela apporte le problème de mutual exclusion, de deadlock et de starvation.
  \item Les processus sont indirectement conscient des autres:
    Ne connaissent pas les autres processus,
    mais partage le même objet (I/O buffer,...).
    C'est donc une relation de coopération par partage.
    Même problème qu'au dessus + problème de data coherence
  \item Les processus sont directement conscients des autres:
    Il peuvent communiquer entre-eux.
    C'est une relation de coopération par communication.
\end{itemize}

Starvation et deadlock sont présents,
ces différents degrés de conscience ont leurs conséquences et leurs problèmes.
Voir table~5.2 \cite[p.~225]{stallings}

\subsubsection{Requirements for Mutual Exclusion}
Toute ce qui veut apporter un support à l'exclusion mutuelle doit respecter les critères suivants:
\begin{enumerate}
  \item L'exclusion mutuelle doit être renforcée.
    Seulement un processus à la fois est autorisé dans la section critique,
    parmis les processus qui ont une section critique pour la même ressource partagée.
  \item Un processus qui s'arrête dans sa section non critique doit le faire sans interférer avec les autres processus.
  \item Il ne peut pas être possible pour un processus demandant de rentrer dans une section critique d'être mis en attente indéfiniment:
    Pas de starvation, ni de deadlock.
  \item Quand aucun processus n'est dans la section critique,
    n'importe quel processus demandant pour rentrer dans cette section critique doit avoir la permission sans attente.
  \item Aucune considération n'est prise par rapport aux vitesses relatives des processus et au nombre de processeurs.
  \item Un processus reste dans une section critique pendant un temps fini.
\end{enumerate}

\subsection{Mutual Exclusion: Hardware Support}
\subsubsection{Interrupt Disabling}
Dans un systeme uniprocesseur, il n'y a pas d'overlapped execution.
Donc il est suffisant de désactiver les interruption d'un processus pendant qu'il est dans sa section critique.
Mais les performances sont nettement dégradées et ça ne marche que pour les machines uniprocesseurs.

\subsubsection{Special Machine Instructions}
Les concepteurs de processeurs ont proposés des instructions machines, sorte de formule magique qui exécute plusieurs actions AUTOMIQUEMENT, et bloque les ressources pendant le temps d'exécution de l'instruction machine.
\begin{itemize}
  \item Compare et Swap Instruction:
    On fait de l'exclusion mutuelle en mettant une variable bolt à 0,
    et le seul processus qui peut accéder à la section critique est celui qui trouve bolt égal à 0.
  \item Exchange Instruction: Une variable bolt est initialisée à 0, chaque processus à une variable initialisée à 1.
    Le seul processus qui peut entrer dans la section critique est celui qui trouve bolt égal à 0 et le met à 1 pour bloquer les autres.
\end{itemize}
Avantages:
\begin{itemize}
  \item Applicable à n'importe que nombre de processus et n'importe que nombre de processeurs
  \item Simple et facile à vérifier
  \item Peut définir plusieurs sections critiques, à chaque section sa variable
\end{itemize}
Désavantages:
\begin{itemize}
  \item Busy waiting: Dépense du temps de processeur pour rien
  \item Starvation possible
  \item DeadLock possible
\end{itemize}

\subsection{Semaphores}
Regardons plutot du coté des mécanismes de l'OS et du langage de programmation pour la concurrence.
Les différents mécanismes fréquemment utilisés sont les suivants:
\begin{itemize}
  \item Semaphore
  \item Binary Semaphore
  \item Mutex
  \item Condition Variable
  \item Monitor
  \item Event Flags
  \item Mailboxes/Messages
  \item SpinLocks
\end{itemize}
Définition et explication du fonctionnement d'un sémaphore \cite[p.~235]{stallings}.

Définition et explication du fonctionnement d'un sémaphore binaire \cite[p.~236]{stallings}.
Définition et explication du fonctionnement d'un mutex \cite[p.~236]{stallings}.
Un mutex est comme un sémaphore sauf que le processus qui a bloqué le mutex DOIT être celui qui va le débloquer.
Si la queue d'attente est une FIFO $\to$ Strong Semaphore, sinon weak semaphore.

\subsubsection{Mutual Exclusion}
Un solution utilisant les sémaphores est donnée \cite[p.~239]{stallings}, avec un schéma d'exécution.

\subsubsection{The Producer/Consummer Problem}
Fonctionne par essais et trouve une solution avec un buffer infini et des sémaphores \cite[p.~241]{stallings}.
Une solution avec un buffer de taille finie est donnée \cite[p.~243]{stallings}.

\subsubsection{Implementation of Semaphores}
Les opérations SemWait et SemSignal doivent être implémentées de manière atomique primitive.
On peut le faire avec l'algorithme de Peterson ou avec une instruction machine.
Voir \cite[p.~245]{stallings} ou \cite[Appendix~A]{stallings}.

\subsection{Monitors}
Le moniteur est une construction en langage de programmation qui fournit les même fonctionalités que les sémaphoses, le tout en étant plus facile à controler.
Il autoriste le programmeur à mettre un lock sur n'importe quel objet.

\subsubsection{Monitors with Signals}
Définition d'un moniteur, plus les condition variable, et ensuite explication de la structure d'un moniteur \cite[p.~248]{stallings}
Solution du producer/consumer avec buffer de taille fixée utilisant des moniteurs \cite[p.~249]{stallings}.
L'avantage avec les sémaphores est que toutes les fonctions de synchronisation sont confinées dans le moniteur.
Moniteur bien programmé $\to$ l'accès aux ressources protégées est correct pour tous les processus
Tous les processus qui ont accès aux ressources protégées sont bien programmés avec des sémaphores
$\to$ L'accès aux ressources protégées est correct.

\subsubsection{Alternate Model of Monitors with Notify and Broadcast}
La définition de moniteur comme vu au point précédent à deux désavantages:
\begin{itemize}
  \item Si le processus qui envoie le csignal n'a pas fini avec le moniteur, alors deux process switch supplémentaires sont requis:
    un pour bloquer ce processus et un pour le résumer quand le moniteur redevient accessible.
  \item Le process scheduling associé avec un signal doit être complement fiable.
\end{itemize}

Voilà pourquoi on voit un autre type de moniteur fait à la base en langage Mesa.
\lstinline|csignal| est remplacé par \lstinline|cnotify|, qui a un fonctionnement différent,
réglant le problème de process switch additionnels et réglant également le problème éventuel de starvation.

\subsection{Message Passing}
L'échange d'informations et l'exclusion mutuelle peuvent être fournis avec le message passing.

\subsubsection{Synchronization}
\begin{itemize}
  \item Send
    \begin{itemize}
      \item blocking: Le processus bloque tant que le message n'a pas été recu
      \item nonblocking : Le processus continue son exécution, peut importe la réception du signal
    \end{itemize}
  \item Receive
    \begin{itemize}
      \item blocking Si aucun message n'est arrivé, il bloque
      \item nonblocking : Si aucun message n'est arrivé, il s'en fout – test for arrival
    \end{itemize}
\end{itemize}
Blocking send, blocking receive $\to$ rendez-vous (synchronisation très étroite) Nonblocking send,
blocking receive $\to$ Combinaison la plus utile (ex:server) Nonblocking send, nonblocking receive.

Le danger avec les nonblocking send, c'est qu'une erreur peut pousser un processus à envoyer sans arrêt des messages, et consumer les ressources du système inutilement.

Les blocking receive peuvent bloquer infiniment si le processus qui devait envoyer se plante avant de le faire.
À côté de ca, des messages peuvent être définitivement perdu si un utilise des nonblocking receive et que le processus qui recoit fait sa demande avant le processus qui envoie.

\subsubsection{Addressing}
\begin{itemize}
  \item Direct
    \begin{itemize}
      \item send: Le processus de destination est spécifiquement identifié
      \item receive :

        explicit : Le processus qui envoie est spécifiquement désigné.
        Le processus sait à l'avance de qui va venir le message
        implicit : ex imprimante, on accepte un message de n'importe qui mais le paramètre ``source''
        possède une valeur retournée après que le message aie été transmis.
      \item Indirect : Les messages sont envoyé à une structure de donnée partagées qui sont des queues qui gardes les messages (mailboxes).
        Il peut y avoir différents types de relation entre sender et receiver:
        one-to-one (lien de communication privé),
        many-to-one (client/server application, on appelle ça un port),
        one-to-many (Un message est traité par plein de processus),
        many-to-many (plusieurs processus server fournissent un service concurrent à plusieurs clients)
        \begin{itemize}
          \item static: Les ports sont souvent statiques
          \item dynamic: Si il ya beaucoup de clients et de va et viens, on peut faire ça
            dynamique avec les primitifs connect et disconnect.
          \item ownership: Si le processus qui recoit dans un port est détruit, le port est détruit.
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Message Format}
\begin{itemize}
  \item Content
  \item Length
    \begin{itemize}
      \item fixed: La taille des messages est petite et fixée, si un grand nombre de donnée doit être transféré,
        le tout est mis dans un fichier et une référence vers le fichier est envoyée
      \item variable: Programmation un peu plus flexible.
        Contient un header, et un body
    \end{itemize}
\end{itemize}

\subsubsection{Queuing Discipline}
Le plus facile est de faire une FIFO mais cela n'est pas suffisant pour des messages urgents,
on peut donc entrevoir la priorité des messages,
ou autoriser le receiver d'inspecter la queue et de choisir le prochain message à traiter.

\subsubsection{Mutual Exclusion}
Exemple avec blocking receive et non blocking send.
\cite[p.~257]{stallings}
Solution au producer/consumer problem avec du message passing \cite[p.~258]{stallings}

\subsection{Readers/Writers Problem}
Il y a une zone de données partagée en plusieurs processus.
A tout moment, il faut que:
\begin{enumerate}
  \item N'importe quel nombre de readers peuvent lire le fichier simultanément
  \item Seulement un writer peut écrire dans le fichier au même moment
  \item Si un writer écrit dans un fichier, aucun lecteur ne peut le lire.
\end{enumerate}
On va apercevoir deux solutions.

\subsubsection{Readers have Priority}
Solution avec sémaphores \cite[p.~260]{stallings}.

\subsubsection{Writers have Priority}
Solution avec sémaphore \cite[p.~261]{stallings}.
Cette solution est plus compliquée que lorsque les reader ont priorité, je me conseille de la relire attentivement.
Une autre solution utilisant le messag passing est donnée \cite[p.~263]{stallings}.

\newpage

%------------------------------------------------------------------------------
%				Chapitre 6
%------------------------------------------------------------------------------

\section{Chapter 6 : Deadlock and Starvation}

\subsection{Principles of DeadLock}
Deadlock : Blocage permanent d'un ensemble de processus qui combattent pour des ressources système ou pour communiquer entre eux.
Exemple avec le carrefour.
Explication du \emph{joint progress diagram}.

\subsubsection{Reusable Ressources}
Ce sont des ressources qu'on peut réutiliser, qui ne sont pas dégradées par un processus (processeur, I/O channels, main et secondary memory,...) .
Un deadlock arrive si chaque processus tient une ressources et demande l'autre.
Une autre sorte de deadlock est par rapport à la main memory, et l'allocation de mémoire: Si deux processus durant leur exécution concurrente demandent plus d'allocation de mémoire que peut fournir la main memory, alors il y a deadlock.
Ce problème est réglé avec l'utilisation de la Virtual Memory.

\subsubsection{Consumable Ressources}
Ressource qui peut être créée et détruite (interrupts, signals, messages,...).
Un deadlock peut arriver si chaque processus attent un message de l'autre pour en renvoyer un à l'autre.
Les deux processus sont en attente et ne recevront jamais rien.
(difficile à trouver).
Il y a un tableau \cite[p.~285]{stallings} qui explique les différentes techniques mises en oeuvre pour combattre les deadlocks.

\subsubsection{Ressource Allocation Graphs}
Explication du \emph{ressource allocation graph} avec l'exemple du carrefour.
C'est un graphe dirigé biparti avec les processus dans le premier ensemble de noeuds
et les ressources dans le deuxième ensemble de noeud.
\begin{itemize}
  \item Il y a une arête d'un processus à une ressource si le processus veut la resources;
  \item il y a une arête d'une ressource à un processus si cete resources est utilisée par le processus.
\end{itemize}

\subsubsection{The Conditions for Deadlock}
On a un deadlock si et seulement si, les 4 conditions suivantes sont satisfaites
\begin{enumerate}
  \item Mutual Exclusion: Un seul processus peut utiliser une ressource en même temps.
  \item Hold and Wait: Un processus peut vouloir une ressource pendant qu'il en utilise une autre.
  \item No preemption: On ne peut pas retirer une ressource à une processus qui en utilise une.
  \item Circular wait $\to$ La plus importante: Il y a un cycle dans le ressource allocation graph.
\end{enumerate}
Les 3 premières impliquent l'existence d'une région fatale dans le joint progress diagram.
La quatrième implique qu'un région fatale a été franchie.

\subsection{Deadlock Prevention}
C'est a dire mettre en place un système qui exclus la possibilité de deadlock.
Deux façon de faire:
Indirecte (Prévenir l'occurence d'une des 3 premières conditions)
ou directe (Prévenir la 4ème condition (circular wait)).

\subsubsection{Mutual Exclusion}
On peut pas s'en passer, c'est comme ça.

\subsubsection{Hold and Wait}
On peut demander que le processus se bloque tant que toutes les ressources qu'il demande ne sont pas libre.
C'est inéfficace pour 2 raisons:
Le processus peut rien foutre pendant longtemps,
alors qu'il pourrait peut être déjà faire quelque chose avec certaines des ressources qu'il demande et les ressources prises
par le processus peuvent être prises pendant longtemps, ce qui emmerde tous les autres

\subsubsection{No Preemption}
Si un processus qui tient plusieurs ressources est bloqué sur une future requête,
il peut tout lacher et recommencer plus tard.
Ou si il demande une ressource qui est tenue par quelqu'un d'autre,
l'autre peut lacher cette ressource.
Ca prévient les deadlock si les deux processus ont la même priorité.
Mais c'est partique seulement si l'état de la ressource peut être sauvé et restauré facilement,
comme dans le cas du processeur.

\subsubsection{Circular Wait}
On choisit un ordre sur les ressources et on impose de demander les ressources dans cet ordre.
Ainsi plus de deadlock possible.
Malheureusement, ça impose des restrictions sur la programmation et ça empêche au programmeur
de demander les ressources incrémentalement.
Ça peut donc aussi causer une utilisation inéfficace des ressources.


\subsection{Deadlock Avoidance}
Autorise plus de concurrence que le deadlock prevention, car autorise les 3 premières condition, mais empêche un deadlock d'arriver.
On a besoin de savoir la demande maximum pour chaque resource $j$ de chaque processus $i$ $C_{ij}$
et la quantité totale de processus que la ressources $j$ peut accepter simultanément $R_j$.

\subsubsection{Process Initiation Denial}
On ne démarre pas un processus si il peut mener à un deadlock.
On vérifie que $\forall j$,
\[ \sum_{i} C_{ij} \leq R_j. \]
Un processus est démarré si la demande maximale de tous les processus courants plus celle des nouveaux processus peut être satisfaite.
Ce n'est pas super optimal car on imagine le pire scénario à chaque fois.

\subsubsection{Ressource Allocation Denial}
On définit un \emph{safe state} comme un état pour lequel,
si on pouvait choisir l'ordre d'exécution des processus,
même dans le pire cas, c'est à dire que le processus $i$ veut $C_{ij}$ pour chaque ressource $j$,
on saurait faire tourner tous les processus jusqu'à complétion.

On s'assure qu'on reste toujours dans un \emph{safe state}.
À chaque demande d'un processus, on vérifie qu'on ne passe pas à un \emph{unsafe state},
si c'est le cas, on le bloque.
On sait alors qu'on aura jamais de deadlock car un état de deadlock est un \emph{unsafe state}.
Et on sait qu'on ne bloquera jamais tous les processus car
par la définition du \emph{safe state}, il existe un processus qui a
assez avec les ressources disponibles pour s'allouer son maximum.
Lorsque ce processus demandera des resources, on ne le bloquera donc jamais.

Ce truc est vraiment cool, mais il y a quelques restrictions:
\begin{itemize}
  \item Le maximum de ressources demandées par chaque processus doit être connu à l'avance.
  \item Les processus considérés doivent être indépendants.
  \item il doit y avoir un nombre fixé de ressources à allouer.
  \item Aucun processus ne peut se barrer (exit) en maintenant des ressources.
\end{itemize}

\subsection{Deadlock Detection}
Les processus obtiennent les ressources dès que possible.
Le but est d'empêcher la 4ème condition.

\subsubsection{Deadlock Detection Algorithm}
L'algorithme marque tous les processus qui ne mènent pas à un deadlock.
Il ne préviens pas un deadlock, mais il détermine si un deadlock existe en ce moment.

\subsubsection{Recovery}
Quand un deadlock est détecté, il y a plusieurs stratégies pour retomber sur ses pattes,
on a, par ordre croissant de complexité:
\begin{enumerate}
  \item On abandonne tous les processus deadlocké (Incroyable mais c'est la technique la plus courante dans les OS)
  \item Retour vers un checkpoint des processus deadlockés.
    Il faut un mécanisme de redémarrage et de rollback dans le sytème
  \item Successivement, on abandonne un par un les processus bloqués jusqu'à ce qu'il n'y ait plus de deadlock
  \item Successivement, preempter une par une les ressources litigieuses.
\end{enumerate}

\subsection{An Integrated Deadlock Strategy}
Vu que chaque technique a ses avantages et inconvénients,
on nous donne ici une manière de raisonner pour choisir la meilleure technique en fonction des cas, exemples à l'appui.

\subsection{Dining Philosophers Problem}
C'est le bazar avec les gens qui doivent choisir leur fourchette.
Ça a l'air con, mais c'est pas facile à résoudre et ça illustre la coordination de ressources partagées.

\subsubsection{Solution Using Semaphores}
Solution avec sémaphore, en forcant qu'il y aie au plus $n-1$ philosophes à table.
Voir \cite[p.~281]{stallings}.

\subsubsection{Solution Using a Monitor}
On a un vecteur de conditions, une condition par fourchette.
Voir \cite[p.~282]{stallings}.

\subsection{UNIX Concurrency Mechanisms}
Unix fourni une variété de mécanismes pour la communication interprocesseurs et la synchronisation.
\begin{itemize}
  \item Pipes: C'est une FIFO, écrite par un processus et lue par un autre.
  \item Messages: associés à une queue qui fonctionne comme une mailbox.
  \item Shared Memory: Bloc de mémoire virtuelle partagé par de multiples processeurs.
  \item Semaphores: Ce sont ceux vu dans le livre qui sont généralisés.
  \item Signals: Similaire à une interruption hardware, mais n'utilise pas le principe de priorité.
\end{itemize}

\subsection{Linux Kernel Concurrency Mechanisms}
Contient les même trucs que UNIX.
Fournit en plus des opérations atomiques pour des entiers avec le type \lstinline|atomic_t|
et pour des bitmap.
Ça permet d'éviter les simple race conditions par exemple.
La liste des opérations et leurs avantages principaux sont repris \cite[p.~306-307]{stallings}.

La technique la plus utilisée dans Linux pour protéger une section critique est le spinlock.
C'est les mutex mais avec de l'attente active.
C'est pratique et efficace seulement si le temps d'attente est censé être très court, de l'ordre de 2 changements de contexte.
Il a aussi un reader-writer version spinlock.

Linux a aussi un mécanisme de sémaphore interne avec counting semaphore,
mutex et une semaphore reader-writer qui agit en mutex pour les writer et en counting semaphore pour les readers.

Linux contient des \emph{Barriers} pour forcer empêcher le compilateur
et le processeur de changer l'ordre des instructions.

\subsection{Solaris Thread Synchrnoization Primitives}
Possède 4 types de primitifs de synchronisation des threads:
\begin{itemize}
  \item Mutual exclusion, mutex et locks
  \item Semaphores
  \item Multiple readers, single writers locks
  \item Condition variables
\end{itemize}

\subsection{Windows 7 Concurrency Mechanisms}
Les méthodes de synchronisation les plus importantes sont:
\begin{itemize}
  \item Executive Dispatcher Objects
  \item User-mode critical sections
  \item Slim reader-writer locks
  \item Condition variables
  \item Lock-free opérations
\end{itemize}

\newpage
\part{Memory}

%------------------------------------------------------------------------------
%				Chapitre 7
%------------------------------------------------------------------------------

\section{Chapter 7 : Memory Management}
\begin{itemize}
  \item Frame: Un bloc à longueur fixée dans la main memory.
  \item Page: Un bloc à longueur fixée dans la secondary memory.
  \item Segment: Un bloc de données à longueur variable dans la secondary memory.
\end{itemize}

\subsection{Memory Management Requirements}
\subsubsection{Relocation}
Vu qu'un programme peut être swappé et déswappé, ce serait limitant de vouloir le replacer chaque fois au même endroit,
donc il faut relocaliser le processus dans différentes zones de mémoire.
On ne sait pas prévoir à l'avance où va se trouver un programme dans la mémoire.
En gros, le processeur et l'OS doivent être capable de traduire les références à la mémoire trouvées dans le code en véritables adresses physiques.

\subsubsection{Protection}
Chaque processus devrait être protégé des interférences non désirées des autres processus.
La protection de la mémoire doit être satisfaite par le processeur plutot que par l'OS
car on ne peut pas faire un process switch pour chaque accès à la main memory.
Les mécanismes qui supportent la relocation supportent également la protection.

\subsubsection{Sharing}
Tout mécanisme de protection doit avoir la flexibilité d'autoriser plusieurs processus à accéder à la même portion de main memory.
Ex: Si un certain nombre de processus exécutent le même programme,
il est avantageux d'autoriser chaque processus à accéder à la même copie du programme plutot que chaque processus en ait leur propre copie.
Les mécanismes qui supportent la relocation supportent également le sharing.

\subsubsection{Logical Organization}
La main memory est organisée comme étant un espace d'adressage linéaire, consistant en une séquence de mots.
La mémoire secondaire est similaire d'un point de vue physique.
Cela ne correspond pas à la façon dont les programmes sont organisés.
Les programmes sont organisés sous forme de modules, ce qui apporte un certain nombre d'avantages:
\begin{itemize}
  \item Les modules peuvent être écrits et compilés indépendemment.
  \item Différents degrés de protection peuvent être donnés aux modules.
  \item On peut introduire des mécanismes avec lesquels les modules peuvent être partagés entre processus.
\end{itemize}

\subsubsection{Physical Organization}
On a vu que la mémoire était structurée en au moins deux niveaux : Main et secondary memory.
L'organisation du flux d'information entre les 2 mémoires est un problème système majeur.
Cela pourrait être de la responsabilité du programmeur, mais ce n'est pas une bonne idée pour 2 raisons:
\begin{itemize}
  \item La main memory disponible pour un programme et ses données peut être insuffisant.
    Il faut donc pratiquer de l'overlaying
    (programme qui fait transiter les données nécessaires et non nécessaires dans un espace réservé à l'exécution du programme).
    Mais même avec une aide du compilateur, cela gaspille du temps pour le programmeur.
  \item Dans un multiprogramming environment,
    le programmeur ne sait pas en codant combien d'espace est disponible pour lui.
\end{itemize}

\subsection{Memory Partitioning}
La Virtual Memory est basée sur la segmentation et le paging.
Avant de parler de ça, on va parler de techniques plus simples qui ne font pas appel à la Virtual Memory, comme le partitioning.
Le tableau 7.2 (\cite[p.~331]{stallings}) est très important car il reprend toutes les techniques développées dans le chapitre et plus loin:
\begin{itemize}
  \item Fixed Partitioning
  \item Dynamic Partitioning
  \item Simple Paging
  \item Simple Segmentation
  \item Virtual Memory Paging
  \item Virtual Memory Segmentation
\end{itemize}

\subsubsection{Fixed Partitioning}
On suppose que l'OS occupe une portion fixe de la mémoire et que le reste es disponible pour les processus.
Le schéma le plus simple pour gérer cette mémoire disponible est de la partitioner en régions à frontières fixes.
On peut faire un partition avec des blocs de taille égales.
Deux problème arrivent:
\begin{itemize}
  \item Un programme trop gros pour une partition doit être redessinée par le programmeur en utilisant des overlays
    pour que seule une partie du programme soit dans la mémoire au même moment.
    (voir animation web)
  \item L'utilisation de la main memory est extrèmement inéfficace.
    Même un tout petit programme occupe une paritition entière.
    Cela s'appelle de l'\emph{internal fragmentation}.
\end{itemize}

On peut utiliser une partition à taille non-fixée, mais cela apporte toujours les même problèmes, malgré le fait que cela les atténue.
Un autre problème arrive néanmoins, celui du placement.
Il y a deux façons de faire: La plus simple est dans mettre chaque processus dans la plus petite partition dans laquelle elle rentre.
Il faut une queue d'attente pour chaque taille de partition.
L'avantage est que cela minimise l'internal fragmentation.
Mais si cela a l'air optimal d'un point de vue d'une partition individuelle, cela ne l'est pas pour le système tout entier.
Il vaut donc mieux avoir une seule file d'attente.
Un désavantage supplémentaire est que le nombre de processus maximal pouvant s'effectué en même temps est fixé par le nombre de partitions.

\subsubsection{Dynamic Partitioning}
Quand un processus est amené dans la mémoire, on lui alloue exactement autant de mémoire qu'il lui faut, pas plus.
L'exemple \cite[p.~334]{stallings} montre bien que la mémoire contient de plus en plus de petits trous,
et l'utilisation de la mémoire se détériore.
Cela s'appelle l'\emph{external fragmentation}.
Une technique utilisée pour éviter l'external fragmentation est la compaction.
C'est à dire que de temps en temps, l'OS déplace tous les processus côtes à côtes pour que tout l'espace libre soit continu.
Le problème est que ça prend du temps et que ça gaspille l'utilisation du processeur.
L'algorithme de placement doit être très intelligent,
car comme la compaction prend du temps,
il doit bien choisir où mettre ses trous dans la mémoire.
Il y a trois algorithmes:
\begin{itemize}
  \item Best-fit (prend le block le plus proche en taille de la requête),
  \item first-fit (prend le premier qui convient depuis le début de la mémoire) et
  \item next-fit (prend le prochain bloc qui convient à partir du dernier placé).
\end{itemize}

Etonnement, first-fit est le meilleur,
next-fit a tendance à trop fragmenter la fin de la mémoire et best-fit produit des trous si petits qu'il faut constemment faire de la compaction.
Il y a un moment où on est obligé de swapper deux processus,
il faut donc un algorithme de remplacement,
mais cela est vu plus loin dans le livre.

\subsubsection{Buddy System}
C'est un compromis.
Les blocs sont de taille $2^L$ jusque $2^U$.
Voir explication \cite[p.~337-338]{stallings}.
C'est un bon compromis qui dépasse les désavantages des deux techniques vues auparavant.

\subsubsection{Relocation}
On fait la disctinction entre plusieurs types d'adresses:
\begin{itemize}
  \item Logical address: Référence à un endroit de la mémoire indépendant de sa position exacte.
    Une traduction est nécessaire pour pouvoir accéder à son adresse physique
  \item Relative address: l'adresse est exprimée par rapport à un point connu.
  \item Physical address: Le varitable emplacement de la valeur dans la main memory.
\end{itemize}
Voir figure~7.8 \cite[p.~340]{stallings} pour comprendre le mécanisme de traduction des adresses.

\subsection{Paging}
La mémoire est divisée en petits bouts de taille fixe (frames) et les processus aussi (pages).
Il y a donc pas de fragmentation externe, et la fragmentation interne correpond à l'espace gaspillé dans la dernière page du processus.
Il faut maintenir une table des pages pour chaque processus, pour savoir où les petits bouts sont mis.
En plus, si on se met d'accord et qu'on pose la taille des pages comme étant une puissance de 2, alors l'adresse relative et l'adresse logique sont les même.
Il y a deux avantages:
\begin{itemize}
  \item L'adresse logique est transparente au programmeur, à l'assembleur, et au linker.
  \item Il est facile d'implémenter une fonction hardware qui fait de la traduction d'adresse
    dynamique.
\end{itemize}

\subsection{Segmentation}
Le programme est divisés en segments qui n'ont pas tous la même taille.
Ca ressemble à de la dynamic partitioning, et ça souffre aussi de l'external fragmentation.
Mais comme les bouts sont plus petit, l'effet est limité.
Et il faut tenir une table des segments contenant la taille et le début de chaque segment pour tous les processus.
L'adresse physique est l'offset additionné au début du segment.

\subsection{Security Issues}
\subsubsection{Buffer Overflow Attacks}
C'est super dangereux.
Voir \cite[p.~327-330]{stallings}.

\subsubsection{Defending against Buffer Overflows}
Il faut programmer intelligemment,
beaucoup d'appels librairies sont connus comme dangereux (e.g. \lstinline|gets|)
et il ne faut plus les utiliser mais utiliser ceux qui ont été développés
qui ne sont pas sensibles les buffers overflows (e.g. \lstinline|fgets|).

Beaucoup de programmes utilisés aujourd'hui ont déjà été mal écrits donc en plus
de bien écrire maintenant, il faut des mécanisme pour se défendre des buffers
overflows.

\newpage

%------------------------------------------------------------------------------
%				Chapitre 8
%------------------------------------------------------------------------------

\section{Chapter 8 : Virtual Memory}
Quelques définitions sont données \cite[p.~361]{stallings}

\subsection{Hardware and Control Structures}
2 caractéristiques de paging et segmentation sont les clés de ce chapitre:
\begin{itemize}
  \item Toutes les références à la mémoire dans un processus sont des adresses logiques qui sont traduites dynamiquement.
  \item Un processus peut être divisé en un nombre de pièrces (segment ou pages) qui n'ont pas besoin d'être côte à côte dans la main memory pendant l'exécution.
\end{itemize}
Un portion de processus qui est actuellement dans la main memory est appelé un resident set du processus.
Si le processeur rencontre une adresse logique qui n'est pas dans la main memory, il génère une memory access fault interruption.
L'OS met le processus interrompu dans un blocking state.
L'OS lance alors une disk I/O request pour pouvoir amener le bloc de mémoire la pièce du processus qui possède l'adresse logique susmentionnée.
Après, l'OS dispatch un autre processus pendant que la disk I/O est en cours.
Après que le bloc soit amené dans la main memory, une I/O interrupt est lancée, ce qui rend le contrôle à l'OS, lequel place le processus bloqué dans l'état ready.
Cette façon de faire amène 2 considérations:
\begin{itemize}
  \item Plus de processus peuvent être présents dans la main memory
  \item Un processus peut être plus grand que la main memory
\end{itemize}
La table à la \cite[p.~363]{stallings} reprend toutes les caractéristiques des (Virtual Memory) paging et segmentation.

\subsubsection{Locality and Virtual Memory}
Il faut faire gaffe à ne pas passe plus de temps à swapper des pièces plutot que d'exécuter des instructions, ce qui s'appelle le trashing.
Il existe des algorithmes pour ca, qui se basent sur le principe de localité qui, pour rappel, établit que le programme et les références aux données d'un processus ont tendance à se regrouper.
Pour que la Virtual Memory soit pratique et efficace, nous avons besoin de 2 ingrédients:
\begin{itemize}
  \item Il doit y avoir un support hardware pour employer le schéma de segmentation et/ou paging
  \item L'OS doit inclure une série de programme pour gérer le mouvement des pages de la main memory à la secondary memory.
\end{itemize}

\subsubsection{Paging}
Explication de la structure d'une page table, et d'une page table entry ainsi que le mécanisme hardware de traduction d'adresses.
Explique ensuite les page tables à plusieurs niveaux de hiérarchie.
Explication des Inverted Page Table qui utilisent une fonction de hashage pour avoir une table plus petite.
Translation Lookaside Buffer: Cache spéciale à haute-vitesse pour les page table entries.
C'est une cache qui contient les PTE qui ont été les récemment plus utilisées.
Voir schéma d'exécution \cite[p.~371]{stallings}.
Il a été prouvé que le TLB augmentait fortement les performances.
Un schéma complet est fourni \cite[p.~372-373]{stallings}.
Voir la discussion très intérressante sur la taille des pages.
\cite[p.~374-375]{stallings}.

\subsubsection{Segmentation}
Les avantages sont les suivants:
\begin{itemize}
  \item Cela simplifie la prise en main de structures de données croissantes
  \item Cela autorie les programmes à être modifiés et recompilés indépendamment.
  \item Cela pousse au partage entre processus
  \item Cela pousse à la protection.
\end{itemize}

\subsubsection{Combined Paging and Segmentation}
Paging, est transparent au programmeur, élimine la fragmentation externe et fournit une utilisation efficace de la main memory.
Segmentation est visible du programmeur, peut gérer les structures de données croissantes, est modulaire et supporte le partage et la protection.
Certains systèmes trop cool supportent les deux techniques.

\subsubsection{Protection and Sharing}
Explique en quoi la segmentation et le pagin fournissent une bonne base pour le partage et la protection.

\subsection{OS Software}
La conception de la partie de geston de la mémoire d'un OS dépend de 3 choix fondamentaux:
\begin{itemize}
  \item Si oui ou non on utilise la technique de mémoire virtuelle
  \item Si on utilise le pagin, la segmentation ou les deux
  \item Quels algorithmes sont employés dans les différents aspect de la gestion de la mémoire.
\end{itemize}
La table à la \cite[p.~381]{stallings} montre les éléments clés du chapitre.
Il est a souligné que dans tout le reste du chapitre, la préoccupation principale est la performance du système.
On veut à tout prix limite le taux de page fault, parce que ça bouffe du temps.

\subsubsection{Fetch Policy}
La fetch policy détermine quand une page devrait être amenée dans la mémoire.
Il y a deux alternatives:
\begin{itemize}
  \item Demand Paging: Une page est amenée dans la méoire seulement quand une référence est faite vers un endroit de cette page.
    Quand un processus est lancé, il y a une pléade de de page faults.
    Mais avec le principe de localité, le nombre de page fault va décroitre vers un niveau très bas.
  \item Prepaging: Amène dans la mémoire d'autres pages que celle demandée.
    Tient compte du mouvement rotationnel et de la latence du disque de mémoire secondaire.
\end{itemize}

\subsubsection{Placement Policy}
La placement policy détermine où dans la real memory un morceau de processus doit être placé.
Dans un système pure-segmentation, on a vu des algorithmes (best-fit,firstfit,...).
Dans un système pure-pagin ou combiné, le placement est généralement inutile car la mécanisme machine de traduction et d'accès peuvent faire le mêmeboulot avec autant d'effiacité.
Ca devient intérressant avec des multiprocesseurs NUMA.

\subsubsection{Replacement Policy}
Revient à selectionner choisir une page dans la main memory à virer pour faire rentrer une nouvelle page.
\begin{itemize}
  \item Basic Algorithms
    \begin{itemize}
      \item Optimal
      \item Least recently used (LRU)
      \item First-in-first-out (FIFO)
      \item Clock
    \end{itemize}
  \item Page Buffering
  \item Replacement policy and cache size
\end{itemize}

\subsubsection{Resident Set Management Cleaning Policy}
\paragraph{Resident set size} L'OS doit décider combien de pages d'un processus il garde en main memory, c'est-à-dire combien de main memory il doit allouer à un certain processus.
Plusieurs facteurs sont importants
\begin{itemize}
  \item Au moins on alloue à un processus, au plus de processus différents il y peut y avoir en main memory.
    C'est mieux car ça accroit la probabilité que l'OS trouve au moins un processus ready et donc diminue le temps perdu à faire des swaps.
  \item Mais si y a trop peu de pages, alors le taux de page fault va être plus haut en vertu du principe de localité.
  \item A partir d'une certaine taille, allouer plus de mémoire pour le processus n'aura pas d'effet notable sur le taux de page fault.
\end{itemize}
Deux politiques sont utilisées dans les OS actuels.
\begin{itemize}
  \item Fixed-allocation policy : donne à un processus un nombre fixé de frames en main memory.
    Ce nombre est fixé à la création du processus et peu être déterminé sur base du type de processus ou peut être fixé par le programmeur.
  \item Variable-allocation policy : nombre de frames peut varier au cours du temps de vie du processus.
    Si le taux de fault est grand (resp.
    petit), on peut lui en donner plus (resp.
    moins).
    Cette technique parait mieux mais l'OS doit alors évaluer le comportement des processus actifs ce qui cause plus d'overhead
\end{itemize}

\paragraph{Replacement scope}
\begin{itemize}
  \item local replacement policy : vire une page du processus qui génère le page fault
  \item global replacement policy : toutes les pages unlocked sont candidates
\end{itemize}

\subsubsection{Cleaning Policy}
opposé de fetch policy, s'intéresse à déterminer quand une page modifiée doit être réécrite en secondary memory
\begin{itemize}
  \item demand cleaning : quand la page a été sélectionnée pour replacement.
    Inconvénient : veut dire que quand un autre processus qui veut lire la page et qui fait un page fault devrait attendre le transfert avant d'être débloqué
  \item precleaning : avant.
    Avantage : on peut le faire par blocs de pages.
    Inconvénient : capacité de transfert vers la secondary memory est limité et ne devrait pas être gaspillé avec des opérations pas nécessaires
    Une meilleure approche incorpore page buffering, à lire dans le livre.
\end{itemize}

\subsubsection{Load Control}
Quel nombre de processus résidant en main memory?
\begin{itemize}
  \item Trop grand $\rightarrow$ chacun a une taille petite $\rightarrow$ beaucoup de page faut
  \item Trop petit $\rightarrow$ seront plein de fois tous blocked  $\rightarrow$ beaucoup de temps de swapping
\end{itemize}

\paragraph{Processus suspension} Si on veut réduire le taux de multiprogramming, on suspend certains processus.
Critères :
\begin{itemize}
  \item lowest-priority
  \item faulting
  \item last activated
  \item smallest résident set
  \item largest
  \item largest remaining execution window
\end{itemize}

\subsection{UNIX and Solaris Memory Management}
\subsubsection{Paging System}
\subsubsection{Kernel Memory Allocation}
\subsection{Linux Memory Management}
\subsubsection{Linux Virtual Memory}
\subsubsection{Kernel Memory Allocation}
\subsection{Windows Memory Management}
\subsubsection{Windows Virtual Adress Map }
\subsubsection{Windows Paging}


\newpage
\part{Scheduling}

%------------------------------------------------------------------------------
%				Chapitre 9
%------------------------------------------------------------------------------

\section{Chapter 9 : Uniprocessor Scheduling}

\subsection{Types of Processor Scheduling}
Short, long et medium scheduling vont être des outils pour déterminer quels processus vont attendre et quels processus vont s'exécuter.
Fondamentalement, le schéduling est la manière d'organiser les queues pour minimiser les temps d'attentes et d'optimiser les performances dans un environnement de queues d'attente.

\subsubsection{Long-Term Scheduling}
Détermine quels programmes dont admis par le système pour s'exécuter.
Il controle le degré de multiprogramming.
Il doit prendre deux décisions:
\begin{itemize}
  \item Quand l'OS doit amener un ou plusieurs processus, ce qui est dirigé par le degré d multiprogramming désiré.
  \item Quels processus doit-on amener ensuite.
    Cela peut se faire sur la base de FirstCome-First-Served (FCFS), ou avec un outils système.
\end{itemize}

\subsubsection{Medium-Term Scheduling}
Fait partie de la fonction de swapping.

\subsubsection{Short-Term Scheduling}
C'est le dispatcher.
C'est le plus fréquemment utilisé des 3 dispositifs, c'est lui qui prend la fine décision de quel processus exécuter ensuite.

\subsection{Scheduling Algorithms }
\subsubsection{Short-Term Scheduling Criteria}
Le but est d'optimiser un ou plusieurs aspects du comportement du système, il y a plusieurs sortes de critères (voir table \cite[p.~401]{stallings}):
\begin{itemize}
  \item User Oriented, Performance Related
    \begin{itemize}
      \item Turnaround time
      \item Response time
      \item Deadlines
    \end{itemize}
  \item User Oriented, Other
    \begin{itemize}
      \item Predictability
    \end{itemize}
  \item System Oriented, Performance Related
    \begin{itemize}
      \item Throughput
      \item Processor utilization
    \end{itemize}
  \item System Oriented, Other
    \begin{itemize}
      \item Fairness
      \item Enforcing priorities
      \item Balancing ressources
    \end{itemize}
\end{itemize}

Les critères user-oriented sont plus importants et ont plus de conséquences que les system-oriented.

\subsubsection{The Use of Priorities}
Le scheduler choisir toujours un processus de plus haute priorité.
Il va donc voir dans les queues de priorité en commancant par la plus haute.
Le risque est que les files de basses priorités peuvent souffrir de starvation.

\subsubsection{Alternative Scheduling Policies}
Voir la table~9.3 pour les différentes fonctions de sélection de processus :
\begin{itemize}
  \item First-Come-First-Served (FCFS): Fonctionne mieux pour les gros processus.
    A tendance à préférer les proccess-bound processes plutot que les I/O-bound processes.
    C'est pas le plus attractif, mais combiné à un principe de priorités, il peut faire un bon scheduler.
  \item Round Robin: Pour éviter les désavantages de FCFS, on utilise le même principe additionné à un principe de préemption basé sur une clock.
    (Time slicing).
    La difficulté est de choisir la longueur de la time slice.
    C'est plutot efficace pour les time-sharing ou transaction processing system, mais cela peut mal fonctionner avec des I/O bound processes.
    On parle aussi dans le livre de Virtual Round Robin (VRR), qui introduit une auxilliary queue, ce que augmente le fairness :)
  \item Shortest Process Next (SPN): Exécute en premier les processus qui on le temps d'exécution estimé le plus court.
    Les grands processus sont désavantagés et cella réduit la prédictability.
    Il faut estimer le temps d'exécution d'un programme et cela se fait à partir de statistiques (calcul exponentiel de moyenne) voir \cite[p.~410-411]{stallings}.
    Risque de starvtion.
    Pas adapté pour time-sharing et transaction processing car non preemptif.
  \item Shortest Remaining Time (SRT): Version préemptive de SPN.
    Prend celui qui à le temps restant d'exécution le plus court.
    Risque de starvation.
    Peut avoir de grns overhead.
  \item Highest Response Ration Next (HRRN): C'est un bon équilibre entre processus courts et longs.
  \item Feedback: Si on a aucune idée de la longueur des processus, SPN, SRT, HRRN sont inutilisables.
    On se focalise alors sur le temps écoulé et non le temps restant.
    Chaque fois qu'un processus est preempté, il perd un degré de priorité.
    Peut souffrir de starvation.
    Chaque niveau de priorité à un time slice double.
\end{itemize}
Voir la figure~9.5 \cite[p.~405]{stallings} qui illustre toutes ces différents comportements.

\subsubsection{Performance Comparison}
Queuing analysis: Voir les graphes.
Les test ont été fait suivant une distribution de poisson.
Les résultats confirment bien la théorie avancée plus avant.
Simulation modeling: Processus groupés par percentiles.
Turnaround time: FCFS très mauvais, surtout pour les petits processus.
Voir analyse \cite[p.~420]{stallings}

\subsubsection{Fair-Share Scheduling}
En gros, si un certains nombre d'utilisateurs font partie d'un groupe, on voudrait que les décisions de scheduling soient prisent à l'intérieur de ce groupe.
C'est-à-dire que les décisions de scheduling devraient être prises pour que chaque groupe recoivent un service similaire.
En UNIX, il existe le Fair-Share Scheduler (FSS).
Chaque groupe recoit un système virtuel qui fonctionne proportionellement plus lentement qu'un système complet.

\subsection{Traditional UNIX Scheduling}
A lire, plutot intéressant.
\cite[p.~423]{stallings}.

\newpage

%------------------------------------------------------------------------------
%				Chapitre 10
%------------------------------------------------------------------------------

\section{Chapter 10 : Multiprocessor and Real-Time Scheduling}

\subsection{Multiprocessor Scheduling}
On peut classer les systèmes multiprocesseurs dans 3 catégories:
\begin{itemize}
  \item Loosely coupled or distributed multiprocessor or cluster: Vus au chapitre 16
  \item Functionally specialized processors: ex I/O processors $\to$ Chapitre 11
  \item Tightly coupled multiprocessor: Ensemble de processeurs qui partagent une main memory commune et qui sont sous le controle intégré d'un OS.
\end{itemize}
But de ce chapitre.

\subsubsection{Granularity}
Un manière de caractériser les multiprocesseurs est de les placer dans le contexte de granularité de synchronisation, ou fréquence de synchronisation entre les processus du système.
\begin{itemize}
  \item Independant parallelism: Pas de synchronisation explicite entre processus (ex:time sharing systems).
  \item Coarse and very-coarse-grained parallelism: Synchronisation entre processus, mais à un niveau très brut.
  \item Medium-grainded parallelism: Ex: Processus qui contient un ensemble de threads.
  \item Fine-grained parallelism: Très complexe, c'est une zone très spécialisée et frangmentée (ex: Source engine de Valve)
\end{itemize}

\subsubsection{Design Issues}
Le scheduling sur multiprocesseurs apporte 3 problèmes interdépendants (\cite[p.~434-435]{stallings}):
\begin{itemize}
  \item L'affectation de processus aux processeurs: On voit deux approches, le master/slave architecture et la peer architecture
  \item L'utilisation de multiprogramming sur des processeurs individuels
  \item Le dispatching reel d'un processus
\end{itemize}

\subsubsection{Process Scheduling}
Un étude à montré que contrairement aux systèmes uniprocesseur, les systèmes multiprocesseurs s'en sortent très bien avec une discipline de scheduling FCFS.

\subsubsection{Thread Scheduling}
La puissance des thread se manifeste véritablement dans les systèmes multiprocesseurs, car ils peuvent pleinement exploiter le parallélisme des applications.
On voit arriver 4 approches (\cite[p.~437-442]{stallings}):
\begin{itemize}
  \item Load sharing: Processus non assigné à un processeur particulier.
  \item Gang scheduling
  \item Dedicated processor assignment
  \item Dynamic scheduling
\end{itemize}

\subsection{Real-Time Scheduling}
\subsubsection{Background}
\subsubsection{Characteristics of Real-Time OS}
\subsubsection{Real-Time Scheduling}
\subsubsection{Deadline Scheduling}
\subsubsection{Rate Monotonic Scheduling}
\subsubsection{Priority Inversion}
\subsection{Linux Scheduling}
\subsection{UNIX SVR4 Scheduling}
\subsection{UNIX FreeBSD Scheduling}
\subsection{Windows Scheduling}
\subsection{Linux Virtual Machine Process Scheduling}


\newpage
\part{Input/Output and Files}


%------------------------------------------------------------------------------
%				Chapitre 11
%------------------------------------------------------------------------------

\section{Chapter 11 : I/O Management and Disk Scheduling}

\subsection{I/O Devices}
Il y a trois type d'appareils externes qui ont des relations I/O avec l'ordinateur:
\begin{itemize}
  \item Human readable : terminal, imprimante, écran,...
  \item Machine readable : Clé USB, senseurs, ...
  \item Communication: modems, ...
\end{itemize}
Il y a encore de grandes différences entre les entités de chaques classes:
\begin{itemize}
  \item Data rate
  \item Application
  \item Complexity of Control
  \item Unit of transfer
  \item Data representation
  \item Error conditions
\end{itemize}

\subsection{Organization of the I/O Function}
Il y a trois techniques pour paratiquer du I/O
\begin{itemize}
  \item Programmed I/O
  \item Interrupt-driven I/O
  \item Direct Memory Adress DMA
\end{itemize}

\subsubsection{The Evolution of the I/O Function}
Explique les différentes étapes par lesquels dont passés les fonctions I/O durant les ages.
Apparition des I/O channel et I/O processor.

\subsubsection{Direct Memory Adress}
Quand le processeur veut faire un read ou un write, il envoie une requête au DMA avec les infos suivantes, chacune envoyées sur une ligne de communication:
\begin{itemize}
  \item Si il veut un read ou un write
  \item L'adresse du dispositif I/O visé
  \item L'adresse de départ du read ou write
  \item Le nombre de mot à lire ou écrire
\end{itemize}
Il y a plusieurs structures de DMA possibles.
Si tous les modules ont le même system bus.
Il y a programmed I/O à l'intérieur du DMA.
C'est inefficace.
On peut faire de deux façon différentes voir \cite[p.~500]{stallings}.

\subsection{OS Design Issues}

\subsubsection{Design Objectives}
Il y a deux objectifs dans la conception I/O: L'efficacité et la généralité.

\subsubsection{Logical Structure of the I/O Function}
Explique que les interractions I/O peuvent être hiérarchisées en couches.
3 cas de figures sont envisagés: Logical peripheral device, communication port et file system.
Les différentes couches sont énoncées également.

\subsection{I/0 Buffering}
Pour éviter les deadlock et la perte d'efficacité lors d'une opération I/O, on utilise une technique appelée buffering.
Attention à bien faire la différence entre les deux types d'appareils I/O: Les block-oriented devices et les stream-oriented devices.

\subsubsection{Single Buffer}
Une zone de la main memory est associée au transfert I/O.
Dès que le block est remplit, il est déplacé en user space et ainsi de suite (Read ahead).
Cool car on peut swapper un processus dehors et en plus on peut lire un bloc pendant que le processus en traite un autre.
Mais ça complexifie la logique de l'OS.

\subsubsection{Double Buffer}
Le processeur remplit un buffer pendant que l'autre est lu, ou inversément.
Ça augmente encore les performances mais aussi la complexité.
C'est efficace que pour les line-at-atime I/O, parce que pour les byte-at-a-time, c'est des intérraction consumer/producer.

\subsubsection{Circular Buffer}
Si le processeur est rapide pour remplir les buffers, on peut préférer le circular buffer.
Simplement un modèle bounded buffer producer/consumer.

\subsubsection{The Utility of the Buffering}
Même avec de multiples buffers, il se peut qu'ils soit tous remplis et que les processus devront attendre qu'il se vide.
Néanmoins, dans un envirennement multiprogrammé, le buffering peut augmenter les performances.

\subsection{Disk Scheduling}
Explique que la vitesse de lecture de disque a augmenté beaucoup plus lentement que la vitesse des processus,...
Important d'améliorer ce truc

\subsubsection{Disk Performance Parameters}
Explique les différents facteurs qui influent sur le temps nécessaire pour réaliser une opération I/O sur un disque.
Cela apporte des temps d'attente tels que: Seek time, rotational time, transfer time, ...
On explique avec un exemple que la sequential organization des données sur le disque a des répercutions très importantes sur la durée des opérations I/O sur le disque.

\subsubsection{Disk Scheduling Policies}
\begin{itemize}
  \item First-In-First-Out: Si tous les accès sont dans le même secteur et qu'il a peu de processus, c'est un bon algorithme, mais c'est pas souvent le cas.
  \item PRI : Bon si beaucoup de processus légers.
    Mauvais pour les systèmes database.
  \item Shortest Service Time First (SSTF): Prend toujours le plus proche.
    Pas dit que c'est le meilleurs, mais c'est un bon algo
  \item SCAN pour éviter la starvation que peut apporter SSTF.
    Prend tout dans une direction, puis change de sens, etc...
    Similaire a SSTF.
  \item Circular-SCAN (C-SCAN) : Tout dans une direction, puis retourne à l'extrémité opposée et recommence à scanner.
    Diminue le temps d'attente pour les nouvelles requetes.
  \item N-STEP-SCAN et FSCAN: Utilise plusieurs queues pour éviter que le bras ne soit immobile trop longtemps.
\end{itemize}

\subsection{RAID}
Standard de design de base de données multi-disques.
Redundant array of independant disks (RAID).
Consiste en 7 niveaux d'architecture.

\subsection{Disk Cache}
Utilisée dans le même ordre d'idée que pour les processus.

\subsubsection{Design Considerations}
Quel algorithme de remplacement adopter.
Il semble que le LFU est meilleur que le LRU, mais il y a qques problèmes.
En effet, le phénomène de localiser peut biaiser le jugement de l'algorithme, et le pousser à prendre de mauvaises décisions.
Le même algorithme est proposé avec une queue un peu différente, voir \cite[p.~524]{stallings}

\subsubsection{Performance Considerations}
Le Frequency-Based Replacement est supérieur au LRU

\subsection{UNIX SVR4 I/O }
\subsection{Linux I/O}
\subsection{Windows I/O}

\newpage

%------------------------------------------------------------------------------
%				Chapitre 12
%------------------------------------------------------------------------------

\section{Chapter 12 : File Management}

\subsection{Overview}
\subsubsection{Files and File Systems}
Le file system permet aux user de créer des données qui ont pour propriété:
\begin{itemize}
  \item Long-term existence
  \item Sharable between processes
  \item Structure
\end{itemize}

\subsubsection{File Structure}
Quand on parle de file, 4 termes arrivent souvent: field, record, file et database.
Les opérations courantes sont reprises \cite[p.~523]{stallings}.
Attention, par exemple, un programme C n'a pas de champ, record, etc...

\subsubsection{File Management Systems}
Donne les objectifs et exigences nécessaires pour avoir un bon file management system.
Les fonctions du file system sont expliquée sur un très joli diagramme \cite[p.~526]{stallings}.

\subsection{File Organization ans Access }
\subsubsection{The Pile}
Les données sont collectées dans l'ordre dans lequel elles arrivent.
Facile, mais pas super pratique dans la plupart des cas.

\subsubsection{The Sequential File}
Tous les records sont de la même longueur.
70

\subsubsection{The Indexed Sequential File}
On ajoute un index et un overflow file.
Chaque record dans l'index file contient 2 champs: Le key field et un pointer vers le main file.

\subsubsection{The Indexed File}

\subsubsection{The Direct or Hashed File}

\subsection{B-Trees}
Les index, c'est cool, mais pour des trucs encore plus gros, on va utiliser un index sous forme de B-TREE.

\subsection{File Directories}

\subsubsection{Contents}

\subsubsection{Structure}

\subsubsection{Naming}

\subsection{File Sharing}

\subsection{Access}

\subsubsection{Rights}

\subsubsection{Simultaneous}

\subsubsection{Access}

\subsection{Record Blocking}

\subsection{Secondary Storage Management}

\subsubsection{File Allocation}

\subsubsection{Free Space Management }

\subsubsection{Volumes}

\subsubsection{Reliability}

\subsection{File System Security UNIX File Management}

\subsection{Linux Virtual File System }
\subsection{Windows File System}

\biblio
\end{document}
