\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{framed}
\usepackage{color}

\title{Analyse de données quantitative\\Question 6}
\date{}

\begin{document}

\maketitle

\textit{Quelles sont les caractéristiques importantes du clustering? Expliquez l’algorithme de clustering de type «fuzzy k-means». Explicitez le critère qui est optimisé et l’algorithme de réallocation-recentrage qui permet d’optimiser le critère. Montrez que cet algorithme réduit l’inertie intra-classe à chaque itération et converge.}

\begin{enumerate}
\item \textbf{Contexte}
  On possede un \textbf{ensemble de données} avec 
  
  \begin{enumerate}
    \item {une \textbf{variable dépendante} dont on aimerait pouvoir prévoir l'apparition}
    \item {et des \textbf{variables explicative} qui nous serviront à expliquer (prévoir) la variable dépendante.}
  \end{enumerate}
  
  Cet ensemble est donc \textbf{déscriptif} \textit{puisqu'il décrit une situation observée}, et l'on aimerait qu'il devienne un ensemble\textbf{ predictif} \textit{pour prédire la realisation de situations.}

On va essayer de répartir la variable dépendante dans différentes classes.

  \begin{description}
\begin{framed}
    \item[Une classe] : Groupement d'observation apparaissant sous des
      conditions semblables.
\end{framed}
  \end{description}

\item \textbf{Clustering}

  \begin{description}
    \item[Clustering] : Volonté de regrouper les données en groupes bien séparé (\textit{Clusters}). $\rightarrow$ Trouver des ensembles de données bien séparé est souvent rare
  \end{description}

Il existe deux types de regroupement : 
\begin{itemize}
  \item celles basé sur la proximité (\textit{càd qui effectue
les calculs sur la matrice de distante et/ou similarité})
\item celles basé sur les variables des données (\textit{càd qui effectue les calculs sur les
  données})
  \end{itemize}

  \begin{description}
\begin{framed}
\item[Notion de proximité] : Elle permet le regroupement sur base de similaritées/différences
  entre les différentes données. \\
  \underline{Note:} On utilise une matrice de proximité pour définir les proximités.
\end{framed}
  \end{description}


Il y a 4 etapes dans le clustering:
\begin{enumerate}
\item \textbf{Le "nettoyage" (pre-processing) des données}
Effacer les données redondantes et correlé et effacer les outliers.
Effectuer un PCA (numérique) ou une MCA (categorielle), pour réduire l'espace des données
\item \textbf{Le choix des proximités}
On crée la matrice de distance et de similarité (Dans les cours il définis une 6-7 methodes de calcul des distances et des similarité)
\item \textbf{La validation du clustering} On regarde si il y a vraiment des groupes bien séparé
\item \textbf{L'interpretation du clustering}
\end{enumerate}


\item \textbf{Fuzzy K-means}

Le debut de l'explication reviens à redefinir l'algorithme K-Means, nous verrons plus tard dans l'explication la différence entre les deux techniques. 

Cet algorithme est un algorithme de regroupement basé sur les variables (les calculs se passeront directement sur les données).

Il se passe en 4 etapes:
\begin{enumerate}
\item On initialise k centres de gravité (centroid) représentant k groupements.
\item On alloue chaque donnée au groupe qui lui est le plus proche
\item On recentre les centroid par rapport aux données
\item On repete (b) et (c)
\item On arrete quand l'algorithme converge
\end{enumerate}

\begin{framed}
  \textbf{Notion d'inertie} indispensable à la justification de cette méthode. Il existe plusieurs
  inertie :
Soit un groupe $G_k (k: 1\rightarrow K)$
\begin{itemize}
  \item L'inertie dans un groupe (\textcolor{red}{intra-class}): $I_k = \sum_{i \in G_k}d^2(x_i,g_k)$
  \item L'inertie de \textcolor{red}{tout} les groupes: $I_W = \sum_{k=1}^{K} I_k$
  \item L'inertie entre les groupes (\textcolor{red}{inter-class}): $I_B = \sum_{k=1}^{K}|G_k|d^2(g_k,g)$
  \item L'inertie \textcolor{red}{totale}: $I_{tot} = I_W + I_B$
\end{itemize}
\end{framed}

L'interet de l'algorithme est de reduire au maximum $I_W$ et d'augmenter $I_B$ afin que les groupes soit le plus séparé possible. 
Comme $I_{tot}$ vaut la somme de ces deux éléments, il est évident que réduire $I_W$ reviendra à augmenter $I_B$.

\underline{Remarque} : si $K$ augmenter alors $I_W$ diminue. Il faut donc trouver le bon nombre de groupements. 

\paragraph{Spécificité de Fuzzy k-means}
Dans cet algorithme, les appartenance à un groupement $p_i(k)$ sont flou (traduction de fuzzy). On fixe les \textbf{contraintes} suivantes.

$$\sum_{k=1}^K p_i(k)=1$$
$$- \sum_i \sum_{k=1}^K p_i(k) log(p_i(k)) = E$$

Avec $E$ le niveau d'entropie. L'entropie etant la quantité d'ordre/de désordre entre les points (pas certains, effectuer une verfication). On va utiliser un lagrangien.

$$L = \sum_{k=1}^K \sum_i p_i(k) ||x_i-g_k||^2 + \sum_i \lambda_i[\sum_{k=1}^K p_i(k) - 1] + \mu [\sum_i\sum_{k=1}^K p_i(k) log p_i(k) + E]$$

On va calculer l'appartenance de chaque points en derivante en fonction de $p_i(k)$

$$\frac{\partial L}{\partial p_i(k)} = 0$$

On obtiens pour un $k$ et $i$ fixe

$$||x_i - g_k||^2 + \lambda_i + \mu (1 + log p_i(k)) = 0$$
$$log p_i(k) = - (\frac{\lambda_i}{\mu} + 1) - \frac{||x_i - g_k||^2}{\mu}$$
$$p_i(k) = \frac{e^{-\frac{||x_i-g_k||^2}{\mu}}}{\sum_{k=1}^m e^{-\frac{||x_i-g_k||^2}{\mu}}}$$

Maintenant on va utiliser la même technique pour calculer les centres de gravité.

$$\frac{\partial L}{\partial g_k} = 0$$

On obtiens

$$\sum_i p_i(k)(x_i-g_k) = 0$$
$$g_k = \frac{\sum_i p_i(k) x_i}{\sum_i p_i(k)}$$

On doit donc faire boucler le programme sur ces deux formules qui calculent respectivement l'appartenance à un groupe k et la position du centre de gravité du groupe.

\end{enumerate}

\end{document}
