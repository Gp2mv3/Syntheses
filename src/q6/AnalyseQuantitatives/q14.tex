\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{float}
\usepackage{pict2e}
\usepackage{framed}
\usepackage{amssymb}


\title{Analyse de données quantitative\\Question 14}
\date{}

\begin{document}

\maketitle

\textit{Dérivez et explicitez la méthode permettant d'obtenir les axes factoriels d'une analyse des correspondances multiple. Interprétez ces résultats - par exemple, que représentent les valeurs propres et comment peut-on obtenir les coordonnées des observations dans le système des axes principaux? Décrivez quelques méthodes de sélection de features.}

\begin{enumerate}

\item \textbf{Contexte}

\begin{figure}[H]
\centering
\begin{tabular}{ll}
\textbf{Type} & \textbf{Contexte} \\
\hline
Analyse en composante principale & Variables numériques\\
Simple analyse des correspondance& Deux variables catégorielles\\
Multiple analyse des correspondance & $p$ variables catégorielles
\end{tabular}
\end{figure}

\item \textbf{Qu'est-ce l'analyse des correspondances multiples}

  L'analyse des correspondance multiple est une \textbf{généralisation de la simple analyse des correspondance} (\textit{qui récupérait un ensemble de variables numérique à partir de variables catégoriques, mais avec seulement deux variables catégoriques}).

\begin{figure}[H]
\centering
\begin{tabular}{ll}
\textbf{Variable} & \textbf{Description} \\
\hline
$y_i$ & de nouvelles variables catégorielle\\
$x_i$ & les variable originales\\
$p$ & le nombre de variables
\end{tabular}
\end{figure}

\paragraph{Objectif :} Trouver une combinaison linéiare $y_i = u_i^Tx^i$ qui maximise la somme des associations entre les variables catégorielles. 

$\rightarrow$ On va donc choisir l'axe qui conserve le maximum de relation entre les variables. 

\textbf{Attention:} le vecteur $x^i$ est un vecteur de modalité pour chaque variable. Par exemple: si j'ai une variable \textit{nationalité} avec 3 nationalités: francais, belge, serbe, alors pour une observation $i$ le vecteur sera soit $[0,0,1]^T$, soit $[0,1,0]^T$ et $[1,0,0]^T$

\textbf{Attention}: les variables ne sont pas centrer et donc la covariance n'est pas centrée alors qu'elle devrait l'être par définition.


\item \textbf{Comment obtenir les axes?}

\begin{framed}
Notre but va donc être d'essayer de maximiser la somme des covariances
\begin{eqnarray}
\sum_{i=1}^p\sum_{j=1}^pcov(y_i,y_j)&=&\sum_{i=1}^p\sum_{j=1}^pE[y_iy_j]\\
&=&E\left[\sum_{i=1}^p\sum_{j=1}^py_iy_j\right]\\
&=&E\left[(\sum_{i=1}^py_i)^2\right]\\
&=&E\left[(\sum_{i=1}^pu_i^Tx^i)^2\right]\\
&=&E\left[(u^Tx)^2\right]\\
&=&E\left[(u^Txu^Tx)\right]\\
&=&E\left[(u^Txx^Tu)\right]\\
&=&u^TE[xx^T]u\\
&=&u^TFu
\end{eqnarray}
De plus, la somme des variance est constante et égale à $p$ par contrainte
$$\sum_{i=1}^pcov(y_i,y_i) = \sum_{i=1}^pvar(y_i)=p$$
\end{framed}

Nous obtenons donc $$\sum_{i=1}^p\sum_{j=1}^pcov(y_i,y_j)=u^TFu  \textrm{ avec }  F=E[xx^T]$$ 
Il va falloir maximiser cette somme.

\begin{framed}
Calculons maintenant la variance
\begin{eqnarray}
var(y_i)&=&E[y_iy_i]\\
&=&E[u_i^Tx^iu_i^Tx^i]\\
&=&u_i^TE[x^i(x^i)^T]u_i\\
&=&u_i^TD_iu_i
\end{eqnarray}
$D_i$ est une matrice diagonale contenant la \textbf{probabilité a priori} d'observer un attribut $k$ (\textit{en effet sur la diagonal il y a la proportion des individus avec un attribut $k$}).
$$[D_i]_{kk}=E[x^i_k]$$
\end{framed}

\paragraph{La somme des variances} vaut donc $u^TDu$, avec $D$ une matrice diagonale et $D_i$ sur sa diagonale. 

\begin{framed}
On va maintenant essayer de maximiser la somme des covariances avec un lagrangien. 
\begin{description}
  \item[Rappel] : les matrices $F$ et $D$ sont symétriques, dès lors leur transposition n'a aucun effet.
  \item[Ligne 19 à 20] : $\sum_{i=1}^pvar(y_i)=p=u^TDu$
\end{description}
\begin{eqnarray}
\mathcal{L} &=& u^TFu + \lambda(p-u^tDu)\\
\frac{\partial \mathcal L}{\partial_u} &=&  u^TF - \lambda u^tD = 0\\
&=&  F^Tu - \lambda D^Tu = 0\\
&=&  Fu - \lambda Du = 0
\end{eqnarray}

$\rightarrow$ Nous obtenons un problème de vecteurs propres/valeurs propres. 

Multiplions l'équation par $u^T$ :
\begin{eqnarray}
u^TFu &=& u^T\lambda Du\\
\frac{u^TFu}{u^TDu} &=& \lambda
\end{eqnarray}
\begin{eqnarray}
\frac{u^TFu}{p} &=& \lambda
\end{eqnarray}
\end{framed}
Nous allons devoir trouver le vecteur propre avec la plus grande valeur propre associée. 

\item \textbf{Interpretation des résultats}

Les valeurs propres sont associés à des vecteurs propres qui forment un nouvel espace pour représenter les différentes variables catégorielles. 

(\textit{la majorité de la réponse à cette question à déja été donné dans les réponses aux autres points})

\item \textbf{Feature Selection}

\textbf{Attention}: Feature Selection $\neq$ Feature Reduction

Il existe trois grandes techniques de \textit{feature selection}:
\begin{enumerate}
\item \textbf{Maximum-relevance}: On calcul l'association entre les variables (par des test de chi-carré ou de student) et on garde les variables qui sont le plus à même d'expliquer la variable dépendante de nos données
\item \textbf{Minimum-redundancy}: On calcul l'association entre les variables et on se débarasse de celles qui sont fortement correlé entre elles
\item \textbf{Stepwize regression}: On enleve/ajoute des variables une a une et on regarde l'effet sur les performance de notre modèle
\end{enumerate}


\end{enumerate}

\end{document}
