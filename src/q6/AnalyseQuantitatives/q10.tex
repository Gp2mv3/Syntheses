\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{float}
\usepackage{pict2e}
\usepackage{framed}
\usepackage{ amssymb }

\title{Analyse de données quantitative\\Question 10}
\date{}

\begin{document}

\maketitle

\textit{Expliquez le fonctionnement d’un réseau de neurones multi-couches dans le cadre de la classification. Décrivez et dérivez l’algorithme de rétro-propagation de l’erreur dans le cas d’un réseau de neurones artificiels multi-couches}

\begin{enumerate}
\item \textbf{Structure}

  \begin{description}
    \item[Réseau de neurone artificiel] est un modèle inspiré du modèle biologique.
    \item {\underline{Composition} : 
\begin{itemize}
\item Une fonction de coût (cfr. construction)
\item Un algorithme d'entrainement (cfr. construction)
\item Une fonction d'activation
\item Des poids
\end{itemize} }
\end{description}

\begin{figure}[H]
\centering
\begin{picture}(300,120)
% Vecteur
\put(10,90){$x_1$}
\put(10,70){$x_2$}
\put(10,50){$...$}
\put(10,30){$x_p$}
\put(5,20){\framebox(20,80)}
% Poids
\put(70,90){$w_1$}
\put(70,70){$w_2$}
\put(70,50){$...$}
\put(70,30){$w_p$}
\put(65,20){\framebox(20,80)}
% Fleche
\put(30,30){\vector(1,0){30}}
\put(30,90){\vector(1,0){30}}
\put(90,90){\vector(1,0){30}}
\put(90,30){\vector(1,0){30}}
% Somme
\put(130,20){\line(0,1){80}}
\put(130,100){\line(1,0){20}}
\put(130,20){\line(1,0){20}}
\put(150,100){\line(2,-4){20}}
\put(150,20){\line(2,4){20}}
\put(140,58){$\sum$}
% Biais
\put(175,60){\vector(1,0){40}}
\put(220,50){\framebox(20,20){$F()$}}
\put(195,60){\line(0,1){30}}
\put(195,100){\circle{20}}
\put(189,98){$w_0$}
% Out
\put(245,60){\vector(1,0){40}}
\end{picture}
\caption{Perceptron - Neurone Artificiel - Unité}
\end{figure}

\begin{description}
  \item {La \textbf{fonction d'activation} sera booléenne.}
  \item {Soit les observations avec le biais $x=[1,x_1,...,x_p]^T$, nous obtenons à la sortie d'un perceptron
      $$y = F\left[\sum_{i=1}^p w_ix_i + w_0\right] = F[W^Tx] $$ }
  \end{description}

Le tableau suivant contient quelques exemples de fonctions d'activation
\begin{figure}[H]
\centering
\begin{tabular}{lc}
Nom & Fonction $F(x)$\\
\hline
Identity & $x$ \\
Heaviside & $\left\{\begin{array}{ll}0 & x < 0\\ 1 & x \geq 0\end{array}\right.$\\
Sigmoïd & $\frac{1}{1+exp[-x]}$\\
\hline
\end{tabular}
\caption{Exemples de fonctions d'activation}
\end{figure}

Un réseau de neuronne se compose aussi de plusieurs couches qui forment le réseau. Il y a une couche d'entrée (input), une valeur de sortie et des couches cachées. 
Les valeurs des noeuds d'entrées correspondront aux valeurs du vecteur d'observation et elle vont passer de couche en couche (chaque perceptron recevra les valeurs de la couche précédente). Le denier neurone, la sortie, renverra la valeurs estimée d'une certaine variable.

\item \textbf{Construction}

  La construction d'un système de neuronne artificiel se fait par \textbf{itération}. Nous allons définir un palier à partir duquel la différence entre la valeur prédite à la sortie du réseau et la valeur réelle est suffisant. A ce moment là nous arreterons les itération.

  \begin{enumerate}
    \item Au départ nous allons générer les poids \textbf{aléatoirement}, et tester les valeurs. 
    \item Nous allons ensuite utiliser notre fonction de couts afin de pouvoir mesurer la distorsion qu'il existe entre les vrais valeur et les valeurs prédite. 
    \item Et nous allons ajuster les poids en essayant d'améliorer cette mesure de distorsion.
  \end{enumerate}

Le nombre de couche caché va aussi avoir un impact non négligeable sur le résultat du réseau. 

D'un points de vue statistique, les réseau de neurones effectuent une \textit{régression non-linéaire}.

\item \textbf{Rétropropagation}

  La \textbf{rétropropagation} est un terme pour parler de l'algorithme qui va estimer les poids et servir à l'entrainement du réseau. Soient 
$$\begin{array}{ll}
y_i(k)=y_i(x_k) & valeurs\ attendue\\
\widehat{y}_i^{(l)}(k)=\widehat{y}_i^{(l)}(x_k) & activation\ neurone\ i\ de\ niveau\ l\ pour\ observation\ x_k\\
w_{ji}^{(l)} & poids\ j\ neuronne\ i\ niveau\ l\\
w_{0i}^{(l)} & biais\\
p(l) & nombre\ de\ neuronne\ au\ niveau\ l\\
N & nombre\ d'observation\\
L & nombre\ de\ couches\\
F & Fonction d'activation sigmoid\\
S & Fonction d'activation multinomial logistic
\end{array}$$

On a la prédiction au niveau $l$ qui est égal à
$$\widehat{y}_i^{(l)}(k) = F[\sum_{j=1}^{p(l-1)}w_{ji}^{(l)}\widehat{y}_i^{(l-1)}(k)+w_{0i}^{(l)}]$$
Et au niveau $L$ (la sortie)
$$\widehat{y}_i^{(L)}(k) = S[\sum_{j=1}^{p(L-1)}w_{ji}^{(L)}\widehat{y}_i^{(L-1)}(k)+w_{0i}^{(L)}]$$

On peut utiliser deux types de fonction de cout:
\begin{itemize}
\item \textbf{Least-square error} $$C = -\frac{1}{2}\sum_{k=1}^n\sum_{i=1}^q[\widehat y_i^{(L)}(k)-y_i(k)]^2$$
\item \textbf{Log likelihood} (pour les problèmes de classification) $$C = log(L) = \sum_{k=1}^n\sum_{i=1}^q y_i(k)log(\widehat y_i^{(L)}(k))$$
\end{itemize}

\begin{framed}
On va essayer de minimiser la fonction de log likelihood
\begin{eqnarray}
\mathcal{L} &=& \sum_{k=1}^n \sum_{i=1}^q y_i(k) log\left[\widehat{y}_i^{(L)}(k)\right]\\
&& +\sum_{k=1}^n\sum_{l=1}^{L-1}\sum_{i=1}^{p(l)}\lambda_i^{(l)}(k)\left[\widehat{y}_i^{(l)}(k) - F[\sum_{j=1}^{p(l-1)}w_{ji}^{(l)}\widehat{y}_i^{(l-1)}(k)+w_{0i}^{(l)}]\right]\\
&& +\sum_{k=1}^n\sum_{i=1}^{p(L)}\lambda_i^{(L)}(k) \left[\widehat{y}_i^{(L)}(k)-S[\sum_{j=1}^{p(L-1)}w_{ji}^{(L)}\widehat{y}_i^{(L-1)}(k)+w_{0i}^{(L)}]\right]
\end{eqnarray}
On va s'amuser à dériver $\frac{\partial \mathcal{L}}{\partial \widehat{y}_j^{(L)}(k)}=0$, nous obtenons simplement,
\begin{eqnarray}
\frac{y_i(k)}{\widehat y_i^{(L)}(k)}+\lambda_i^{(L)}(k) &=& 0 \\
\lambda_i^{(L)}(k) &=& -\frac{y_i(k)}{\widehat y_i^{(L)}(k)}
\end{eqnarray}
Maintenant dérivons $\frac{\partial \mathcal{L}}{\partial \widehat{y}_j^{(l)}(k)}=0$. Nous n'allons garder que la partie de l'equation qui n'est pas nulle. Ensuite nous allons exprimer l'equation en fonction de la couche suivante.
\begin{eqnarray}
\lambda_i^{(l)}(k)\left[\widehat{y}_i^{(l)}(k) - F[\sum_{j=1}^{p(l-1)}w_{ji}^{(l)}\widehat{y}_i^{(l-1)}(k)+w_{0i}^{(l)}]\right]\\
\lambda_i^{(l)}(k)\left[\widehat{y}_i^{(l+1)}(k) - F[\sum_{j=1}^{p(l)}w_{ji}^{(l+1)}\widehat{y}_i^{(l)}(k)+w_{0i}^{(l+1)}]\right]\\
\lambda_i^{(l)}(k)\widehat{y}_i^{(l+1)}(k) - \lambda_j^{(l+1)}(k)F[\sum_{j=1}^{p(l)}w_{ji}^{(l+1)}\widehat{y}_i^{(l)}(k)+w_{0i}^{(l+1)}]
\end{eqnarray}
Dérivons
\begin{eqnarray}
\lambda_i^{(l)}(k)- \sum_{j=1}^{p(l)} \lambda_j^{(l+1)}(k)w_{ji}^{(l+1)}{F'}_j^{(l+1)}(k)&=& 0\\
\lambda_i^{(l)}(k)&=& \sum_{j=1}^{p(l)} w_{ji}^{(l+1)}[\lambda_j^{(l+1)}(k){F'}_j^{(l+1)}(k)]\\
\end{eqnarray}
\textbf{DANS LE COURS LA SOMME VA JUSQUE p(l+1), j'imagine que c'est une erreur}

Si l'on pose $\delta_i^{(l)}(k) = \lambda_j^{(l)}(k){F'}_j^{(l)}(k)$ on obtiens
\begin{eqnarray}
\delta_i^{(l)}(k) &=&\lambda_j^{(l)}(k){F'}_j^{(l)}(k) \\
&=& {F'}_j^{(l)}(k)\sum_{j=1}^{p(l)} w_{ji}^{(l+1)}[\lambda_j^{(l+1)}(k){F'}_j^{(l+1)}(k)]\\
&=& {F'}_j^{(l)}(k)\sum_{j=1}^{p(l)} w_{ji}^{(l+1)}\delta_j^{(l+1)}(k)
\end{eqnarray}
\textbf{Une fois de + DANS LE COURS LA SOMME VA JUSQUE p(l+1), j'imagine que c'est une erreur}

Et pour la dernière couche, vu que l'on ne peut pas l'exprimer en fonction de la couche suivante, on doit utiliser la least square error
\begin{eqnarray}
\delta_i^{(l)}(k) &=& -{F'}_{i}^{(L)}(k)[\widehat y_i^{(L)}(k)-y_i(k)]
\end{eqnarray}

On ne peut pas dériver en fonction de $w_{ji}^{(l)}$ car c'est beaucoup trop compliqué, alors on doit utiliser un algorithme à gradient ascendant. 
$$w_{ji}^{(l)}(t+1) \leftarrow w_{ji}^{(l)}(t) + \eta\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}}$$
avec
$$\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \sum_{k=1}^n\delta_i^{(l)}(k)\widehat y_j^{(l-1)}(k)$$
\end{framed}

Il y a donc 6 étapes à répeter avant la convergence:
\begin{enumerate}
\item Envoyer les données dans la couche d'entrée $l=0$ avec $\widehat y^{(0)}(k)=x_k$
\item Propager les données jusqu'a la sortie en utiliser les fonctions $F$ et $S$
\item Calculer l'erreur à la dernière couche $L$ (ou pour un problème de classification le log-likelihood)\footnote{les dérivées que j'ai faites peuvent être réalisée pour la fonction d'erreur, amusez vous bien}
\item Calculer l'erreur à l'envers par la retropropagation
$$
\left\{
\begin{array}{l}
\delta_i^{(l)}(k) = -{F'}_{i}^{(L)}(k)[\widehat y_i^{(L)}(k)-y_i(k)]\\
\delta_i^{(l)}(k) = {F'}_j^{(l)}(k)\sum_{j=1}^{p(l)} w_{ji}^{(l+1)}\delta_j^{(l+1)}(k)
\end{array}
\right.
$$
\textbf{JE ME REPETE MAIS DANS LE COURS LA SOMME VA JUSQUE l+1}
\item On calcul le gradient $$\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}} = \sum_{k=1}^n\delta_i^{(l)}(k)\widehat y_j^{(l-1)}(k)$$
\item On mets à jour les poids
$$w_{ji}^{(l)}(t+1) \leftarrow w_{ji}^{(l)}(t) + \eta\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}}$$
\end{enumerate}


\item \textbf{Avantages et inconvénients}

  \underline{Avantages}
\begin{itemize}
\item Les prédictions sont précises
\item Une prediction rapide
\end{itemize}
\underline{Inconvénients}
\begin{itemize}
\item Entraintement assez long
\item Difficile à optimiser
\item Difficile de comprendre les résultats
\item Pas facile à extraire les données
\end{itemize}

\end{enumerate}

\end{document}
