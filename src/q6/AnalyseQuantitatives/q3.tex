\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{framed}


\title{Analyse de données quantitative\\Question 3}
\date{}

\begin{document}

\maketitle

\textit{Dérivez de manière détaillée les règles de décision à appliquer dans le cas d’un problème de classification à plus de deux classes pour lequel les différents types d’erreur n’ont pas le même coût. Expliquez également en quoi consiste un problème de classification avec «reject option» et en quoi cela affecte la méthode de classification.}

\begin{enumerate}
\item \textbf{A quoi sert d'introduire un coûts?}

  Il est interessant des fois de considerer \textbf{l'impact} que pourrait avoir une fausse prédiction. (\textit{Exemple, un mauvais diagnostic dans le cadre médical}).
  Il est donc parfois nécessaire d'induire des coûts lors de nos prédictions.

\item \textbf{Décision à plus de deux classe avec différents types d'erreur.}

  \begin{description}
    \item {Soit le vecteur d'observation $x$}
    \item {Nous allons poser un coût pour chaque erreur possible, avec $c_{ij}$ le coût lorsque l'on associe un vecteur d'observation à une classe $\omega_j$ quand celle-ci appartient en fait à la classe $\omega_i$.}
    \item {\underline{Note} : \textit{Nous avons aussi $c_{ii} = 0$ (lorsqu'une prédiction est vraie il n'y a pas de coût) et $c_{ij} \geq 0$}}.
  \end{description}

Nous allons maintenant établir le calcul du risque d'une prédiction, soient $y$ la vrai classe et $d$ la décision prise, nous avons
$$R = \sum_{i=1}^q\sum_{j=1}^qP(y=\omega_i, d=\omega_j)c_{ij}$$

Objectif : minimiser le risque.

\begin{framed}
  \begin{description}
  \item [Ligne 2] : il faut utiliser une somme si les valeurs sont discrète. 
\item [Ligne 3] : nous utilisons $P(X \cap Y) = P(X,Y) = P(Y|X)P(X)$.
    \item [Ligne 4 à 5] : on soumet que la décision ne peut pas dépendre d'un evenement $y$ n'ayant pas encore eu lieu.
  \end{description}

\begin{eqnarray}
R &=& \sum_{i=1}^q\sum_{j=1}^qP(y=\omega_i, d=\omega_j)c_{ij}\\
&=& \int_x \sum_{i=1}^q\sum_{j=1}^qP(x,y=\omega_i, d=\omega_j)c_{ij} dx\\
&=& \int_x \left[\sum_{i=1}^q\sum_{j=1}^qP(y=\omega_i, d=\omega_j|x)c_{ij}\right] P(x) dx\\
&=& \int_x \left[\sum_{i=1}^q\sum_{j=1}^qP(d=\omega_j|y=\omega_i,x)P(y=\omega_i|x)c_{ij}\right] P(x) dx\\
&=& \int_x \left[\sum_{i=1}^q\sum_{j=1}^qP(d=\omega_j|x)P(y=\omega_i|x)c_{ij}\right] P(x) dx
\end{eqnarray}
En posant $u_j(x) = P(d = \omega_j|x)$ (juste pour mettre en evidence qu'il s'agit du terme qui variera), on obtiens
\begin{eqnarray}
R &=& \int_x \left[\sum_{i=1}^q\sum_{j=1}^qu_j(x)P(y=\omega_i|x)c_{ij}\right] P(x) dx
\end{eqnarray}
Comme tout les termes sont positifs cela revient à minimiser le risque conditionnel suivant:
\begin{eqnarray}
R(u|x) &=& \sum_{i=1}^q\sum_{j=1}^qP(y=\omega_i|x)u_j(x)c_{ij}
\end{eqnarray}
Le \textit{vecteur de choix} $u(x) = [u_1(x), ..., u_j(x), ..., u_q(x)]^T$ avec la contrainte $\sum_{j=1}^q u_j(x) = 1$ (c'est une probabilité c'est assez évident) ou encore avec $e=[1, ..., 1, ...,1]^T$ on a $u^Te=1$.
\end{framed}

La décision optimale va donc être
$$u^*(x) = argmin_{u;u^Te=1}[R(u|x)]$$
\begin{framed}
Maintenant nous allons remplacer la probabilité par son estimation et puis on aura le coût espéré $\widehat{l}_j(x) = \sum_{i=1}^q \widehat{y}_i(x)c_{ij}$.
\begin{eqnarray}
\widehat{R}(u|x) &=& \sum_{j=1}^q[\sum_{i=1}^q\widehat{y}_i(x)c_{ij}]u_j(x) \\
&=& \sum_{j=1}^q\widehat{l}_j(x)u_j(x)\\
u^*(x) &=& argmin_{u;u^Te=1}\left[\sum_{j=1}^q\widehat{l}_j(x)u_j(x)\right]
\end{eqnarray}
\end{framed}

Le calcul de minimisation est en fait assez simple, la décision optimale est $k=argmin_j[\widehat{l}_j(x)]$ et l'on ecris 
$$
\left\{
\begin{array}{ll}
u^*(x) = 1 & j=k\\
u^*(x) = 0 & j\neq k
\end{array}
\right.
$$
Exemple: \textit{imaginons le cas à 3 classes. On doit minimiser $l_1u_1+l_2u_2+l_3u_3$. Imaginons les coûts $l_1=4\ \ l_2 = 5\ \ l_3=2$, on veut minimiser $4u_1+5u_2+2u_3$. Il semble évident que les valeurs qui minimisent ce calcul sont $u_1=u_2=0$ et $u_3 = 1$}.

\item \textbf{Reject option dans les méthodes de classification}

  Dans une reject option, on pose un seuil au dessus duquel il est nécessaire de rejetter une prédiction car elle est trop couteuse (\textit{trop dangereuse}). Si nous posons un seuil $\theta$, nous allons rejeter la décision de l'appartenance d'un élément à une classe $\omega_k$ si tout les coûts dépassent le seuil ($l_k(x) = min_j[l_j(x)] > \theta$). Toute ces observations seront stocké dans une classe de rejet $\omega_0$. 

\end{enumerate}
\end{document}
