\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{framed}


\title{Analyse de données quantitative\\Question 15}
\date{}

\begin{document}

\maketitle

\textit{Dérivez et expliquez la technique classique de « multidimensional scaling ». Démontrez la formule permettant de déduire les produits scalaires en fonction des distances et décrivez ses liens avec l’analyse en composantes principales. Quel est finalement la procédure permettant
de représenter les données sur base des distances ?}

\begin{enumerate}

\item \textbf{Contexte}

\item \textbf{Multidimensional scaling}

  La technique du \textbf{multidimensional scaling} permet de représenter $n$ individus (données), entre lesquels nous avons une notion de \textbf{proximité et de distance}, dans un \textbf{espace euclidien}.

  \begin{description}
    \item {Pour cela nous allons calculer la matrice des distance euclidienne ou des similarité
$$d^2_{ij}=[D]_{ij}=||x_i-x_j||^2$$
$$s_{ij} = [S]_{ij}$$}
\item {Il est aussi facile de passer d'une matrice des similarité à une matrice de distance
    $$d_{ij}=(s_{ii}-2s_{ij}+s_{jj})^{1/2}$$}
  \item {On peut aussi utiliser dans notre calcul une matrice de produits entre les observations, qui pourra nous être utile pour récuperer les matrices de distances et/ou de similarité.
      $$k_{ij} = [K]_{ij} = x_i^Tx_j$$}
  \end{description}

  Le \textbf{multidimensionnal scaling} s'effectue en deux étapes:
\begin{enumerate}
\item Calculer $K$ à partir de $D$
\item Une fois que $K$ a été trouvé on peut représenter les données dans un espace euclidiens en conservant les distances
\end{enumerate}

\item \textbf{Démonstration}

  Nous pouvons exprimer la \textbf{matrice des distances} en fonction de la 
  \textbf{matrice des produits}. Avec $e = [1,...,1]^T$, nous avons
\begin{eqnarray}
d^2_{ij}&=&||x_i-x_j||^2\\
&=& ||x_i||^2 + ||x_j||^2 - 2x_i^Tx_j\\
&=&k_{ii}+k_{jj}-2k_{ij}\\
D &=& diag(K)*e^T + e*diag(K)^T-2K
\end{eqnarray}

\paragraph{Remarque :} A contrario, il est très difficile d'exprimer la matrice des produits
avec la matrice des distances puisqu'il n'existe pas une seule solution.


Il faut imposer que $K$ soit centré (en effet des distance peuvent être les mêmes mais les points peuvent être à des endroits différents).
$$e^TK = 0$$
Cette formule veut dire que le centre de gravité vaut 0. Donc nous avons $\forall j$
\begin{eqnarray}
\sum_i k_{ij} &=& \sum_i x_i^Tx_j\\
&=& \sum_i x_j^Tx_i\\
&=& x_j^T (\sum_ix_i)\\
&=& 0
\end{eqnarray}

Soit $H = I-\frac{ee^T}{n}$, une matrice, nous avons
$$K = -\frac{1}{2}HDH$$

\paragraph{Propriété de cette matrice $H$}
\begin{enumerate}
\item Multiplier $H$ à un vecteur $x$ centre le vecteur
\begin{eqnarray}
Hx &=& \left(I-\frac{ee^T}{n}\right)x\\
&=& x - \left(\frac{e^Tx}{n}\right)e\\
&=& x - \left(\frac{\sum_{i=1}^n x_i}{n}\right)e\\
&=& x - mean(x)e
\end{eqnarray}
\item Multiplier n'importe quel matrice $M$ par la gauche et par la droite par $H$ ($HMH$) centre la matrice (la somme de ses éléments est nulle) (je dirais que $HM$ centre chaque collone de la matrice $M$ et $MH=HM^T$ ($H$ est symétrique) centre chaque ligne, donc $HMH$ centre ligne et collone donc toute la matrice).
\end{enumerate}

\textbf{Preuve de l'expression avec la matrice $H$}
\begin{framed}
  \begin{description}
    \item[Ligne 15] : $He = (I-ee^T/n)e = e-mean(e)e=e-1*e=e-e=0$
  \end{description}
\begin{eqnarray}
-\frac{1}{2}HDH &=& -\frac{1}{2}H(diag(K)e^T + e*diag(K)^T-2K)H\\
&=& -\frac{1}{2}(Hdiag(K)e^TH + He*diag(K)^TH-2HKH)\\
&=& -\frac{1}{2}(0 + 0-2HKH)\\
&=& HKH \\
&=& K
\end{eqnarray}
\end{framed}

Supposons maintenant que $K$ est une \textbf{matrice de produit valide}. La théorie des matrices nous dit que toute matrice symétrique admet une décomposition spectrale\footnote{On est apparement sensé connaitre ca d'un cours précédent}.
$$K = U\Lambda U^T = (U\Lambda^{\frac{1}{2}})(U\Lambda^{\frac{1}{2}})^T$$
\begin{description}
  \item {Avec $U$ contenant les vecteurs propres de $K$ sur ses collones et $\Lambda$ une matrice diagonale possédant les valeurs propres sur sa diagonale.} 
  \item {Comme $K=XX^T$, on peut définir que $X=U\Lambda^{\frac{1}{2}}$.}
\end{description}

\textbf{Attention}: Pour que les données soit réeeles il faut que $\Lambda$ soit non-négative (on fait une racine carré) et donc $K$  doit être positif semi-définies ($\forall x$ on a $xKx\geq0$). Si $K$ n'est pas positive semi-définies, les distances initiales ne sont pas euclidienne et ne peuvent être représenté dans l'espace euclidien.

\item \textbf{Procédure de représentation}

  \begin{description}
    \item {Une fois que l'on a la matrice $K$, on cherche maintenant à représenter les données.}

    \item {Il faut montrer que les vecteurs propres de $K$ correspondens aux coordonnées des individus avec les axes de l'ACP (analyse en composante principale) redimensionné (pour rappel l'acp définis de nouveaux axes pour les données afin de réduire la correlation entre les variables).}
  \end{description}
Soit $u_1$ le premier axe (que l'on a obtenus par l'ACP sur les données $X$). Nous allons simplement projeter les données sur les axes de l'ACP et auront donc $Xu_1$ un vecteur propre de $XX^T = K$ (c'est simplement la projection de $X$ sur l'axe $u_1$). 
\end{enumerate}

\end{document}
