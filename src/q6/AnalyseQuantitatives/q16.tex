\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{framed}
\usepackage{float}
\usepackage{color}

\title{Analyse de données quantitative\\Question 16}
\date{}

\begin{document}

\maketitle

\textit{En plus des questions énoncées, synthétisez et structurez le contenu du cours, explicitez ses motivations, les grandes classes de techniques utilisées et les différents moyens de valider les modèles. Décrivez le projet que vous avez réalisé. Quelle méthodologie suivriez-vous si vous étiez confrontés à un problème (1) de classification, (2) de clustering.}

\begin{enumerate}

\item \textbf{Introduction}

Le cours rentre dans le concept d'\textbf{apprentissage machine}. On sait que l'on a plein d'\textbf{information} autour de nous et le but est d'extraire de la \textbf{connaissance} depuis cette information.

Nous rencontrerons deux types de problèmes, les problèmes de classification et de clustering.

Nous possédons $n$ données qui possèdent chacune d'entre elles $p$ variables que nous pouvons rassembler dans une matrice $[X]_{ij}$ avec les observations sur les lignes et les variables (ou caractéristiques) sur les colonnes. On possède plusieurs types de données, les données catégorielles et les données numériques. 

Une \textbf{variable dépendante} est une variable dont ont essaye de predire les conditions d'apparition en se basant sur des \textbf{variables explicatives}.

\item \textbf{Feature selection and extraction (Part II)}

  Cette partie du cours traite du traitement des données. Nous allons essayer de conserver exclusivement les informations interessante en traitant les données. Il existe trois grandes techniques de \textcolor{red}{\textit{feature selection}}:
\begin{enumerate}
\item \textbf{Maximum-relevance}: On calcul l'association entre les variables (par des test de chi-carré ou de student) et on garde les variables qui sont le plus à même d'expliquer la variable dépendante de nos données
\item \textbf{Minimum-redundancy}: On calcul l'association entre les variables et on se débarasse de celles qui sont fortement correlé entre elles
\item \textbf{Stepwize regression}: On enleve/ajoute des variables une a une et on regarde l'effet sur les performance de notre modèle
\end{enumerate}

On peut aussi effectuer des \textcolor{red}{\textit{features extraction}}. Cette technique va consister à créer un nouvel espace de données avec de nouvelles variables en posant une condition préalable.

\begin{enumerate}
\item \textbf{Analyse en composante principale (ACP)}: On va essayer de trouver l'axe qui maximise la variance et essayer de garder un nombre de dimensions qui conserve au moins 80\% de la variance entre les données. Cette technique s'applique à des données numériques

\item \textbf{Simple analyse des correspondances}: C'est l'équivalent de l'ACP mais pour deux variables catégorielles

\item \textbf{Analyse des correspondances multiple}: C'est l'équivalent de l'ACP mais pour $p$ variables catégorielles.

\item \textbf{Analyse des correlations canoniques}: On possède deux ensembles de données $X$ et $Y$ et on va essayer de trouver un espace qui maximise les relations entre les deux ensembles.

\item \textbf{Analyse discriminante}: Comme la ACP mais on va essayer de maximiser la variance entre des groupes de données. La différence avec l'ACP c'est la notion de classification induite dans l'analyse discriminante.

\item \textbf{Multidimensional Scaling}: Cette technique va servir à représenter les données dans un espace euclidien afin de pouvoir les visualiser. On se sert des distances entre les individus ici. 
\end{enumerate}

\item \textbf{Clustering (Part III)}

  \begin{description}
    \item[Clustering] : Volonté de regrouper les données en groupes bien séparé (\textit{Clusters}). $\rightarrow$ Trouver des ensembles de données bien séparé est souvent rare
  \end{description}

Il existe deux types de regroupement : 
\begin{itemize}
  \item celles basé sur la proximité (\textit{càd qui effectue
les calculs sur la matrice de distante et/ou similarité})
\item celles basé sur les variables des données (\textit{càd qui effectue les calculs sur les
  données})
  \end{itemize}

  \begin{description}
\begin{framed}
\item[Notion de proximité] : Elle permet le regroupement sur base de similaritées/différences
  entre les différentes données. \\
  \underline{Note:} On utilise une matrice de proximité pour définir les proximités.
\end{framed}
  \end{description}


  Il y a 4 \textcolor{red}{etapes} dans le clustering:
\begin{enumerate}
\item \textbf{Le "nettoyage" (pre-processing) des données}
Effacer les données redondantes et correlé et effacer les outliers.
Effectuer un PCA (numérique) ou une MCA (categorielle), pour réduire l'espace des données
\item \textbf{Le choix des proximités}
On crée la matrice de distance et de similarité (Dans les cours il définis une 6-7 methodes de calcul des distances et des similarité)
\item \textbf{La validation du clustering} On regarde si il y a vraiment des groupes bien séparé
\item \textbf{L'interpretation du clustering}
\end{enumerate}

Il existe aussi 2 \textcolor{red}{types} de clustering:
\begin{enumerate}
\item \textbf{Hierarchical clustering}: On va construire les groupes par regroupement de sous-groupes ou par division d'un grand groupe
\item \textbf{Fixed numer of clusters}: On fixe le nombre de groupes à l'avance et l'on répartit les données dans les différents groupes.
\end{enumerate}

\item \textbf{Classification (Part IV)}

Le but de la \textbf{classification supervisé} est donc de trouver un modèle pour pouvoir établir une prédiction sur base de certaines variables. Pour cela, nous utiliserons un \textbf{training set}.

\paragraph{}\textit{Exemple d'utilisation:}
\begin{itemize}
\item Reconnaitre la voix d'une personne (avec comme variable
explicative: la vitesse, la frequence).

\item Prevoir ce qui dans un produit attire un client (sur base des couleurs,
de formes, et aussi des données personelles au client)
\end{itemize}

\paragraph{}La classification se passe en 4 étapes:
\begin{enumerate}
\item \textbf{Construction du modèle}

  Nous allons construire un \textbf{modèle de classification}. 
  
  Il existe deux \textcolor{red}{types} de modèles, 
  \begin{enumerate}
    \item les modèles \textbf{direct} qui vont directement estimer la probabilite d'appartenance d'une observation $x$ à une classe $\omega_k$ ($P(\omega_k|x)$),
    \item et les modèles \textbf{indirect} qui calcul la probabilité d'appartenance à une classe $\omega_k$ puis edéduisent $P(\omega_k|x)$ via le \textit{théorème de Bayes}
  \end{enumerate}

\begin{framed}
\textbf{Théorème de Bayes}
$$P(\omega_k|x)=\frac{P(\omega_k)P(x|\omega_k)}{P(x)}$$
\end{framed}

Voici les \textcolor{red}{différents modèles} vus dans le cours:
\begin{enumerate}
\item \textbf{Decision trees}: On construit un arbre avec une variable à chaque noeuds et chacune des sous branche divise les données de facon binaire ou multiple. La classe majoritaire dans une feuille de l'arbre (un noeud sans sous branche) est selectionnée comme etant la classe du noeud. Introduire une donnée dans l'arbre reviens à la faire parcourir les noeuds jusqu'a atteindre une feuille.
\item \textbf{Nearest Neighbors classifier}: On va prendre les $k$ plus proches voisins d'une données et considerer que la classe la plus proche est celle de l'observation dont on regarde les voisins. Il faut faire attention dans le choix du $k$. Il peut etre interessant aussi de redimenssioner certaines variables afin que chaque variable impacte de la même façon dans le calcul des distances (par exemple comparer des grammes et des metres ca n'a pas de sens il faut ramener sur une echelle commune). 
\item \textbf{Naive Bayes classifier}: Ce classifieur effectue une prediction probabilistique en utilisant le théorème de Bayes. On evalue $P(x|\omega_k)$ comme etant une distribution gaussienne et l'on va trouver $P(\omega_k|x)$.
\item \textbf{Regression logistique}: La regression logistique va directement estimer la probabilité d'appartenance à posteriori en séparant les différentes classes par des hyperplan. \item \textbf{Réseau de neurones multicouches}: C'est un modèle qui utilise différentes couches de neurones. La premiere couche recoit les valeurs de différentes variables, les valeurs vont transiter d'une couche à une autre tout en etant multiplié par des poids et sommé entre elles et la prédiction va sortir à la dernière couche. 
\end{enumerate}

\item \textbf{Comparaison des performances}

  Nous allons calculer pour chaque classification le \textbf{taux d'erreur} qui nous donnera la proportion d'individu ayant été associé à des classes auxquels ils ne devraient pas appartenir. 

$\rightarrow$ On va pouvoir comparer les différentes performances entre les différents modèles en comparant les taux d'erreur.

\item \textbf{Evaluation des performances}

Il intervient aussi dans le calcul des performances la notions d'\textbf{overfitting} et d'\textbf{underfitting}. 
\begin{description}
  \item[L'overfitting] a lieu lorsqu'un modèle est trop précis et s'adapte mal à de nouveaux cas.
  \item[L'underfitting] arrive quand le modèle est trop général au points de nous donner aucune information. 
\end{description}

Pour evaluer les performances, nous allons utiliser un \textbf{test set} ou \textbf{validation set}, un ensemble qui n'a pas ete utilisé pour la construction du modèle mais qui permettra de tester le modèle.

Il existe différentes \textcolor{red}{techniques} pour evaluer les performances:
\begin{enumerate}
\item \textbf{Holdout method}: on divise nos données de base en deux ensembles l'un servira pour construire le modèle et l'autre pour le valider
\item \textbf{Cross-validation}: On divise nos données en $k$ groupes. On va construire le modèle avec $k-1$ groupes et tester avec 1 groupe et repeter l'operation k fois (on test avec chaque groupe).
\item \textbf{Leave-one-out}: C'est une cross-validation mais avec $k$ qui est égal au nombre de données
\item \textbf{Bootstrap}: On choisis aléatoirement des données et chaque donnée choisie est mise dans le training set. La donnée peut etre resélectionnée de multiples fois. On répète l'operation $k$ fois.
\end{enumerate}

\item \textbf{Application à de nouveaux cas}
\end{enumerate}

On peut introduire la notiion de \textbf{coût lors de la décision} qui va associer
à chaque décision un coùt.

$\rightarrow$ On posera une option de rejet, c'est-à-dire un seuil de risque au dessus duquel on prendra la décision de ne prendre aucune décision quand à la classification d'une observation.

\item \textbf{Projet}

Notre projet consistait à tester différents modèles de classification et à comparer les performances entre ceux-ci pour essayer de classifier une base de données que nous avions préalablement choisis. Nous avons appliqué pour cela les notions de feature selection/extraction et de classification vue dans le cours.

\item \textbf{Méthodologie}

  Deux \textcolor{red}{méthodologies} différentes s'applique suivant les types de problèmes:
\begin{enumerate}
\item \textbf{Problème de classification}

  \begin{description}
  \item { Dans le cas d'un \textbf{problème de classification} on va dans un premier temps reduire l'espace des données et enlever les données grandement correlée entre elles.}
    \item {Ensuite nous allons utiliser la cross validation pour tester les performances de plusieurs modèles et evaluer quel modèle est le plus adapté dans notre problème.}
\end{description}

Une fois le modèle sélectionné nous pouvons l'utiliser dans de nouveaux cas.

\item \textbf{Problème de clustering}

  \begin{description}
    \item {Comme pour la classification nous allons réduire l'espace des données.}
    \item {Nous allons ensuite, suivant les données que nous possèdons, choisir un algorithme de clustering et diviser les données en différents groupes.}
  \end{description}

\end{enumerate}

\end{enumerate}

\end{document}
