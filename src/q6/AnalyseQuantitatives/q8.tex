\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{framed}
\usepackage{fourier-orns}

\title{Analyse de données quantitative\\Question 8}
\date{}

\begin{document}

\maketitle

\begin{framed}
\textbf{Rappel}

Soient $A=\left(\begin{array}{l}a\\b\end{array}\right)$ et $B=\left(\begin{array}{l}c\\d\end{array}\right)$, 
$$(A-B)^T(A-B) = ||A-B||^2$$
$$A^TB = B^TA$$
\end{framed}

\textit{Dérivez et explicitez la décomposition de la variance within et between cluster ainsi que la méthode permettant d’obtenir les axes factoriels d’une analyse discriminante. Interprétez ces résultats – comment peut-on interpréter les valeurs propres résultantes.Décrivez quelques méthodes de sélection de features.}

\begin{enumerate}
\item \textbf{Contexte}

  \begin{description}
    \item[Analyse discriminant] est une méthode de \textbf{feature extraction} (\textit{càd qu'on va créer de nouvelles
      dimension pour réduire la corrélation entre les varriables et conserver l'information interessante}).
      C'est aussi une méthode de \textbf{classification}.
  \end{description}

  La \textbf{décompositon de la variance} rentre dans le contexte de l'analyse discriminante.
On va chercher l'axe qui maximise le rapport de la variance entre classe et la variance totale des données projetée (voir schéma slide 48 part 2).
 

  \danger Cela s'applique \textbf{exclusivement} aux données numériques. 

\item \textbf{Decomposition de la variance between et within cluster}

\begin{framed}
Calculons d'abord la variance totale des points. 
\begin{description}
  \item[Ligne 2 à 3] $A^TB = B^TA$
\end{description}

\begin{eqnarray}
\sigma^2 &=& \frac{1}{2n^2}\sum_{i=1}^n\sum_{j=1}^n (x_i-x_j)^T(x_i-x_j)\\
&=& \frac{1}{2n^2}\sum_{i=1}^n\sum_{j=1}^n ((x_i-g)+(g-x_j))^T((x_i-g)+(g-x_j))\\
&=& \frac{1}{2n^2}\sum_{i=1}^n\sum_{j=1}^n (x_i-g)^T(x_i-g)+(g-x_j)^T(g-x_j)+2(g-x_j)^T(x_i-g)
\end{eqnarray}
Or on sait que
\begin{eqnarray}
\sum_{i=1}^n\sum_{j=1}^n (x_i-g)^T(x_i-g)&=&n\sum_{k=1}^n (x_k-g)^T(x_k-g)\\
\sum_{i=1}^n\sum_{j=1}^n (g-x_j)^T(g-x_j)&=&n\sum_{k=1}^n (g-x_k)^T(g-x_k)
\end{eqnarray}
Comme il s'agit de la distance d'un point quelconque au \textbf{centroid} on a
\begin{eqnarray}
\sum_{k=1}^n (x_k-g)^T(x_k-g)&=&\sum_{k=1}^n (g-x_k)^T(g-x_k)
\end{eqnarray}
(3) devient donc
\begin{eqnarray}
\sigma^2 &=& \frac{1}{n}\sum_{i=1}^n (x_i-g)^T(x_i-g)+ \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n(g-x_j)^T(x_i+g)
\end{eqnarray}
De plus, 
\begin{eqnarray}
\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n(g-x_j)^T(x_i+g) &=& 0
\end{eqnarray}
En effet, nous avons la distance d'un point à un autre puis la même distance dans le sens inverse ce qui annule leur somme. 

Au final, nous obtenons
\begin{eqnarray}
\sigma^2 &=& \frac{1}{n}\sum_{i=1}^n (x_i-g)^T(x_i-g)\\
&=& \frac{1}{n}\sum_{k=1}^q\sum_{i\in C(k)} (x_i-g)^T(x_i-g)\\
&=& \frac{1}{n}\sum_{k=1}^q SS(k)
\end{eqnarray}
\end{framed}

$\rightarrow$ Finalement, \textbf{la variance vaut la somme au carré de l'inertie
de toute les classes $k$}.
\paragraph{}
Nous allons maintenant diviser cette somme et mettre en évidence la variance à l'intérieur 
(\textbf{within}) et entre (\textbf{between}) les groupes.

\begin{framed}
Soit $g(k)$ le centre de gravité du groupe $k$
\begin{eqnarray}
SS(k) &=& \sum_{i\in C(k)} (x_i-g)^T(x_i-g)\\
&=& \sum_{i\in C(k)} ((x_i-g(x))+(g(x)-g))^T((x_i-g(x))+(g(x)-g))\\
&=& \sum_{i\in C(k)} (||x_i-g(x)||^2 + ||g(x)-g||^2 + 2(x_i-g(x))^T(g(x)-g))
\end{eqnarray}
On peut annuler la partie de droite car avec $n(k)$ le nombre d'éléments dans $k$,
\begin{eqnarray}
\sum_{i\in C(k)} 2(x_i-g(x))^T(g(x)-g) &=& 2(g(x)-g)^T\sum_{i\in C(k)} (x_i-g(x))\\
g(x) &=& \frac{1}{n(k)} \sum_{i\in C(k)} x_i\\
\sum_{i\in C(k)} 2(x_i-g(x))^T(g(x)-g) &=& 2(g(x)-g)^T\left[\sum_{i\in C(k)} x_i- \sum_{i\in C(k)} g(x)\right]\\
&=& 2(g(x)-g)^T\left[n(k)g(x)- n(k)g(x)\right]\\
&=& 0
\end{eqnarray}
Et donc,
\begin{eqnarray}
SS(k) &=& \sum_{i\in C(k)} (||x_i-g(x)||^2 + ||g(x)-g||^2)\\
&=& \sum_{i\in C(k)} ||x_i-g(x)||^2 + \sum_{i\in C(k)} ||g(x)-g||^2\\
&=& \sum_{i\in C(k)} ||x_i-g(x)||^2 + n(k) ||g(x)-g||^2\\
\end{eqnarray}
\end{framed}

Nous avons montré que $SS(k)$ est \textbf{la somme de l'inertie entre les groupes ($n(k) ||g(x)-g||^2$) et dans un groupe ($\sum_{i\in C(k)} ||x_i-g(x)||^2$)}. 

\paragraph{}Continuons le calcul de la variance

\begin{framed}
\begin{eqnarray}
\sigma^2 &=& \frac{1}{n}\sum_{k=1}^q SS(k)\\
&=& \frac{1}{n}\left[\sum_{k=1}^q\sum_{i\in C(k)} ||x_i-g(x)||^2 + \sum_{k=1}^qn(k) ||g(x)-g||^2\right]\\
&=& \frac{1}{n}\sum_{k=1}^q n(k) \left[\left(\frac{1}{n(k)}\sum_{i\in C(k)} ||x_i-g(x)||^2\right)+ ||g(x)-g||^2\right]\\
&=& \frac{1}{n}\sum_{k=1}^q n(k) [\sigma_{(w)}^2(k)+\sigma_{(b)}^2(k)]\\
&=&\sigma_{(w)}^2+\sigma_{(b)}^2
\end{eqnarray}
\end{framed}

\item \textbf{Obtenir les axes factoriels}

On va chercher maintenant un axe qui maximise la variance entre les différents groupes. Il faut pour cela définir une matrice de projection $\pi = VV^T$ avec $V^TV=1$. Nous allons projeter la variance sur les axes. Un autre rappel

\begin{framed}
\begin{description}
  \item[Ligne 31 à 32] : $VV^T*VV^T = VV^T$ (c'est trivial à montrer mais long).
\end{description}
  Nous allons projeter la variance \textbf{within} cluster $\sigma_{v(w)}^2$. 
\begin{eqnarray}
\sigma_{v(w)}^2 &=& \frac{1}{n}\sum_{k=1}^q\sum_{i\in C(k)} (\pi x_i-\pi g(k))^T(\pi x_i-\pi g(k))\\
 &=& \frac{1}{n}\sum_{k=1}^q\sum_{i\in C(k)}(VV^T x_i-VV^T g(k))^T(VV^T x_i-VV^Tg(k))\\
&=& \frac{1}{n}\sum_{k=1}^q\sum_{i\in C(k)}(x_i-g(k))^T(VV^TVV^T)(x_i-g(k))\\
&=& \frac{1}{n}\sum_{k=1}^q\sum_{i\in C(k)}(x_i-g(k))^TVV^T(x_i-g(k))\\
&=& V^T\left[\frac{1}{n}\sum_{k=1}^q\sum_{i\in C(k)}(x_i-g(k))(x_i-g(k))^T\right]V\\
&=& V^T\left[\frac{1}{n}\sum_{k=1}^qn(k)\left(\frac{1}{n(k)}\sum_{i\in C(k)}(x_i-g(k))(x_i-g(k))^T\right)\right]V\\
&=& V^T\left[\frac{1}{n}\sum_{k=1}^qn(k)W_k\right]V\\
&=& V^TWV
\end{eqnarray}
Maintenant nous allons projeter la variance \textbf{between} avec le même genre de procédé.
\begin{eqnarray}
\sigma_{v(b)}^2 &=& \frac{1}{n} \sum_{k=1}^q n(k) (\pi g(k) - \pi g)^T (\pi g(k) - \pi g)\\
&=& \frac{1}{n} \sum_{k=1}^q n(k) (g(k) - g)^TVV^T(g(k) - g)\\
&=& V^T\left[\frac{1}{n}\sum_{k=1}^q n(k) (g(k) - g)(g(k) - g)^T\right]V\\
&=& V^TBV
\end{eqnarray}
Et enfin la \textbf{variance totale}
\begin{eqnarray}
\sigma_v^2 &=& \frac{1}{n}\sum_{i=1}^n (\pi x_i-\pi g)^T(\pi x_i-\pi g)\\
&=& V^T\left[\frac{1}{n}\sum_{i=1}^n (x_i- g)(x_i- g)^T\right]V\\
&=& V^T\sum V
\end{eqnarray}
\end{framed}

\begin{description}
  \item {On sait que $\sigma_v^2 = \sigma_{v(w)}^2 + \sigma_{v(b)}^2$ et donc $1 = \frac{\sigma_{v(w)}^2}{\sigma_v^2} + \frac{\sigma_{v(b)}^2}{\sigma_v^2}$}
  \item {De plus, la variance est positive.}
\item {Nous avons donc : }
    $$0 < \frac{\sigma_{v(b)}^2}{\sigma_v^2} < 1$$
\end{description}
Notre objectif va être de \textbf{maximiser la variance entre les différents clusters}. C'est à dire
$$max\left(\frac{\sigma_{v(b)}^2}{\sigma_v^2}\right)=max\left(\frac{V^TBV}{V^T\sum V}\right)$$

\begin{framed}
Dérivons. Nous allons pouvoir faire $BV^T = VB$ car les matrices $\sum$, $B$ et $W$ sont des matrices symétriques (\textbf{Evident mais je demande vérification et aussi pour le calcul}).
\begin{eqnarray}
\partial_v\left(\frac{V^TBV}{V^T\sum V}\right) &=& 0\\
\frac{(V^T\sum V)(V^TB)-(V^TBV)(V^T\sum)}{(V^T\sum V)^2} &=& 0\\
(V^T\sum V)(V^TB)-(V^TBV)(V^T\sum) &=& 0\\
(V^T\sum V)BV-(V^TBV)\sum V &=& 0\\
BV &=& \frac{(V^TBV)}{(V^T\sum V)}\sum V
\end{eqnarray}
\end{framed}

Nous allons poser $0 < \lambda= \frac{(V^TBV)}{(V^T\sum V)} < 1$. Il suffit de résoudre
$$\sum^{-1} B V = \lambda V$$

\item \textbf{Interpretation}

$\lambda$ est la valeur propre du vecteur propre qui répond à la maximisation de la variance entre les clusters. Il y a au moins $(q-1)$ valeurs propres différentes de 0. Une fois que nous avons trouvé l'axe nous allons le normaliser et projeter les données sur celui-ci $z_i = V^Tx_i$. Nous allons récuperer de nouvelles dimensions dans lesquelles les données sont bien séparé en plusieurs groupes. 

\item \textbf{Feature Selection}

\textbf{Attention}: Feature Selection $\neq$ Feature Reduction

Il existe trois grandes techniques de \textit{feature selection}:
\begin{enumerate}
\item \textbf{Maximum-relevance}: On calcul l'association entre les variables (par des test de chi-carré ou de student) et on garde les variables qui sont le plus à même d'expliquer la variable dépendante de nos données
\item \textbf{Minimum-redundancy}: On calcul l'association entre les variables et on se débarasse de celles qui sont fortement correlé entre elles
\item \textbf{Stepwize regression}: On enleve/ajoute des variables une a une et on regarde l'effet sur les performance de notre modèle
\end{enumerate}


\end{enumerate}

\end{document}
