\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{framed}


\title{Analyse de données quantitative\\Question 12}
\date{}

\begin{document}

\maketitle

\textit{Dérivez et explicitez la formule à appliquer lorsqu’on rencontre un problème de classification pour lequel les probabilités a priori d’appartenance aux classes des données réelles diffèrent de celles de l’ensemble d’apprentissage. Expliquez également comment estimer les performances d’un modèle de classification (cross-validation, etc) et en quoi cela est important.}

\begin{enumerate}

\item \textbf{Contexte}

  Soient un ensemble de donnée $[X]_{ij}$ tel que les lignes sont les observations et les colonne les variables. Nous allons avoir au sein de ces données une \textbf{variable dépendante} et des 
  \textbf{variables explicative}. 

  $\rightarrow$ Il va falloir se servir des variables explicative pour essayer de prévoir la valeur de la variable dépendante.

\item \textbf{Evaluation du cas}

  Supposons que nous avons un \textbf{classifieur} qui nous fournir une probabilité à posteriori $P_t(y=\omega_i|x)$ avec un certain \textbf{training set} $t$ tel que les valeurs sont
  différentes des valeurs réelles :
$$
\left\{
\begin{array}{l}
P_t(y=\omega_i) \neq P(y=\omega_i)\\
P_t(x|y=\omega_i) = P(x|y=\omega_i)\\
\end{array}
\right.
$$

\begin{framed}
  Nous allons chercher la \textbf{propabilité a posteriori réelle} $P(y=\omega_i|x)$. 
  
  Utilisons Bayes,
$$
\left\{
\begin{array}{l}
P_t(x|y=\omega_i) = \frac{P_t(y=\omega_i|x)P_t(x)}{P_t(y=\omega_i)}\\
P(x|y=\omega_i) = \frac{P(y=\omega_i|x)P(x)}{P(y=\omega_i)}\\
\end{array}
\right.
$$
Des lors reprennons l'egalite au dessus
\begin{eqnarray}
P_t(x|y=\omega_i) &=& P(x|y=\omega_i)\\
\frac{P_t(y=\omega_i|x)P_t(x)}{P_t(y=\omega_i)} &=& \frac{P(y=\omega_i|x)P(x)}{P(y=\omega_i)}\\
P(y=\omega_i|x) &=& \frac{P_t(x)}{P(x)}\frac{P(y=\omega_i)P_t(y=\omega_i|x)}{P_t(y=\omega_i)}
\end{eqnarray}
Posons $f(x) = \frac{P_t(x)}{P(x)}$ et $odds(y=\omega_i) = \frac{P(y=\omega_i)}{P_t(y=\omega_i)}$
\begin{eqnarray}
P(y=\omega_i|x) &=& f(x)*P_t(y=\omega_i|x)*odds(y=\omega_i)
\end{eqnarray}
Et comme $\sum_{i=1}^q P(y = \omega_i|x) = 1$ on peut écrire
\begin{eqnarray}
f(x) &=& \left[\sum_{i=1}^qP_t(y=\omega_i|x)*odds(y=\omega_i)\right]^{-1}
\end{eqnarray}
Et donc on peut écrire une nouvelle formule estimant la probabilité a posteriori.
\begin{eqnarray}
P(y=\omega_i|x) &=& \frac{P_t(y=\omega_i|x)*odds(y=\omega_i)}{\sum_{i=1}^qP_t(y=\omega_i|x)*odds(y=\omega_i)}
\end{eqnarray}
\end{framed}

Lorsque la probabilité n'est pas connue à l'avance ce que l'on peut faire c'est estimer la probabilité avec cette technique et puis réestimer la probabilitée avec l'estimation et iterer ainsi. 


\item \textbf{Importance de calculer la performance d'un modele}

  \begin{description}
    \item{ Il est important de pouvoir évaluer la \textbf{performance d'un modele}, afin de pouvoir choisir quel modèle doit être utiliser pour une situation donnée. En effet, certains modèles fonctionnent mieux dans certaines conditions (\textit{certain marchent mieux avec beaucoup de données, d'autre avec des données de types numérique, ...}).}

    \item {Il est aussi important de pouvoir evaluer la \textbf{complexité d'un modèle}. 
      Si un modele est trop précis, il répondra mal à l'entrée de nouvelles donnée (appellé \textbf{l'overfitting}). A l'inverse, si un modele est trop peu précis, il ne nous apportera aucune information (appellé l'underfitting).}
  \end{description}

\item \textbf{Comment calculer la performance d'un modele}

  Le calcul de performance apparait avec la volonté de \textbf{réduire l'overfitting}. L'idée est d'evaluer la performance sur des données indépendante, c'est à dire des données qui n'ont pas été utilisé lors de la construction du modèle.

  \paragraph{}
Il existe 4 techniques permettant de gerer les données pour la construction du modèles et une fois les modèles construit on regarde la différence entre les previsions attendues sur les données de test et les resultats effectivement obtenus à la sortie du modèle.

Les 4 methodes sont les suivantes:

$\rightarrow$ \textbf{Holdout method}: Avec les données à notre disposition, on divise l'ensemble de ces données en deux : on définis un \textcolor{red}{training set} (\textit{utilisé pour la construction du modèle}) et un \textcolor{red}{validation set} (\textit{utilisé pour tester le modèles}). Les données du validation set sont des observations "sacrifiée" à la verification.

$\rightarrow$ \textbf{Cross Validation}: On divise les données en k groupes de taille égale et on choisi un des groupes comme \textcolor{red}{validation set} et les autres comme \textcolor{red}{training set}. On recommence l'operation k fois, une fois par groupe. 

On calcul dans les deux methodes l'erreur quadratique moyenne, et dans le cas de la Cross-Val, on fait la moyenne des k erreurs.

$\rightarrow$ \textbf{Leave-one-out}: Comme pour la cross-validation mais on divise les données en k groupes de 1 données. 

$\rightarrow$ \textbf{Bootstrap}: On selectionne une donnée aleatoirement et on l'ajoute au training set. Les données peuvent être selectionné plusieurs fois. 

Une version très utilisé du bootstrap est le bootstrap .632, dans lequel on effectue n fois une selection aleatoire,  où n est le nombre de données.
Toute les données qui ne sont pas dans le training set (qui representeront 1-63.2\% des données) formeront le validation set.
Et on repete l'operation k fois.

\end{enumerate}

\end{document}
