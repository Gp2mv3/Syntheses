\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{framed}
\usepackage{color}

\title{Analyse de données quantitative\\Question 5}
\author{Malengreau Symeon, Houtain Nicolas et Kabasele Nicolas}
\date{}

\begin{document}

\maketitle

%QUESTION 5
\textit{Quelles sont les caractéristiques importantes du clustering? Expliquez l’algorithme de clustering de type «k-means». Montrez que cet algorithme réduit l’inertie intra-classe à chaque itération et converge.Expliquez également comment le k-means peut être adapté pour
une matrice de distances au lieu d’une « data matrix » (features)}

\begin{enumerate}
\item \textbf{Contexte}
  On possede un \textbf{ensemble de données} avec 
  
  \begin{enumerate}
    \item {une \textbf{variable dépendante} dont on aimerait pouvoir prévoir l'apparition}
    \item {et des \textbf{variables explicative} qui nous serviront à expliquer (prévoir) la variable dépendante.}
  \end{enumerate}
  
  Cet ensemble est donc \textbf{déscriptif} \textit{puisqu'il décrit une situation observée}, et l'on aimerait qu'il devienne un ensemble\textbf{ predictif} \textit{pour prédire la realisation de situations.}

On va essayer de répartir la variable dépendante dans différentes classes.

  \begin{description}
\begin{framed}
    \item[Une classe] : Groupement d'observation apparaissant sous des
      conditions semblables.
\end{framed}
  \end{description}

\item \textbf{Clustering}

  \begin{description}
    \item[Clustering] : Volonté de regrouper les données en groupes bien séparé (\textit{Clusters}). $\rightarrow$ Trouver des ensembles de données bien séparé est souvent rare
  \end{description}

Il existe deux types de regroupement : 
\begin{itemize}
  \item celles basé sur la proximité (\textit{càd qui effectue
les calculs sur la matrice de distante et/ou similarité})
\item celles basé sur les variables des données (\textit{càd qui effectue les calculs sur les
  données})
  \end{itemize}

  \begin{description}
\begin{framed}
\item[Notion de proximité] : Elle permet le regroupement sur base de similaritées/différences
  entre les différentes données. \\
  \underline{Note:} On utilise une matrice de proximité pour définir les proximités.
\end{framed}
  \end{description}


Il y a 4 etapes dans le clustering:
\begin{enumerate}
\item \textbf{Le "nettoyage" (pre-processing) des données}
Effacer les données redondantes et correlé et effacer les outliers.
Effectuer un PCA (numérique) ou une MCA (categorielle), pour réduire l'espace des données
\item \textbf{Le choix des proximités}
On crée la matrice de distance et de similarité (Dans les cours il définis une 6-7 methodes de calcul des distances et des similarité)
\item \textbf{La validation du clustering} On regarde si il y a vraiment des groupes bien séparé
\item \textbf{L'interpretation du clustering}
\end{enumerate}

\item \textbf{K-means}

Cet algorithme est un algorithme de regroupement basé sur les variables (les calculs se passeront directement sur les données).

Il se passe en 4 etapes:
\begin{enumerate}
\item On initialise k centres de gravité (centroid) représentant k groupements.
\item On alloue chaque donnée au groupe qui lui est le plus proche
\item On recentre les centroid par rapport aux données
\item On repete (b) et (c)
\item On arrete quand l'algorithme converge
\end{enumerate}

\textbf{Note:} Cet algorithme nécessite de fixer arbitrairement les groupes en ``pré-condition''.

\begin{framed}
  \textbf{Notion d'inertie} indispensable à la justification de cette méthode. Il existe plusieurs
  inertie :
Soit un groupe $G_k (k: 1\rightarrow K)$
\begin{itemize}
  \item L'inertie dans un groupe (\textcolor{red}{intra-class}): $I_k = \sum_{i \in G_k}d^2(x_i,g_k)$
  \item L'inertie de \textcolor{red}{tout} les groupes: $I_W = \sum_{k=1}^{K} I_k$
  \item L'inertie entre les groupes (\textcolor{red}{inter-class}): $I_B = \sum_{k=1}^{K}|G_k|d^2(g_k,g)$
  \item L'inertie \textcolor{red}{totale}: $I_{tot} = I_W + I_B$
\end{itemize}
\end{framed}

L'interet de l'algorithme est de reduire au maximum $I_W$ et d'augmenter $I_B$ afin que les groupes soit le plus séparé possible. 
Comme $I_{tot}$ vaut la somme de ces deux éléments, il est évident que réduire $I_W$ reviendra à augmenter $I_B$.

\underline{Remarque} : si $K$ augmenter alors $I_W$ diminue. Il faut donc trouver le bon nombre de groupements. 

\paragraph{Implémentation des étapes de l'algorithme}
\begin{enumerate}
\item Fixer les centres de gravité $g(k)$ en minimisant $I_W$. 
\begin{eqnarray}
I_k &=& \sum_{i \in G_k}d^2(x_i,g_k)\\
    &=& \sum_{i \in G_k}||x_i - g(k)||^2 \\
I_W &=& \sum_{k=1}^K I_k \\
&=& \sum_{k=1}^K \sum_{i \in G_k}||x_i - g(k)||^2 
\end{eqnarray}
Pour minimiser $I_W$, il suffit de $\partial_{g(k)}I_W = 0$. Ce qui nous donne
$$\sum_{i \in G_k}(x_i - g(k)) = 0$$
$$g(k) = \frac{1}{n(k)}\sum_{i \in G_k}x_i$$

\item (Re)allouer les individus $x_i$ dans chaque groupe afin de minimiser $I_W$. Posons $p_i(k)$ la probabilité d'associer un individu i au groupe k.
$$I_W = \sum_i \sum_{k=1}^{K} p_i(k) ||x_i - g(k)||^2$$

\item Comme à chaque iteration on réduit l'inertie entre les groupes et que celle-ci ne peut pas être négative, la procédure converge.

\end{enumerate}

\paragraph{Points négatifs de cette technique:}
\begin{itemize}
\item On doit fixer le nombre de groupe à l'avance
\item Le resultat est très dépendant des centroids choisis de façon initial.
\item Il arrive souvent qu'il y ait un groupe vide
\end{itemize}

\item \textbf{Distances au lieu d’une « data matrix »}

L'algorithme peut facilement être adapté pour fonctionner avec une matrice de distance à la place d'une matrice de donnée. Pour chaque groupe $G_k$ on définis un prototype $p_k$ qui est un individus qui appartient au grupe. 

La distance entre un individu et un groupe sera la distance du prototype du groupe.
$$dist[i,G_k] = d_{ip_k}$$

L'inertie au sein d'un cluster deviens
$$I_k = \sum_{i\in G_k} dist^2[i, G_k]$$

L'algorithme s'effectue de la façon suivante:
\begin{enumerate}
\item Initialiser les prototypes $p_k$
\item Alouer les individus aux groupe les plus proches tout en maintenant les prototypes fixes
\item Recalculer les prototypes
$$p_k = argmin_j\left\{\sum_{i \in G_k} d^2_{ij}\right\}$$
\end{enumerate}

\end{enumerate}


\end{document}
