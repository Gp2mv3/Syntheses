\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{framed}
\usepackage{amssymb}

\title{Analyse de données quantitative\\Question 13}
\date{}

\begin{document}

\maketitle

\textit{Dérivez et expliquez l’analyse des corrélations canonique et dérivez la méthode permettant d’obtenir les axes factoriels. Interprétez ces résultats – par exemple, que représentent les valeurs propres et comment peut-on obtenir les coordonnées des observations dans le système des axes principaux ? Décrivez quelques méthodes de sélection de features.}

\begin{enumerate}

\item \textbf{Contexte}

\item \textbf{Analyse des corrélations canonique}

  L'analyse des corrélations canonique permet d'analyser les \textbf{relations entre deux ensembles de variables} (des bases de données) $X$ et $Y$. 
  Pour cela, on crée un espace qui rassemble la maximum de relation entre les deux ensemble (un maximum de corrélation).

Soient $x$ et $y$ deux vecteurs qui réalisent les données $X$ et $Y$. Nous allons calculer deux scores $z_x$ et $z_y$ qui sont correlé au maximum. On définis les centres de gravité
$$
\left\{
\begin{array}{l}
g_x=E[x]\\
g_y=E[y]\\
\end{array}
\right.
$$
Et les scores
$$
\left\{
\begin{array}{l}
z_x=u_x^Tx\\
z_y=u_y^Ty\\
\end{array}
\right.
$$
Notre objectif va être de trouver $u_x$ et $u_y$ de facon à maximiser la covariance entre les scores, avec la contrainte que $var(z_x)=1$ et $var(z_y)=1$
$$max_{u_x,u_y}(cov(z_x,z_y))$$
Enfin nous définissons les scores centré comme étant
$$
\left\{
\begin{array}{l}
\tilde z_x=u_x^T(x-g_x)\\
\tilde z_y=u_y^T(y-g_y)\\
\end{array}
\right.
$$

\begin{framed}
Trouvons d'abord la covariance. 
\begin{description}
  \item[Rappel ]: $cov(X,Y) = E[(X-E(X))(Y-E(Y))]$, $var(X) = E[(X-E[X])^2]$ et $A^TB = B^TA$.
\end{description}
\begin{eqnarray}
cov(z_x,z_y) &=& E[\tilde z_x, \tilde z_y]\\
&=& E[u_x^T(x-g_x)u_y^T(x-g_y)]\\
&=& E[u_x^T(x-g_x)(x-g_y)^Tu_y]\\
&=& u_x^TE[(x-g_x)(x-g_y)^T]u_y\\
var(z_x) &=& E[\tilde z_x^2]\\
&=& E[u_x^T(x-g_x)u_x^T(x-g_x)]\\
&=& u_x^TE[(x-g_x)(x-g_x)^T]u_x\\
var(z_y) &=& u_y^TE[(y-g_y)(y-g_y)^T]u_y
\end{eqnarray}
Nous allons définir 3 matrice de variance-covariance
$$
\left\{
\begin{array}{l}
S_{xy}=E[(x-g_x)(y-g_y)^T]\\
S_{xx}=E[(x-g_x)(x-g_x)^T]\\
S_{yy}=E[(y-g_y)(y-g_y)^T]
\end{array}
\right.
$$
\textbf{Attention}: $S_{xy}$ n'est pas une matrice carrée.

Et l'estimation de ces matrices est (se rappeler de l'estimateur de l'espérance), avec $n$ le nombre d'observations
$$
\left\{
\begin{array}{l}
\Sigma_{xy}=\frac{1}{n-1}\sum_{k=1}^n[(x_k-g_x)(y_k-g_y)^T]\\
\Sigma_{xx}=\frac{1}{n-1}\sum_{k=1}^n[(x_k-g_x)(x_k-g_x)^T]\\
\Sigma_{yy}=\frac{1}{n-1}\sum_{k=1}^n[(y_k-g_y)(y_k-g_y)^T]
\end{array}
\right.
$$
\end{framed}

\item \textbf{Trouver les axes}

  Nous allons en fait \textbf{maximiser l'estimateur de la covariance entre les deux ensembles} 
$$max_{u_x,u_y}(u_x^T\Sigma_{xy}u_y)$$
Avec les contraintes
$$
\left\{
\begin{array}{l}
u_x^T\Sigma_{xx}u_x=1\\
u_y^T\Sigma_{yy}u_y=1
\end{array}
\right.
$$

\begin{framed}
Nous devons donc définir un lagrangien
$$\mathcal{L} = u_x^T\Sigma_{xy}u_y+\frac{\lambda_x}{2}(1-u_x^T\Sigma_{xx}u_x)+\frac{\lambda_y}{2}(1-u_y^T\Sigma_{yy}u_y)$$
Avec 
$$
\left\{
\begin{array}{l}
\partial_{u_x}\mathcal{L}=0\\
\partial_{u_y}\mathcal{L}=0
\end{array}
\right.
$$
Ce qui nous donne (rappel: $\frac{\partial (u_x^T\Sigma_{xx}u_x)}{\partial u_x} = 2\Sigma_{xx}u_x$)
$$
\left\{
\begin{array}{l}
\Sigma_{xy}u_y-\lambda_x\Sigma_{xx}u_x=0\\
\Sigma_{yx}u_x-\lambda_y\Sigma_{yy}u_y=0
\end{array}
\right.
$$
\end{framed}

Nous avons donc le problème de vecteur propre valeur propre $\lambda = \lambda_x\lambda_y$.

\begin{framed}
Calculons les deux parties de ce système de facon séparée en se souvenant que $u_y^T\Sigma_{yy}u_y=1=u_x^T\Sigma_{xx}u_x$.
\begin{eqnarray}
\Sigma_{xy}u_y-\lambda_x\Sigma_{xx}u_x&=&0\\
u_x^T(\Sigma_{xy}u_y-\lambda_x\Sigma_{xx}u_x)&=&u_x^T0\\
u_x^T\Sigma_{xy}u_y-\lambda_xu_x^T\Sigma_{xx}u_x&=&0\\
u_x^T\Sigma_{xy}u_y-\lambda_x*1&=&0\\
u_x^T\Sigma_{xy}u_y&=&\lambda_x
\end{eqnarray}
Et de même facon
\begin{eqnarray}
u_y^T\Sigma_{yx}u_x&=&\lambda_y
\end{eqnarray}
Multiplions les deux lambda, quelque soit le sens de la multiplication le resultat est le même et multiplions ensuite par $u_x$ et $u_y$. 
\begin{eqnarray}
\lambda_x\lambda_y&=&u_x^T\Sigma_{xy}u_yu_y^T\Sigma_{yx}u_x\\
\lambda u_x&=&u_xu_x^T\Sigma_{xy}u_yu_y^T\Sigma_{yx}u_x\\
\lambda u_y&=&u_yu_y^T\Sigma_{yx}u_xu_x^T\Sigma_{xy}u_y
\end{eqnarray}
Si l'on reprends les formules de tantot on a
\begin{eqnarray}
u_y^T\Sigma_{yy}u_y&=&1\\
u_y^T\Sigma_{yy}u_yu_y^T&=&u_y^T\\
u_yu_y^T&=&u_y^Tu_y^{-1T}\Sigma_{yy}^{-1}\\
u_yu_y^T&=&\Sigma_{yy}^{-1}\\
\end{eqnarray}
Il faut se rappeler que les matrices $\Sigma_{xx}$ et $\Sigma_{yy}$ sont symétriques et carrées. Nous obtennons
\begin{eqnarray}
\lambda u_x&=&\Sigma_{xx}^{-1}\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}u_x\\
\lambda u_y&=&\Sigma_{yy}^{-1}\Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy}u_y
\end{eqnarray}
\end{framed}

\begin{description}
  \item {Au final $cov(z_x,z_y)=\lambda_x=\lambda_y$ }
  \item {et donc $\lambda = \lambda_x\lambda_y = cov^2(z_x,z_y) \geq 0$.}
\end{description}

Si $u_{x1}$ est le premier vecteur propre associé à la plus grande valeur propre associé au vecteur $x$, le score d'un vecteur $x_k$ sur ces nouveaux axes est
$$\tilde z_{x1} = u_{x1}^T(x_k-g_x)$$

\item \textbf{Feature Selection}

\textbf{Attention}: Feature Selection $\neq$ Feature Reduction

Il existe trois grandes techniques de \textit{feature selection}:
\begin{enumerate}
\item \textbf{Maximum-relevance}: On calcul l'association entre les variables (par des test de chi-carré ou de student) et on garde les variables qui sont le plus à même d'expliquer la variable dépendante de nos données
\item \textbf{Minimum-redundancy}: On calcul l'association entre les variables et on se débarasse de celles qui sont fortement correlé entre elles
\item \textbf{Stepwize regression}: On enleve/ajoute des variables une a une et on regarde l'effet sur les performance de notre modèle
\end{enumerate}


\end{enumerate}

\end{document}
