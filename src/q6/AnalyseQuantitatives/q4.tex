\documentclass[a4paper, 11pt, onecolumn]{article}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{framed}
\usepackage{ amssymb }


\title{Analyse de données quantitative\\Question 4}
\date{}

\begin{document}

\maketitle

\textit{Dérivez de manière détaillée et explicitez la méthode permettant d’obtenir les axes factoriels d’une analyse en composantes principales (une méthode basée sur les données empiriques, l’autre basée sur les définitions mathématiques). Interprétez ces résultats - par exemple, que représentent les valeurs propres? Comment peut-on obtenir les coordonnées des observations sur les axes principaux ? Décrivez quelques méthodes de sélection de features.}

\begin{enumerate}
\item \textbf{Qu'est-ce qu'une Analyse en composante principale}

Le principe d'une analyse en composante principale (PCA) est de 
\textbf{réduire la corrélation entre les différentes variables} 
en créant un nouveau système de coordonnées tout en conservant une certaines quantité de 
\textbf{variance}.
Nous allons essayer de conserver au minimum (\textit{par convention}) 80\% de la variance des variables. 

Cette méthode s'applique aux variables continues, pour les variables catégorielle il faut regarder une MCA (qui est une PCA multiple en fait). 

\item \textbf{Comment récupérer les axes}

  \begin{description}
    \item {Le \textbf{premier axe} de ce nouveau système de coordonnées est 
      l'axe qui conserve le maximun de variance (\textit{c'est à dire que la variance en projetant les différents points sur cet axe sera maximale}). }
    \item {Les autres axes seront perpendiculaires à celui-ci. Nous allons regarder le pourcentage d'information que chaque axe garde et en conserver une certains quantité de variance.}
  \end{description}

$\rightarrow $ Mais comment trouver mathématiquement ce premier axe? 

\paragraph{Premièrement} il faut trouver la variance du nuage de points.
\begin{framed}
Le développement suivant est assez simple. Il faut se rappeler que $A^TB = B^TA$ (ligne 3).
\begin{eqnarray}
\sigma^2 &=& \frac{1}{2n(n-1)} \sum_{i=1}^n\sum_{j=1}^n(x_i - x_j)^T(x_i-x_j)\\
&=& \frac{1}{2n(n-1)} \sum_{i=1}^n\sum_{j=1}^n((x_i-g)+(g-x_j))^T((x_i-g)+(g-x_j))\\
&=& \frac{1}{2n(n-1)} \sum_{i=1}^n\sum_{j=1}^n((x_i-g)^T(x_i-g)+(g-x_j)^T(g-x_j)+2(x_i-g)^T(g-x_j))\\
\end{eqnarray}
Nous pouvons réécrire les parties de ce calculs comme étant deux sommes de $k$.
$\rightarrow$ Il s'agit en fait de la distance d'un points au centre de gravité ce qui signifie que ces deux parties sont égales.
\begin{eqnarray}
\frac{1}{2n(n-1)} \sum_{i=1}^n\sum_{j=1}^n(x_i-g)^T(x_i-g) &=& \frac{n}{2n(n-1)}  \sum_{i=1}^n (x_i-g)^T(x_i-g)\\
 &=& \frac{n}{2n(n-1)}  \sum_{k=1}^n (x_k-g)^T(x_k-g)\\
\frac{1}{2n(n-1)} \sum_{i=1}^n\sum_{j=1}^n(g-x_j)^T(g-x_j) &=& \frac{n}{2n(n-1)} \sum_{j=1}^n (g-x_j)^T(g-x_j)\\
 &=& \frac{n}{2n(n-1)} \sum_{j=1}^n (g-x_k)^T(g-x_k)
\end{eqnarray}
\begin{description}
  \item {Comme la deuxième partie du calcul est la distance de tout les points au centre de
    gravité et du centre de gravité à tout les points,}
  \item {nous avons un chemin dans un sens puis dans l'autre. \textbf{Ce qui annule
    l'expression}}
\end{description}
\begin{eqnarray}
\sigma^2 &=& \frac{1}{(n-1)}  \sum_{k=1}^n (x_k-g)^T(x_k-g) + \frac{2}{2n(n-1)}\sum_{i=1}^n\sum_{j=1}^n(x_i-g)^T(g-x_j)\\
\sigma^2 &=& \frac{1}{(n-1)}  \sum_{k=1}^n (x_k-g)^T(x_k-g)
\end{eqnarray}
\end{framed}

\paragraph{Deuxièmement}, nous allons projeter les données sur un axe et chercher à
maximiser la variance projetée parce que l'axe qui possède le maximim de variance possède aussi
le maximum d'information.
\begin{description}
  \item {Nous avons le vecteur directeur de cet espace $V$ tel que $V^T V=1$ et 
    $\pi$ la matrice symétrique de projection tel que $\pi = VV^T$.}
  \item {Dès lors $\pi$ est l'operateur d'une projection orthogonale 
      (\textit{c'est une matrice qui définis cet axe}).}
    \item {$\rightarrow$ Projeter des données dans ce nouveau système revient à multiplier la donnée par la matrice de projection. }
  \end{description} 
      Calculons la variance projetée:

\begin{framed}
  \begin{description}
    \item [Ligne 13] : La matrice $VV^T$ est symétrique et dès lors sa transposition reviens à reprendre la même matrice 
    \item [Ligne 14] : Comme $V^TV = 1$, la multiplication de $VV^T$ avec elle-même reviens à $VV^T$ (\textit{trivial à prouver mais il faut faire des matrices de taille nxn et c'est assez long à écrire}).
  \end{description}

\begin{eqnarray}
\sigma^2_v &=& \frac{1}{(n-1)}  \sum_{k=1}^n (\pi(x_k-g))^T(\pi(x_k-g))\\
&=& \frac{1}{(n-1)}  \sum_{k=1}^n (VV^T(x_k-g))^T(VV^T(x_k-g))\\
&=& \frac{1}{(n-1)}  \sum_{k=1}^n (x_k-g)^T(VV^TVV^T)(x_k-g)\\
&=& \frac{1}{(n-1)}  \sum_{k=1}^n (x_k-g)^TVV^T(x_k-g)\\
&=& \frac{1}{(n-1)}  \sum_{k=1}^n ((x_k-g)^TV)(V^T(x_k-g))\\
&=& \frac{1}{(n-1)}  \sum_{k=1}^n V^T(x_k-g)(x_k-g)^TV\\
&=& \frac{1}{(n-1)}  V^T\left[\sum_{k=1}^n (x_k-g)(x_k-g)^T\right]V\\
&=& V^T\sum V
\end{eqnarray}
Avec la matrice de variance-covariance $\sum$. C'est une matrice symétrique positive et définie (une matrice est définie positive si ses valeurs sont réelles strictement positive).
\end{framed}

Notre objective va être de maximiser la variance projetée, il suffit pour cela d'écrire un bête lagrangien $\mathcal{L} = V^T\Sigma V + \lambda (1-V^TV)$ avec comme condition $\partial_v \mathcal{L} = 0$.

\begin{framed}
Il ne faut pas oublier que $\Sigma$ est une matrice symétrique carrée, dès lors sa transposée est égale à elle-même ($\Sigma^T = \Sigma$).
\begin{eqnarray}
\frac{\partial V^T\Sigma V + \lambda (1-V^TV)}{\partial V} &=& 0\\
\frac{\partial V^T\Sigma V + \lambda - \lambda V^TV}{\partial V} &=& 0\\
\frac{\partial V^T\Sigma V}{\partial V} + \frac{\partial \lambda}{\partial V} &=& \frac{\partial\lambda V^TV}{\partial V}\\
\frac{\partial V^T\Sigma V}{\partial V} &=& \frac{\partial\lambda V^TV}{\partial V}\\
V^T\Sigma &=& \lambda V^T\\
\Sigma V &=& \lambda V
\end{eqnarray}
L'axe qui a la direction de $V$ est appellé \textbf{axe principal}. Comme la matrice de variance-covariance est symétrique et définie positive, les valeurs propres sont toute réelles et non négative.
\end{framed}

Et donc si nous reprenons la formule de la variance projetée, nous obtenons $\sigma_v^2 = V^T\sum V = V^T \lambda V = \lambda$ (pour rappel $V^TV = 1$). La solution est donc le vecteur propre avec la valeur propre maximale $\lambda$. L'axe qui a la direction de $V$ est appellé \textbf{axe principal}. 

Maintenant il va falloir trouver le second axe, il faut simplement maximiser à nouveau $V^T \sum V$ mais sous les conditions que $V^TV = 1$ et $V^TV_1 = 0$ (c'est à dire que le vecteur est perpendiculaire au premier vecteur, ici $V_1$ correspond au vecteur propre de valeur propre $\lambda_1$).

\item \textbf{Interpretation des résultats}

La variance totale du système est la somme de toute les valeurs propres, ainsi on sait représenter la proportion de variance portée par chaque vecteur du nouveau système.
$$\sigma^2 = \sum_{i=1}^n \lambda_i$$.

Pour projeter une observation $x$ sur l'axe de composante principale on calcule $V^T x$.

\item \textbf{Feature Selection}

\textbf{Attention}: Feature Selection $\neq$ Feature Reduction

Il existe trois grandes techniques de \textit{feature selection}:
\begin{enumerate}
\item \textbf{Maximum-relevance}: On calcul l'association entre les variables (par des test de chi-carré ou de student) et on garde les variables qui sont le plus à même d'expliquer la variable dépendante de nos données
\item \textbf{Minimum-redundancy}: On calcul l'association entre les variables et on se débarasse de celles qui sont fortement correlé entre elles
\item \textbf{Stepwize regression}: On enleve/ajoute des variables une a une et on regarde l'effet sur les performance de notre modèle
\end{enumerate}

\end{enumerate}
\end{document}
