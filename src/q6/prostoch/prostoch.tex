\input{../../lib.tex}

\usepackage{qtree}

\def\Perp{\perp\!\!\!\perp}

\renewcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}
\DeclareMathOperator{\var}{\mathbb{V}ar}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\bias}{Bias}

% Distributions
\DeclareMathOperator{\bin}{Bin}
\DeclareMathOperator{\expo}{Expo}
\DeclareMathOperator{\gammad}{Gamma}
\DeclareMathOperator{\po}{Po}

\newcommand{\fourier}{\mathcal{F}}
\newcommand{\esth}{\hat{\theta}}
\newcommand{\esthls}{\esth^{LS}}
\newcommand{\esthcm}{\esth^{CM}}
\newcommand{\esthmap}{\esth^{MAP}}
\newcommand{\esthml}{\esth^{ML}}
\newcommand{\esthblue}{\esth^{BLUE}}
\newcommand{\esthlmmse}{\esth^{BLUE}}
\newcommand{\eqmth}{\mathrm{EQM}_{\esth}}

\hypertitle{Probabilité et Statistiques}{5}{FSAB}{1105}
{Benoît Legat}
{Anour El Ghouch et Rainer von Sachs}

\newcommand{\Ex}[1]{\E\left\{#1\right\}}

% NO randomness, no info

\section{Probabilités et variables aléatoires}
\subsection{Notations}
On met les signaux déterministes en minuscule et les signaux aléatoires en majuscule (comme pour les variables aléatoires en probabilité).
On met aussi un signal aléatoire en minuscule lorsqu'on parle d'une de ses instances (comme pour proba à nouveau, on met par exemple $P(x \geq X)$ où $x$ est une valeur de la variable aléatoire $X$ et $X$ est la variable aléatoire).

\subsection{Probabilité conditionnelles et formule de Bayes}
Soient $E_1, E_2$ des évènements et $H_1, H_2, \ldots, H_n$ des évènements mutuellement exclusifs tels que $p_{H_1 \cup H_2 \cup \cdots \cup H_n} = 1$.
On a
\begin{align*}
  p_{E_1 \cup E_2} & = p_{E_1} + p_{E_2} - p_{E_1 \cap E_2}\\
  p_{E_1 \cap E_2} & = p_{E_1|E_2} p_{E_2}\\
                   & = p_{E_2|E_1} p_{E_1}\\
  p_{E_1}          & = \sum_{i=1}^n p_{E_1|H_i} p_{H_i}.
\end{align*}

\subsubsection{Probabilité à postiori}
La loi de Bayes vient simplement de l'égalité $p_{E_2|E_1} p_{E_1} = p_{E_1 \cap E_2} = p_{E_1|E_2} p_{E_2}$, qui donne
\begin{align*}
  p_{E_2|E_1} & = \frac{p_{E_1|E_2}p_{E_2}}{p_{E_1}}\\
  p_{H_i|E_1} & = \frac{p_{E_1|H_i}p_{H_i}}{p_{E_1}}\\
  p_{H_i|E_1} & = \frac{p_{E_1|H_i}p_{H_i}}{\sum_{i=1}^n p_{E_1|H_i} p_{H_i}}.
\end{align*}

\subsection{Variable aléatoire}
Soit une VA (Variable Aléatoire) $X$.
On note $T_X(x)$ sa densité de probabilité,
$m_n$ son moment d'ordre $n$, $\mu_n$ son moment \emph{centré} d'ordre $n$
et $X_c = X - m_1$ la variable centrée.
\begin{align*}
  m_n   & = \Ex{X^n}\\
  \mu_n & = \Ex{(X - m_1)^n}\\
        & = \Ex{X_c^n}.
\end{align*}
La relation classique dans ces nouvelles notations est
\[
  \mu_2 = \sigma_X^2 = m_2 - m_1^2.
\]

\subsubsection{Fonction caractéristique}
On définit la fonction caractéristique $\phi_X(q)$ comme la transformée fourier
(avec $q = -\omega$) de $T_X$, c'est à dire
\begin{align*}
  \phi_X(q) & = \Ex{e^{jqx}}\\
  T_X(x) & = \frac{1}{2\pi} \int_{-\infty}^{\infty} \phi_x(q) e^{-jqx} \dif q.
\end{align*}

$\phi_X$ est très pratique car tout comme $T_X$, elle détermine
entièrement la distribution d'une VA: $\phi_X = \phi_Y$ ssi $T_X = T_Y$.
Il permet donc facilement de trouver $T_Y$ lorsqu'on connait $T_{X_i}$ et $Y = \sum_i X_i$,
car $e^{jq\sum_i X_i} = \prod_i e^{jqX_i}$ et si les $X_i$ sont \emph{indépendants},
$\prod_Y(q) = \Ex{\prod_i e^{jqX_i}} = \prod_i \Ex{e^{jqX_i}} = \prod_i \phi_{X_i}(q)$.

Les moments sont obtenus aisément par
\[ m_n = \frac{1}{j^n} \left.\frac{\dif^n \phi_x}{\dif q^n}\right|_{q = 0}. \]

\subsubsection{Variable aléatoire à plusieurs dimensions}
Lorsqu'on parle d'une VA à plusieurs dimensions,
$m$ et $\mu$ changent de sens et on n'a que des notations pour
les moments d'ordre 1 et 2.
Les nouvelles notations sont reprises dans la \tabref{momentdim}.
\begin{table}
  \centering
  \begin{tabular}{|l|c|c|}
    \hline
            & non-centré & centré\\
    \hline
    Ordre 1 & $m_i$      & $\mu_i$\\
    \hline
    Ordre 2 & $R_{ij}$   & $C_{ij}$\\ % FIXME ou $\sigma_{ij}$ ?
    \hline
  \end{tabular}
  \caption{Tables des moments pour une VA à plusieurs dimensions.}
  \label{tab:momentdim}
\end{table}
$R_{ii}$ et $C_{ii}$ se racourcissent bien évidemment en $R_i$
et $C_i$ ou $\sigma_i^2$.
\begin{align*}
  \sigma_i^2 & = R_i - m_i^2\\
  C_{ij}   & = R_{ij} - m_im_j.
\end{align*}

On a donc, en tout généralité,
\begin{align*}
  R_{ij} & = \Ex{X_iX_j^*}\\
  C_{ij} & = \Ex{(X_i-m_i)(X_j-m_j)^*}\\
         & = \Ex{X_{i;c}X_{j;c}^*}
\end{align*}
où $^*$ représente la matrice adjointe,
c'est à dire la matrice transposée de la conjuguée.

On définit aussi la matrice des moments d'ordre 2 et la \emph{matrice de covariance}
\begin{align*}
  R_X    & = \Ex{X X^*}\\
  R_{XY} & = \Ex{X Y^*}\\
  C_X    & = \Ex{X_c X_c^*}\\
  C_{XY} & = \Ex{X_c Y_c^*}.
\end{align*}
En particulier, on a donc
\begin{align*}
  R_X & =
    \begin{pmatrix}
      R_1        & R_{12} & \cdots & R_{1n}\\
      R_{21}     & R_2 & \cdots & R_{2n}\\
      \vdots     & \vdots & \ddots & \vdots\\
      R_{n1}     & R_{n2} & \cdots & R_n
    \end{pmatrix} &
  C_X & =
  \begin{pmatrix}
    \sigma_1^2 & C_{12}     & \cdots & C_{1n}\\
    C_{21}     & \sigma_2^2 & \cdots & C_{2n}\\
    \vdots     & \vdots     & \ddots & \vdots\\
    C_{n1}     & C_{n2}     & \cdots & \sigma_n^2
  \end{pmatrix}.
\end{align*}

Une propriété très importante sur les matrices $R_{XY}$ et $C_{XY}$ est
qu'elles sont \emph{hermitiennes} ($R_{XY}^* = R_{XY}$ et $C_{XY}^* = C_{XY}$)
et \emph{semi-définies positives}.
Comme elle est hermitienne, elle est aussi diagonalisable par une famille
de vecteurs propres orthonormés,
c'est à dire qu'il existe une matrice orthogonale $M$ telle que
$R_{XY}$ ou $C_{XY}$ vaut $M \Lambda M^*$.

\subsubsection{Indépendance}
$X_1$ et $X_2$ sont indépendantes, noté $X_1 \Perp X_2$,
si et seulement si $p_{X_1|X_2} = p_{X_1}$ si $X_1$ et $X_2$ sont des évènements et
$T_{X_1|X_2} (x_1|x_2) = T_{X_1}(x_1)$ si c'est des VA.

Les conséquences de l'indépendance entre $X_1$ et $X_2$ sont,
pour toutes fonctions $f, g$,
\begin{align*}
  f(X_1) & \Perp g(X_2)\\
  \E(X_1 X_2) & = \E(X_1) \E(X_2)\\
  \phi_{X_1 + X_2}(q) & = \phi_{X_1}(q) \phi_{X_2}(q)\\
  \var(a + bX_1 + cX_2) & = b^2\var(X_1) + c^2\var(X_2)\\
  C_{12} & = 0.
\end{align*}
Ce ne sont que des conséquences, \emph{pas} de conditions nécessaires
à être indépendant.
Lorsque $C_{12} = 0$ on dit que $X_1$ et $X_2$ sont décorrelés ou orthogonales.
La correlation entre $X_1$ et $X_2$ est d'ailleurs
\begin{align*}
  \rho & = \Ex{\left(\frac{X_1-m_1}{\sigma_1}\right)\left(\frac{X_2-m_2}{\sigma_2}\right)}\\
       & = \frac{C_{12}}{\sigma_1\sigma_2}.
\end{align*}
Si $X_1$ et $X_2$ sont normales, $C_{12} = 0$ est aussi une condition
suffisante pour que $X_1 \Perp X_2$.

\section{Fonctions aléatoires et processus stochastiques}
\subsection{Fonctions aléatoires}
Les notations pour une FA sont les mêmes que pour une VA à plusieurs dimensions
sauf qu'on utilise plus d'indice mais plutôt des arguments,
voir~\tabref{momentfun}.
\begin{table}
  \centering
  \begin{tabular}{|l|c|c|}
    \hline
            & non-centré & centré\\
    \hline
    Ordre 1 & $m(t)$      & $\mu(t)$\\
    \hline
    Ordre 2 & $R(t_i,t_j)$   & $C(t_i,t_j)$\\ % FIXME ou $\sigma(t_i,t_j)$ ?
    \hline
  \end{tabular}
  \caption{Tables des moments pour une FA.}
  \label{tab:momentfun}
\end{table}
$C(t,t)$ se racourcit en $\sigma^2(t)$.
On appelle $C$ la \emph{fonction d'autocovariance}.

On a donc, en tout généralité,
\begin{align*}
  R_{X_1X_2}(t_1,t_2) & = \Ex{X_1(t_1)X_2^*(t_2)}\\
  C_{X_1X_2}(t_1,t_2) & = \Ex{(X_1(t_1)-m_1(t_1))(X_2(t_1)-m_2(t_2))^*}\\
                      & = \Ex{X_{1;c}(t_1)X_{2;c}^*(t_2)}.
\end{align*}

On peut calculer $R$ ou $C$ entre deux FA différentes $X_1,X_2$,
on notera alors implement $C_{X_1X_2}(t_1,t_2)$ et le premier temps
(ici $t_1$) réfère à la première variable, donc $X_1(t_1)$ et $m_{X_1}(t_1)$.

Tout comme $C_{XY}$ était hermitienne,
on a
\begin{equation}
  \label{eq:faherm}
  C_{X_1X_2}(t_1,t_2) = C_{X_2X_1}(t_2,t_1).
\end{equation}

\subsubsection{Stationnarité d'une FA}
\paragraph{Stationnarité au sens stricte}
On dit qu'une FA est stationnaire au sens stricte si,
elle est complètement indépendante de l'origine du temps,
c'est à dire que pour tout $t_1,t_2,\ldots,t_n$,
\[ T_X(x(t_1),x(t_2),\ldots,x(t_n)) = T_X(x(0),x(t_2-t_1),x(t_3-t_1),\ldots,x(t_n-t_1)). \]

Imposer la stationnarité au sens stricte est un peu difficile en pratique,
on introduit alors la stationnarité au sens faible.

\paragraph{Stationnarité au sens faible}
Une FA est stationnaire au sens faible (abrégé WSS) lorsque
ses deux premiers moments sont stationnaires,
c'est à dire que
\begin{align*}
  m_X(t)       & = m_X\\
  C_X(t_1,t_2) & = C_X(\tau = t_1 - t_2). % FIXME not t_2 - t_1 ?
\end{align*}
La variance est alors aussi constante,
$\sigma_X^2(t) = C_X(t,t) = C_X(0) = \sigma_X^2$.
\eqref{eq:faherm} devient $C_X(-\tau) = C_X^*(\tau)$.
On a également $|C_x(\tau)| \leq |C_X(0)| = \sigma_X^2$
\footnote{$|\sigma_X^2| = \sigma_X^2$ car $\sigma_X^2$ est un réel positif car
$(X(t)-m_X(t))(X(t)-m_X(t))^* = |X(t)-m_X(t)|$ est un réel positif}.

\subsubsection{Représentation spectrale}
On définit $\mathcal{X}(w) = \fourier(X_c(t))$ la transformée de fourier
\emph{centrée} de $X_c$.
Comme $\mathcal{X}$ est aussi centré,
$C_\mathcal{X} = \Ex{\mathcal{X}(w)\mathcal{X}^*(w)}$.

Si $X$ est \emph{WSS}, $\mathcal{X}$ est à valeures \emph{décorrelé},
c'est le théorème de Cramér-Loève. On a donc
\[
  C_\mathcal{X}(w,w') = 2\pi \gamma_X(w) \delta(w - w')
\]
avec $\gamma_X(w) = \fourier(C_x(\tau))$
(on voit que $\gamma_x(w)$ n'a du sens que si $X$ est WSS).

\paragraph{Effet d'un système LTI sur une FA}
Si $Y = g * X$ où $g$ est une fonction de transfert déterministe,
on a
\begin{align*}
  m_Y & = G(0) m_X & m_Y & = G(1) m_X = G(e^{j0}) m_X.
\end{align*}

Si $X$ est \emph{WSS}, $Y$ l'est également !
$\gamma_Y$ est donc aussi défini et% FIXME ", on a le \emph{théorème de Wiener Kintchine}" sur Wikipedia, ils ont pas pareil
% FIXME L'ordre est important dans le cas discret car c'est des matrices mais là c'est pas des matrices,
%       mais si on fait inverse z transform c'est re des matrices.
\begin{align*}
  \gamma_Y(s) & = G(s)\gamma_X(s)G^*(-s^*) & \gamma_Y(z) & = G(z) \gamma_X(z) G^*(1/z^*).
\end{align*}

% TODO add C_Y = C_h * C_X ? C_h est calculé comme un ergodique

\subsubsection{Factorisation spectrale}
\begin{mydef}[Bruit blanc]
  Une FA \emph{WSS} $W$ est appelée bruit blanc si elle est à valeur décorrelées.
  On a donc
  \begin{align*}
    C_W(\tau) & = \sigma_W^2 \delta(\tau)\\
    \gamma_W(w) & = \sigma_W^2.
  \end{align*}
\end{mydef}

\begin{mydef}[Processus régulier]
  Un processus régulier est un processus $Y$ qui peut être exprimé
  la sortie d'un système LTI avec en entrée un bruit blanc.
  Il existe donc $L(\omega)$ et $\sigma$ tels que
  \[ \gamma_Y(\omega) = L(\omega) L^*(\omega) \sigma^2. \]
\end{mydef}

La factorisation spectrale consiste à trouver $L(\omega)$ à partir
de $\gamma_Y(\omega)$.
Voyons la méthode pour trouver un $L(s)$ stable et à minimum de phase\footnote{
  ``stable'' signifie que les pôles sont stables et ``minimum de phase'' signifie
  que les zéros sont stables.
}
dans le cas où $Y$ est réel et que $\gamma_Y$ est sous forme d'une fraction de
polynômes à coefficients réels.
Comme $Y$ est réel, $C_Y$ aussi et donc
$C_Y(-\omega) = C_Y^*(\omega) = C_Y(\omega)$.
$C_Y$ est donc pair et donc
\begin{align*}
  \gamma_Y(-\omega) & = \int_{-\infty}^{\infty} e^{j\omega\tau} C_Y(\tau) \dif \tau\\
                    & = \int_{\infty}^{-\infty} e^{j\omega(-\tau)} C_Y(-\tau) \dif (-\tau)\\
                    & = \int_{-\infty}^{\infty} e^{-j\omega\tau} C_Y(\tau) \dif \tau\\
                    & = \gamma_Y(\omega).
\end{align*}
On sait que alors que $\gamma_Y(-\omega) = \gamma_Y(\omega)$,
$\gamma_Y$ est donc pair et comme c'est une fraction de polynômes ça signifie
que $\gamma_Y$ est en fonction de $\omega^2$.
On passe alors à la transformée en $s$.
On sait que la transformée de fourier est
la transformée en $s$ pour $s = j\omega$.
La transformée en $s$ est donc vraissemblablement $\gamma_Y$ où on remplace
$\omega^2$ par $-s^2$.
On mets alors $\gamma_Y$ sous la forme
\begin{align*}
  \gamma_Y(s) & = \sigma^2\frac{\prod_i (z_i^2-s^2)}{\prod_i (p_i^2-s^2)}\\
              & = \sigma^2\frac{\prod_i (z_i+s)}{\prod_i (p_i+s)}\frac{\prod_i (z_i-s)}{\prod_i (p_i-s)}\\
              & = \sigma^2 L(s) L(-s)
\end{align*}
où $z_i$ et $p_i$ ont une partie réelle positive.
Comme $L(s)$ a des pôles en $-p_i$, il est stable et comme il
a des zéros en $-z_i$, il est à minimum de phase.

\paragraph{En discret}
En discret, on s'impose aussi que $L(z)$ soit monique, c'est à dire que le coefficient
de degré le plus élevé du numérateur et dénominateur doit être 1.
La parité implique que $\gamma_Y$ est fonction de
$\cos(\omega) = \frac{e^{j\omega} + e^{-j\omega}}{2}$.
On remplace $e^{j\omega}$ par $z$, ce qui donne $\cos(\omega) = z^-1\frac{z^2+z}{2}$
et on a
\begin{align*}
  \gamma_Y(s) & = {\sigma'}^2\frac{z^{-n_z}\prod_i^{n_z} (z-z_i^{-1})(z-z_i)}{z^{-n_p}\prod_i^{n_p} (z-p_i^{-1})(z-p_i)}\\
              & = \frac{z_i^{n_z}}{p_i^{n_p}}{\sigma'}^2\frac{\prod_i^{n_z} (z-z_i^{-1})(z^{-1}-z_i^{-1})}{\prod_i^{n_p} (z-p_i^{-1})(z^{-1}-p_i^{-1})}\\
              & = \sigma^2 L(z) L(z^{-1})
\end{align*}
où $z_i$ et $p_i$ ont un module plus grand que 1.
Comme $L(z)$ a des pôles en $p_i^{-1}$, il est stable et comme il
a des zéros en $z_i^{-1}$, il est à minimum de phase.
Le terme de plus haute puissance au numérateur et au dénominateur sont
respectivement $z^{n_z}$ et $z^{n_p}$.
$L(z)$ est donc bien monique.

\subsection{Processus stochastiques}
Les différents processus stochastiques sont repris par la \figuref{processus}.
\begin{figure}
  \Tree [.{Box et Jenkins\\$Y(k) = \frac{B(z^{-1})}{A(z^{-1})}U(k)+\frac{C(z^{-1})}{D(z^{-1})}E(k)$}
    [.{ARMAX\\$A(z^{-1})Y(k)=B(z^{-1})U(k)+C(z^{-1})E(k)$}
      [.{ARMA\\$A(z^{-1})Y(k)=C(z^{-1})E(k)$}
        [.{AR\\$A(z^{-1})Y(k)=E(k)$}
          [.{Markov\\$(1-Az^{-1})Y(k) = E(k)$}
            [.{Wiener\\$(1-z^{-1})Y(k) = E(k)$} ]
          ]
        ]
        [.{MA\\$Y(k)=C(z^{-1})E(k)$} ]
      ]
    ]
  ]
  \caption{Les différents processus du cas le plus général au plus particuliers.
  $A(z^{-1})$, $B(z^{-1})$ et $C(z^{-1})$ sont des polynômes moniques et stables
  (toutes les racines sont à l'intérieur du cercle unité)
  et $B(z^{-1})$ est quelconque.
  Markov peut être vu comme un processus AR avec $A(z^-1) = 1 - Az^{-1}$
  et $A$ Hurwitz (toutes ses valeurs propres à l'intérieur du cercle unité)
  pour que $A(z^-1)$ soit stable.}
  \label{fig:processus}
\end{figure}

\section{Théorie de l'estimation}
On considère $Z \in \Rn$ et $\theta \in \R^p$ tels que $Z = h(\theta)$.
On aimerait estimer $\theta$ en ayant observé une réalisation $z$.
\begin{mydef}[Estimateur]
  Un estimateur $\esth$ de $\theta$ est une fonction déterministe $g(Z)$ qui
  à toute réalisation $z$ de $Z$ fait correspondre une estimation $\esth=g(z)$.
\end{mydef}
Le problème, c'est que $h$ n'est pas déterministe,
on ne peut donc pas simplement prendre $g = h^{-1}$.

On peut se trouver dans 3 cas
\begin{description}
  \item[Bayes] $\Theta$ est un vecteur aléatoire et $Z$ est un vecteur aléatoire
    dont la densité de probabilité dépend d'une réalisation $\theta$
    de $\Theta$.
    Lorsqu'on observe une ou plusieurs réalisation $z$ de $Z$, on peut donc
    se donner une idée de la densité de probabilité de $Z$ et donc de $\theta$.
    On sera cependant influencé par $T_\Theta$ et on ne donnera donc pas à
    tous les $\theta$ la même chance d'être $\esth$.
  \item[Fisher] $\theta$ est déterministe, ce cas peut être considéré comme un cas
    particulier du cas précédent où $\Theta$ est une loi uniforme
    ($T_\Theta(\theta)$ est uniforme).
    On donne donc à tous les $\theta$ autant de chance d'être $\esth$.
  \item[Mesure bruitée] $Z$ n'est qu'une mesure bruitée de $\theta$, on a donc
    un $h$ \emph{déterministe} et un bruite $V$ tel que
    \[ Z = h(\theta) + V. \]
    Ça peut être considéré comme un cas particulier du cas précédent
    où $h'(\theta) = h(\theta) + V$.
    % FIXME un peu restrictif
    %On va alors être plus influencé par les composantes de $Z$ qui sont
    %moins bruitées.
\end{description}

\subsection{Propriétés d'un estimateur}

\begin{mydef}[Biais d'un estimateur]
  Le biais, $b_{\esth}$ est défini comme suit
  \begin{description}
    \item[Fisher]
      \[ b_{\esth;\theta} = m_{\esth} - \theta \]
      où
      $m_{\esth} \Ex{g(Z)|\theta}$.
    \item[Bayes] Dans le cas de Bayes, on peut être moins restrictif et
      ne pas l'imposer pour tout $\theta$ et donner plus de poids
      si $T_\Theta(\theta)$ est plus grand.
      On va donc prendre
      \[ b_{\esth} = m_{\esth} - m_\theta \]
      où $m_{\esth} = \Ex{g(Z)}$, on prend donc l'espérance en considérant
      tous les thetas à la fois.
  \end{description}
  Un estimateur $\esth$ est sans biais si $b_{\esth} = 0$.
  Pour Fisher, on doit avoir $b_{\esth;\theta} = 0$ $\forall \theta$,
  c'est donc plus restrictif que pour Bayes.
\end{mydef}

\begin{mydef}[Variance d'un estimateur]
  L'estimateur est une VA et sa matrice de covariance est défine comme pour une quelconque VA,
  c'est à dire
  \[ C_{\esth} = \Ex{(\esth - m_{\esth})(\esth - m_{\esth})^*}. \]
\end{mydef}

\begin{mydef}[Erreur quadratique moyenne d'un estimateur]
  \[ \eqmth = \Ex{(\esth - \theta)(\esth-\theta)^*}. \]
\end{mydef}

Dans le cas de \emph{Fisher}, on a
\begin{equation}
  \label{eq:fisheqm}
  \eqmth = C_{\esth} + b_{\esth;\theta}b_{\esth;\theta}^*.
\end{equation}

\begin{mydef}[Estimateur efficace ou estimateur à variance minimale]
  Un estimateur \emph{non-biaisé} $\esth$ de $\theta$ est efficace si
  pour tout autre estimateur \emph{non-biaisé} $\esth'$ de $\theta$, on a
  \[ C_{\esth} \preceq C_{\esth'} \]
  où $C_{\esth} \preceq C_{\esth'}$ signifie que $C_{\esth'} - C_{\esth}$
  est semi-définie positive.
\end{mydef}

\begin{mydef}[Matrice d'information de Fisher]
  Si $T_{Z|\theta}(z|\theta)$ est différentiable par rapport à $\theta$,
  la matrice d'information de Fisher $I(\theta)$ est définie par
  \[ I_{k,j}(\theta) = -\Ex{\fpart{^2\ln T_{Z|\theta}(z|\theta)}{\theta_k\partial\theta_j}}. \]
\end{mydef}

\begin{myineg}[Inégalité de Cramér-Rao]
  Dans le cas de \emph{Fisher},
  pour tout estimateur \emph{non-biaisé} $\esth$ de $\theta$,
  on a
  \[ C_{\esth} \succeq I^{-1}(\theta). \]
\end{myineg}

On peut donc prouver qu'un estimateur $\esth$ est efficace en calculant
la matrice d'information de Fisher
et en montrant que $C_{\esth} = I^{-1}(\theta)$.

\subsection{Propriétés asymptotiques d'un estimateur}
Lorsqu'on estime $\theta$ sur $N$ observation,
on peut s'intéresser au comportement de $\esth_N = g(Z^N)$ l'estimation
de $\theta$ sur ces $N$ observations.
\begin{mydef}[Estimateur asymptotiquement non-biaisé]
  $m_{\esth}$ est calculé de la même façon que précédemment
  en fonction du cadre dans lequel on est (Fisher ou Bayes).
  Un estimateur est asymptotiquement non-biaisé si
  \begin{description}
    \item[Bayes]
      \[ \lim_{N\to\infty} m_{\esth} = m_\theta \]
    \item[Fisher]
      \[ \lim_{N\to\infty} m_{\esth} = \theta \]
      $\forall \theta$.
  \end{description}
\end{mydef}
\begin{mydef}[Convergence en moyenne quadratique]
  Une SA $X_k$ converge en moyenne quadratique vers $X$ si
  \[ \lim_{k\to\infty} \Ex{\|X_k-X\|^2} = 0. \]
\end{mydef}
Pour Fisher,
$\esth_N$ converge en moyenne quadratique vers $\theta$ si
et seulement s'il est asymptotiquement non-biaisé et que la varaince tend
vers 0 par l'\eqref{eq:fisheqm}.

\begin{mydef}[Estimateur asymptotiquement efficace]
  Soit $\esth_N$ un estimateur qui \emph{converge en moyenne quadratique} vers $\theta$.
  $\esth_N$ est asymptotiquement efficace s'il existe $N_0 > 0$ tel que
  pour tout $\esth'_N$, et pour tout $N > N_0$, on a
  \[ C_{\esth} \preceq C_{\esth'}. \]
\end{mydef}

\subsection{Bayes}
\subsubsection{Moyenne conditionnelle}
La moyenne conditionnelle revient à prendre
\[ \esthcm(Z) = \int_{\Theta} \theta T_{\Theta|Z}(\theta|z) \dif \theta. \]

Il est \emph{sans-biais} et \emph{efficace}.

\subsubsection{Estimateur linéaire à la MSE minimale (LMMSE)}
L'estimateur à la MSE minimale (Linear Minimum MSE).
revient à prendre
\[ \esthlmmse(Z) = m_\theta + C_{\Theta Z} C_{Z}^{-1}(Z - m_z). \]

Il est \emph{sans-biais} et c'est l'estimateur linéaire
qui a la plus petite MSE.
Il a aussi l'avantage de ne nécessiter les moments d'ordre
1 de $T_\Theta$ et $T_Z$ et d'ordre 2 de $T_{Z|\Theta}$.

Il obéit également au \emph{principe d'orthogonalité}:
\[ \Ex{(\esthlmmse - \theta)Z^*} = 0 \]
ce qui implique que
\[ \Ex{(\esthlmmse - \theta)(\esthlmmse)^*} = 0 \]
qui avec $\Ex{\esthlmmse} = \theta$ détermine entièrement
$\esthlmmse$ et peut être utilisé pour le déterminer.

\subsubsection{Maximum a posteriori (MAP)}
Le maximum a posteriori revient à prendre
\[ \esthmap(Z) = \argmax_{\Theta} T_{Z|\Theta}(z|\theta) T_{\Theta}(\theta). \]

\subsection{Fisher}
\subsubsection{Maximum de Vraisemblance (ML)}
ML est simplement l'adaptation de MAP pour Fisher.
Comme $T_{\Theta}$ est uniforme, on a
\[ \esthml(Z) = \argmax_{\Theta} T_{Z|\Theta}(z|\theta). \]

\begin{mydef}[Convergence en probabilité]
  Une SA $X_k$ converge en moyenne quadratique vers $X$ si
  $\forall \epsilon > 0$,
\[ \lim_{k\to\infty} P(\|X_k-X\| > \epsilon) = 0. \]
\end{mydef}
La convergence en probabilité est \emph{plus faible} que la convergence en moyenne quadratique.

$\esthml_N$ converge en probabilité vers $\theta$ et est
asymptotiquement normal et efficace. %% FIXME pour être efficace, il faut converger en moyenne quadratique pourtant convergence en proba plus faible :o

Lorsqu'on connait $T_{\Theta}$, il faut mieux utiliser $\esthmap$ car il
a typiquement une covariance inférieure à $\esthml$.

\subsection{Mesure bruitée}
\subsubsection{Estimateur des moindres carrés (LS)}
Comme $h$ et déterministe, on peut simplement prendre $\esth$ tel que
$Z - h(\esth)$ est minimum.
Mais comme c'est des vecteurs, le minimum est pas vraiment défini.
On va donc plutôt dire qu'on minimise sa ``norme''\footnote{
  on ne prend pas une norme car on laisse $W$ être semi-définie positive et
  non définie positive}
$(Z-h(\theta))^TW(Z-h(\theta))$
où $W$ est une matrice semi-définie positive.
$\esthls$ dépend donc du choix de $W$.
On a donc
\[ \esthls(Z) = \argmin_\theta[(Z-h(\theta))^TW(Z-h(\theta))]. \]
Si $h$ est linéaire, c'est à dire qu'il existe $H \in \R^{n \times p}$ tel que
$h(\theta) = H\theta$, ça devient
\[ \esthls(Z) = (H^TWH)^{-1}H^TWZ. \]

$W$ donne l'importance qu'on donne aux différentes erreurs.
$W = I$ donne la même importance à toutes les erreurs et correspond à l'estimateur
des moindres carrés ``classique'' qu'on a toujours utilisé $\argmin_\Theta ||z-h(\theta)||_2^2$.
On l'utilise quand on a pas d'information sur le bruit $V$.

Comme on le verra à la \sectionref{lingauss},
avec un bruit \emph{Gaussien}, il est opportun de donner
une importance moindre au bruit d'une composante lorsque celle-ci est affectée
d'un bruit de grande variance car elle est moins ``fiable''.
Par exemple,
si les composantes du bruit sont indépendantes les unes des autres, $R$ a la variance
de chaque composante sur sa diagonale et $R^{-1}$ a leur inverse.
À la limite, si une composante, si certaines composantes ne sont pas bruitées,
on leur donnera un poids infini et on commencera à minimiser leurs erreurs
et on s'intéressera aux erreurs des autres composantes uniquement si on a
trouvé des solutions qui les rendent nulles.

\subsubsection{Estimateur linéaire à variance minimale (BLUE)}
En supposant que $h$ est linéaire et que
$V$ a une moyenne nulle et une covariance $R$
(comme $m_V = 0$, $V_c = V$ et donc $R_V = C_V$),
on a
\[ Z = H\theta + V \]
et
l'estimateur linéaire à covariance minimale (Best Linear Unbiased Estimator)
est
\[ \esthblue(Z) = (H^TR^{-1}H)^{-1}H^TR^{-1}z. \]

$\esthblue$ est non-biaisé et est l'estimateur linéaire avec la plus ``petite'' covariance
(il n'est donc pas d'office efficace mais, c'est le mieux qu'on peut faire avec un estimateur linéaire).

\subsection{Modèle linéaire et Gaussien}
\label{sec:lingauss}
Prenons un modèle linéaire mais Bayésien
\[ Z = H\Theta + V \]
où $\Theta$ et $V$ (et donc $Z$ aussi) sont Gaussiens
\begin{itemize}
  \item $\Theta \sim \N(m_\Theta, Q)$,
  \item $V \sim \N(0, R)$,
  \item $\Theta$ et $V$ sont indépendants.
\end{itemize}

$\esthmap$, $\esthcm$ et $\esthlmmse$ sont pareils et sont
bien entendu linéaires (car LMMSE est linéaire) et valent
\[ \esthmap = \esthcm = \esthlmmse = (Q^{-1} + H^*R^{-1}H)^{-1}(Q^{-1}m_\Theta + H^*R^{-1}z). \]
Ils sont non-baisés ce qui est sans surprise car c'est un CM et un LMMSE
mais qui n'était pas garanti pour le MAP.
Sa covariance vaut
\[ \cov(\esthmap) = \cov(\esthcm) = \cov(\esthlmmse) = (Q^{-1} + H^*R^{-1}H)^{-1})^{-1}. \]

Même si ici on connait $T_\Theta$, on peut
appliquer nos estimateurs du modèle de Fisher et
de la mesure bruitée en considérant que $T_\Theta$ est uniforme.
On va se rendre compte qu'on a des covariances moins bonne ce
qui est attendu car on utilise pas toutes l'information disponible.

L'estimateur ML est l'estimateur MAP avec $T_\Theta$ uniforme et
donc $Q$ infini et $Q^{-1} = 0$.
On a donc $\esthml = \esthblue$ et
\begin{align*}
  \esthml & = (H^*R^{-1}H)^{-1}H^*R^{-1}Z &
  \esthls & = (H^TWH)^{-1}H^TWZ\\
  \cov(\esthml) & = (H^TR^{-1}H)^{-1} &
  \cov(\esthls) & = (H^*WH)^{-1}H^*WRWH(H^TWH)^{-1}.
\end{align*}

Comparons les efficacités. On a
\[ \cov(\esthmap) \preceq \cov(\esthml) \preceq \cov(\esthls) \]
avec égalité pour la première inégalité si $Q^{-1} = 0$ et
égalité pour la seconde si $W = R^{-1}$.
On est donc ``moins efficace'' que pour l'estimateur Bayésien.

On peut en conclure que pour un bruit Gaussien, $W = R^{-1}$
est un bon choix et que lorsqu'on a de l'information sur $T_\Theta$,
c'est bon de l'utiliser et de travailler avec le modèle de Bayes.

\end{document}
