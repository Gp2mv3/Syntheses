\documentclass[en]{../../../../../../eplexam}

\hypertitle{Stochastic processes}{6}{INMA}{1731}{2019}{June}{Majeure}
{Alice Borb√°th \and Olivier Leblanc \and Julien Verecken}
{Pierre-Antoine Absil and Luc Vandendorpe}

Exam during 3h with 3 questions: 2 questions by PAA and 1 question by LV.

\section{PAA 1}

Let $G(z)$ be an LTI system which is stable and causal. 
One inputs a white noise signal $X$ of variance $\sigma_X^2$ in the system and outputs $Y$.
The transfer function is given by:
\begin{equation}
    G(z)=\frac{z}{\left(z-\frac{1-\epsilon}{\sqrt{2}}(1+j)\right)\left(z-\frac{1-\epsilon}{\sqrt{2}}(1-j)\right)}
\end{equation}
where $0<\epsilon\ll1$.
\begin{enumerate}
    \item Compute the power spectral density of the output $Y$, i.e. $\gamma_Y(z)$.
    \item Sketch $\gamma_Y(e^{j\Omega})$. In other words, show what the graph of the function $\Omega \mapsto \gamma_Y(e^{j\Omega})$ looks like. Give an approximation of its local extrema (position and value).
    \item Define the system by a recurrence equation.
    \item What kind of finite-dimensional model do you obtain?
\end{enumerate}

\nosolution

\section{PAA 2}

Let $Z$ be an observed random vector in $\Rn$ and $g \colon \Rn \to \R$ an estimation rule for a stochastic estimation $\Theta$ (Bayesian estimation). Suppose that $\mathbb{E}[g(Z)]=\mathbb{E}[\Theta]$ and that we have a function $h \colon \Rn \to \R$ such that $\mathbb{E}[h(Z)]=m_{h(Z)}$ is known. Consider the new estimation rule
\begin{equation}
    \Tilde{g}(Z)=g(Z)-\alpha(h(Z)-m_{h(Z)}),
\end{equation}
where $\alpha \in \R$. Propose a principled way to choose $\alpha$ assuming that the covariance of $[\Theta, g(Z), h(Z)]$ is known.

\nosolution

\section{LV 1}

One investigates the linear estimation of a zero-mean signal $x[k]$ from a signal $y[k]$ containing a colored (non-white) noise. More precisely,
\begin{equation}
    y[k]=\sum_{l=0}^{L-1}h[l]x[k-l]+\nu[k],
\end{equation}
where
\begin{itemize}
    \item $h[l]$ is the finite impulse response of a deterministic causal filter,
    \item the vector process ($x[k]$ $y[k]$)$^T$ is WSS,
    \item $\nu[k]$ is a zero-mean additive (uncorrelated with $x[k]$) NON-white noise, with autocovariance function $C_\nu[k]$ and power spectrum density $\gamma_\nu(\e^{j\Omega})$.
\end{itemize}
One investigates the optimum smoother corresponding to 
\begin{equation}
    \hat{x}[k]=\sum_{l=-\infty}^{\infty}w[l]y[k-l].
\end{equation}
\begin{enumerate}
    \item Derive and provide the optimum smoother coefficients $W(\e^{j\Omega})$ for an estimator optimized according to the MMSE or Wiener criterion. Express your result as a function of the power spectral densities of $x[k]$ and $\nu[k]$, and $H(\e^{j\Omega})$.
\end{enumerate}

We then investigate a solution where the first step is to whiten the noise (make it white). We know that there exists a filter $L_\nu(\e^{j\Omega})$ such that $\gamma_\nu(\e^{j\Omega})=L_\nu(\e^{j\Omega})$. By passing the signal $y[k]$ in a filter with frequency response $\frac{1}{L_\nu(\e^{j\Omega})}$, we obtain a new signal $y_2[k]$ from which we investigate the MMSE estimation of $x[k]$ with the help of an optimum smoother with coefficients $w_2[k]$, that is 
\begin{equation}
    \hat{x}'[k]=\sum_{l=-\infty}^{\infty}w_2[l]\: y_2[k-l]
\end{equation}

\begin{enumerate}[resume]
    \item Derive and provide the optimum smoother coefficients $W_2(\e^{j\Omega})$ for an estimator of $x[k]$ optimized according to the MMSE or Wiener criterion. Express your result as a function of the power spectral densities of $x[k]$ and $\nu[k]$, and $H(\e^{j\Omega})$.
    \item How does the estimate $\hat{x}'[k]$ computed by means of $w_2[l]$ compare with $\hat{x}[k]$ obtained thanks to $w[l]$? Justify.
\end{enumerate}

\nosolution

\end{document}
