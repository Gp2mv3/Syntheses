\documentclass[fr]{../../../../../../eplexam}

\hypertitle{Processus Stochastiques}{6}{INMA}{1731}{2018}{Août}{Majeure}
{Nicolas Dourt}
{Pierre-Antoine Absil \and Luc Vandendorpe}

\section{PAA}
\begin{itemize}
    \item Give a very short introduction to estimation theory, including the concept of unbiaised estimator. You should use at most one page with normal handwriting size.
    \item Let $x$ be a gaussian vector with mean $m_x$ and covariance $C_{xx}$. let vectors $a$ and $b$ be given. Give and unbiaised estimator of $\theta := a^Tx$ based on $z := b^Tx$, with a \sout{variance} \textbf{MSE} (il a changé pdt l'examen) as small as possible. Give your development in detail.
\end{itemize}

\section{LV}
One considers the Wiener or MMSE estimation of a signal $X(k)$ on the basis of another signal $Y(k)$.
\begin{itemize}
    \item What are the assumptions that the signals $X(k)$ and $Y(k)$ must fulfil ? Mention them and explain mathematically what they mean.
    \item We assume a problem where $\hat{X}(k)$, the estimate of $X(k)$ is computed as 
    $$ \hat{X}(k) = \sum_{l=0}^{N-1} x(l)Y(k-l) $$
    Where $N$ is the length of the impulse response $w(l)$. Provide the mathematical derivations to show how we arrive at the orthogonality principle that must be fulfilled by the coefficients $w(l)$ to be MMSE or Wiener Optimum. Please note that all signals can be complex valued.
    \item Expand the orthogonality principle and provide (derive) the final set of equations that must be fulfilled by the coefficients $w(l)$.
    \item How do we know we get a minimum ?
    \item What do these equations become when $X(k) = Y(k-2)$ ? Explain.
\end{itemize}

\end{document}
