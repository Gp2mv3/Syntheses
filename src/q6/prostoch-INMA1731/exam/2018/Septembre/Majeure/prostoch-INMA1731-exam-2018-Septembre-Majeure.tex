\documentclass[fr]{../../../../../../eplexam}
\usepackage[normalem]{ulem}

\hypertitle{Processus Stochastiques}{6}{INMA}{1731}{2018}{Août}{Majeure}
{Nicolas Dourt}
{Pierre-Antoine Absil et Luc Vandendorpe}

\section{PAA}
\begin{itemize}
    \item Give a very short introduction to estimation theory, including the concept of unbiased estimator. You should use at most one page with normal handwriting size.
    \item Let $x$ be a gaussian vector with mean $m_x$ and covariance $C_{xx}$. Let vectors $a$ and $b$ be given. Give an unbiased estimator of $\theta := a^Tx$ based on $z := b^Tx$, with a \sout{variance} \textbf{MSE}\footnote{Le professeur a changé de terme pendant l'examen.} as small as possible. Give your development in detail.
\end{itemize}

\nosolution

\section{LV}
One considers the Wiener or MMSE estimation of a signal $X(k)$ on the basis of another signal $Y(k)$.
\begin{itemize}
    \item What are the assumptions that the signals $X(k)$ and $Y(k)$ must fulfill ? Mention them and explain mathematically what they mean.
    \item We assume a problem where $\hat{X}(k)$, the estimate of $X(k)$ is computed as 
    $$ \hat{X}(k) = \sum_{l=0}^{N-1} x(l)Y(k-l) $$
    where $N$ is the length of the impulse response $w(l)$. Provide the mathematical derivations to show how we arrive at the orthogonality principle that must be fulfilled by the coefficients $w(l)$ to be MMSE or Wiener Optimum. Please note that all signals can be complex-valued.
    \item Expand the orthogonality principle and provide (derive) the final set of equations that must be fulfilled by the coefficients $w(l)$.
    \item How do we know we get a minimum ?
    \item What do these equations become when $X(k) = Y(k-2)$ ? Explain.
\end{itemize}

\nosolution

\end{document}
