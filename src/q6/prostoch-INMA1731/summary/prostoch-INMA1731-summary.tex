\documentclass[fr]{../../../eplsummary}

\usepackage{qtree}

\def\Perp{\perp\!\!\!\perp}

\newcommand{\E}{\mathbb{E}}
\renewcommand{\N}{\mathcal{N}}
\DeclareMathOperator{\var}{\mathbb{V}ar}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\bias}{Bias}

% Distributions
\DeclareMathOperator{\bin}{Bin}
\DeclareMathOperator{\expo}{Expo}
\DeclareMathOperator{\gammad}{Gamma}
\DeclareMathOperator{\po}{Po}

\newcommand{\fourier}{\mathcal{F}}
\newcommand{\esth}{\hat{\theta}}
\newcommand{\esthn}[1]{\esth^{\mathrm{#1}}}
\newcommand{\esthls}{\esthn{LS}}
\newcommand{\esthcm}{\esthn{CM}}
\newcommand{\esthmap}{\esthn{MAP}}
\newcommand{\esthml}{\esthn{ML}}
\newcommand{\esthblue}{\esthn{BLUE}}
\newcommand{\esthlmmse}{\esthn{LMMSE}}
\newcommand{\msethf}{\mathrm{MSE}_{\esth;\theta}}
\newcommand{\msethb}{\mathrm{MSE}_{\esth}}

\hypertitle{Processus stochastiques}{6}{INMA}{1731}
{Benoît Legat}
{Pierre-Antoine Absil et Luc Vanderdorpe}

\newcommand{\Ex}[1]{\E\left\{#1\right\}}

% No randomness, no info

\section{Probabilités et variables aléatoires}
\subsection{Notations}
On met les signaux déterministes en minuscule et les signaux aléatoires en majuscule (comme pour les variables aléatoires en probabilité).
On met aussi un signal aléatoire en minuscule lorsqu'on parle d'une de ses instances (comme pour proba à nouveau, on met par exemple $P(x \geq X)$ où $x$ est une valeur de la variable aléatoire $X$ et $X$ est la variable aléatoire).

\subsection{Probabilité conditionnelles et formule de Bayes}
Soient $E_1, E_2$ des évènements et $H_1, H_2, \ldots, H_n$ des évènements mutuellement exclusifs tels que $p_{H_1 \cup H_2 \cup \cdots \cup H_n} = 1$.
On a
\begin{align*}
  p_{E_1 \cup E_2} & = p_{E_1} + p_{E_2} - p_{E_1 \cap E_2}\\
  p_{E_1 \cap E_2} & = p_{E_1|E_2} p_{E_2}\\
                   & = p_{E_2|E_1} p_{E_1}\\
  p_{E_1}          & = \sum_{i=1}^n p_{E_1|H_i} p_{H_i}.
\end{align*}

\subsubsection{Probabilité à postiori}
La loi de Bayes vient simplement de l'égalité $p_{E_2|E_1} p_{E_1} = p_{E_1 \cap E_2} = p_{E_1|E_2} p_{E_2}$, qui donne
\begin{align*}
  p_{E_2|E_1} & = \frac{p_{E_1|E_2}p_{E_2}}{p_{E_1}}\\
  p_{H_i|E_1} & = \frac{p_{E_1|H_i}p_{H_i}}{p_{E_1}}\\
  p_{H_i|E_1} & = \frac{p_{E_1|H_i}p_{H_i}}{\sum_{i=1}^n p_{E_1|H_i} p_{H_i}}.
\end{align*}

\subsection{Variable aléatoire}
Soit une VA (Variable Aléatoire) $X$.
On note $T_X(x)$ sa densité de probabilité,
$m_n$ son moment d'ordre $n$, $\mu_n$ son moment \emph{centré} d'ordre $n$
et $X_c = X - m_1$ la variable centrée.
\begin{align*}
  m_n   & = \Ex{X^n}\\
  \mu_n & = \Ex{(X - m_1)^n}\\
        & = \Ex{X_c^n}.
\end{align*}
La relation classique dans ces nouvelles notations est
\[
  \mu_2 = \sigma_X^2 = m_2 - m_1^2.
\]

\subsubsection{Fonction caractéristique}
On définit la fonction caractéristique $\phi_X(q)$ comme la transformée fourier
(avec $q = -\omega$) de $T_X$, c'est à dire
\begin{align*}
  \phi_X(q) & = \Ex{e^{jqx}}\\
  T_X(x) & = \frac{1}{2\pi} \int_{-\infty}^{\infty} \phi_x(q) e^{-jqx} \dif q.
\end{align*}

$\phi_X$ est très pratique car tout comme $T_X$, elle détermine
entièrement la distribution d'une VA: $\phi_X = \phi_Y$ ssi $T_X = T_Y$.
Il permet donc facilement de trouver $T_Y$ lorsqu'on connait $T_{X_i}$ et $Y = \sum_i X_i$,
car $e^{jq\sum_i X_i} = \prod_i e^{jqX_i}$ et si les $X_i$ sont \emph{indépendants},
$\prod_Y(q) = \Ex{\prod_i e^{jqX_i}} = \prod_i \Ex{e^{jqX_i}} = \prod_i \phi_{X_i}(q)$.

Les moments sont obtenus aisément par
\[ m_n = \frac{1}{j^n} \left.\frac{\dif^n \phi_x}{\dif q^n}\right|_{q = 0}. \]

\subsubsection{Variable aléatoire à plusieurs dimensions}
Lorsqu'on parle d'une VA à plusieurs dimensions,
$m$ et $\mu$ changent de sens et on n'a que des notations pour
les moments d'ordre 1 et 2.
Les nouvelles notations sont reprises dans la \tabref{momentdim}.
\begin{table}
  \centering
  \begin{tabular}{|l|c|c|}
    \hline
            & non-centré & centré\\
    \hline
    Ordre 1 & $m_i$      & $\mu_i$\\
    \hline
    Ordre 2 & $R_{ij}$   & $C_{ij}$\\ % FIXME ou $\sigma_{ij}$ ?
    \hline
  \end{tabular}
  \caption{Tables des moments pour une VA à plusieurs dimensions.}
  \label{tab:momentdim}
\end{table}
$R_{ii}$ et $C_{ii}$ se racourcissent bien évidemment en $R_i$
et $C_i$ ou $\sigma_i^2$.
\begin{align*}
  \sigma_i^2 & = R_i - m_i^2\\
  C_{ij}   & = R_{ij} - m_im_j.
\end{align*}

On a donc, en tout généralité,
\begin{align*}
  R_{ij} & = \Ex{X_iX_j^*}\\
  C_{ij} & = \Ex{(X_i-m_i)(X_j-m_j)^*}\\
         & = \Ex{X_{i;c}X_{j;c}^*}
\end{align*}
où $^*$ représente la matrice adjointe,
c'est à dire la matrice transposée de la conjuguée.

On définit aussi la matrice des moments d'ordre 2 et la \emph{matrice de covariance}
\begin{align*}
  R_X    & = \Ex{X X^*}\\
  R_{XY} & = \Ex{X Y^*}\\
  C_X    & = \Ex{X_c X_c^*}\\
  C_{XY} & = \Ex{X_c Y_c^*}.
\end{align*}
En particulier, on a donc
\begin{align*}
  R_X & =
    \begin{pmatrix}
      R_1        & R_{12} & \cdots & R_{1n}\\
      R_{21}     & R_2 & \cdots & R_{2n}\\
      \vdots     & \vdots & \ddots & \vdots\\
      R_{n1}     & R_{n2} & \cdots & R_n
    \end{pmatrix} &
  C_X & =
  \begin{pmatrix}
    \sigma_1^2 & C_{12}     & \cdots & C_{1n}\\
    C_{21}     & \sigma_2^2 & \cdots & C_{2n}\\
    \vdots     & \vdots     & \ddots & \vdots\\
    C_{n1}     & C_{n2}     & \cdots & \sigma_n^2
  \end{pmatrix}.
\end{align*}

Une propriété très importante sur les matrices $R_{XY}$ et $C_{XY}$ est
qu'elles sont \emph{hermitiennes} ($R_{XY}^* = R_{XY}$ et $C_{XY}^* = C_{XY}$)
et \emph{semi-définies positives}.
Comme elle est hermitienne, elle est aussi diagonalisable par une famille
de vecteurs propres orthonormés,
c'est à dire qu'il existe une matrice orthogonale $M$ telle que
$R_{XY}$ ou $C_{XY}$ vaut $M \Lambda M^*$.

\subsubsection{Indépendance}
$X_1$ et $X_2$ sont indépendantes, noté $X_1 \Perp X_2$,
si et seulement si $p_{X_1|X_2} = p_{X_1}$ si $X_1$ et $X_2$ sont des évènements et
$T_{X_1|X_2} (x_1|x_2) = T_{X_1}(x_1)$ si c'est des VA.

Les conséquences de l'indépendance entre $X_1$ et $X_2$ sont,
pour toutes fonctions $f, g$,
\begin{align*}
  f(X_1) & \Perp g(X_2)\\
  \E(X_1 X_2) & = \E(X_1) \E(X_2)\\
  \phi_{X_1 + X_2}(q) & = \phi_{X_1}(q) \phi_{X_2}(q)\\
  \var(a + bX_1 + cX_2) & = b^2\var(X_1) + c^2\var(X_2)\\
  C_{12} & = 0.
\end{align*}
Ce ne sont que des conséquences, \emph{pas} de conditions nécessaires
à être indépendant.
Lorsque $C_{12} = 0$ on dit que $X_1$ et $X_2$ sont décorrelés ou orthogonales.
La correlation entre $X_1$ et $X_2$ est d'ailleurs
\begin{align*}
  \rho & = \Ex{\left(\frac{X_1-m_1}{\sigma_1}\right)\left(\frac{X_2-m_2}{\sigma_2}\right)}\\
       & = \frac{C_{12}}{\sigma_1\sigma_2}.
\end{align*}
Si $X_1$ et $X_2$ sont normales, $C_{12} = 0$ est aussi une condition
suffisante pour que $X_1 \Perp X_2$.

\section{Fonctions aléatoires et processus stochastiques}
\subsection{Fonctions aléatoires}
Les notations pour une FA sont les mêmes que pour une VA à plusieurs dimensions
sauf qu'on utilise plus d'indice mais plutôt des arguments,
voir~\tabref{momentfun}.
\begin{table}
  \centering
  \begin{tabular}{|l|c|c|}
    \hline
            & non-centré & centré\\
    \hline
    Ordre 1 & $m(t)$      & $\mu(t)$\\
    \hline
    Ordre 2 & $R(t_i,t_j)$   & $C(t_i,t_j)$\\ % FIXME ou $\sigma(t_i,t_j)$ ?
    \hline
  \end{tabular}
  \caption{Tables des moments pour une FA.}
  \label{tab:momentfun}
\end{table}
$C(t,t)$ se racourcit en $\sigma^2(t)$.
On appelle $C$ la \emph{fonction d'autocovariance}.

On a donc, en tout généralité,
\begin{align*}
  R_{X_1X_2}(t_1,t_2) & = \Ex{X_1(t_1)X_2^*(t_2)}\\
  C_{X_1X_2}(t_1,t_2) & = \Ex{(X_1(t_1)-m_1(t_1))(X_2(t_1)-m_2(t_2))^*}\\
                      & = \Ex{X_{1;c}(t_1)X_{2;c}^*(t_2)}.
\end{align*}

On peut calculer $R$ ou $C$ entre deux FA différentes $X_1,X_2$,
on notera alors implement $C_{X_1X_2}(t_1,t_2)$ et le premier temps
(ici $t_1$) réfère à la première variable, donc $X_1(t_1)$ et $m_{X_1}(t_1)$.

Tout comme $C_{XY}$ était hermitienne,
on a
\begin{equation}
  \label{eq:faherm}
  C_{X_1X_2}(t_1,t_2) = C_{X_2X_1}(t_2,t_1).
\end{equation}

\subsubsection{Stationnarité d'une FA}
\paragraph{Stationnarité au sens stricte}
On dit qu'une FA est stationnaire au sens stricte si,
elle est complètement indépendante de l'origine du temps,
c'est à dire que pour tout $t_1,t_2,\ldots,t_n$,
\[ T_X(x(t_1),x(t_2),\ldots,x(t_n)) = T_X(x(0),x(t_2-t_1),x(t_3-t_1),\ldots,x(t_n-t_1)). \]

Imposer la stationnarité au sens stricte est un peu difficile en pratique,
on introduit alors la stationnarité au sens faible.

\paragraph{Stationnarité au sens faible}
Une FA est stationnaire au sens faible (abrégé WSS) lorsque
ses deux premiers moments sont stationnaires,
c'est à dire que
\begin{align*}
  m_X(t)       & = m_X\\
  C_X(t_1,t_2) & = C_X(\tau = t_1 - t_2). % FIXME not t_2 - t_1 ?
\end{align*}
La variance est alors aussi constante,
$\sigma_X^2(t) = C_X(t,t) = C_X(0) = \sigma_X^2$.
\eqref{eq:faherm} devient $C_X(-\tau) = C_X^*(\tau)$.
On a également $|C_x(\tau)| \leq |C_X(0)| = \sigma_X^2$
\footnote{$|\sigma_X^2| = \sigma_X^2$ car $\sigma_X^2$ est un réel positif car
$(X(t)-m_X(t))(X(t)-m_X(t))^* = |X(t)-m_X(t)|$ est un réel positif}.

\subsubsection{Représentation spectrale}
On définit $\mathcal{X}(w) = \fourier(X_c(t))$ la transformée de fourier
\emph{centrée} de $X_c$.
Comme $\mathcal{X}$ est aussi centré,
$C_\mathcal{X} = \Ex{\mathcal{X}(w)\mathcal{X}^*(w)}$.

Si $X$ est \emph{WSS}, $\mathcal{X}$ est à valeures \emph{décorrelé},
c'est le théorème de Cramér-Loève. On a donc
\[
  C_\mathcal{X}(w,w') = 2\pi \gamma_X(w) \delta(w - w')
\]
avec $\gamma_X(w) \eqdef \fourier(R_X(\tau))$\footnote{
C'est bien $R$ et non $C$.
Dans le syllabus,
ils mettent $C$ car on prend $X_c$ qui est centré et donc $C_{X_c} = R_{X_c}$.
Par après avec Wiener, on voit bien qu'on prend $R$ et non $C$.}
(on voit que $\gamma_X(w)$ n'a du sens que si $X$ est WSS).

\paragraph{Effet d'un système LTI sur une FA}
Si $Y = g * X$ où $g$ est une fonction de transfert déterministe,
on a
\begin{align*}
  m_Y & = G(0) m_X & m_Y & = G(1) m_X = G(e^{j0}) m_X.
\end{align*}

Si $X$ est \emph{WSS}, $Y$ l'est également !
$\gamma_Y$ est donc aussi défini et% FIXME ", on a le \emph{théorème de Wiener Kintchine}" sur Wikipedia, ils ont pas pareil
% FIXME L'ordre est important dans le cas discret car c'est des matrices mais là c'est pas des matrices,
%       mais si on fait inverse z transform c'est re des matrices.
\begin{align*}
  \gamma_Y(s) & = G(s)\gamma_X(s)G^*(-s^*) & \gamma_Y(z) & = G(z) \gamma_X(z) G^*(1/z^*).
\end{align*}

% TODO add C_Y = C_h * C_X ? C_h est calculé comme un ergodique

\subsubsection{Factorisation spectrale}
\begin{mydef}[Bruit blanc]
  Une FA \emph{WSS} $W$ est appelée bruit blanc si elle est à valeur décorrelées.
  On a donc
  \begin{align*}
    C_W(\tau) & = \sigma_W^2 \delta(\tau)\\
    \gamma_W(w) & = \sigma_W^2.
  \end{align*}
\end{mydef}

\begin{mydef}[Processus régulier]
  Un processus régulier est un processus $Y$ qui peut être exprimé
  la sortie d'un système LTI avec en entrée un bruit blanc.
  Il existe donc $L(\omega)$ et $\sigma$ tels que
  \[ \gamma_Y(\omega) = L(\omega) L^*(\omega) \sigma^2. \]
\end{mydef}

La factorisation spectrale consiste à trouver $L(\omega)$ à partir
de $\gamma_Y(\omega)$.
Voyons la méthode pour trouver un $L(s)$ stable et à minimum de phase\footnote{
  ``stable'' signifie que les pôles sont stables et ``minimum de phase'' signifie
  que les zéros sont stables.
}
dans le cas où $Y$ est réel et que $\gamma_Y$ est sous forme d'une fraction de
polynômes à coefficients réels.
Comme $Y$ est réel, $C_Y$ aussi et donc
$C_Y(-\omega) = C_Y^*(\omega) = C_Y(\omega)$.
$C_Y$ est donc pair et donc
\begin{align*}
  \gamma_Y(-\omega) & = \int_{-\infty}^{\infty} e^{j\omega\tau} C_Y(\tau) \dif \tau\\
                    & = \int_{\infty}^{-\infty} e^{j\omega(-\tau)} C_Y(-\tau) \dif (-\tau)\\
                    & = \int_{-\infty}^{\infty} e^{-j\omega\tau} C_Y(\tau) \dif \tau\\
                    & = \gamma_Y(\omega).
\end{align*}
On sait que alors que $\gamma_Y(-\omega) = \gamma_Y(\omega)$,
$\gamma_Y$ est donc pair et comme c'est une fraction de polynômes ça signifie
que $\gamma_Y$ est en fonction de $\omega^2$.
On passe alors à la transformée en $s$.
On sait que la transformée de fourier est
la transformée en $s$ pour $s = j\omega$.
La transformée en $s$ est donc vraissemblablement $\gamma_Y$ où on remplace
$\omega^2$ par $-s^2$.
On mets alors $\gamma_Y$ sous la forme
\begin{align*}
  \gamma_Y(s) & = \sigma^2\frac{\prod_i (z_i^2-s^2)}{\prod_i (p_i^2-s^2)}\\
              & = \sigma^2\frac{\prod_i (z_i+s)}{\prod_i (p_i+s)}\frac{\prod_i (z_i-s)}{\prod_i (p_i-s)}\\
              & = \sigma^2 L(s) L(-s)
\end{align*}
où $z_i$ et $p_i$ ont une partie réelle positive.
Comme $L(s)$ a des pôles en $-p_i$, il est stable et comme il
a des zéros en $-z_i$, il est à minimum de phase.

\paragraph{En discret}
En discret, on s'impose aussi que $L(z)$ soit monique, c'est à dire que le coefficient
de degré le plus élevé du numérateur et dénominateur doit être 1.
La parité implique que $\gamma_Y$ est fonction de
$\cos(\omega) = \frac{e^{j\omega} + e^{-j\omega}}{2}$.
On remplace $e^{j\omega}$ par $z$, ce qui donne $\cos(\omega) = z^{-1}\frac{z^2+z}{2}$
et on a
\begin{align*}
  \gamma_Y(s) & = {\sigma'}^2\frac{z^{-n_z}\prod_i^{n_z} (z-z_i^{-1})(z-z_i)}{z^{-n_p}\prod_i^{n_p} (z-p_i^{-1})(z-p_i)}\\
              & = \frac{z_i^{n_z}}{p_i^{n_p}}{\sigma'}^2\frac{\prod_i^{n_z} (z-z_i^{-1})(z^{-1}-z_i^{-1})}{\prod_i^{n_p} (z-p_i^{-1})(z^{-1}-p_i^{-1})}\\
              & = \sigma^2 L(z) L(z^{-1})
\end{align*}
où $z_i$ et $p_i$ ont un module plus grand que 1.
Comme $L(z)$ a des pôles en $p_i^{-1}$, il est stable et comme il
a des zéros en $z_i^{-1}$, il est à minimum de phase.
Le terme de plus haute puissance au numérateur et au dénominateur sont
respectivement $z^{n_z}$ et $z^{n_p}$.
$L(z)$ est donc bien monique.

\subsection{Processus stochastiques}
Les différents processus stochastiques sont repris par la \figuref{processus}.
\begin{figure}
  \Tree [.{Box et Jenkins\\$Y(k) = \frac{B(z^{-1})}{A(z^{-1})}U(k)+\frac{C(z^{-1})}{D(z^{-1})}E(k)$}
    [.{ARMAX\\$A(z^{-1})Y(k)=B(z^{-1})U(k)+C(z^{-1})E(k)$}
      [.{ARMA\\$A(z^{-1})Y(k)=C(z^{-1})E(k)$}
        [.{AR\\$A(z^{-1})Y(k)=E(k)$}
          [.{Markov stationnaire\\$(1-Az^{-1})Y(k) = E(k)$}
            [.{Wiener\\$(1-z^{-1})Y(k) = E(k)$} ]
          ]
        ]
        [.{MA ou FIR\\$Y(k)=C(z^{-1})E(k)$} ]
      ]
    ]
  ]
  \caption{Les différents processus du cas le plus général au plus particuliers.
  $A(z^{-1})$, $B(z^{-1})$ et $C(z^{-1})$ sont des polynômes moniques et stables
  (toutes les racines sont à l'intérieur du cercle unité)
  et $B(z^{-1})$ est quelconque.
  Markov peut être vu comme un processus AR avec $A(z^-1) = 1 - Az^{-1}$
  et $A$ Hurwitz (toutes ses valeurs propres à l'intérieur du cercle unité)
  pour que $A(z^-1)$ soit stable.}
  \label{fig:processus}
\end{figure}

\section{Théorie de l'estimation}
On considère $Z \in \Rn$ et $\theta \in \R^p$ tels que $Z = h(\theta)$.
On aimerait estimer $\theta$ en ayant observé une réalisation $z$.
\begin{mydef}[Estimateur]
  Un estimateur $\esth$ de $\theta$ est une fonction déterministe $g(Z)$ qui
  à toute réalisation $z$ de $Z$ fait correspondre une estimation $\esth=g(z)$.
\end{mydef}
Le problème, c'est que $h$ n'est pas déterministe,
on ne peut donc pas simplement prendre $g = h^{-1}$.

On peut se trouver dans 3 cas
\begin{description}
  \item[Bayes] $\Theta$ est un vecteur aléatoire et $Z$ est un vecteur aléatoire
    dont la densité de probabilité dépend d'une réalisation $\theta$
    de $\Theta$.
    Lorsqu'on observe une ou plusieurs réalisation $z$ de $Z$, on peut donc
    se donner une idée de la densité de probabilité de $Z$ et donc de $\theta$.
    On sera cependant influencé par $T_\Theta$ et on ne donnera donc pas à
    tous les $\theta$ la même chance d'être $\esth$.
  \item[Fisher] $\theta$ est déterministe, ce cas peut être considéré comme un cas
    particulier du cas précédent où $\Theta$ est une loi uniforme
    ($T_\Theta(\theta)$ est uniforme).
    On donne donc à tous les $\theta$ autant de chance d'être $\esth$.
  \item[Mesure bruitée] $Z$ n'est qu'une mesure bruitée de $\theta$, on a donc
    un $h$ \emph{déterministe} et un bruite $V$ tel que
    \[ Z = h(\theta) + V. \]
    Ça peut être considéré comme un cas particulier du cas précédent
    où $h'(\theta) = h(\theta) + V$.
    % FIXME un peu restrictif
    %On va alors être plus influencé par les composantes de $Z$ qui sont
    %moins bruitées.
\end{description}

\subsection{Propriétés d'un estimateur}

\begin{mydef}[Biais d'un estimateur]
  Le biais, $b_{\esth}$ est défini comme suit
  \begin{description}
    \item[Fisher]
      \[ b_{\esth;\theta} = m_{\esth} - \theta \]
      où
      $m_{\esth} \Ex{g(Z)|\theta}$.
    \item[Bayes] Dans le modèle de Bayes, on peut être moins restrictif et
      ne pas l'imposer pour tout $\theta$ et donner plus de poids
      si $T_\Theta(\theta)$ est plus grand.
      On va donc prendre
      \[ b_{\esth} = m_{\esth} - m_\Theta \]
      où $m_{\esth} = \Ex{g(Z)}$, on prend donc l'espérance en considérant
      tous les thetas à la fois.
  \end{description}
  Un estimateur $\esth$ est sans biais si $b_{\esth} = 0$.
  Pour Fisher, on doit avoir $b_{\esth;\theta} = 0$ $\forall \theta$,
  c'est donc plus restrictif que pour Bayes.
\end{mydef}

\begin{mydef}[Variance d'un estimateur]
  L'estimateur est une VA et sa matrice de covariance est défine comme pour une quelconque VA,
  c'est à dire
  \[ C_{\esth} = \Ex{(\esth - m_{\esth})(\esth - m_{\esth})^T}. \]
\end{mydef}

\begin{mydef}[Erreur quadratique moyenne d'un estimateur (MSE)]
  La MSE n'est pas le même dans le modèle de Bayes que dans le modèle de Fisher.
  En effet, comme dans le modèle de Fisher, $\theta$ est déterministe,
  la MSE dépend de $\theta$ alors que dans le modèle de Bayes,
  la MSE pondère l'erreur sur tous les $\theta$.
  \begin{description}
    \item[Fisher]
      \[ \msethf = \Ex{(\esth - \theta)(\esth-\theta)^T}. \]
    \item[Bayes]
      \[ \msethb = \Ex{(\esth - \Theta)(\esth-\Theta)^T}. \]
  \end{description}
\end{mydef}

On peut remarquer que,
si $\esth$ est \emph{non-biaisé},
$\msethf = \cov(\esth - \theta)$ et $\msethb = \cov(\esth - \Theta)$.
En effet, pour Fisher (le raisonnement est le même pour Bayes),
$\Ex{\esth - \theta} = 0$ s'il est non-baisié,
on a alors
\[ \cov(\esth - \theta) = \Ex{(\esth-\theta - 0)(\esth-\theta - 0)^T} = \msethf. \]

Dans le modèle de \emph{Fisher}, on a
\begin{equation}
  \label{eq:fisheqm}
  \msethf = C_{\esth} + b_{\esth;\theta}b_{\esth;\theta}^T.
\end{equation}

\begin{mydef}[Estimateur efficace ou estimateur à variance minimale]
  Un estimateur \emph{non-biaisé} $\esth$ de $\theta$ est efficace si
  pour tout autre estimateur \emph{non-biaisé} $\esth'$ de $\theta$, on a
  \[ C_{\esth} \preceq C_{\esth'} \]
  où $C_{\esth} \preceq C_{\esth'}$ signifie que $C_{\esth'} - C_{\esth}$
  est semi-définie positive.
\end{mydef}

\begin{mydef}[Matrice d'information de Fisher]
  Si $T_{Z|\Theta}(z|\theta)$ est différentiable par rapport à $\theta$,
  la matrice d'information de Fisher $I(\theta)$ est définie par
  \[ I_{k,j}(\theta) = -\Ex{\fpart{^2\ln T_{Z|\Theta}(z|\theta)}{\theta_k\partial\theta_j}}. \]
\end{mydef}

\begin{myineg}[Inégalité de Cramér-Rao]
  Dans le modèle de \emph{Fisher},
  pour tout estimateur \emph{non-biaisé} $\esth$ de $\theta$,
  on a
  \[ C_{\esth} \succeq I^{-1}(\theta). \]
\end{myineg}

On peut donc prouver qu'un estimateur $\esth$ est efficace en calculant
la matrice d'information de Fisher
et en montrant que $C_{\esth} = I^{-1}(\theta)$.

\subsection{Propriétés asymptotiques d'un estimateur}
Lorsqu'on estime $\theta$ sur $N$ observation,
on peut s'intéresser au comportement de $\esth_N = g(Z^N)$ l'estimation
de $\theta$ sur ces $N$ observations.
\begin{mydef}[Estimateur asymptotiquement non-biaisé]
  $m_{\esth}$ est calculé de la même façon que précédemment
  en fonction du cadre dans lequel on est (Fisher ou Bayes).
  Un estimateur est asymptotiquement non-biaisé si
  \begin{description}
    \item[Bayes]
      \[ \lim_{N\to\infty} m_{\esth} = m_\Theta \]
    \item[Fisher]
      \[ \lim_{N\to\infty} m_{\esth} = \theta \]
      $\forall \theta$.
  \end{description}
\end{mydef}
\begin{mydef}[Convergence en moyenne quadratique]
  Une SA $X_k$ converge en moyenne quadratique vers $X$ si
  \[ \lim_{k\to\infty} \Ex{\|X_k-X\|^2} = 0. \]
\end{mydef}
Pour Fisher,
$\esth_N$ converge en moyenne quadratique vers $\theta$ si
et seulement s'il est asymptotiquement non-biaisé et que la variance tend
vers 0 par l'\eqref{eq:fisheqm}.

\begin{mydef}[Estimateur asymptotiquement efficace]
  Soit $\esth_N$ un estimateur qui \emph{converge en moyenne quadratique} vers $\theta$.
  $\esth_N$ est asymptotiquement efficace s'il existe $N_0 > 0$ tel que
  pour tout $\esth'_N$, et pour tout $N > N_0$, on a
  \[ C_{\esth} \preceq C_{\esth'}. \]
\end{mydef}

\subsection{Bayes}
\subsubsection{Moyenne conditionnelle}
La moyenne conditionnelle revient à prendre
\[ \esthcm(Z) = \int_{\Theta} \theta T_{\Theta|Z}(\theta|z) \dif \theta. \]

Il est \emph{sans-biais} et \emph{efficace}.

\subsubsection{Estimateur linéaire à la MSE minimale (LMMSE)}
L'estimateur à la MSE minimale (Linear Minimum MSE).
revient à prendre
\[ \esthlmmse(Z) = m_\Theta + C_{\Theta Z} C_{Z}^{-1}(Z - m_Z). \]

Il est \emph{sans-biais} et c'est l'estimateur linéaire
qui a la plus petite MSE.
Il a aussi l'avantage de ne nécessiter les moments d'ordre
1 de $T_\Theta$ et $T_Z$ et d'ordre 2 de $T_{Z|\Theta}$.

Il obéit également au \emph{principe d'orthogonalité}:
\[ \Ex{(\esthlmmse - \Theta)Z^T} = 0 \]
ce qui implique que
\[ \Ex{(\esthlmmse - \Theta)(\esthlmmse)^T} = 0 \]
qui avec $\Ex{\esthlmmse} = m_\Theta$ détermine entièrement
$\esthlmmse$ et peut être utilisé pour le déterminer.

\subsubsection{Maximum a posteriori (MAP)}
Le maximum a posteriori revient à prendre
\[ \esthmap(Z) = \argmax_{\Theta} T_{Z|\Theta}(z|\theta) T_{\Theta}(\theta). \]

\subsection{Fisher}
\subsubsection{Maximum de Vraisemblance (ML)}
ML est simplement l'adaptation de MAP pour Fisher.
Comme $T_{\Theta}$ est uniforme, on a
\[ \esthml(Z) = \argmax_{\Theta} T_{Z|\Theta}(z|\theta). \]

\begin{mydef}[Convergence en probabilité]
  Une SA $X_k$ converge en moyenne quadratique vers $X$ si
  $\forall \epsilon > 0$,
\[ \lim_{k\to\infty} P(\|X_k-X\| > \epsilon) = 0. \]
\end{mydef}
La convergence en probabilité est \emph{plus faible} que la convergence en moyenne quadratique.

$\esthml_N$ converge en probabilité vers $\theta$ et est
asymptotiquement normal et efficace. %% FIXME pour être efficace, il faut converger en moyenne quadratique pourtant convergence en proba plus faible :o

Lorsqu'on connait $T_{\Theta}$, il faut mieux utiliser $\esthmap$ car il
a typiquement une covariance inférieure à $\esthml$.

\subsection{Mesure bruitée}
\subsubsection{Estimateur des moindres carrés (LS)}
Comme $h$ et déterministe, on peut simplement prendre $\esth$ tel que
$Z - h(\esth)$ est minimum.
Mais comme c'est des vecteurs, le minimum est pas vraiment défini.
On va donc plutôt dire qu'on minimise sa ``norme''\footnote{
  on ne prend pas une norme car on laisse $W$ être semi-définie positive et
  non définie positive}
$(Z-h(\theta))^TW(Z-h(\theta))$
où $W$ est une matrice semi-définie positive.
$\esthls$ dépend donc du choix de $W$.
On a donc
\[ \esthls(Z) = \argmin_\theta[(Z-h(\theta))^TW(Z-h(\theta))]. \]
Si $h$ est linéaire, c'est à dire qu'il existe $H \in \R^{n \times p}$ tel que
$h(\theta) = H\theta$, ça devient
\[ \esthls(Z) = (H^TWH)^{-1}H^TWZ. \]

$W$ donne l'importance qu'on donne aux différentes erreurs.
$W = I$ donne la même importance à toutes les erreurs et correspond à l'estimateur
des moindres carrés ``classique'' qu'on a toujours utilisé $\argmin_\Theta ||z-h(\theta)||_2^2$.
On l'utilise quand on a pas d'information sur le bruit $V$.

Comme on le verra à la \sectionref{lingauss},
avec un bruit \emph{Gaussien}, il est opportun de donner
une importance moindre au bruit d'une composante lorsque celle-ci est affectée
d'un bruit de grande variance car elle est moins ``fiable''.
Par exemple,
si les composantes du bruit sont indépendantes les unes des autres, $R$ a la variance
de chaque composante sur sa diagonale et $R^{-1}$ a leur inverse.
À la limite, si une composante, si certaines composantes ne sont pas bruitées,
on leur donnera un poids infini et on commencera à minimiser leurs erreurs
et on s'intéressera aux erreurs des autres composantes uniquement si on a
trouvé des solutions qui les rendent nulles.

\subsubsection{Estimateur linéaire à variance minimale (BLUE)}
En supposant que $h$ est linéaire et que
$V$ a une moyenne nulle et une covariance $R$
(comme $m_V = 0$, $V_c = V$ et donc $R_V = C_V$),
on a
\[ Z = H\theta + V \]
et
l'estimateur linéaire à covariance minimale (Best Linear Unbiased Estimator)
est
\[ \esthblue(Z) = (H^TR^{-1}H)^{-1}H^TR^{-1}z. \]

$\esthblue$ est non-biaisé et est l'estimateur linéaire avec la plus ``petite'' covariance
(il n'est donc pas d'office efficace mais, c'est le mieux qu'on peut faire avec un estimateur linéaire).

\subsection{Modèle linéaire et Gaussien}
\label{sec:lingauss}
Prenons un modèle linéaire mais Bayésien
\[ Z = H\Theta + V \]
où $\Theta$ et $V$ (et donc $Z$ aussi) sont Gaussiens
\begin{itemize}
  \item $\Theta \sim \N(m_\Theta, Q)$,
  \item $V \sim \N(0, R)$,
  \item $\Theta$ et $V$ sont indépendants.
\end{itemize}

Un premier estimateur $\tilde{\esth}$ serait $m_\Theta$
qui n'utilise aucune des observation.
On a $\msethb = \Ex{(m_\Theta-\Theta)(m_\Theta-\Theta)^T} = Q$.
Regardons si on arrive à quand même faire mieux.

$\esthmap$, $\esthcm$ et $\esthlmmse$ sont pareils
(voir \annexeref{maplmmselingauss}) et sont
bien entendu linéaires (car LMMSE est linéaire) et valent
\[ \esthmap = \esthcm = \esthlmmse = (Q^{-1} + H^TR^{-1}H)^{-1}(Q^{-1}m_\Theta + H^TR^{-1}z). \]
Ils sont non-baisés ce qui est sans surprise car c'est un CM et un LMMSE
mais qui n'était pas garanti pour le MAP.
Son $\msethb$ est
\[ \cov(\esthmap-\Theta) = (Q^{-1} + H^TR^{-1}H)^{-1}. \]

Même si ici on connait $T_\Theta$, on peut
appliquer nos estimateurs du modèle de Fisher et
de la mesure bruitée en considérant que $T_\Theta$ est uniforme.
On va se rendre compte qu'on a des covariances moins bonne ce
qui est attendu car on utilise pas toutes l'information disponible.

L'estimateur ML est l'estimateur MAP avec $T_\Theta$ uniforme et
donc $Q$ infini et $Q^{-1} = 0$.
On a donc $\esthml = \esthblue$ et
\begin{align*}
  \esthml & = (H^TR^{-1}H)^{-1}H^TR^{-1}Z &
  \esthls & = (H^TWH)^{-1}H^TWZ\\
  \cov(\esthml - \Theta) & = (H^TR^{-1}H)^{-1} &
  \cov(\esthls - \Theta) & = (H^TWH)^{-1}H^TWRWH(H^TWH)^{-1}.
\end{align*}

Comparons les efficacités. On a
\[ \cov(\esthmap-\Theta) \preceq \cov(\esthml-\Theta) \preceq \cov(\esthls-\Theta) \preceq \cov(\tilde{\esth}-\Theta) \]
avec égalité pour la première inégalité si $Q^{-1} = 0$,
égalité pour la seconde si $W = R^{-1}$
et égalité pour la dernière si $R^{-1} = 0$,
c'est à dire qu'on a une covariance infinie sur le bruit,
$Z$ est donc inintéressant.
On est donc ``moins efficace'' que pour l'estimateur Bayésien
mais toujours plus précis que $\tilde{\esth}$.

On peut en conclure que pour un bruit Gaussien, $W = R^{-1}$
est un bon choix et que lorsqu'on a de l'information sur $T_\Theta$,
c'est bon de l'utiliser et de travailler avec le modèle de Bayes.

\section{Filtrage et prédiction}
On va essayer de développer des filtres linéaires qui permettent de
retrouver un signal utile dans ce signal bruité.
On va abordé le filtre de Wiener et de Kalman.
Wiener est en fait un cas particulier de Kalman lorsque
le processus est \emph{stationnaire}. % FIXME verify that

\subsection{Filtre de Wiener}
Le filtre de Wiener consiste en le choix de $w(l_1), \ldots, w(l_2)$
donnant le filtre $w$ permettant d'estimer $x(k)$ à partir des
mesures $y(k-l_1), \ldots, y(k-l_2)$ avec $x = w * y$ autrement dit
\[ x[k] = \sum_{l=l_1}^{l_2} w(l)y(k-l). \]

\begin{itemize}
  \item Si $l_1 = 0$, on appelle ça du filtrage.
  \item Si $l_1 > 0$, on appelle ça de la prédiction.
  \item Si $l_1 < 0$, on appelle ça du lissage.
\end{itemize}

Le filtre de Wiener calcule les $w(l)$ en essayant
de minimiser $\xi = \Ex{|E(k)|^2}$
où $E(k) = X(k) - \hat{X(k)}$.
On remarque que ça implique
le \emph{principe d'orthogonalité} aussi appelé
\emph{théorème de projection}
\[ \Ex{E(k)Y^*(k-l)} = 0 \]
$\forall l$.
Il en découle que $R_{\hat{X}E}(0) = \Ex{\hat{X}(k)E^*(k)} = 0$
et que
\[ R_{XY}(k) = \sum_{l=l_1}^{l=l_2} w(l)R_Y(k-l) \]
d'où les \emph{équations de Wiener-Hopf}
\[
  \begin{pmatrix}
    R_Y(0) & R_Y^*(1) & \cdots & R_Y^*(l_2-l_1)\\
    R_Y(1) & R_Y(0) & \cdots & R_Y^*(l_2-l_1-1)\\
    \vdots & \vdots & \ddots & \vdots\\
    R_Y(l_2-l_1) & R_Y(l_2-l_1-1) & \cdots & R_Y(0)
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    w(l_1)\\w(l_1+1)\\\vdots\\w(l_2)
  \end{pmatrix}
  =
  \begin{pmatrix}
    R_{XY}(l_1)\\
    R_{XY}(l_1+1)\\
    \vdots\\
    R_{XY}(l_2)
  \end{pmatrix}.
\]


\annexe
\section{$\esthmap = \esthlmmse$ for linear gaussian model}
\label{ann:maplmmselingauss}
On a
\begin{align*}
  \esthlmmse(Z) & = m_\Theta + C_{\Theta Z}C_{Z}^{-1}(Z - m_Z)\\
                & = m_\Theta + QH^T(HQH^T+R)^{-1}(Z - Hm_\Theta)\\
  \esthmap(Z) & = (Q^{-1}+H^TR^{-1}H)^{-1}(Q^{-1}m_\Theta+H^TR^{-1}Z)
\end{align*}
On doit donc prouver que
\[ (Q^{-1}+H^TR^{-1}H)^{-1}(Q^{-1}m_\Theta+H^TR^{-1}Z) = m_\Theta + QH^T(HQH^T+R)^{-1}(Z - Hm_\Theta) \]
En multipliant par $(Q^{-1}+H^TR^{-1}H)$ par la gauche, on a
\begin{multline*}
   Q^{-1}m_\Theta+H^TR^{-1}Z = Q^{-1}m_\Theta + H^TR^{-1}H m_\Theta + H^T(HQH^T+R)^{-1}(Z - Hm_\Theta)\\
   + H^TR^{-1}HQH^T(HQH^T+R)^{-1}(Z - Hm_\Theta)
\end{multline*}
où on peut simplifier $Q^{-1}m_\Theta$ et puis multiplier par $(H^T)^{-1}$ par la gauche et on obtient
\begin{multline*}
  R^{-1}Z = R^{-1}H m_\Theta + (HQH^T+R)^{-1}(Z - Hm_\Theta)\\
  + R^{-1}HQH^T(HQH^T+R)^{-1}(Z - Hm_\Theta)
\end{multline*}
où on peut faire passer $R^{-1}Hm_\Theta$ à droite et simplifier tout par la droite par $(z-Hm_\Theta)^{-1}$
\[
  R^{-1} = (HQH^T+R)^{-1} + R^{-1}HQH^T(HQH^T+R)^{-1}
\]
et en multipliant par $(HQH^T+R)$ par la droite, on a
\[
  R^{-1}HQH^T + I = I + R^{-1}HQH^T.
\]
La commutativité de la somme matricielle nous permet alors de conclure sans trop de mal.

\end{document}
