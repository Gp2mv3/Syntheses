\documentclass[fr,license=none]{../../../eplsummary}
\usepackage[french]{varioref} % \vref and \vpageref
\usepackage{graphicx} % images
\usepackage{float} % images
\usepackage{csquotes}
\usepackage{url}
	\urlstyle{sf}
\usepackage[backgroundcolor=yellow]{todonotes} %% todonotes: \listoftodos & \todo{Some note or other.} & \missingfigure{}

% draw
\usepackage{tikz}
\usetikzlibrary{arrows,automata,calc, positioning}
\tikzset{
    %Define standard arrow
    >=stealth',
    % Define arrow style
    pil/.style={
           ->,
           thick,
           shorten <=2pt,
           shorten >=2pt,}
}
% \usepackage{qtree}    % dessiner des arbres %% => texlive-humanities

\definecolor{codeBlue}{rgb}{0,0,1}
\definecolor{webred}{rgb}{0.5,0,0}
\definecolor{codeGreen}{rgb}{0,0.5,0}
\definecolor{codeGrey}{rgb}{0.6,0.6,0.6}
\definecolor{webdarkblue}{rgb}{0,0,0.4}
\definecolor{webgreen}{rgb}{0,0.3,0}
\definecolor{webblue}{rgb}{0,0,0.8}
\definecolor{orange}{rgb}{0.7,0.1,0.1}

\usepackage{caption}
\renewcommand{\familydefault}{\sfdefault}

\usepackage{listings}		% Pour l'insersion de fichiers de codes sources.
\lstdefinelanguage{diff}{
  morecomment=[f][\color{blue}]{@@},     % group identifier
  morecomment=[f][\color{red}]-,         % deleted lines
  morecomment=[f][\color{green}]+,       % added lines
  morecomment=[f][\color{magenta}]{---}, % Diff header lines (must appear after +,-)
  morecomment=[f][\color{magenta}]{+++},
}
\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}
\lstset{
	  language=scala,
	  frame=single,
	  flexiblecolumns=true,
	  numbers=none, % left
	  stepnumber=1,
	  numberstyle=\ttfamily\tiny,
	  keywordstyle=\ttfamily\textcolor{blue},
	  stringstyle=\ttfamily\textcolor{red},
	  commentstyle=\ttfamily\textcolor{green},
	  breaklines=true,
	  extendedchars=true,
	  basicstyle=\ttfamily\footnotesize,
	  showstringspaces=false
	}

\IfFileExists{fourier.sty}{\usepackage{fourier}}{\typeout{! WARNING: Fourier package not included: skip it}}

\hypertitle{Languages and Translators}{8}{INGI}{2132}
{Matthieu Baerts\and Benoît Baufays\and Julien Colmonts\and Alex Vermeylen\and Hélène Verhaeghe}
{Pierre Schauss}

\listoftodos

\todo{ben}

CFG: Context-Free Grammar

PEG: Parsing Expression Grammar : grammaire non-ambigue qui est exprimmée comme une CFG mais pour éviter les ambiguités, on prend tjs la première dérivation qui marche.


\section{Chapitre 1 : Compilation}
\subsection{Compiler}

\paragraph{Compilateur} Programme qui traduit un programme source
écrit dans un langage haut niveau en un programme équivalent écrit
en langage de bas niveau (code machine le plus souvent)
qui peut être exécuté directement par l'ordinateur.

\begin{figure}[!h]
    \centering
\begin{tikzpicture}
	\node[text width=2.5cm,align=center] (A) {Source language program};
    \node[draw, shape=rectangle] (B) [right=of A] {Compiler};
	\node[text width=2.5cm,align=center] (C)[right=of B] {Program Code Machine};
	\path[->] (A) edge (B)
                (B) edge (C);
\end{tikzpicture}
\caption{Compilation}
\end{figure}


\subsubsection{Programming language}

\paragraph{Langage de programmation} Un langage artificiel dans lequel
un programmeur écrit un programme pour contrôler le comportement d'une
machine.


On définit un langage de programmation en 3 étapes:
\begin{itemize}
    \item les \textbf{tokens}: \enquote{mot-clé} du langage
    \item la \textbf{syntaxe} et la \textbf{construction} du langage: comment écrire un programme qui fonctionne
    \item la \textbf{sémantique} du langage: sens des phrases.
\end{itemize}

\subsubsection{Machine language}

\paragraph{Langage machine (ou ensemble d'instructions)} est un langage
facilement interpreté par l'ordinateur (le processeur plus précisement) lui-même. Chacune des
instruction occupe un byte ou plus et est facilement
accédé et interprété. On parle également d'architecture pour
l'ensemble d'instructions et le comportement d'une machine.

\paragraph{Complexité instruction}
On distingue plusieurs niveaux de complexités (CISC: \textit{Complex
Instruction Set Computer} ou RISC: \textit{Reduced Instructive Set Computer}).
Il faut plusieurs instructions RISC pour obtenir une instruction CISC.


Comme les registres sont très rapides, un ordinateur va essayer de
garder au maximum les données dans les registres.

\paragraph{Machine virtuelle} L'architecture de la machine est implémentée en
software et cette implémentation exécute un programme comme une machine
physique. Exemple d'une machine virtuelle: la JVM (java virtual machine).
L'avantage d'une machine virtuelle est que les programmes générés sont
indépendents de l'architecture réelle de la machine, il suffit qu'il existe une
machine virtuelle pour cette architecture.

Les étapes de l'analyse du compilateur pour produire le programme de sortie sont:
\begin{itemize}
    \item Mapper les noms aux adresses mémoires, frames de la pile et registres.
    \item Générer la liste linéaire des instructions machines.
    \item détecter les erreurs
\end{itemize}

\subsubsection{Interpreteur Vs Compilateur}

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}

        \begin{tikzpicture}
            \node[text width=1cm,align=center] (A) {Source Code};
            \node[draw, shape=rectangle] (B) [right=of A] {Compilers};
            \node[text width=1cm,align=center] (C)[right=of B] {Exec};
            \node[text width=1cm,align=center] (D)[above=of C] {Data};
            \node[text width=1cm,align=center] (E)[below=of C] {Output};


            \path[->] (A) edge (B)
            (B) edge (C)
            (D) edge (C)
            (C) edge (E);
        \end{tikzpicture}
    \end{subfigure}
    \quad
    \quad
    \begin{subfigure}[b]{0.4\textwidth}
        \begin{tikzpicture}
             \node[text width=1cm,align=center] (AA) {Source Code};
             \node[text width=1cm,align=center] (DD)[below=of AA] {Data};
             \node[draw, shape=rectangle] (BB) [right=of AA] {Interpreters};
             \node[text width=1cm,align=center] (EE)[right=of BB] {Output};

             \path[->] (AA) edge (BB)
             (DD) edge (BB)
             (BB) edge (EE);
         \end{tikzpicture}
     \end{subfigure}
     \caption{Compilation}
 \end{figure}

Dans le cas d'un interpréteur, le code high-level est exécuté directement.

\begin{center}
\begin{tabular}{|p{0.5\linewidth}|p{0.5\linewidth}|}
    \hline
    \multicolumn{1}{|c|}{Avantages compilateur} & \multicolumn{1}{c|}{Avantage interpréteur} \\
    \hline

\begin{itemize}
    \item \textbf{Performances}: le code machine est plus rapidement
exécuter, l'interpréteur doit lui redécoder à chaque fois les lignes
de codes pour les transformer en instructions.
    \item \textbf{Secret}: le code source est très difficile à déduire depuis un code machine.
\end{itemize}
&

\begin{itemize}
    \item L'overhead de l'interpreteur ne nécessite pas toujours la mise
        en place d'un compilateur (ex: les scripts)
    \item \textbf{Contrôle}: le code source est disponible, facilement modifiable.
    \item \textbf{Portabilité}: le langage machine dépend de la machine alors qu'un interpréteur est portable.
\end{itemize}
\\
    \hline
\end{tabular}
\end{center}

\subsection{Why should we study compilers?}
\begin{itemize}
	\item Ce sont de très grands programmes (comme ceux que nous étudierons plus tard)
    \item Il fait usage de tout ce que nous avons vu avant
    \item Cela permet d'apprendre sur le langage
    \item On en apprend sur la machine visée
    \item Il existe encore du boulot de compilation pour les nouveaux langages
    \item Ils se retrouvent partout
    \item Les programmes utilisent du XML se servent de technologies de compilation
    \item Mix de théorie et pratique
    \item Comme un compilateur peut être écrit en étapes, c'est un cas d'étude d'ingénérie logicielle
    \item écrire des programmes est chouette
\end{itemize}

\subsection{How does a compiler work? Phases of compilation}
\begin{figure}[!h]
\begin{tikzpicture}
	\node[text width=2.5cm,align=center] (A) {Programme source};
    \node[draw, shape=rectangle] (B) [right=of A] {Frontend};
	\node[text width=2.5cm,align=center] (C)[right=of B] {Intermediate Representation};
    \node[draw, shape=rectangle] (D) [right=of C] {Backend};
	\node[text width=2.5cm,align=center] (E) [right=of D] {native code};
	\path[->] (A) edge (B)
              (B) edge (C)
              (C) edge (D)
              (D) edge (E);
\end{tikzpicture}
\caption{Compiler work}
\end{figure}

\subsubsection{Frontend}
\begin{itemize}
	\item Analyse du programme pour en deviner le sens
    \item dépend uniquement du langage de départ
\end{itemize}
\begin{figure}[!h]
\begin{tikzpicture}
	\node[text width=1.5cm,align=center] (A) {Programme source};
    \node[draw, shape=rectangle] (B) [right=of A] {Scanner};
	\node (C)[right=of B] {Tokens};
    \node[draw, shape=rectangle] (D) [right=of C] {Parser};
	\node[text width=1.5cm,align=center] (E) [right=of D] {Abstract Syntax Tree};
    \node[draw, shape=rectangle] (F) [right=of E] {Sementics};
	\node (G) [right=of F] {IR};
	\path[->] (A) edge (B)
              (B) edge (C)
              (C) edge (D)
              (D) edge (E)
              (E) edge (F)
              (F) edge (G);
\end{tikzpicture}
\caption{Frontend}
\end{figure}

\subsubsection{Backend}
\begin{itemize}
	\item Prend le IR et produit le code machine
    \item Dépend de la machine d'arrivée uniquement
\end{itemize}
\begin{figure}[!h]
\begin{tikzpicture}
	\node (A) {IR};
    \node[draw, shape=rectangle] (B) [right=of A] {Codegen};
	\node[text width=1.5cm,align=center] (C)[right=of B] {Target machine code};
    \node[draw, shape=rectangle] (D) [right=of C] {Peephole};
	\node[text width=1.5cm,align=center] (E) [right=of D] {(Better) Target machine code};
    \node[draw, shape=rectangle] (F) [right=of E] {Object};
	\node[text width=1.5cm,align=center] (G) [right=of F] {Target machine program};
	\path[->] (A) edge (B)
              (B) edge (C)
              (C) edge (D)
              (D) edge (E)
              (E) edge (F)
              (F) edge (G);
\end{tikzpicture}
\captionsetup{singlelinecheck=off}
\caption[legend]{Backend :
    \centering
\begin{itemize}
	\item Codegen: Choisi les instructions adéquates
    \item Peephole: Cherche à faire un peu d'optimisation locale du code
    \item Object: liste les différents modules et construit un programme exécutable
\end{itemize}
}
\end{figure}

\subsubsection{Middle end}
Il peut y avoir l'ajout d'un optimisateur entre le frontend et le backend. Celui-ci à pour but d'améliorer le code intermédiare:
\begin{itemize}
	\item Il organise le programme en blocks
    \item Recherche la durée de vie des variables (next-use information)
    \item Élimine des sous-expressions et des \textit{constaints folding} ($x+4=9$ si on sait que $x=5$), regarde à l'allocation des registres
    \item Met les invariants hors des boucles, remplace les multiplication par des additions (car moins couteuse)
\end{itemize}

\subsubsection{Avantage de la décomposition}
\begin{itemize}
	\item Réduit la complexité
    \item Permet un dévelopement en parallèle
    \item Permet une réutilisation (un seul backend pour tous les langages pour une même machine)
\end{itemize}

\begin{figure}[!h]
    \begin{tikzpicture}
        \node (A1) {Java program};
        \node[draw, shape=rectangle] (B1) [right=of A1] {Java front end};

        \node (C) [below right=0.5cm of B1] {IR};

        \node[draw, shape=rectangle] (B2) [below left = 0.5cm of C] {C front end};
        \node (A2) [left = of B2] {C program};

        \node[draw, shape=rectangle] (C1) [above right =0.5cm of C] {Intel back end};
        \node (D1) [right = of C1] {Intel core program};

        \node[draw, shape=rectangle] (C2) [below right =0.5cm of C] {SPARC back end};
        \node (D2) [right = of C2] {SPARC program};

        \path[->] (A1) edge (B1)
        (A2) edge (B2)
        (B1) edge (C)
        (B2) edge (C)
        (C) edge (C1)
        (C) edge (C2)
        (C1) edge (D1)
        (C2) edge (D2);

    \end{tikzpicture}
    \caption{Decomposition}
\end{figure}


\subsubsection{Compiling to a Virtual Machine: New Boundaries}

\paragraph{.class}
Les programmes Java sont d'abord compilés en un fichier \texttt{.class}
(a \textbf{byte-code})
éxécutable sur la machine virtuelle. Le \texttt{.class} peut donc être
considérer comme un \textbf{IR} (Intermediate Representation), le
\texttt{JavaC} comme le frontend et la machine virtuel comme le
backend.

\paragraph{Machine virtuel}
La machine virtuelle est un interpréteur qui observe le code qu'on lui donne et
compile les parties critiques (\textit{hotspot}) qui sont les plus souvent
appelées. Il fait aussi du \textit{in-lining}, c'est-à-dire remplacer
des appels de méthode par leur code.

\paragraph{Note:}
Sur Windaube, ils ont implémenté une machine CLR (Commun Langage
Runtime). Suivant une technique appelée JIT (Just In Time), le CLR
compile chaque méthode en code natif et utilise ce code lorsque la
méthode est appelée.

\subsubsection{Compiling JVM code sur une architecture en registre}

En effet, le JVM code à une architecture basé sur la stack et il peut
y avoir différent challenge à résoudre pour mapper beacoup de variables
vers un nombre limité de registre rapide.


La stratégie consistant à fournir du code intermédiare pour une
machine virtuelle a plusieurs avantages :

\begin{itemize}
	\item Le code intermédiare est compact
    \item De gros efforts ont eété fait pour optimiser ces machines virtuelles pour qu'elles s'exécute très rapidement (ex: Java, LOL)
    \item Le fait de ne compiler que les régions spéciales permettent d'aller plus vite que de compiler l'entièreté du code.
\end{itemize}

\subsection{An overview of the j-\-- to SVM compilers}

j-\-- est un sous-ensemble de Java: non-trivial, orienté objet,
supportant les classes, méthodes, champs, messages, statements,
expressions et types primitifs.

Son compilateur est écrit en Java d'une manière orienté objet.

\subsubsection{j-\-- compiler organization}

Le début est la \texttt{main}. Après avoir lu les arguments, elle
crée le parser et le scanner. Ensuite:
\begin{itemize}
	\item \texttt{compilationUnit()} au parser pour obtenir l'AST.
    \item \texttt{preAnalyze()} au n\oe ud principal de l'arbre pour déclarer les types et classes dans la table des symboles.
    \item \texttt{analyze()} pour déclarer les noms et chercher les types
    \item \texttt{codegen()} pour gérer le code
    \item S'il y a des erreurs, on en cherche d'autres et puis on arrête.
\end{itemize}

\begin{figure}[!h]
    \includegraphics[width=\linewidth]{img/j--.png}
    \caption{j-\-- compiler}
\end{figure}

\subsubsection{Scanner}

Le scanner scanne les tokens à la demande et les classes en :
\begin{itemize}
    \item \textbf{Identifiant} (\texttt{main, myMethod})
    \item \textbf{tokens réservés} (\texttt{import, public, class, operateurs,
        separateurs,\ldots})
    \item \textbf{litéral} (tout le reste sans les séparateurs: \texttt{"Hello World", 3}, etc.).
\end{itemize}

\subsubsection{Parser}

Le parser est spécifique à la syntaxe du langage et est effectué en
\textit{recursive descent}.

\subsubsection{AST}

L'abstract syntax tree est une représentation sous forme d'arbre
du programme source. Il permet de rendre explicite la structure
syntaxique. Chacune des classes des nœuds étendent une même classe (\texttt{JAST})
et implémente 3 méthodes:
\begin{itemize}
    \item \texttt{preAnalyze()} (déclarer les types)
    \item \texttt{analyze()} (déclarer les variables locales)
    \item \texttt{codegen()} (pour générer le code).
\end{itemize}

\subsubsection{Types}

Un type indique comment une varaible doit être interpreté et
se comporte. Java est dit statiquement typé car les types sont
déterminés à la compilation.


Dans le compilateur j-\--, on crée une classe type pour placer les
différents types.

%TODO

\subsubsection{Symbol table}

%TODO

\subsubsection{\texttt{preAnalyze()} and \texttt{analyze()}}

\paragraph{\texttt{preAnalyze()}} l'étape de pré analyse doit traverse l'AST mais seleument jusqu'à un certain point pour :
\begin{itemize}
\item declarer les types importés
\item declarer les classe définie pas le programmeur
\item declarer les attributs des classes
\item declarer la signature des méthodes (nom et type des paramètres)
\end{itemize} Il construit une partie de la table des symbol qui est
au dessus du AST.

Cette étape est nécessaire car dans le code source on peut référencer certains types avant de les avoir déclarés.

\paragraph{\texttt{analyze()}} reprend là où \texttt{preAnalyze()} s'est arrêté.
Il s'occupe de:
\begin{itemize}
    \item Type checking: trouver le type de chaque expression
    \item Accessibilité: Regarde aux \texttt{public}, \texttt{protected} et \texttt{private}
    \item Member finding: vérifie les signatures.
    \item Tree rewriting: réécrit certains tree
\end{itemize}

\subsubsection{Stack Frames}

L'analise s'occupe aussi de calculer l'offset des variables locales
dans la stack frame (block continu de mémoire au-dessus de la pile
\textit{run-time}). On peut ainsi savoir l'espace repris par une
méthode à chacune de ses invocations.

\subsubsection{\texttt{codegen()}}

Permet de générer le byte code pour la JVM. Un outil
(\texttt{CLEmitter}) existe et permet de construire la table des
constantes et de générer les références utilisées par la JVM
pour les noms et constantes. Permet de calculer des offsets de
\textit{brckets} et les adresses. Calcule les coûts (espace mémoire)
des variables et l'espace de calcul nécessaire dans une méthode. Et
pour finir, il construit le fichier \texttt{.class}.

\subsubsection{j-\-- compiler source tree}

Pour ajouter une opération, on doit modifier le scanner pour
reconnaitre les nouveaux tokens, modifier le parser pour reconnaitre
l'expression, implémenter l'analyse sémentique et finalement la
génération du code allant avec cette opération.



\section{Chapitre 2 : Lexical Analysis}
\subsection{Introduction}

\begin{figure}[!h]
    \centering
\begin{tikzpicture}
	\node (A) {string};
    \node[draw, shape=rectangle] (B) [right=of A] {Lexical Analysis};
	\node[text width=4.5cm,align=center] (C)[right=of B] {Token : <Class, string>};
    \node[draw, shape=rectangle] (D) [right=of C] {Parsing};
	\path[->] (A) edge (B)
              (B) edge (C)
              (C) edge (D);
\end{tikzpicture}

    \caption{Lexical analysis}
\end{figure}

L'analyse lexical parse donc l'input en une série de \textbf{token}.

\paragraph{Lexical tokens}: éléments composant les programmes. Ils
sont décrits par la syntaxe lexicale du langage.

\paragraph{Lexical analysis}: procédé permettant d'identifier
les lexical tokens dans le programme. On peut séparer les tokens
lexicaux en catégories:
\begin{itemize}
    \item les identifiants
    \item les mots réservés, opérateurs et séparateurs
    \item les literaux
\end{itemize}

\subsection{Scanning tokens}

\paragraph{Scanner} Un scanner est un programme
écrit par le programmeur (\textit{handwritten}) ou bien généré
automatiquement par une liste de regex comme JavaCC.

Les espaces peuvent servir à séparer des tokens mais pas tout le temps, par
exemple si on est entrain de scanner un \texttt{integer} et que l'on tombe
sur une lettre, on peut en conclure que l'\texttt{integer} est fini.

\subsection{Lexical analysis hand written}

Dans le cas d'un scanner écrit à la main, il est très utile de
décrire le scanner comme un \textbf{state transition diagram}.

\subsubsection{State transition diagram} diagramme dans lequel les nœuds
représentent les états, les flèches dirigées représentes des
mouvement d'un état à l'autre suivant ce qui est scanné. Si un
caractère scanné n'est sur aucune flèche, on choisi celle non
étiquetée.

\begin{figure}[!ht]
	\begin{center}
		\begin{tikzpicture}[node distance=3cm, on grid, auto]
			\node[initial,state, initial text={}]					(start) 	{$start$};
			\node[state, above right =of start]		(id) 		{$id$};
			\node[state, below right =of start]		(integer) 	{$integer$};
			\node[state, accepting, right =of id]  	(idEnd)		{$idEnd$};
			\node[state, accepting, right =of integer] (intEnd) {$intEnd$};

			\path[->] (start)	edge				node {letter,digit,\_,\$}		(id);
			\path[->] (id)		edge 				node {} 	(idEnd);
			\path[->] (id) 		edge [loop above] 	node {letter, digit, \_, \$}	(id);
			\path[->] (integer) edge 				node {} (intEnd);
			\path[->] (integer) edge [loop above]	node {digit} (integer);
			\path[->] (start)	edge 				node [left] {1\ldots9} (integer);
		\end{tikzpicture}
	\caption{State diagram}
	\end{center}
\end{figure}

\paragraph{Implémentation}

\begin{itemize}
    \item Les choix sont représenté par des \texttt{if} (ou \texttt{switch})
    \item Les cyles sont des \texttt{while}
    \item Les mots réservés ne sont pas intégré dans la machine à
        états car cela est trop complexe. A la place on liste les
        mots réservés dans une table et faire un \textit{lookup} à chaque
        fois qu'on reconnait un identifiant.
    \item Les opérateur sont intégré dans la machine à états finis (via un
        \texttt{switch} entre les différents opérateurs possible)
    \item Les espaces sont ignorés (indique la fin d'un token)
    \item Les commentaires sont ignorés
\end{itemize}


\subsection{Lexical analysis generator}

\begin{figure}[!h]
\begin{tikzpicture}
    \node[draw, shape=rectangle] (A)  {RegEx};
    \node[draw, shape=rectangle] (C) [right=2cm of A] {NFA};
    \node[draw, shape=rectangle] (E) [right=2cm of C] {DFA};
    \node[draw, shape=rectangle] (G) [right=2cm of E] {DFA minimizing};
    \node[draw, shape=rectangle] (I) [right=2cm of G] {Table driven};

    \path[->] (A) edge node[above] {\textcolor{red}{Thompson}} (C)
              (C) edge node[above] {\textcolor{red}{$\epsilon$-closure}} (E)
              (E) edge node[above] {\textcolor{red}{Partition}} (G)
              (G) edge node [above] {\textcolor{red}{Algo}} (I);
\end{tikzpicture}
\caption{Generator lexical analysis}
\end{figure}



\subsubsection{Regular Expressions (regex)}

On dit que les regex définissent un langage de strings sur un alphabet.

\begin{table}[!h]
\begin{tabular}{lccl}
    Input & Language & regex & contient \\
    \hline
    $a \in$ alphabet & $L(a)$ & $a$ & Le string $a$ \\

    $r$ et $s$ des regex & $L(rs)$ &  $rs$ & Toute les combinaisons de concaténation $rs$ \\

    $r$ et $s$ des regex & $L(r|s)$ & $r|s$ & $L(r) \cup L(s)$ \\

    $r$ une regex & $L(r*)$ & $r*$ & Concaténation de zéro ou plusieurs instance de $r$ \\

    \hline
    \multicolumn{4}{p{11cm}}{
    \begin{itemize}
    \item $\epsilon$ est le langage contenant uniquement la string vide
    \item $r$ et $(r)$ sont les mémes regex. Les parenthèses sont
     utilisées pour grouper les regex.
     \end{itemize}
 }

\end{tabular}
\caption{Formes possibles des RegEx}
\end{table}

\subsubsection{Finite State Automate (FSA)}

Pour tout langage décrit par une regex, il existe un diagramme à
transition d'état.

\textit{Un automate fini reconnait un language.}

\paragraph{Automate à états finis} (Finite State Automate) c'est $F =
(\Sigma, S, s_0, M, F)$ où:

\begin{itemize}
	\item $\Sigma$ est l'alphabet
    \item $S$ est l'ensemble des états
    \item $s_0 \in S$ est l'état initial
    \item $M$ est l'ensemble de mouvements possibles d'un état à un
        autre ; $M(r, a) = S$ où $r, s \in S$ et $a \in \Sigma$
    \item $F \in S$ est l'ensemble des états finaux
\end{itemize}
\paragraph{Une phrase appartient au language} si en partant du start, on
arrive sur un état final après avoir suivit les changement d'états
correspondant à la phrase.

Il existe un moyen automatique de générer un FSA à partir de regex.
\paragraph{Exo:} Spécifiez un automate à état fini pour (ab)*ab
\subsubsection{Non-Deterministic Finite State Automate (NFA) vs. Deterministic FSA (DFA)}

\paragraph{\textbf{DFA}} automate à états finis où il n'y a pas de
$\epsilon$-move et où un seul chemin existe à partir d'un état pour
un input.

On ne peut donc aps avoir $M(r, a) = s$ et $M(r, a) = t$ tel que $s \neq t$.

\begin{itemize}
    \item DFA are faster to execute and easy to implement (by
        table driven implementation)
\end{itemize}

\paragraph{\textbf{NFA}} automate à états finis qui permet plusieurs chemins
à partir d'un état pour le même input, et autorise $\epsilon$-move.

\begin{itemize}
    \item NFA are in general smaller (exponentially)
\end{itemize}

\subsubsection{Regular expressions to NFA}

On utilise les constructions de \textbf{Thompson}.


\begin{table}[!h]
\begin{scriptsize}
    \centering
    \begin{tabular}{m{2cm}m{10cm}}

	\large $a$ &
    \begin{tikzpicture}
		\node[state] (A) [initial, initial text={}] {S};
    	\node[state] (B) [right=of A, accepting] {E};
		\path[->] (A) edge node[above] {$a$} (B); % [bend left]
	\end{tikzpicture}
	\\

	\large$rs$ &
    \begin{tikzpicture}
		\node[state] (A) [initial, initial text={}] {S$_r$};
    	\node        (B) [right=of A] {$N_r$};
    	\node[state] (C) [right=of B] {E$_r$};
		\node[state] (D) [right=of C] {S$_s$};
    	\node        (E) [right=of D] {$N_s$};
    	\node[state] (F) [right=of E, accepting] {E$_s$};
		\path[->] (C) edge (D);
        \draw[thick] ($(A.north west)+(-0.25,0.25)$) rectangle ($(C.south east)+(0.25,-0.25)$);
        \draw[thick] ($(D.north west)+(-0.25,0.25)$) rectangle ($(F.south east)+(0.25,-0.25)$);
	\end{tikzpicture}
	\\
	\large$r|s$ &
    \begin{tikzpicture}
    	\node[state] (0) [initial, initial text={}] {S};
		\node[state] (A) [above right=0.5cm and 1cm of 0]{S$_r$};
    	\node        (B) [right=of A] {$N_r$};
    	\node[state] (C) [right=of B] {E$_r$};
		\node[state] (D) [below right=0.5cm and 1cm of 0] {S$_s$};
    	\node        (E) [right=of D] {$N_s$};
    	\node[state] (F) [right=of E] {E$_s$};
    	\node[state] (G) [below right=of C, accepting]{E};
		\path[->] (0) edge[bend left] node[above] {$\epsilon$} (A);
		\path[->] (0) edge[bend right] node[below] {$\epsilon$} (D);
		\path[->] (C) edge[bend left] node[above] {$\epsilon$} (G);
		\path[->] (F) edge[bend right] node[below] {$\epsilon$} (G);
        \draw[thick] ($(A.north west)+(-0.25,0.25)$) rectangle ($(C.south east)+(0.25,-0.25)$);
        \draw[thick] ($(D.north west)+(-0.25,0.25)$) rectangle ($(F.south east)+(0.25,-0.25)$);
	\end{tikzpicture}
	\\
	\large$r*$ &
    \begin{tikzpicture}
    	\node[state] (0) [initial, initial text={}] {S};
		\node[state] (A) [right=of 0]{S$_r$};
    	\node        (B) [right=of A] {$N_r$};
    	\node[state] (C) [right=of B] {E$_r$};
    	\node[state] (D) [right=of C, accepting]{E};
		\path[->] (0) edge node[above] {$\epsilon$} (A);
		\path[->] (0) edge[bend right=50] node[above] {$\epsilon$} (D);
		\path[->] (C) edge[bend right=50] node[above] {$\epsilon$} (A);
		\path[->] (C) edge node[below] {$\epsilon$} (D);
        \draw[thick] ($(A.north west)+(-0.25,0.25)$) rectangle ($(C.south east)+(0.25,-0.5)$);
	\end{tikzpicture}
	\\
	\large$\epsilon$ &
    \begin{tikzpicture}
		\node[state] (A) [initial, initial text={}] {S};
    	\node[state] (B) [right=of A, accepting] {E};
		\path[->] (A) edge node[above] {$\epsilon$} (B);
	\end{tikzpicture}
	\\
\end{tabular}
\end{scriptsize}
\caption{Construction de thompson}
\end{table}

\subsubsection{NFA to DFA}

Il est évident qu'un NFA nécessite du \textbf{backtracking} poour
parser un input (puisqu'il peut emprunter le mauvais choix) qui prend du
temps\ldots Heureusment, pour toutes NFA il existe une DFA équivalente.
On arrive à la trouver grâce aux $\epsilon$-closure.


\paragraph{$\epsilon$-closure($s$)} c'est un ensemble incluant $s$ ainsi
que tous les états atteignables à partir de $s$ uniquement avec
des $\epsilon$-moves.

\paragraph{$\epsilon$-closure($S$)} c'est un ensemble incluant
l'ensemble $S$ ainsi que tous les états atteignables à partir de
chaque élément $s$ de $S$ uniquement avec des $\epsilon$-moves.

\begin{figure}
\begin{lstlisting}[mathescape]
Input: a set of states, S.
Output: $\epsilon$-closure(S)

Stack P.addAll(S);
Set C.addAll(S);
while (!P .empty() ) {
    s = P.pop();
    for (r $\in$  move(s, $\epsilon$)){
        if (r $\notin$ C) {
            P.push( r );
            C.add( r );
        }
    }
}
return C;
\end{lstlisting}
\caption{$\epsilon$-closure algorithm}
\end{figure}

\begin{figure}
\begin{lstlisting}[mathescape]
Input: an NFA, N = ($\sum$, S, $s_0$ , M, F)
Output: DFA, D = ($\sum$, $S_D$ , $s_{D0}$ , $M_D$ , $F_D$ )

Set $S_{D0}$ = $\epsilon$-closure($s_0$ );
Set $S_D$ .add($S_D0$ );
Moves $M_D$ ;
Stack stk.push($S_{D0}$ );
i = 0;
while (! stk.empty()) {
    t = stk.pop();
    for (a : $\sum$) {
        $S_{Di+1}$ = $\epsilon$-closure(M(t, a));
        if ($S_{Di+1}$ != {}) {
            if ($S_{Di+1}$ $\notin$ $S_D$ ) {
                // We have a new state.
                $S_D$ .add($S_{Di+1}$ );
                stk.push($S_{Di+1}$ );
                i = i+ 1;
                $M_D$ .add($M_D$(t, a) = i ) ;
            }
            else if ($\exists$ j, $S_j \in S_D$ $\wedge$ $S_{Di+1} == S_j$ ) {
                .. In the case that the state already exists.
                $M_D$ .add($M_D$(t,a) = j ) ;
            }
        }
    }
}

Set $F_D$ ;
for ($s_D$ : $S_D$ )
    for (s : $s_D$ )
        if (s $\in$ F)
            $F_D$ .add($s_D$ );

return D = ($\sum$, $S_D$ , $s_{D0}$ , $M_D$ , $F_D$ );

\end{lstlisting}
\caption{DFA construction algorithm}
\end{figure}

%TODO : explication algorithm

\subsubsection{Minimal DFA}

L'idée est de fusionner des états non-distinguable (càd qu'on ne peut
distinguer d'un autre pour chaque input). On va recréer des états
grace à des partitions.

\paragraph{Partitions}
\begin{itemize}
    \item Les partitions initialles sont \textit{final} et \textit{non-final}.
    \item On scinde en différente partition quand les états sont distinguables
    \item Pour chaque partition, un input doit induire un mouvement vers
        une même partition.

    \item[Note:] On doit s'assurer que pour chacun des états, le mouvement d'un symbole
        particulier reflète celui dans l'ancien DFA.
\end{itemize}

\subsubsection{Table driven}

For each DFA, a Table driven can be implemented as a 2D table (states and input).
\begin{lstlisting}[mathescape]
For every $M(r, a) = s$, $T[r, a] = s$.
\end{lstlisting}

\paragraph{Recognize the longest match}

Un automate reconnait les états finaux avec deux variables :
\begin{itemize}
    \item \texttt{Last-Final} : le dernier état final rencontré
    \item \texttt{Input-Position-at-Last-Final}
\end{itemize}

On met à jour ces deux variables quand un état final est rencontré.
Quand a \textit{dead} état (càd un état qui n'a pas l'ouput correspondant
à l'input) est atteint, les variables disent quels tokens ont matché et où ils
se sont fini.

\subsubsection{JavaCC: Tool for generating scanner}

JavaCC est un outil pour générer une analyse lexicale à partir de
regex ainsi qu'un parseur à partir d'une grammaire hors contexte.


Lorsque l'on scanne un token, on considère tous les regex correspondant
à l'état actuel et on choisi celui qui contient le plus de
caractères.


Il existe 4 types de regex:
\begin{itemize}
	\item SKIP: regex jeté
    \item MORE: on continue après
    \item TOKEN: crée un token et est retourné au parseur
    \item SPECIAL-TOKEN: crée un token spécial ne participant pas au parsing
\end{itemize}

\section{Chapitre 3 : Parsing}

\subsection{Introduction}

\paragraph{Parsing} action de mettre les tokens ensemble pour créer des
entités syntaxiques plus grandes (expressions, instructions, méthodes,
définition de class, ...).

Fonction du parsing :
\begin{itemize}
    \item Assumer la validité syntaxique du programme.
        \begin{enumerate}
            \item Identifier une erreur et la reporter
                (erreur et le numéro de ligne).
            \item Ne pas s'arrêter après une erreur et chercher d'autres erreurs
                (pour ne pas reporter qu'une seule erreur à la fois).
        \end{enumerate}
    \item Fournir une représentation du programme parsé, souvent en
    arbre (\textbf{AST}) car ce type de représentation est facile à
    analyser et à "décorer" d'informations.

\end{itemize}

\subsection{Context-Free Grammars and Languages}

CFG est plus puissant que RegExp. En effet, impossible en RegExp
de décrire un string tel que une suite de ``a'' est suivit d'une suite de ``b''
tel que les deux séquences ont la même taille.

Cela est impossible puisque les DFA ne peuvent pas compter!

\begin{lstlisting}[mathescape]
S := a S b
S := $\epsilon$
\end{lstlisting}

\subsubsection{Backus-Naur Form (BNF) and its extensions}

On définit les langages de programmation de manière récursive.
\begin{itemize}
	\item[$::=$] : "\textit{peut être écrit comme}", sigle de la définition
    \item[$|$] : "ou"
    \item[$[X\rfloor$] : $X$ est optionnel
    \item[$\{X\}$] : \textit{Keene closure}, $X$ apparait de 0 à plusieurs fois.

\end{itemize}

\subsubsection{Grammar and the Language it describes}
\paragraph{Context-Free grammar} Une grammaire sans contexte est un tuple $G=(N,T,S,P)$ où:
\begin{itemize}
	\item[$N$] est un ensemble de symboles non terminaux
    \item[$T$] est un ensemble de symboles terminaux
    \item[$S \in N$] est un non terminal appelé symbole de départ
    \item[$P$] est un ensemble de règles de production
\end{itemize}

\paragraph{Derivation} Séquence d'applications de règles de
production, démarrant du symbole de départ pour arriver à
l'expression voulue («~$\Rightarrow$~» est le symbole de la relation
de dérivation).

\paragraph{Directly derive} Lorsqu'un string de symboles dérive d'un
autre en une seule étape.

\subparagraph{E directly derives T + T} :
$ E \Rightarrow T + T$

\paragraph{Language $L(G)$} On dit que le langage $L(G)$, décrit par
une grammaire $G$, consiste en tous les strings ne comprenant que des
symboles terminaux pouvant être dérivés du symbole de départ.

\paragraph{Left-most derivation} Dérivation dans laquelle, à chaque
étape, le string suivant est dérivé en applicant une règle de
production réécrivant le non terminal le plus à gauche.

\paragraph{Right-most derivation} Dérivation dans laquelle, à chaque
étape, le string suivant est dérivé en applicant une règle de
production réécrivant le non terminal le plus à droite.

\paragraph{Sentential form} Chaque string de terminaux ou non terminaux
qui peut être dérivé à partir d'un symbole de départ.

\paragraph{Parse tree} Arbre illustrant les dérivations effectuées
depuis un symbole de départ (racine) pour arriver à une expression
(feuilles).

\subsubsection{Ambiguous Grammars and unambiguous grammars}

Detecter si une context free grammar est ambiguous est \textbf{indécidable}.
Notons que les grammaires LL(k) et LR(k) sont non-ambiguous.

\begin{itemize}
    \item \textbf{Ambiguous sentence} : Une phrase par laquelle il existe
        plusieurs dérivations possibles. (2 arbres pour représenter $id + id * id$)
\item \textbf{Ambiguous grammar} : Une grammaire qui décrit au-moins une
        phrase ambigüe.
    \item \textbf{Unambiguous grammar} : Une grammaire ne décrivant que
        des phrases à dérivation unique (qu'elle soit \textit{left} ou
        \textit{right}).
\end{itemize}

\paragraph{IF..ELSE ambiguous}
Pour résoudre cet ambiguité, on définit une règle telle que
le \texttt{else} se groupe avec le \texttt{if} précédent.

\begin{center}
\begin{tikzpicture}
    \node[text width=2.5cm,align=center] (A)
    {{\begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
S ::= if (E) S
   | if (E) S else S
   | s
E ::= e
if (e) if (e) s else s
\end{lstlisting}}};

	\node[text width=2.5cm,align=center,  above right=-1.5cm and 1.5cm of A] (B)
	{{\begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
if (e)
    if (e)
        s
    else
        s
\end{lstlisting}}};
	\node[text width=2.5cm,align=center, below right=-1.5cm and 1.5cm of A] (C)
	{{\begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
if (e)
    if (e)
        s
else
    s
\end{lstlisting}}};

		\path[->] (A) edge (B)
		(A) edge (C);

\end{tikzpicture}
\end{center}


\paragraph{x.y.z ambiguous}

Il y a aussi ambiguité avec $x.y.z$: est-ce un \textit{field
selection}, un nom de package, un nom de class, etc.


Pour résoudre cela, un nœud \texttt{Ambiguous Name} est placé dans
l'\textbf{AST} et est revu par le compileur après la déclation des types
(\textit{type declaration}).


\subsubsection{Parseur}

Pour les parseur des langages décrits par une grammaire
\textit{context-free}, on utilise une \textit{pushdown} stack avec
\textit{backtracking}.

Lorsque le \textit{backtracking} n'est pas nécessaire, on parle de
parsing déterministe. Deux types de parsing déterministes:

\begin{itemize}
    \item Top-Down: le parser commence avec le \texttt{start symbol}
        et étape par étape dérive l'input sentence pour produire l'\textbf{AST}
        du root aux feuilles.
    \item Bottom-up: le parser commence avec l'input sentence et le scanne
        de gauche à droite et applique les règles. L'\textbf{AST} est construit
        du bas vers le root.
\end{itemize}


\subsection{Top-Down Deterministing Parsing}

Deux types de top-down:
\begin{itemize}
    \item recursive descent
    \item $LL(1)$.
\end{itemize}

Le parser commence au symbol de départ (root) et agrandit
l'AST en déscendant vers les feuilles\ldots il remplace le symbole
par le coté droite d'une règle BNF.

Il va scanner la phrase d'input et parser les entités syntaxique
représentées par le symbole de départ.

\subsubsection{Parsing by recursive descent}

Parser par descente récursive invoque une procédure écrite
pour parser chaque non-terminal selon qui lui est associé.
La procédure choisit la règle a appliquer selon le prochain non-scanné
symbol. (Ne nécessite pas de backtracking!)

Ces méthodes construisent également les nœuds AST!

\paragraph{Utils}
Le scanner a quelques méthodes utiles pour aider à décider de la
règle de production adéquate :

\begin{itemize}
    \item \texttt{have(TOKEN)}: renvoie \texttt{true} si le token est présent,
        et le scanne.
    \item \texttt{mustBe(TOKEN)}: renvoie \texttt{true} si le token est
        présent et le scanne ou une erreur sinon.
    \item \texttt{see(TOKEN)}: regarde si le token est présent dans le scanner.
\end{itemize}

\paragraph{Lookahead}
%TODO
Pour certaines grammaires, il faut regarder plus loin que le token
suivant pour savoir ce qui est présent en premier et pour savoir
quelle loi de dérivation utiliser. On a un \textit{Lookahead} scanner
définit qui permet de voir plus loin qu'un symbole.

\paragraph{Error recovery}
%TODO
Pour récupérer des erreurs, les \texttt{mustBe}, lorsqu'il rencontre
un token inatendu, scanne jusqu'à trouver celui qu'il veut jusqu'à se
remettre dans un état à nouveau correct.
\subsubsection{Left recursion removal}
Parfois pour faire un parsing de descente récursive, il faut
modifier la grammaire même si elle n'est pas ambigüe. Par
exemple pour la grammaire:
\begin{itemize}
	\item Y ::= Y \textcolor{red}{a}
	\item Y :: = \textcolor{green}{b}
\end{itemize}

on ne peut pas  faire le parseur à cause de la left-recursion.
On a de la left-recursion quand le symbole non-terminal le plus
à gauche de la règle de production et le méme que le
déclencheur. Il faut retirer cette recursion
en faisant:
\begin{itemize}
	\item Y ::= \textcolor{green}{b} Y'
	\item Y' ::= \textcolor{red}{a}Y'
	\item Y' ::=$\epsilon$
\end{itemize}
L'algo qui permet de le faire est le suivant:
\begin{lstlisting}[mathescape]
Enumerate the non-terminal of G: X1,X2,..,Xn
for i := 1 to n do
	for j:= 1 to i-1 do
		Replace each rule in P having the form
		$X_i$::= $X_j\alpha$ by the rules
			$X_i$::= $\beta_1\alpha|\beta_2\alpha|...|\beta_k\alpha$ where $X_j$ ::= $\beta_1|\beta_2|..|\beta_k$
			Eliminate any immediate left recursion

\end{lstlisting}
\paragraph{Indirect left recursion}
Dans ce cas la, une première manipulation est nécessaire avant
d'appliquer la régle de base.

\begin{center}
\begin{tikzpicture}
    \node[text width=2.5cm,align=center] (A)
    {{\begin{lstlisting}
S ::= A a | b
A ::= S c | d
\end{lstlisting}}};

	\node[text width=2.5cm,align=center,  right= of A] (B)
	{{\begin{lstlisting}
S ::= A a | b
A ::= A ac | bc | d
\end{lstlisting}}};
		\path[->] (A) edge (B);

\end{tikzpicture}
\end{center}


\paragraph{Left Factoring}
Left Factoring consiste à retirer les facteurs gauche communs dans
des règles de productions du même symbole terminal. Par exemple
\begin{itemize}
	\item Y ::= $\alpha \beta$
	\item Y ::= $\alpha \gamma$
\end{itemize}
deviens
\begin{itemize}
	\item Y ::= $ \alpha$ Y'
	\item Y' ::= $\beta | \gamma$
\end{itemize}
Cette technique est utile parce qu'elle permet d'éviter le
backtracking (donc utile pour les lookahead scanner)


\subsubsection{$LL(1)$ parsing}

Contrairement au recursive descent, ici, la stack est explicite.
Le 1\ier $L$ veut dire \textit{left-to-right scan}, le 2\ieme pour
\textit{left-most derivative} et le $1$ pour le fait qu'on regarde un
seul symbole à la fois.\\


Chaque grammaire a une table de parsing unique. Pour créer la table, si
$\alpha$ et $\beta$ sont des strings (pouvant être vides) de terminaux
et non terminaux :
\begin{center}
$table[Y,a]=i$ où $i$ est le numéro de la règle,
$Y ::= X_1, X_2,..., X_n$ si soit $X_1, X_2,..., X_n \Rightarrow
a\alpha$ ou $X_1, X_2,..., X_n \xrightarrow{*} \epsilon$ et il y a une
dérivation $S$\# $ \Rightarrow \alpha Y a \beta$.\\
\end{center}

Pour arriver à établir cette table, on fait usage de deux ensemble :
\texttt{first} et \texttt{follow}.


\paragraph{LL(1) parsing algorithm}

Au tout début, on place le symbole terminal initial sur la pile.
Suivant le token scanné, on remplace le symbole sur la pile par celui
qui correspond suivant la table de parsing.

Lorsqu'un non-terminal est trouvé sur la pile et si c'est bien celui
qui est scanné, il est retiré de la pile et un nouveau token est
scanné.

\begin{lstlisting}
Initially,
Stack stk initially contains the terminator # and the start symbol S, with S on top;
Symbol sym points to the first symbol in the sentence w.
for (;;) {
    Symbol top = stk.pop();
    if (top == sym == #) halt successfully;
    else if (top is a terminal symbol) {
        if (top == sym) {
            advance sym to point to the next symbol in w;
        } else {
            halt with error: a sym found where a top was expected;
        }
    } else if (top is a non-terminal, Y) {
        int index = table[Y,sym];
        if (index != err)} {
        rule = rules[index];
        Say rule is Y ::= X 1 X 2 ...X n
        Push X n,..., X 2, X 1 onto the stack stk, with X 1 on top
        }
        else {
            halt with an error.
        }
    }
}

\end{lstlisting}

\paragraph{First}
%TODO : rendre joli

Correspond à l'ensemble des terminaux qui peuvent commencer une règle.
\textbf{First may contain $\epsilon$ ($X_1 X_2 ... X_n \Rightarrow *
\epsilon$)}

$$first(X_1 X_2 ... X_n) = \{ a \quad | \quad X_1 X_2 ... X_n
\Rightarrow * a \alpha, \quad a \in T\}$$

\begin{enumerate}
    \item Calcul de $first(X)$ pour tous les $X$ dans la grammaire:
        \begin{lstlisting}[mathescape]
For each terminal $x$, $first(x) = \{x\}$.
For each non-terminal $X$, $first(X) = \{\}$.
If there is a rule $X ::= \epsilon$ in $P$
     $first(X)$ += $\epsilon$

Repeat until no new symbols are added to any set:
    For each rule$ Y ::= X_1 X_2 ... X_n \in P$
        $first(Y)$ += all symbols from $first(X_1 X_2 ... X_n)$
        \end{lstlisting}

    \item Calcul de $first(X_1 X_2 ... X_n)$:
        \begin{lstlisting}[mathescape]
Set S = first(X$_1$).
i = 2
While $\epsilon$ $\in$ S and i $\le$ n {
    S = S - $\epsilon$
    S += first(X$_i$)
    i = i+1
return S
\end{lstlisting}
\end{enumerate}

\paragraph{Follow}

Correspond à l'ensemble des terminaux qui peuvent suivre une règle.
\textbf{Follow cannot contain $\epsilon$ }

$$follow(X) = \{ a \quad | \quad S \Rightarrow * wX\alpha \quad and \quad \alpha \Rightarrow * a ...\}$$

Calcul de $follow(X)$:
\begin{lstlisting}[mathescape]
$follow(S) = \{$\#$\}$.
$follow(X) = \{\}$ for all non-terminals X $\ne$ S.
Repeat until no new symbols are added to any set:
    for each rule $Y ::= X_1 X_2... X_n \in P$,
        for each non-terminal $X_i$,
            $follow(X_i)$ += $first(X_{i+1} X_{i+2}... X_n) - {\epsilon}$
            if $X_i$ is the rightmost symbol OR $first(X_{i+1} X_{i+2}... X_n)$ contains $\epsilon$,
                $follow(X_i)$ += $follow(Y)$
\end{lstlisting}

\paragraph{Parsing table}

Construire une table de parsing $LL(1)$ pour une grammaire $G=(N,T,S,P)$:
\begin{lstlisting}[mathescape]
For each non-terminal $Y$ in $G$,
    For each rule $Y ::= X_1 X_2...X_n \in P$ with number $i$
        For each terminal $a$ $\in$ $first(X_1 X_2... X_n) - \{\epsilon\}$
            $table[Y, a]$ += $i$

        If $first(X_1 X_2... X_n)$ contains $\epsilon$
            for each terminal (or \#) in $follow(Y)$
                $table[Y, a]$ += $i$
\end{lstlisting}

Une grammaire est dite $LL(1)$ si la table de parsing produite n'a pas de conflits. Toute grammaire $LL(1)$ est non ambigue.\\
On peut avoir des grammaire non $LL(1)$ mais $LL(k)$ avec $k>1$ (on regarde alors $k$ symboles).

\paragraph{Error recovery}
Si il reçoit un input non attendu pour terminal/non-terminal ($T$) dans la stack,
il va simplement skipper les tokens jusqu'a obtenir un token $\in$ Follow($T$).


\subsection{Bottom-up deterministic parsing}
On passe!!

\subsection{Parser Generation using Java CC}

On utilise une structure syntaxique EBNF (\textit{extended BNF}). On
peut définir ainsi des parseurs $LL(k)$ récursive descent. Le fichier
\texttt{j--.jj} contient donc les regex pour la structure lexicale ainsi
que les règles syntaxiques du langage.


Entre \texttt{PARSER\_BEGIN(JavaCCParser)} et
\texttt{PARSER\_END(JavaCCParser)} sont définies des fonctions utiles
et utilisées dans le parser généré : \texttt{reportParserError()} et
\texttt{recoverFromError()}.


\subsubsection{Specification syntaxique}

La syntaxe EBNF est:
\begin{itemize}
	\item $[a]$: 0 ou 1
    \item $(a)^*$: 0 ou plusieurs
	\item $a|b$: $a$ ou $b$
    \item $()$: grouping
\end{itemize}

\subsubsection{Déclaration de symbol non-terminaux}

On définit premièrement un symbole de départ
(\texttt{compilationUnit}) qui est un ``haut niveau'' de non-terminal
et référence le niveau inférieur de non-terminal. Ceux ci reference
les tokens définit dans la spécification lexical.

La déclaration de non terminaux ressemble à une méthode java. Il y a
un type de retour, un nom, peut avoir des arguments et à un corps de
méthode. Il y a un bloc précédent celui-là dans lequel les variables
locales sont déclarées.

%TODO : example page 70 chapitre 03

\subsubsection{Lookahead}
Lorsqu'il est nécessaire de regarder plusieurs symboles après
l'actuel, on utilise la méthode \texttt{LOOKAHEAD(<token>)}.

Il y a deux méthodes pour récupérer des erreurs : \textit{shallow}
et \textit{deep} (utilisé par j-\--). Dans le corps d'un non terminal,
on catch les erreurs du parseur. On choisit alors à quel token on veut
reprendre (\textit{skip to token}). Il y a donc premièrement affichage
d'une erreur puis récupération par shipping.

\subsubsection{Avantage}
Les avantages d'un parser autogénéré (JavaCC) sont:

\begin{itemize}
	\item Structure lexicale mieux expliquée
    \item Construction EBNF possible
    \item Lookahead possible et facile
    \item Conflit de choix reporté
    \item Mécanisme de récupération d'erreur sophistiqué utilisable.
\end{itemize}


\subsection{Bactracking parsing (not in the book)} %LN

Avec le backtracking parsing, on peut parser n'importe quel langage
décrit par une CFG. On va dériver en left most à partir du symbol
initial en choisisant la première règle de dérivation. Tant que les
terminaux déjà obtenu match le string qu'on veut matcher alors on
continue, sinon on revient à l'étape pérédente et on choisit la
règle de dérivation suivante.

Exemple:

Suivant la grammaire :
\begin{lstlisting}
S ::= ASA
S ::= ASB
S ::= A
A ::= a
B ::= b
\end{lstlisting}

Si on souhaite parser $aab$, on a les étapes suivantes :
\begin{lstlisting}
S
S => ASA
S => ASA => aSA
S => ASA => aSA => aASAA
S => ASA => aSA => aASAA => aaSAA
S => ASA => aSA => aASAA => aaSAA => aaASAAA
S => ASA => aSA => aASAA => aaSAA => aaASAAA => aaaSAAA
!! backtrack !!
S => ASA => aSA => aASAA => aaSAA => aaASAAA
!! backtrack a nouveau car pas d autre regle pour A !!
S => ASA => aSA => aASAA => aaSAA
S => ASA => aSA => aASAA => aaSAA => aaASBAA
S => ASA => aSA => aASAA => aaSAA => aaASBAA => aaaSBAA
!! backtrack !!
S => ASA => aSA => aASAA => aaSAA => aaASBAA
!! backtrack a nouveau car pas d autre regle pour A !!
S => ASA => aSA => aASAA => aaSAA
S => ASA => aSA => aASAA => aaSAA => aaAAA
S => ASA => aSA => aASAA => aaSAA => aaAAA => aaaAA
!! backtrack !!
S => ASA => aSA => aASAA => aaSAA => aaAAA
!! backtrack a nouveau car pas d autre regle pour A !!
S => ASA => aSA => aASAA => aaSAA
!! backtrack a nouveau car pas d autre regle pour S !!
S => ASA => aSA => aASAA
!! backtrack a nouveau car pas d autre regle pour A !!
S => ASA => aSA
S => ASA => aSA => aASBA
S => ASA => aSA => aASBA => aaSBA
... (on a compris l idee)
\end{lstlisting}

\subsection{Packrat parsing (not in the book)} %LN

Pour éviter le coût du backtrack parsing (qui peut être exponentiel
si à chaque fois on se retrouve à devoir prendre la dernière
dérivation), on utilise de la mémorization. On obtient donc un
algorithme en O(input*\#production).

On va pour chaque index dans l'input essayer de parser via toutes les
règles possibles et stocker le résultat ainsi que l'index de fin de
l'expression. On peut ainsi aller de l'index le plus grand au plus petit
en utilisant les résultats précedemment calculés.

\section{Chapitre 4 : Type checking}

\subsection{Introduction}

Du \textit{type checking}, c'est faire de l'analyse sémantique. On y
applique les opérations suivantes :

\begin{itemize}
	\item Détermine le type de tous les noms et expressions.
    \item Type checking: on vérifie les types des opérations
    \item Analyse du stockage pour savoir la place reprise par les variables locales
    \item Réécriture de l'arbre AST pour rendre explicite certaines constructions.
\end{itemize}

\subsection{j-\-- types}
\subsubsection{Introduction}

On a soit des \textbf{types primitifs} (\texttt{int, boolean, char}), soit des
\textbf{types référentiels} (\texttt{array}, type définit par une déclaration
de classe, \texttt{java.lang.Object}, etc.).

\subsubsection{Type representative and class objects}

On défini deux objets \textit{placeholder} pour prendre la place des
types non résolu encore :

\begin{itemize}
    \item Type Name: est résolu en faisant un lookup dans le contexte
        (représentation dy symbol table)
    \item Array Type Name: on résoud le type des éléments de l'array
        et on l'encapsule dans une \textit{Class Object} utilisé ensuite dans
        l'array.
\end{itemize}

On utilise cette technique car le type défini ce qu'il nous faut et on
peut utiliser des types temporaire avant la résolution.

\subsection{j-\-- Symbol Table}

\subsubsection{Contexts And Idefns: Declaring and Looking Up Types and Local Variables}

La table des symboles est représentée par un arbre d'objets
contexts, chacun correspondant à une région de scope du programme
et contenant un mapping des noms vers leur définition (type, parameter, local variables).

\begin{figure}[!h]
    \centering
    \begin{tikzpicture}[level/.style={sibling distance = 5cm}]
        \node { Context }
        child{ node {CompilationUnitContext} }
        child{ node {LocalContext} }
        child{ node {ClassContext} };

    \end{tikzpicture}
    \caption{Context tree}
\end{figure}

Dans chacun des contextes associés aux méthodes de classes, on associe
un champ \textit{surrounding context} qui pointe vers le contexte
englobant le bloc correspondant.

\paragraph{Contexts}
A chaque point d'execution, la table des symbols ressemble à une \textbf{stack
de context} :

\begin{figure}[!h]
\begin{tikzpicture}
	\node[draw, shape=rectangle] (A) {LocalContext};
    \node[draw, shape=rectangle] (B) [right=of A] {MethodContext};
    \node[draw, shape=rectangle] (C) [right=of B] {ClassContext};
    \node[draw, shape=rectangle] (D) [right=of C] {CompilationUnitContext};
	\path[->] (A) edge (B)
              (B) edge (C)
              (C) edge (D);
\end{tikzpicture}
\caption{Stack context at time t}
\end{figure}

Durant l'analyse, quand le compilateur rencontre une \textbf{variable} il regarde à
cette variable dans la table des symbols en commencant par le \texttt{localContext}.

\begin{itemize}
    \item \texttt{CompilationUnitContext} : scope du programme
        entier et contient un mapping des noms vers les \textbf{types}:
        \begin{enumerate}
            \item Les type implicits (java.lang.Object, java.lang.String,\ldots)
            \item Les types importés
            \item Les types introduit dans la déclaration de classe
        \end{enumerate}
    \item \texttt{ClassContext} : scope d'une classe et ne contient pas
        de mapping. (Constructor, methodes et champs sont enregistré dans l'objet
        Class)
    \item \texttt{MethodContext} : scope d'une méthode dont les paramètres
        sont déclarés dedans
    \item \texttt{LocalContext} : scope d'un block (\{ \}) contenant les variables locales
\end{itemize}


\begin{figure}[!h]
    \includegraphics[width=10cm]{img/structure-context.png}
    \caption{Structure d'un context}
\end{figure}

\paragraph{Idefns} est un type d'interface pour la définition de la
table des symboles.

Deux types de Idefns dans les entrées de la table:
\begin{itemize}
    \item \texttt{TypeNameDefn} qui encapsule juste un type
    \item \texttt{LocalVariableDefs} qui défini une variable locale en
        encapsulant son nom, type et un offset dans la frame stack.
\end{itemize}


\begin{figure}
\begin{tabular}{l}
    \includegraphics[height=19cm]{img/AST-context.png}
\\
\begin{lstlisting}
package pass;
import java.lang.System;

public class Factorial {
    public static int factorial(int n) {
        if (n <= 0) return 1;
        else return n * factorial(n - 1);
    }
    public static void main(String[] args) {
        int x = n;
        System.out.println(x + "! = " + factorial(x));
    }
    static int n = 5;
}
\end{lstlisting}
\end{tabular}
\caption{Example AST with context }
\end{figure}

\subsubsection{Finding Method and Field Names in Type Objects}
\todo{LN}

\subsection{Two phase analysis}

L'analyse sémantique besoin de deux traversés de l'AST car des classes et
méthodes peuvent être déclarées après une référence.

Globalement, \textbf{preAnalyze()} fait ComputationUnitcontext et ClassContext
ainsi que les informations à propose des types importés/déclaré.
\textbf{analyze()} fait MethodContext et LocalContext, continue la création
des tables de symbol, réécrit l'arbre et fait plusieurs checking.

\subsection{Pre-Analysis of j-\-- program}

\texttt{PreAnalyze()} va:
\begin{itemize}
    \item déclarer les types importés (\texttt{JCompilationUnit} level)
    \item déclarer les class définies par l'user (\texttt{JClassDeclaration} level)
    \item déclarer des champs (\texttt{JFieldDeclaration} level)
    \item déclarer les méthodes (\texttt{JMethodDeclaration} et
        \texttt{JConstructorDeclaration} level)
\end{itemize}

\subsubsection{\texttt{JCompilationUnit.preAnalyse(Context c)}}
\begin{itemize}
    \item Créer le \texttt{CompilationUnitContext}
    \item Déclarer les types implicites \texttt{java.lang.String}
        et \texttt{java.lang.Object}
    \item Déclarer les types importés
    \item Déclarer les types définis par les déclarations de classe
    \item Invoquer \texttt{preAnalyze()} pour chacun des déclarations
        de type dans la Compilation Unit
\end{itemize}

\subsubsection{\texttt{JClassDeclaration.preAnalyse(Context c)}}
\begin{itemize}
    \item Créer le \textit{Class context} avec \texttt{surroundingContext}
        qui pointe vers \texttt{CompilationUnitContext}
	\item Résoudre le type \texttt{super} de la classe (check si on n'étend
        pas un class \textit{final})
	\item Créer le \texttt{CLEmitter} associé et y ajoute le class header,
	    le nom et tout les modifications
	\item Invoquer \texttt{preAnalyze} sur les membres de la classe
	\item Rajouter un constructeur explicite s'il n'y a pas
	\item Produire le \textit{Class object}.
\end{itemize}

\subsubsection{\texttt{JMethodDeclaration.preAnalyse(Context c, CLEmitter cle)}}
\begin{itemize}
	\item Résoud les types des paramètres et du \texttt{return}
	\item Vérifie si l'\textit{abstract modifier} est propre

    \item Calcule du descripteur de méthode (codification de la
    signature en string)

        (\texttt{(I)I}: consomme un \texttt{int}, renvoie un
        \texttt{int} ; ([L\texttt{java.lang.String;}])V : prend une
        liste ([]) de strings (L\texttt{java.lang.String;}), revoie
        \texttt{void})
    \item Appelle \texttt{partialCodeGen()} pour générer le code
        de la méthode sans le corp pour que l'objet class ait au-moins les
        informations de l'interface des méthodes (paramètre et le type du
        \texttt{return})
\end{itemize}

\subsubsection{\texttt{JFieldDeclaration.preAnalyse(Context c, CLEmitter cle)}}
\begin{itemize}
	\item Vérifie qu'un champs n'est pas déclaré \texttt{abstract}
	\item Résoud le type du champ
	\item Génère le code bytestream pour la JVM pour la déclaration des champs
\end{itemize}

\subsubsection{Symbol Table built by \texttt{preAnalyse()}}

L'étape de pré-analyse ne déclare aucune variable locale. On a donc
un arbre partiel qui est construit.

%TODO: image?

\subsection{Analayse of j-\-- programs}
Lors de l'analyse, on va effectuer une discrete analyze recursive:

\begin{itemize}
    \item Réécriture de champs et d'initialisation de variables locales
        comme des assignements de variables normales
    \item Déclaration des paramètre formet et des variables locales
    \item Allocation des ressources sur la stack frame
    \item Calcul des types des expressions
    \item Reclassification des noms ambigus
    \item Modification de l'arbre
\end{itemize}


\subsubsection{\texttt{JCompilationUnit.analyse(Context c)}}
Appelle analyze() pour descendre dans l'arbre pour chaque type
de déclaration.

\subsubsection{\texttt{JClassDeclaration.analyse(Context c)}}
Après avoir fait l'analyze() pour chaque field,
il copie l'initialisation des fields dans son node.

Il va aussi les séparer en deux : \texttt{staticFieldInitializations}
et \texttt{instanceFieldInitializations}.

\subsubsection{\texttt{JFieldDeclaration.analyse(Context c)}}

Récrit le field initialisé comme un assignement explicit stocké dans
la liste d'initialisation.

\subsubsection{\texttt{JMethodDeclaration.analyse()}}

\begin{enumerate}
    \item Crée un \textbf{MethodContext} dont \texttt{surroundingContext}
        pointe vers le \texttt{ClassContext} précédant
    \item Le premier stack frame offset  =  0.

        (0 is allocate for \texttt{this} for a instance method)
    \item Les paramètres sont déclaré comme des variables locales
        et alloué sur des offsets consécutif
    \item Il analyse le corps de la méthode comme un context différent.
\end{enumerate}


\paragraph{Stack-frame method}
Nous avons besoin de calculer les offsets de tout les paramètres
ainsi que des variables local pour savoir l'espace nécessaire à
la méthode.

\subsubsection{\texttt{JBlock.analyse()}}

\begin{enumerate}
    \item Crée un nouveau \texttt{LocalContext} dont \texttt{surroundingContext}
        pointe vers le \texttt{MethodContext} précédent et le \textbf{newtOffset}
        est copié du précédent context.
    \item Analyze son body et chaque \texttt{JVariableDeclarations} declare
        sa variable dans le \texttt{LocalContext} du step 1
\end{enumerate}


\subsubsection{Rewriting a field}

\begin{figure}[!h]
    \includegraphics[width=15cm]{img/field-init.png}
    \caption{Rewriting a field Initialization.
JClassDeclaration (c) et JFieldDeclaration (b) analyse}
\end{figure}

\subsubsection{Method context and local context}

\begin{figure}[!h]
\begin{tabular}{m{6cm}m{4cm}m{4cm}}
\begin{lstlisting}
public class Locals {
    public int foo(int t, String u) {
        int v = u.length();
        {
            int w = v + 5, x = w + 7;
            v = w + x;
        }
        {
            int y = 3;
            int z = v + y;
            t = t + y + z;
        }
        return t + v;
    }
}
\end{lstlisting}
&
    \includegraphics[width=4cm]{img/method-context1.png}
&
    \includegraphics[width=4cm]{img/method-context2.png}
\end{tabular}
\caption{Example method context and local context }
\end{figure}


\subsubsection{Analysing local variable declaration and their initialization}

Avant d'effectuer la méthode analyze(), l'AST contient uniquement les
\texttt{JVariableDeclarator}.

L'analyse va :
\begin{itemize}
    \item Créer une \texttt{JVariableDefns} avec l'offset de la stack-frame
        correspondant
    \item Vérifié que la variable déclaré n'a pas un nom déja existant
    \item Déclaré la variable dans le \texttt{LocalContext}
    \item Réécrire tout les initialization comme des assignements
        explicits
\end{itemize}

\begin{figure}[!h]
    \includegraphics[width=\linewidth]{img/analysis-variable.png}
    \caption{Analysis variable initialization}
\end{figure}

\subsubsection{Simple variable}

\begin{itemize}
    \item Recherche dans la table des symbols la variable et récupère
        son \texttt{LocalVariableDefn}
    \item Enregistre le \texttt{LocalVariableDefn} pour son utilisation
        dans le code generation et copie le Type
\end{itemize}

\begin{figure}[!h]
    \centering
    \includegraphics[width=10cm]{img/simple-variable.png}
    \caption{Analysis simple variable}
\end{figure}

\paragraph{Note: } Une variable peut représenter un field d'une classe,
dans ce cas la variable est analyser différement.

\begin{figure}[!h]
    \includegraphics[width=\linewidth]{img/variable-field.png}
    \caption{Analysis d'un field}
\end{figure}

\subsubsection{Field selection and message expression}
%TODO : page 29


\subsubsection{Typing Expressions and Enforcing the Type Rules}
%TODO

\subsubsection{Cast operation}
%TODO

\subsection{The Visitor Pattern and the AST Traversal Mechanism}
%TODO


\subsection{Visitor Pattern and the AST traversal Mechanism}

\section{Chapitre 5 : JVM and Bytecode} %TODO : lire chapter5

\subsection{Introduction}

Une machine virtuelle est un programme implémentant une machine
qui exécute un programe comme une machine physique.

\begin{figure}[!h]
    \centering
    \begin{tikzpicture}[->]
        \node (A) { Byte code (.class) }
        child{ node {JVM} child{ node {windows}} }
        child{ node {JVM} child{ node {Linux}} }
        child{ node {JVM} child{ node {Mac}} };

        \node (B) [above= of A] {Javac};
        \node (C) [above= of B] {Java code (.java)};

        \node (D) [right= of B] {Scalac};
        \node (E) [right= of C] {Scala};

        \node (F) [right= of D] {\ldots compilers};
        \node (G) [right= of E] {\ldots languages};

        \path[->] (C) edge (B)
                  (B) edge (A)
                  (E) edge (D)
                  (D) edge (A)
                  (G) edge (F)
                  (F) edge (A);
    \end{tikzpicture}
    \caption{JVM}
\end{figure}

D'autres language choisissent de rejoindre la JVM car c'est facile de générer
le code et le byte code est compacte, le code est sur une architecture indépendante
et donc compatible et finalement JVM est efficace et très testé!


\begin{tabular}{m{4.5cm}m{8cm}}
Une instruction \textbf{bytecode} =
&
\begin{itemize}
    \item Un byte \textbf{opcode}
    \item Un nombre variable d'argument
    \item Offset ou pointeur vers la Constant pool
\end{itemize}
\end{tabular}

Une instuction byte code consumme et produit certain élément de la stack.
Constantes, locals et les éléments de la stack sont typé (adresse ou type
primitifs).

\begin{tabular}{m{4.5cm}m{8cm}}
Un fichier \textbf{.class} =
&
\begin{itemize}
    \item Magic number (0xCAFEBABE)
    \item Version
    \item Constant pool
    \item Access flag
    \item This class
    \item Super class
    \item Interfaces
    \item Fields
    \item Methods
    \item Atrributes
\end{itemize}
\end{tabular}

\subsection{Components}

\begin{itemize}
    \item Stack par thread : stocke les stack frames
    \item Heap : mémoire alloué dynamiquement
    \item Constant pool : stocke les constantes et les références
        vers les champs/méthodes
    \item Code segment : stocke code du programme (JVM instruction)
    \item Counter du programme par thread : contient l'addresse de la prochaine instruction
\end{itemize}

\subsection{Stack}

JVM a décidé d'utiliser une stack machine car cela réduit la taille
du byte-code et surtout toutes les opérations travailles sur peu d'élément
au dessus de la stack!

\begin{center}
(\textit{Pour une operation de division d'entier, la machine RISC à
32 registres on a besoin $32*31$ différents opcode (pour chaque paire
possible de registre), alors que la stacke machine n'a besoin que de 1
opcode puisqu'il travaille toujours sur la deux éléments en haut de la
stack})
\end{center}

\subsubsection{Stack loading process}

Les classes (dont le nom doit correspondre avec celui du fichier) sont
chargées de manière \textit{lazy} lors du premier accès, juste après
les super-classes dont elles dépendent. Ensuite le byte-code est vérifié,
les fiels static sont alloué et recoivent leur valeur par défault. Enfin,
les initialiseur static sont exécuté.

\subsubsection{Frame}
Une nouvelle frame est créé et push sur la stack pour chaque appel
de méhtode. Les stacks sont ensuite pop lorsque la méthode se termine
normalement ou si une exception est \textit{throw}.

\begin{tabular}{m{2.5cm}m{9cm}m{3cm}}
Une frame =
&
\begin{itemize}
    \item Un tableau de variable locale
    \item Une valeur de retour
    \item Operand stack
    \item Référence vers le \texttt{runtime constant pool} pour la
        classe de la méthode courante.
\end{itemize}
&
\includegraphics[width=3cm]{img/frame.png}
\end{tabular}

\subsubsection{Operand stack}

Appliquer des opérations se fait sur des éléments
en haut de la stack. Dans cet example deux éléments sont consommé et un produit.

\begin{figure}[!h]
\begin{tikzpicture}
    \node[text width=6.5cm,align=center] (A)
    {{\begin{lstlisting}
public int add(int v1, int v2) {
    int c = 8;
    return v1+v2+c;
}
\end{lstlisting}}};

\node[text width=3.5cm,align=center] (B) [right= of A]
    {{\begin{lstlisting}
0 bipush 8
2 istore_3 [c]
3 iload_1 [v1]
4 iload_2 [v2]
5 iadd
6 iload_3 [c]
7 iadd
8 ireturn
\end{lstlisting}}};

	\path[->] (A) edge (B);
\end{tikzpicture}
\caption{Exemple d'utilisation des opérations de stack}
\end{figure}


\subsection{JVM instruction}

\paragraph{Arithmetic}
\texttt{ineg, i2c, iadd, isub, imul, idiv, irem, iinc k i}

\paragraph{Branch}
\texttt{goto L, ifeq L, ifne L, ifnull L, ifnonnull L} : compare
l'élément au top de la stack par rapport à 0.

\texttt{if\_icmpeq L, if\_icmpne L, if\_icmplt L, if\_icmple L,
if\_acmpeq L, if\_acpmne L, if\_icmgt, if\_icmpge} : compare les 2
éléments au top de la stack

\paragraph{Constant loading}
\texttt{iconst\_0, ..., iconst\_5, aconst\_null, ldc i, lds string}.

\paragraph{Field}
\texttt{getfield f sig, putfield f sig, getstatic f sig, putstatic f sig}

\paragraph{Stack}
\texttt{dup, pop, swap, nop, dup\_x1, dup\_x2}

\paragraph{Class}
\texttt{new C, instanceof C, checkcast C}

\paragraph{Méthode}
\texttt{invokestatic name sig, invokevirtual name sig, ireturn, areturn, return}


\subsection{Verification}

Un bytecode nécessite d'être vérifié avant son exécution. Cette vérification
est effectué : au chargement de la class et à l'exécution (runtime).

\paragraph{Note:} Un compilateur java doit générer du code vérifiable

\subsubsection{File}
Grace au code magique (0xCAFEBABE)

\subsubsection{Constant and header}
Les classes final ne sont pas hérité, final méthode non réécrite,
chaque classe à une superclass (sauf Object), field et méthode
référence a signature valide.

\subsubsection{Instruction}
Seulement les offset légaux sont référencé, les constantes ont des types
appropriés, toutes les instructions sont complète.

\subsubsection{Dataflow and type checking}
%TODO

\subsection{CLEmitter}
%TODO


\section{Garbage Collection}

\begin{itemize}
    \item Explicit memory management
        \begin{enumerate}
            \item Peut amener à beaucoup d'erreur (double free/malloc, seg fault,\ldots)
            \item Programmation modulaire plus complexe (accord sur qui free/malloc,\ldots)
            \item trouver de corriger les bugs est très compliqué car
                parfois un bug apparait longtemps après le lancement du programme.

        \end{enumerate}

    \item Automatic memory management
        \begin{enumerate}
            \item Illusion d'une infinité de mémoire
        \end{enumerate}
\end{itemize}

\paragraph{Allocation}
\begin{itemize}
    \item On maintient plusieurs \textbf{freelist} tel que \texttt{freelist[i]}
        contient les records de taille $i$
    \item Lorsqu'un objet demande un chunk, on peut donner n'importe
quel block de taille >= celle demandée.
\end{itemize}


\begin{table}[!h]
    \centering
    \begin{tabular}{|p{4cm}|p{9cm}|}
    \hline
        Nom & Avantage/inconvénient \\
    \hline
    \hline

        Reference counting & \begin{itemize}
            \item[+] Rapide, sûre
            \item[-] Ne fonctionne pas avec des ref circulaires
        \end{itemize}
        \\
    \hline
    Mark \& sweep classique & \begin{itemize}
        \item[-] Complexité spatiale $\bigoh(H)$, fragmentation et coût d'allocation
    \end{itemize}
    \\
    \hline

    Mark \& sweep reversibel & \begin{itemize}
        \item[+] La stack est ``contenue'' dans le graph pointer
        \item[-] Fragmentation et coût d'allocation
    \end{itemize}
    \\
    \hline

    Copying collection & \begin{itemize}
        \item[+] allocation facile et peu couteuse, pas de
            fragmentation (compactage automatique), le temps est proportionnel au
            nombre d'objets vivants
             \begin{itemize}
                 \item[$\to$] efficient
             \end{itemize}
        \item[-] Il faut le double d'espace mémoire.
    \end{itemize}
    \\
    \hline

    Generation collection & Peut être utilisé pour \begin{itemize}
        \item Mark \& sweep
        \item Copying
    \end{itemize}
    \\
    \hline

    \end{tabular}
    \caption{Garbage solution}
\end{table}

\subsection{Reference Counting}

\begin{itemize}
    \item Chaque objet retient le nombre de réference vers lui-même.
        \begin{itemize}
            \item[$\to$] \texttt{cnt++}/\texttt{cnt--} lors decréation/destruction de
                référence
        \end{itemize}
    \item Suppression lorsque le \texttt{cnt == 0}
    \item[]
    \item[+]
        Méthode rapide, sûre et marche dans la plupart des cas.
    \item[-]
        Problème si il y a des références circulaire (A ref B et B ref A)
\end{itemize}



\subsection{Mark \& Sweep}

\begin{figure}[!h]
\centering
\includegraphics[width=5cm]{img/marksweet.png}
\caption{Mark \& sweet principe}
\end{figure}

\begin{enumerate}

    \item \textbf{Mark} : On démare de root, on parcour la mémoire comme un graph (object graph) en
marquant tout les objets atteignables à partir des roots.

\begin{figure}[!h]
\begin{tabular}{cc}
\begin{minipage}{0.4\linewidth}
\begin{lstlisting}[mathescape]
function DFS(x)
    if $x$ == pointer into the heap AND record $x$ is not marked
        mark $x$

        foreach field $f_i$ of record $x$
            DFS($x.f_i$)
\end{lstlisting}
\end{minipage}
&
\begin{minipage}{0.4\linewidth}
\begin{lstlisting}[mathescape]
function DFS(x)
    if $x$ == pointer into the heap AND record $x$ is not marked
        mark $x$

        t=1
        stack[t] = x

        while t > 0
            x = stack[t--]

            foreach field $f_i$ of record $x$
                if $x.f_i$ == pointer AND record $x.f_i$ not marked
                    mark $x.f_i$
                    stack[++t] = $x.f_i$
\end{lstlisting}
\end{minipage}
\end{tabular}
\caption{Implementation mark phase sans re}
\end{figure}

\begin{itemize}
    \item Complexité temporel : proportionnel au number de node mark.

    \item Complexité spatial : Puisque DFS est recursive, la
complexité pour un heap de size $H$ peut être de $\bigoh(H)$. $\to$
Pas acceptable
\end{itemize}

\item{\textbf{Mark avec Pointer reversal}} :

\begin{tabular}{m{7cm}m{3cm}m{0.5cm}m{3cm}}
l'idée est de faire un DFS et d'encoder
la stack dans le graph pointer lui-même en inversant les pointeurs.
&
\includegraphics[width=3cm]{img/reverse1.png}
& $\to$ &
\includegraphics[width=3cm]{img/reverse2.png}
\end{tabular}

\begin{lstlisting}[mathescape]
function DFS(x)
    if $x$ == pointer AND record $x$ not marked
        $t$ = nil
        mark $x$
        done[$x$] = 0


        while true
            $i$ = done[$x$]

            if $i$ < $\#$ of fields in record $x$
                $y$ = $x.f_i$

                if $y$ == pointer AND record $y$ not marked
                    $x.f_i$ = $t$
                    $t$ = $x$
                    $x$ = $y$

                    mark $x$
                    done[$x$] = 0
                else
                    done[$x$] = ++$i$
            else
                $y$ = $x$
                $x$ = $t$

                if $x$ = nil then return
                $i$ = done[$x$]
                $t$ = $x.f_i$
                $x.f_i$ = $y$
                done[$x$] = ++$i$
\end{lstlisting}

    \item \textbf{Sweep} : on itère sur tout les objets du heap, on
supprime les objets non marqué et on clear les marks.

\begin{figure}[!h]
    \centering
\begin{lstlisting}[mathescape]
function DFS(x)
    $p$ = first address in heap
    while $p$ < last adress in heap
        if record $p$ marked
            unmark $p$
        else  %% (let f_1 the first field in p
            $f_1$ = freelist
            freelist = $p$

        $p$ = $p$ + sizeof($p$)
\end{lstlisting}
\caption{Implementation mark phase}
\end{figure}


\end{enumerate}

Le mark and Sweep permet de gérer les cycles et ne déplace pas les
objets dans la mémoire ce qui lui permet d'être applicable à des
langages comme C/C++. Par contre, il peut y avoir des problèmes de
fragmentation de la mémoire et un cout d'allocation élevé.

\subsubsection{Conclusion}
On peut le faire en DFS mais contraignant pour la complexité
spatiale donc une meilleur solution est d'utiliser l'idée du pointer reverse.

Le pointer reverse est utilisé pour ne pas utiliser de la mémoire
lors du garbage collection, en effet on lance en général un garbage
collector quand on est a court de mémoire alors si il doit lui-même
utiliser de la mémoire pour s'exécuter il va avoir un problème.

De plus l'espace mémoire requis pour le garbadge collector avec Mark \&
Sweep n'est pas borné, il dépend du nombre d'éléments a concerner.


\paragraph{Garbage}

Le cout peut être très élevé si il y a peu à supprimer.

Une meilleur solution est de mesuré $H$ (heap size) and $R$ (reachable data in heap)
tel que si $\frac{R}{H} > 0.5$ on augmente la taille du heap.

$$\textrm{Cost: } \frac{c_1 R + c_2 H}{H-R}, \quad \textrm{avec c constant number of instructions}$$


\paragraph{$\to$}
Le mark and Sweep permet de gerer les cycles. Par contre, il peut y
avoir des problème de fragmentation et un cout d'allocation grand.


\subsection{Copying collection : Cheney algorithm}

On splitte la heap en 2 : from-space (old space) et to-space (new space)

\begin{figure}[!h]
    \centering
    \begin{tabular}{cc}
    \includegraphics[width=7cm]{img/cheney.png}
    &
    \includegraphics[width=7cm]{img/cheney1.png}
\end{tabular}

    \caption{Cheney principle}
\end{figure}

\begin{itemize}
	\item Pour les references venant de la stack:
		\begin{enumerate}
			\item Si l'objet référencé est dans l'espace from, il faut l'amener
			dans l'espace to et laisser un pointer qui pointer vers le nouvel
			emplacement (forwarder pointer).
			\item Si l'objet référencé est déjà dans l'espace to, il faut mettre
			à jour la référence à partir du forwarder pointer.
		\end{enumerate}
	\item Pour les références venant de l'espace to, on effectue les même
	opération que au dessus.
\end{itemize}

\paragraph{Etapes}
\begin{enumerate}
    \item Initialement tout les chunck utilisé sont dans le from-space.
    \item Au lieu
        de marquer les chuncks utilisés, on les copie dans le to-space tout
        en maintenant à l'ancien emplacement une référence vers le nouvel
        emplacement.
    \item[$\to$]On déplace ainsi petit-à-petit en suivant les liens.

    \item Lorsque l'algo est fini, on inverse les utilisations des deux espaces
        (from devient to et to deviens from).
\end{enumerate}

\begin{figure}[!h]
\begin{tabular}{cc}
\begin{minipage}{0.4\textwidth}
\begin{lstlisting}[mathescape]
function Forward(p)
    if $p$ points to from-space
        if $p.f_1$ points to to-space
            return $p.f_1$
        else
            foreach field $f_i$ of $p$
                $next.f_i = p.fi$
            $p.f_i = next$
            $next = next$ + size of recod $p$
            return $p.f_i$

    else
        return $p$
\end{lstlisting}
\end{minipage}
&
\begin{minipage}{0.4\textwidth}
\begin{lstlisting}[mathescape]
scan = next = beginning of to-space

foreach root $r$
    $r$ = Forward(r)

while $scan < next$
    foreach field $f_i$ of record at $scan$
        $scan.f_i$ = Forward($scan.f_i$)

    $scan = scan$ + size of record at $scan$
\end{lstlisting}
\end{minipage}
\end{tabular}
\caption{Cheney algorithm}
\end{figure}


\subsubsection{Avantages/inconvenient}

\begin{itemize}
    \item Simple: pas besoin de stack
    \item Allocation facile et peu couteuse: \texttt{malloc(n)} implémenté avec \texttt{next = next + n}
    \item Pas de fragmentation (compactage automatique)
    \item Le temps est proportionnel au nombre d'objets vivants.

\item le garbage collection des objets est également peu couteux spécialement si beaucoup d'objet sont à jeter
\item[]
\item Mais vu que ça déplace les objets dans la mémoire certain langages ne le suportte pas C/C++
\end{itemize}

Par contre, il faut le \textbf{double} d'espace mémoire.


\subsubsection{Remarque}
Par contre le même problème qu'avec mark and sweep sans la technique
des pointeur inversé se pose. Il faut traverser tous les elements sans
utiliséde l'espace mémoire pour le faire.

Pour cela le to-space (new space) va être divisé en 3 parties :

\begin{itemize}
\item copier et scanner : les pointeurs contenu dans l'objet ont été scanné et "suivit"
\item copier non scanner : les pointeurs contenu dans l'objet n'ont pas été traité
\item empty
\end{itemize}

\paragraph{Etapes}:
\begin{enumerate}
    \item Copier dans le to-space les objets pointés par les roots et
de créer les "forwarding pointers"

\item Scanner les objets non scanner et copier tous les objets pointer par ceux ci dans les to-space

\item[$\to$] Si on rencontre un objet déjà copié il suffit de suivre son forwarding pointer pour savoir ou il se trouve.
    \end{enumerate}


\subsection{Generational collection}

On divise la heap en deux ou plusieur parties, les jeunes et les vieux
objets. Quand on alloue de la place pour un objet il est mis dans la
partie jeune.

\begin{itemize}
    \item On ne fait du garbage collection que dans les objets concidéré comme "jeune"
        a un rythme \textbf{soutenu}.
    \item Les vieux objets sont ceux qui ont survécu à plusieurs passage du garbage
        collector. Le rythme est plus lent.
    \item En effet, la majorité des objects meurent ``\textbf{jeune} et ceux
        qui reste atteignable pour longtemps sont souvent : global, referencé depuis la main
    \item[$\Rightarrow$] On a alors une siscion dans la mémoire entre jeune et vieux.
        Lorsqu'un objet devient vieux, il est déplacé dans la partie vieux.
\end{itemize}

Un problème survient quand on a des objets dans la partie vieux et que
ils référence des objets dans la partie jeune. Quand on va garbage
collection les jeunes, on ne va pas voir que un objet est référencé
par un "vieux". Pour cela on crée un table (remember set) qui contient
tous les pointeurs des vieux vers les jeunes.



\begin{figure}[!h]
    \centering
    \begin{tabular}{cc}
    \includegraphics[width=7cm]{img/generation.png}
    &
    \includegraphics[width=7cm]{img/generation1.png}
\end{tabular}

    \caption{Generation principle}
\end{figure}


\section{DSL \& Scala}

\subsection{Basic}
\subsubsection{Variables}

\paragraph{Mot-clef}

\begin{tabular}{m{3cm}m{9cm}}
\begin{itemize}
    \item \texttt{val}
    \item \texttt{var}
    \item \texttt{lazy val}: evalué uniquement au premier accès
\end{itemize}
&
\begin{lstlisting}
var jizz = "jizz"
jizz = "baufays"          // OK

val poulpe = "poulpy"
poulpe = "manteau louche" // NOP
\end{lstlisting}
\end{tabular}


\paragraph{Type}

\begin{tabular}{cc}
    \begin{minipage}{0.4\linewidth}
        \begin{lstlisting}
val sum = 1 + 2 + 3
val nums = List(1, 2, 3)
        \end{lstlisting}
    \end{minipage}
    &
    \begin{minipage}{0.4\linewidth}
        \begin{lstlisting}
val sum: Int = 1 + 2 + 3
val nums: List[Int] = List(1, 2, 3)
        \end{lstlisting}
    \end{minipage}
    \\
    Type inference & Explicit types \\
\end{tabular}



\subsubsection{Mutability Vs non-mutability}

On encourage val plutôt que var, ainsi que des collections immutable.

\subsubsection{Pattern matching}
\begin{tabular}{cccc}
    \begin{minipage}{0.4\linewidth}
        \begin{lstlisting}
val times = 1
times match {
    // On value
    case 1 => ...

    // With guards
    case i if i == 2 => ...

    // On types
    case j: Int  => ...
    case d: Double if d > 0.0 => ...

    // Default
    case _ => ...
}
        \end{lstlisting}
    \end{minipage}
&
    \begin{minipage}{0.5\linewidth}
        \begin{lstlisting}
object Test extends App {
    def bookType(b: Book) = b mathc {
        case Book(5) => ``Revue''
        case Book(3) => ``Journal''
        case Book(i) => ``default: %s''.format(i)
    }
}
        \end{lstlisting}
    \end{minipage}
    \\
\end{tabular}


\subsubsection{Imports}
Les imports peuvent apparaitre n'importe où dans le code.

\subsubsection{Exception}
\texttt{Nothing} est un subtype of tout les autres types.

\begin{lstlisting}
def error(message: String): Nothing =
    throw new RuntimeException(message)

def divide(x: Int, y: Int): Int =
    if (y != 0) x / y
    else error(``cannot divide by zero'')

// Return Int est respecte comme Nothing est un subtype de Int
\end{lstlisting}

\subsection{Orienté object}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{img/hierarchy.png}
    \caption{Hierarchie}
\end{figure}

\subsubsection{Pure O.O.}

\begin{itemize}
    \item Tout est un object: \texttt{1.toString}
    \item Toute opération est un appel de méthode: \texttt{1 + 2 + 3}
        $\to$ \texttt{(1).+(2).+(3)}
    \item Can omit . and ()
\end{itemize}


\subsubsection{classes}

App est utilisé comme la classe main en Java.

\paragraph{Les constructeurs}

\begin{tabular}{cc}
    \begin{minipage}{0.4\linewidth}
        \begin{lstlisting}
class vehicle(name: String, price:Int) {

}

val car = new vehicle("Ferrari", 100000);
        \end{lstlisting}
    \end{minipage}
    &
    \begin{minipage}{0.4\linewidth}
        \begin{lstlisting}
class truck(name: String, price:Int) {

    def this(name: String) = {
        this(name, 10)
    }
}
        \end{lstlisting}
    \end{minipage}
    \\
    Constructeur simple par défaut & Constructeur custom \\
\end{tabular}


\paragraph{object}

Les \texttt{object} sont comme des classes statiques.
Ils représentent la déclaration d'une classe anonyme qui étend App ainsi
que la création d'une unique instance "Test" de cette classe anonyme.

\begin{lstlisting}
object Test extends App
\end{lstlisting}

$\to$ Les méthodes définies à l'intérieur d'un object sont des
méthodes statiques, au sens \#Java8 du terme.

\paragraph{getters and setters}

Pas de getter/setter en scala, on y accède comme un champs:
\texttt{nomDeLaClasse.Champ = 'Hélène roule en tracteur'}

Il y a aussi une autre technique si vos attributs sont privé vous pouvez les accéder comme ceci :
\begin{lstlisting}
private var battery = 0
def power = battery // getter
def power_= (p: Int) { battery = p } // setter
\end{lstlisting}

la ligne power\_= est un peu particulière, cela permet de définir un nom de fonction comprenant un espace. "power ="

\paragraph{Companion object and apply}

Un objet compagnon est un objet qui porte le même nom que la classe
qu'il accompagne.

La spécificité étant que la classe peut accéder au membres privés
de l'objet et vice versa.
Le compagnion object peut être utilisé comme un genre de container.

\paragraph{\texttt{apply}} est une méthode qui agit un peu comme un
constructeur pour les objects et qui peut être utilisée avec les
classes comme sucre syntaxique :

\begin{lstlisting}
//Exemple 1

class Calculator(brand: String) {
	def add(m: Int, n: Int): Int = m + n
}

object Calculator {
	def apply(brand: String) = new Calculator(brand)
}

object Test extends App {

val c1 = new Calculator("HP")  // Utilise le constructeur de Calculator
val c2 = Calculator("HP")      // Utilise apply() du compagnon
}
\end{lstlisting}

Un compagnion object permet de construire un object sans utiliser de new.

\paragraph{Operator overload}

Il suffit de réecrire une méthode avec le même nom.

\subsection{Functional}
\subsubsection{Fonction anonymes}

\begin{lstlisting}
val plusOne = (x:Int) => x+1

plusOne(5)    // va retourner 6
\end{lstlisting}

\subsubsection{Closure}

Function anonyme qui peut référencé n'importe quelle valeur/variable
dans le scope.

\begin{lstlisting}
//	plusFoo	can reference any values/variables in scope
var foo = 1
val plusFoo = (x:Int) => x + foo

plusFoo(5)    //return 6
foo = 5
plusFoo(5)    //now return 10
\end{lstlisting}

\subsubsection{Higher order function}
\begin{tabular}{m{4cm}m{7cm}}
\begin{enumerate}
    \item L'utilisation de \_
    \item Une fonction peut prendre une autre fonction en paramètre.
\end{enumerate}
&
\begin{lstlisting}
val nums = List(1,2,3)

nums.exists(_ == 2)     // true
nums.map(x => x+1)      // List(2,3,4)
\end{lstlisting}
\end{tabular}


\subsubsection{Currying}

On peut définir plus qu'une séquence d'arguments.

\begin{tabular}{cm{7cm}}
\begin{lstlisting}
def add(a: Int)(b: Int) = a + b

// Partially applied function
val f = add(1)(_)
\end{lstlisting}
&
\begin{lstlisting}
def add(a: Int) = (b: Int) => a + b
\end{lstlisting}
\end{tabular}


\subsection{Null and options}

La valeur \texttt{null} est difficile à gérer pour empecher le risque
de \texttt{NullPointerException}.

\begin{tabular}{m{6cm}m{7cm}}
\texttt{Option[T]}
\begin{itemize}
    \item Some(x) if x existe
    \item None sinon
\end{itemize}
&
\begin{lstlisting}
trait Option[T] {
    def isDefined: Boolean
    def get: T
    def getOrElse(t: T): T
}
\end{lstlisting}
\end{tabular}


\subsection{Call by name}

Il existe deux types de passage d'argument :
\begin{itemize}
    \item \textbf{By value}: Les arguments sont toujours évalué \textsc{AVANT}
        l'exécution de la fonction de la même manière que les opérateurs.

        L'idée est de réduire une expression à une valeur.

         \begin{itemize}
    \item[$\to$] Si on utilise une fonction en argument elle ne sera évalué qu'une
        seule fois
\end{itemize}

    \item \textbf{By name}: Les arguments sont évalués lorsque l'on en a besoin.
        ($\approx$ \textit{lazy})

\begin{itemize}
    \item[$\to$] Si on utilise une fonction en argument elle sera évalué à
        chaque appel (jamais évalué si on ne l'utilise pas).

        \begin{tabular}{m{6cm}m{2cm}m{6cm}}
            \begin{lstlisting}
def foo(y: => Int): Int = y
            \end{lstlisting}
            & Compilé en $\to$ &
            \begin{lstlisting}
def foo(y: () => Int): Int = y
            \end{lstlisting}
            \\
            \multicolumn{3}{c}{By name est un sucre synthaxique pour la définition de closure}
        \end{tabular}

\end{itemize}
    \item[]
    \item Si CBV se termine $\Rightarrow$ CBN se termine aussi.
\end{itemize}

\paragraph{Default}

En scala, le \textbf{call by value} est le comportement par défaut,
et pour faire du \textbf{call by name} il faut ajouter $\Rightarrow$
devant le type de l'argument.


\subsubsection{Fonctionnement}

Les deux méthodes sont équivalentes dans le cas de fonctions pures et
si les exécutions terminent. Par exemple :

\begin{lstlisting}
object Test {
   def main(args: Array[String]) {
        delayed(time());
   }

   def time() = {
      println("Getting time in nano seconds")
      System.nanoTime
   }
   def delayed( t: => Long ) = {
      println("In delayed method")
      println("Param: " + t) //1
      wait(5000)
      println("Param: " + t) //2
   }
}
\end{lstlisting}

Ici, si on fait du by value, les deux print imprimeront la même chose.
Par contre, dans le cas du by name, la fonction sera recalculée et on
aura deux valeur différente à imprimer. \todo{correct ?}\\


\subsubsection{Stream}

Un stream est une liste dans laquelle le \textbf{tail} n'est executé
que lorsque l'on en a besoin.

\begin{tabular}{m{6cm}m{9cm}}
\begin{lstlisting}
abstract class MyStream {
  def isEmpty: Boolean
  def head: Int
  def tail: MyStream

  def apply(i: Int): Int = {
    i match {
      case 0 => head
      case i => tail(i-1)
    }
  }

  def filter(p: Int => Boolean): MyStream = {
    if( p(head) )
      MyStream.cons(head, tail.filter(p))
    else
      tail filter(p)
    }
  }
}
\end{lstlisting}
&
\begin{lstlisting}
object MyStream {
  def cons(hd: Int, tl: => MyStream) : MyStream = new MyStream {
    def isEmpty = false
    def head: Int = hd
    def tail: MyStream = tl
  }

  val empty = new MyStream {
    def isEmpty = true
    def head = throw new NoSuchElementException("empty.head")
    def tail = throw new NoSuchElementException("empty.tail")
  }
}
\end{lstlisting}
\end{tabular}

\paragraph{Utilisation}:

\begin{lstlisting}
object TestStream extends App{
  def streamFrom(from: Int): MyStream =
    MyStream.cons(from,streamFrom(from+1))

  def streamFilter(from: Int, f: (Int) => Boolean): MyStream =
    streamFrom(from).filter(x => f(x))


  def nextPrime(from: Int, i: Int): Int =
    streamFilter(from, (x) => isPrime(x)) (i)

  def isPrime(i: Int): Boolean = {
    if (i <= 1) false
    else if (i == 2) true
    else !(2 to (i - 1)).exists(x => i \% x == 0)
  }
}
\end{lstlisting}

\subsubsection{Flow-control}

\begin{tabular}{m{8cm}m{2cm}m{1cm}m{1cm}m{1cm}}
\begin{lstlisting}
def myLoop(n: Int)(body: Unit): Unit = {
def myLoop(n: Int)(body: () => Unit): Unit = {
def myLoop(n: Int)(body: => Unit): Unit = {

    if (n > 0) {
        println(n)

        body
        body()
        body

        myLoop(n-1) (body)
    }
}

myLoop(5) {
myLoop(5) { () =>
myLoop(5) {

    println("hello")
}
\end{lstlisting}
&
Résultat
&
\begin{lstlisting}
hello
5
4
3
2
1
\end{lstlisting}
&
\begin{lstlisting}
5
hello
4
hello
3
hello
2
hello
1
hello
\end{lstlisting}
&
\begin{lstlisting}
5
hello
4
hello
3
hello
2
hello
1
hello
\end{lstlisting}


\\
\end{tabular}

\subsubsection{Storing by-name parameters}

\begin{lstlisting}
class Observable {
    private var listeners: List[() => Unit] = List()
    def changed(): Unit = listeners.foreach(l =>l())
    def onChange(b: => Unit): Unit = {
        listeners = (() => b) :: listeners
    }
}
\end{lstlisting}

\paragraph{Example}

\begin{tabular}{m{6cm}m{4cm}}
\begin{lstlisting}
class Counter() extends Observable {
    private var c: Int = 0
    def value = c
    def incr(): Unit = {
        c += 1
        changed()
    }
}
\end{lstlisting}
&
\begin{lstlisting}
val c = new Counter()
c.onChange {
    println(c.value)
}
c.incr()
c.incr()
\end{lstlisting}
\end{tabular}

\subsection{Implicit}
\subsubsection{implicit conversion}

Permet de convertir automatiquement un type en un autre pour pouvoir appliquer certaines opérations.

\begin{lstlisting}
import ComplexImplicits._

object ComplexImplicits {
	implicit def Double2Complex(value : Double) = new Complex(value,0.0)
}

object MyComplex extends App {
	val d = Complex(1)(2)
	val e = 1 + d	//Ici, le Int 1 va automatiquement etre converti en Complex
                    //via la methode Double2Complex
}
\end{lstlisting}

\subsubsection{implicit classes}

Une classe implicite permet d'enrichir une classe existante en y ajoutant des méthodes comme par exemple :

\begin{lstlisting}
object TestImplicits extends App {
implicit class PrimeTester(i: Int) {
    def isZero: Boolean = i == 0
}

3.isPrime
\end{lstlisting}

Le type string a maintenant une nouvelle méthode, method1 qui ne fait ... rien ! mais c'est quand même la classe.

\subsubsection{implicit parameters}

\begin{lstlisting}
def speakImplicitly (implicit greeting : String) = println(greeting)
speakImplicitly("Goodbye world")
speakImplicitly // error: no implicit argument matching parameter type String was found.
implicit val hello = "Hello world"
speakImplicitly
\end{lstlisting}

Le keyword implicit va dire au compilateur de lui-même procurer un
paramètre à la méthode. Pour cela il faut qu'il y ai un paramètre
implicit déclaré et visible par la fonction et qu'il ai le type
requis.

\subsection{Traits}

Les traits sont à la frontière entre les interfaces et les classes abstraites.

\begin{itemize}
    \item[+ interface]
    \item Contient des implementation $\Rightarrow$ plus riche que les interfaces
    \item[- abstract]
    \item Pas de paramètre dans le constructeur
    \item Permet un héritage multiple car il est lié dynamiquement

    \item[]
    \item[$\Rightarrow$] Ce n'est pas un object et on ne peut donc pas l'instancier
\end{itemize}


\begin{lstlisting}
trait Comp {

def < (that : A): Boolean
def > (that : A): Boolean = {...}
def >= (that : A): Boolean = {...}
...
}
\end{lstlisting}

\subsubsection{Stackable trait}

\begin{tabular}{m{9cm}m{6cm}}
    \begin{itemize}
        \item Le trait (ou classe abstraite) de \textbf{base} défini
            une interface abstraite que tout les cores ou stackables extend
        \item Le trait (ou classes) de \textbf{core} implémente les méthodes abstraite
        \item Chaque \textbf{stackable} override un ou plusieurs méthodes définie dans le
            \textbf{base} trait.
    \end{itemize}
    &
    \includegraphics[width=6cm]{img/stackable.png}
\end{tabular}


\begin{figure}[!h]
    \centering
    \begin{tabular}{m{6cm}m{5cm}}
        \begin{lstlisting}
class Animal
trait Furry extends Animal
trait HasLegs extends Animal
trait FourLegged extends HasLegs
class Cat extends Animal with Furry with FourLegged
\end{lstlisting}
&
    \includegraphics[width=5cm]{img/linearization.png}
\end{tabular}
    \caption{Linearisation des héritages}
\end{figure}


\subsection{Monads}

Tous les types \texttt{M[T]} donc en gros des listes de \texttt{T}
(structure de données génériques) et qui ont deux opérations
spéciales: \texttt{flatMap} et \texttt{unit}

Il s'agit d'une structure de donnée $M$ qui a un types paramétrique
$M[T]$ qui défini:
\begin{itemize}
    \item \texttt{flatMap}:  idem que map mais avec un flatten en plus.
        \begin{center}
        (\texttt{[1, [2]]} $\Rightarrow$ \texttt{[1, 2]})
    \end{center}

    \item \texttt{unit}: Prend une valeur en argument (du type
    générique de la structure) et va créer une structure avec cette
    élément dedans.

    \begin{itemize}
        \item[Ex]: \texttt{List(x)}: on aura une liste avec \texttt{x} comme
        premier élément de la structure.
\end{itemize}

    \item[]
    \item \texttt{map} : prend un iterable et applique la fonction passée en argument


\end{itemize}

\begin{lstlisting}
trait M[T] {
    def flatMap[U](f: T => M[U]): M[U]
    def map[U](f: T => U): M[U]

    def withFilter[U](p: T => Boolean): M[T]
    def foreach(f: T => Unit): Unit
}

def unit[T](x: T): M[T]

\end{lstlisting}

\begin{lstlisting}
val l : List[Option[Int]] = List( Some(5), None, Some(7), Some(8))

// Link with for( ; ) yield
println( l map(_ map(x => x)) flatten)
println( l flatMap(_ map(x => x)))
println( l flatMap(_ flatMap(x => Some(x))))
println( for(i <- l; j <- i) yield j)

=> List(5, 7, 8)

// Link with for( ) yield
println( l map(x => x))
println( l flatMap(x => Some(x)))
println( for(i <- l) yield i)

=> List(Some(5), None, Some(7), Some(8))

// Link with for
println( l foreach(x => print(x)))
println( for(i <- l) print(i))

=> Some(5)NoneSome(7)Some(8)()
\end{lstlisting}


\subsubsection{map - flatmap}
\begin{lstlisting}
m map f == m flatMap ( x => unit(f(x)) )

m flatMap f == flatten( m map f)
\end{lstlisting}

\subsubsection{Monad law}
\begin{tabular}{m{4cm}m{12cm}}
    Associativity & \begin{lstlisting}
m flatMap f flatMap g ==  m flatMap (x => f(x) flatMap g)
\end{lstlisting}\\
    Left unit & \begin{lstlisting}
unit(x) flatMap f == f(x)
\end{lstlisting}\\
    Right unit &\begin{lstlisting}
 m flatMap unit == m
\end{lstlisting}
\end{tabular}

\paragraph{Example}:

\begin{lstlisting}
// Monads Laws
val m : List[Int] = List(1, 2, 3, 4, 5)
val x = 5
val f = (i : Int) => List(i * 2)
val g = (i: Int) => List(i + 10)

// Associativity
println ( m.flatMap(f(_)).flatMap(g(_)) == m.flatMap( f(_).flatMap(g(_))) )

// Left Unit
println ( List(x).flatMap(f(_)) == f(x) )

// Right Unit
println ( m.flatMap(List(_)) == m )
\end{lstlisting}


\subsubsection{Exercice}
\begin{lstlisting}
  def sequenceOption[A](x: List[Option[A]]): Option[List[A]] = {
    x match {
      case Nil => Some(Nil)
      case h :: t => h flatMap (x => sequenceOption(t) map (x :: _))
    }
  }
\end{lstlisting}

\subsection{Yield}
Va faire une espère de link dans le type du premier du for:
\begin{lstlisting}
val A = Array(1,2)
val B = List(1,2)

// Array((1,1),	(1,2), (2,1), (2,2))
for (i <- A; j <- B) yield (i,j)

// List((1,1), (1,2), (2,1), (2,2))
for (i <- B; j <- A) yield (i,j)
\end{lstlisting}

\subsubsection{Translation}
\begin{tabular}{m{7.5cm}m{0.2cm}m{8.85cm}}
\begin{lstlisting}
for (x <- expr1) yield expr2

for (x <- expr1 if expr2) yield expr3

for (x <- expr1; y <- expr2; seq) yield expr3
\end{lstlisting}
& $\to$ &
\begin{lstlisting}
expr1.map(x => expr2)

for (x <- expr1 withFilter (x => expr2)) yield expr3

expr1.flatMap(x => for (y <- expr2; seq) yield expr3)
\end{lstlisting}
\end{tabular}


\subsection{Try}
\begin{lstlisting}
abstract class Try[+T]
case class Success[T](x: T) extends Try[T]
case class Failure(ex: Exception) extends Try[Nothing]

object Try {
    def apply[T](expr: => T): Try[T] =
    try Success(expr) catch {
        case NonFatal(ex) => Failure(ex)
    }
}

abstract class Try[T] {
    def flatMap[U](f: T => Try[U]): Try[U] = this match {
        case Success(x) => try f(x) catch { case NonFatal(ex) => Failure(ex) }
        case fail: Failure => fail
    }

    def map[U](f: T => U): Try[U] = this match {
        case Success(x) => Try(f(x))
        case fail: Failure => fail
    }
}
\end{lstlisting}

\paragraph{Example}:

\begin{lstlisting}
def divide: Try[Int] = {
    val dividend = Try(Console.readLine("Enter an Int to divide:\n").toInt)
    val divisor = Try(Console.readLine("Enter an Int to divide by:\n").toInt)
    val problem = dividend.flatMap(x => divisor.map(y => x / y))

    problem match {
        case Success(v) =>
            println("Result of " + dividend.get + "/" + divisor.get + " is: " + v)
            Success(v)
        case Failure(e) =>
            println("You must've divided by zero. Try again!")
            println("Info from the exception: " + e.getMessage)
            divide
    }
}
\end{lstlisting}


\end{document}
