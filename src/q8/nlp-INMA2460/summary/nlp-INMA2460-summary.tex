\documentclass[en]{../../../eplsummary}

\usepackage{multicol}
\usepackage{pgfplots}

\DeclareMathOperator{\Lin}{Lin}
\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\vol}{vol}

\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}

\usepackage{algorithm}
\usepackage{algorithmic}

\hypertitle{Nonlinear programming}{8}{INMA}{2460}
{Benoît Legat}
{Yurii Nesterov}

\begin{center}
  ``The main fact, which should be known to any person dealing with optimization models,
  is that, in general, \emph{the optimization problems are unsolvable}.''
  \hfill\cite[p.~5]{nesterov2004introductory}
\end{center}

\section{The World of Nonlinear Optimization}
\label{sec:nlp}
We are interested in the problem
\begin{align*}
  \min & f_0(x)\\
  f_j(x) & \leq 0 & j = 1, \ldots, m\\
  x & \in S.
\end{align*}
where $f_0$ is the \emph{objective} function,
the vector function $f(x) = (f_1(x), \ldots,f_m(x))$ is called the \emph{functional constraint},
the set $S$ is called the \emph{basic feasible set} and the set
\[ Q = \{\, x \in S \mid f_j(x) \leq 0, j = 1, \ldots, m \, \} \]
is called the \emph{feasible set} of the optimization problem.

$S$ stands for \emph{structural} constraints, like non-negativity or boundedness of some variables.
% TODO link that to chapter 4

\begin{itemize}
  \item It is called \emph{unconstrained} if $Q \equiv S \equiv \Rn$ and \emph{constrained} otherwise.
  \item It is called \emph{smooth} if all the $f_i$ are differentiable and \emph{nonsmooth} otherwise.
\end{itemize}

A \emph{method} $\mathcal{M}$ for a \emph{class} $\mathcal{F}$ is given as input a problem $\mathcal{P}$ of $\mathcal{F}$.
Since $\mathcal{M}$ deals with the whole class $\mathcal{F}$,
it does not have the complete description of the problem $\mathcal{P}$ but rather the description of the class $\mathcal{F}$.
In addition it has access to an oracle $\mathcal{O}$ to collect specific information about $\mathcal{P}$.

The pair $(\mathcal{F}, \mathcal{O})$ defines a \emph{model}.
The \emph{peformance} of a method $\mathcal{M}$ on a model $(\mathcal{F}, \mathcal{O})$ is its performance on the \emph{worst} problem $\mathcal{P}_w$ of $\mathcal{F}$ for $\mathcal{M}$.

Once we define what ``a solution $x$ with accuracy $\epsilon$'' means (e.g. $\|x - x^*\| \leq \epsilon$, $|f(x)-f(x^*)| \leq \epsilon$, ...) % TODO do a list
we can define the complexity of the problem $\mathcal{P}$ for the method $\mathcal{M}$
\begin{description}
  \item[Analytical complexity]
    The number of calls to the oracle required for an accuracy $\epsilon$.
  \item[Arithmetical complexity]
    The number of arithmetic operations required for an accuracy $\epsilon$,
    including operations done by the oracle and by the method.
\end{description}

We will analyse three type of oracle of input $x \in \mathbb{R}^n$%TODO on les analyse vraiment tous ?
\begin{description}
  \item[Zero-order oracle] outputs the value $f(x)$.
  \item[First-order oracle] outputs the value $f(x)$ and the gradient $f'(x)$.
  \item[Second-order oracle] outputs the value $f(x)$, the gradient $f'(x)$ and the Hessian $f''(x)$.
\end{description}

\subsection{Global optimization}
The purpose of this section is to show that global optimization is impossible and we should work on smaller classes of problems.

Clearly, if we do not make any continuity assumption on the objective, the oracle won't give us a lot of information.
Also, since we do not consider that $f$ is convex, we cannot search for the \emph{global} optimal solution in an infinite domain.

Let % TODO the objective is f or f_0 ? f is (f_1, ..., f_m)...
\[ Q = B_n = \{\, x \in \Rn \mid 0 \leq x_i \leq 1, i = 1, \ldots, n \,\} \]
and $f_0$ be $L$-Lipschitz continuous
\begin{align*}
  |f(x) - f(y)| & \leq L\|x - y\| & \forall x,y \in B_n.
\end{align*}
We consider here a Zero-order oracle but the lower complexity bound will be the same for a smooth $f_0$ of for higher order oracle~\cite[p.~18]{nesterov2004introductory}.

The grid method $\mathcal{G}(p)$ consists in calling the oracle at all the $(p+1)^n$ points
of the set $\{0,1/p,2/p,\ldots,1\}^n$ and return the point of the grid giving the minimum value for the objective $f_0$.

\begin{tabular}{lll} % TODO find exact bounds
  $L$-Lipschitz with norm & $\mathcal{G}$ & Lower\\
  2
  & $\big(\lfloor \frac{L\sqrt{n}}{2\epsilon} \rfloor + 2\big)^n$
  & $\big(\lfloor \frac{L}{2\epsilon} \rfloor\big)^n$\\
  $\infty$
  & $\big(\lfloor \frac{L}{2\epsilon} \rfloor + 2\big)^n$
  & $\big(\lfloor \frac{L}{2\epsilon} \rfloor\big)^n$
\end{tabular}

\subsection{Nonlinear optimization}
\subsubsection{Function classes}
If $f$ is differentiable at $\bar{x}$ then for $y \in \Rn$ we have
\[ f(x) = f(\bar{x}) + \langle f'(\bar{x}), y-\bar{x}\rangle + o(\|y-\bar{x}\|) \]
where $o(r)$ is some function of $r > 0$ such that $\lim_{r \downarrow 0} \frac{1}{r}o(r) = 0$ and $o(0) = 0$.

\begin{mynota}
  We denote by $C^{k,p}_L(Q)$ the class of functions $f$ that are $k$ times continuously differentiable on $Q$
  and its $p$th derivative is Lipschitz continuous on $Q$ with the constant $L$:
  \[ \| f^{(p)}(x) - f^{(p)}(y) \| \leq L \| x - y \| \]
  for all $x,y \in Q$.
\end{mynota}

We have seen that we cannot win the game of optimization in General or with a Lipschitz continuous fonction.
Any function can be approximated as close as possible with a smooth function so with need First or Second order Lipschitz.
\paragraph{First order Lipschitz}
\begin{mylem}
  A function $f \in C^2$ belongs to $C^{2,1}_L(\Rn) \subseteq C^{1,1}_L(\Rn)$ if and only if
  \[ \| f''(x) \| \leq L \]
  for all $x \in \Rn$.
\end{mylem}

\begin{multicols}{2}
  \begin{myprop}
    Let $f \in C^{1,1}_L$. Then for any $x,y \in \Rn$ we have
    \begin{equation}
      \label{eq:lipschitzfirst}
      | f(y) - f(x) - \langle f'(x), y-x \rangle| \leq \frac{L}{2} \|y - x\|^2.
    \end{equation}
  \end{myprop}

  \begin{tikzpicture}[x=3cm,y=0.6cm]
    \draw[color=red, thick, domain=-1:1] plot
    (\x, {-\x/2+(\x)^2});
    \draw[color=blue, thick, domain=-1:1] plot
    (\x, {-\x/2});
    \draw[thick, domain=-1:1] plot
    (\x, {-(\x)^3/2 - \x*\x*sin(60-\x*420)/6-\x/2});
    \draw[color=red, thick, domain=-1:1] plot
    (\x, {-\x/2-(\x)^2});
  \end{tikzpicture}
\end{multicols}

\paragraph{Second order Lipschitz}
\label{sec:secondorderLipschitz}
We have the exact same property thant for First order Lipschitz with $f'$ since $f'$ is first order Lipschitz
\begin{myprop}
  Let $f \in C^{2,2}_M$. Then for any $x,y \in \Rn$ we have
  \[ | f'(y) - f'(x) - \langle f''(x), y-x \rangle| \leq \frac{M}{2} \|y - x\|^2. \]
\end{myprop}
\begin{mycorr}
  Let $f \in C^{2,2}_M$. Then for any $x,y \in \Rn$ we have
  \begin{equation}
    \label{eq:lipschitzsecondeig}
    f''(x) - MrI_n \leq f''(y) \leq f''(x) + MrI_n.
  \end{equation}
  where $r = \|y-x\|$.
  Remember that $A \leq B$ means that $B-A$ is positive semidefinite.
\end{mycorr}

If $f \in C_M^{2,2}$ and there is a local minimum $x^*$ such that $f'(x^*) = 0$ and $f''(x^*)$ is positive definite.
Let $0 < l \leq L < \infty$ such that the eigenvalues of the hessian $f''(x^*)$ are in the interval $[l,L]$.

Since $f'(x^*)=0$, we can see that $f'(x) = G(x)(x-x^*)$ where
$G(x) = \int_0^1 f''(x^* + \tau(x-x^*)) \dif \tau$.
If we use \eqref{eq:lipschitzsecondeig} and integrate,
we see that the eigenvalues of $G(x)$ are between $l-\frac{r}{2}M$ and $L+\frac{r}{2}M$
where $r = \|x - x^*\|$.

What that means is that from a point $x$,
the directional derivative in the direction of $x^*$ $p = \frac{x^*-x}{\|x^*-x\|}$ which is
$p^T f'(x) = -(x-x^*)^TG(x)(x-x^*)/\|x^*-x\|$ is between $-(L+\frac{r}{2}M) r$ and $-(l-\frac{r}{2}M) r$.
\begin{multicols}{2}
  We see that this direction is always a decrease for $x \in B[x^*, \bar{r}]$ where $\bar{r} \eqdef \frac{2l}{M}$.
  Geometrically, that means that there is no hill between $x$ and the target, the path is monotone to $x^*$.
  Also the decrease is bounded from both sides.
  However, this direction is ideal since we do not know $x^*$.

  \begin{tikzpicture}[x=3cm,y=0.6cm]
    % derivative should be between xl - 1/2*x^2M and xL + 1/2*x^2M
    % I fix l = 1, L = 2 and M = 1 so bar{r} = 2
    % I want the derivative to be 1.5*x + sin(x) * x^2
    % So the function is 1.5x^2/2 - cos(x) * x^2 + 2sin(x)*x + 2cos(x)
    % Or                 1.5x^2/2 - (k^2*cos(k*x) * x^2 + k*2*sin(k*x)*x + 2cos(k*x)) / k^3
    % I set k to 10 to increase frequency of noise
    \draw[color=red, thick, domain=-2:0] plot
    (\x, {(\x)^2 + abs(\x)^3/6}); % 2/2x^2 + 1/6x^3
    \draw[color=red, thick, domain=-2:0] plot
    (\x, {(\x)^2/2 - abs(\x)^3/6}); % 1/2x^2 + 1/6x^3
    \draw[thick, domain=-2:0] plot
    (\x, {1.5*(\x)^2/2 + (-100*cos(10*180/pi*(\x))*(\x)^2 + 10*2*sin(10*180/pi*(\x))*(\x) + 2*cos(10*180/pi*(\x))-2)/2/1000}); % 1/2x^2 + 1/6x^3
    \draw[thick, domain=-2:0] plot
    (\x, {1.5*(\x)^2/2 + (-100*cos(10*180/pi*(\x))*(\x)^2 + 10*2*sin(10*180/pi*(\x))*(\x) + 2*cos(10*180/pi*(\x))-2)/2/1000}); % 1/2x^2 + 1/6x^3
    \node[above] at (-2, 3) {$x$};
    \node[below] at (0, 0) {$x_0$};
    \draw (-2, 0) -- (0,0);
    \draw (-2, 0.1) -- (-2,-0.1);
    \node[below] at (-2, 0) {$\bar{r}$};
  \end{tikzpicture}
\end{multicols}


\subsection{Local methods in unconstrained minimization}
\begin{mydef}
  We call a sequence $\{a_k\}_{k=1}^\infty$ a \emph{relaxation sequence} if $a_{k+1} \leq a_k$ for all $k \geq 0$.
\end{mydef}
Remember that every non-increasing sequence bounded from below is Cauchy.
If we generate a relaxation sequence $\{f(x_k)\}_{k=0}^\infty$ and $f$ is bounded from below it converges.

\subsubsection{Gradient method}
The gradient method iteration is
\begin{equation}
  \label{eq:graditer}
  x_{k+1} = x_k - h_kf'(x_k).
\end{equation}
If follows the steepest descent $f'(x_k)$.
$h_k$ can be fixed a priori to constant or a function of $k$, e.g. $h/\sqrt{k+1}$ for some constant $h$.
But it can also be computed at each iteration.
Theoritically, we would like to pick
\[ h_k = \argmin_{h \geq 0} f(x_k - hf'(x_k)). \]
That is called a \emph{full relaxation}.

In practice we rather use the Goldstein-Armijo rule
\[ \alpha\langle f'(x_k), x_k - x_{k+1}\rangle \leq f(x_k) - f(x_{k+1}) \leq \beta\langle f'(x_k), x_k - x_{k+1}\rangle \]
\begin{multicols}{2}
  \noindent
  which in the case of the gradient method \eqref{eq:graditer} is
  \[ \alpha h_k\|f'(x_k)\|^2 \leq f(x_k) - f(x_{k+1}) \leq \beta h_k\|f'(x_k)\|^2 \]
  since $x_k - x_{k+1} = -h_kf'(x_k)$.

  \begin{tikzpicture}[x=3cm,y=0.6cm]
    \draw[color=red, thick, domain=0:2] plot
    (\x, {-0.2*4*\x+1.75});
    \draw[color=red, thick, domain=0:1.3] plot
    (\x, {-0.55*4*\x+1.75});
    \draw[color=blue, thick, domain=0:0.6] plot
    (\x, {-4*\x+1.75});
    \draw[thick, domain=0:0.26] plot
    (\x, {2*(\x-1)^2 - cos(\x*420)/4});
    \draw[green, domain=0.27:0.56] plot
    (\x, {2*(\x-1)^2 - cos(\x*420)/4});
    \draw[thick, domain=0.57:0.89] plot
    (\x, {2*(\x-1)^2 - cos(\x*420)/4});
    \draw[green, domain=0.90:1.54] plot
    (\x, {2*(\x-1)^2 - cos(\x*420)/4});
    \draw[thick, domain=1.55:2] plot
    (\x, {2*(\x-1)^2 - cos(\x*420)/4});
  \end{tikzpicture}
\end{multicols}

\paragraph{First order Lipschitz}
If $f \in C^{1,1}_L$, \eqref{eq:lipschitzfirst} with \eqref{eq:graditer} becomes
\[ f(x_{k+1}) \leq f(x_k) - h_k\Big(1 - \frac{h_k}{2}L\Big) \|f'(x_k)\|^2. \]

We see that for $0 \leq h_k \leq 2/L$, $\{f(x_k)\}_{k=0}^\infty$ is a relaxation sequence.
We decrease at least as much as
\begin{equation}
  \label{eq:gradientguarantee}
  f(x_{k+1}) \leq f(x_k) - \frac{w}{L} \|f'(x_k)\|^2
\end{equation}
where $w = 1/2$ for a constant $h_k = 1/L$ or for a full relaxation and $w = 2\alpha(1-\beta)$ for Goldstein-Armijo.

Summing for $k = 0, \ldots, N-1$ we get
\[ \frac{w}{L} \sum_{k=0}^N \| f'(x_k) \|^2 \leq f(x_0) - f(x_N) \leq f(x_0) - f^*. \]
If we define $g_N^* = \min_{0 \leq k \leq N} g_k$ we have
\[ g_N^* \leq \frac{1}{\sqrt{N+1}} \Big[ \frac{L}{w} (f(x_0) - f^*) \Big]^2. \]

Note that we cannot say anything about the rate of convergence of the sequence $\{f(x_k)\}$ or $\{x_k\}$
and without additional very strict assumption we cannot guarantee the convergence to a minimum.
The fact that $\|f'(x_k)\| \to 0$ as $k \to \infty$ only ensures us that we converge to a stationary point.
%TODO lower bound are not known + cite

\paragraph{Second order Lipschitz}
If $f \in C^{2,2}$ and there is a local minimum $x^*$ such that $f'(x^*) = 0$ and $f''(x^*)$ positive definite.
Let $0 < l \leq L < \infty$ such that the eigenvalues of the hessian $f''(x^*)$ are in the interval $[l,L]$. % TODO Same assumption than for Newton, stop restating it

Using $f'(x_k) = G_k(x_k-x^*)$, \eqref{eq:graditer} gives
\[ r_{k+1} = (I-h_kG_k)r_k \]
and we have seen that the eigenvalues of $I-h_kG_k$
are between $1 - h_k(L+\frac{r_k}{2}M)$ and $1 - h_k(l-\frac{r_k}{2}M)$.

If $x_0$ is close enough to a local minimum $x^*$, i.e. $r_0 < \bar{r}$ where $r_k = \|x_k - x_0\|$,
and for all our steps we choose $h_k$ such that $0 \leq h_k \leq \frac{2}{L+\frac{r_k}{2}M}$ then
for $k > 0$, $x_k$ is also be in the ball $B(x^*,\bar{r})$ and
$\{r_k\}$ is a relaxation sequence.

With the step size $h_k = 2/(L+l)$ we can ensure a linear rate of convergence~\cite[Theorem~1.2.4]{nesterov2004introductory}
\[ \|x_k - x^*\| \leq \frac{\bar{r}r_0}{\bar{r}-r_0} \Big(1 - \frac{l}{L+l}\Big)^k. \]
This rate of convergence is called \emph{linear}.

\subsubsection{Newton method}
Initially, the Newton method was proposed for finding a root of a function of one variable $\phi(t)$, $t \in \R$, $\phi(t^*) = 0$.
Using a linear approximation $\phi(t+\Delta t) \approx \phi(t) + \phi'(t)\Delta t$ which gives $t_{k+1} = t_k - \phi(t_k)/\phi'(t_k)$.
It can be generalized for a function $F : \Rn \to \Rn$ where the scheme becomes $x_{k+1} = x_k - [F'(x_k)]^{-1}F(x_k)$.

In \emph{unconstrained} minimization, it is necessary that $f'(x) = 0$ for $x$ to be a local minimum%
\footnote{In constrainted minimization we need to use KKT condition instead}.
Since $f' : \Rn \to \Rn$, we can apply
\begin{equation}
  \label{eq:newton}
  x_{k+1} = x_k - [f''(x_k)]^{-1}f'(x_k).
\end{equation}

This method has two drawbacks.
First, it can break down if $f''(x_k)$ is degenerate.
Second, the Newton process can diverge (see \cite[Example~1.2.4]{nesterov2004introductory}).
To escape from the possible divergence, we can apply a \emph{damped Newton method}
\[ x_{k+1} = x_k - h_k[f''(x_k)]^{-1}f'(x_k) \]
where $h_k > 0$ is the step-size parameter.
At the initial stage when we are far from $x^*$ we can use the same step-size strategies than for the gradient method.
At the final stage it is reasonable to choose $h_k = 1$.

From \eqref{eq:newton}, we have
\[ x_{k+1} - x^* = [f''(x_k)]^{-1}[f''(x_k)](x_k - x^*) - [f''(x_k)]^{-1}f'(x_k) \]
so
\[ x_{k+1} - x^* = [f''(x_k)]^{-1}[f''(x_k)-G_k](x_k - x^*). \]
Let $G'_k = f''(x_k)-G_k$, we can see~\cite[p.~36]{nesterov2004introductory} that $\|G'_k\| \leq \frac{r_k}{2}M$.

\paragraph{Second order Lipschitz}
If $f \in C_M^{2,2}(\Rn)$ and there exists a local minimum of $f$ $x^*$ with positive definite Hessian and let $l>0$ be a lower bound for the eigenvalues of $f''(x^*)$.
From \eqref{eq:lipschitzsecondeig}, we know that the eigenvalues of $f''(x_k)$ are greater than $l-Mr_k$.
If $r_k < l/M = \bar{r}/2$, $f''(x_k)$ is positive definite and the eigenvalues of $f''(x_k)$ are smaller than $(l-Mr_k)^{-1}$ so $\|[f''(x_k)]^{-1}\| \leq (l-Mr_k)^{-1}$.
We have
\[ r_{k+1} \leq \frac{Mr_k^2}{2(l-Mr_k)}. \]
One can verify from this equation that if $r_k < 2l/(3M) = \bar{r}/3$, $\{r_k\}$ is a relaxation sequence.
The rate of convergence of this type is called \emph{quadratic}.

\subsubsection{Variable metric method (or quasi-Newton)}
\label{sec:varmet}
Note that the gradient $f'(x)$ of a nonlinear function $f(x)$ is defined with
respect to the standard Euclidean inner product $\langle x, y\rangle = y^Tx$ on $\Rn$.
If we define $f'_A = A^{-1}f'(x)$ as the gradient and Hessian with respect to $\langle Ax, y\rangle = y^TAx$,
we have the linear approximation
\begin{align*}
  f(y) & = f(x) + \langle f'(x), y-x \rangle + o(\|y-x\|)\\
       & = f(x) + \langle A^{-1}f'(x), y-x \rangle_A + o(\|y-x\|)\\
       & = f(x) + \langle f'_A(x), y-x \rangle_A + o(\|y-x\|)
\end{align*}
The direction $A^{-1}$ is therefore simply the gradient with respect to a different norm.

The scheme can be seen as a gradient method with $h=1$ but with a different norm.

One other way to see the scheme is the minimization at each step of a the quadratic approximation
\begin{align*}
  f(y) & \approx \phi(x) = f(x) + \langle f'(x), y-x \rangle + \langle A(y-x), y-x \rangle
\end{align*}
The minimization gives the step $A^{-1}f'(x)$.
With $A = f''(x)$ we have the classical approximation corresponding to the second order Taylor expansion and the scheme is the Newton method.

\begin{multicols}{2}
  If $f \in \mathcal{C}_L^{1,1}$,
  with $A = I/h$ and $0 < h \leq 1/L$, we see with \eqref{eq:lipschitzfirst} that $\phi$ is an upper approximation of $f(y)$.
  The minimum of $\phi$ has a lower value of $\phi$ than $x$, since it is an upper approximation, it is even lower.
  Therefore, we are sure to have a relaxation sequence.

  \begin{tikzpicture}[x=6cm]
    \draw[color=red, thick, domain=0:1.2] plot
    (\x, {-\x/2+(\x)^2});
    \draw[color=blue, thick, domain=0:1.2] plot
    (\x, {-\x/2});
    \draw[thick, domain=0:1.2] plot
    (\x, {-(\x)^3/2 - \x*\x*sin(60-\x*420)/6-\x/2});
    \draw[dashed, thick, domain=0:0.7] plot
    (\x, {-\x/2+4.2*(\x)^2});
    \draw[dashed, thick, domain=0:0.95] plot
    (\x, {-\x/2+2.6*(\x)^2});
    \draw[dashed, thick, domain=0:1.1] plot
    (\x, {-\x/2+1.8*(\x)^2});
    \draw[dashed, thick, domain=0:1.2] plot
    (\x, {-\x/2+1.4*(\x)^2});
    \draw[dashed, thick, domain=0:1.2] plot
    (\x, {-\x/2+1.2*(\x)^2});
    \draw[dashed, thick, domain=0:1.2] plot
    (\x, {-\x/2+1.1*(\x)^2});
  \end{tikzpicture}
\end{multicols}
This is the Gradient method.
Actually as we have seen we are also sure to have a relaxation sequence if $\frac{1}{L} < h < \frac{2}{L}$ but it is no more an upper approximation.

The variable metric methods consist in using different matrices $A$ at each iteration.
Those matrices are built only from the zero and first order oracle, not from the Hessian.
Let $H_k = A_k^{-1}$.
We do the iteration
\[ x_{k+1} = x_k - h_kH_kf'(x_k) \]
where $h_k$ is chosen the same way as with the gradient method (e.g. Goldstein-Armijo).

$H_{k+1}$ is computed using $H_k$, $f(x_k),f(x_{k+1})$ and $f'(x_k),f'(x_{k+1})$.
In the quadratic case, $f(x) = \alpha + \langle a, x \rangle + \langle Ax, x \rangle/2$, $f'(x) = Ax + a$ so
$f'(x_{k+1}) - f'(x_k) = A(x_{k+1}-x_k)$.
From this we have the \emph{quasi-Newton rule}:
\[ H_{k+1}(f'(x_{k+1}-f'(x_k)) = x_{k+1} - x_k. \]

There are several example of variable metric schemes, see \cite[Example~1.2.3]{nesterov2004introductory}.
BFGS is considered as the most stable scheme.

For quadratic functions, the variable metric usually terminates in $n$ iterations.
For general functions,
in the neighborhood of a strict minimum they have a \emph{superlinear} rate of convergence:
For $x_0 \in \Rn$,  there exists $N$ such that for all $k \geq N$ we have
\[ \|x_{k+1} - x^*\| \leq \constant \cdot \|x_k - x^*\| \cdot \|x_{k-n} - x^*\|. \]
but in theory they do not have better global convergence than the gradient method.

\subsubsection{Conjugate gradient method}
The idea of the conjugate gradient is to base the scheme on the quadratic function
\begin{align*}
  f(x)
  & = \alpha + \langle a, x \rangle + \frac{1}{2} \langle Ax, x \rangle\\
  & = \alpha - \frac{1}{2} \langle Ax^*, x^* \rangle + \frac{1}{2} \langle A(x-x^*), x-x^* \rangle\\
  & = \alpha - \frac{1}{2} \langle Ax^*, x^* \rangle + \frac{1}{2} \|x-x^*\|_A
\end{align*}
We see that $f'(x) = A(x-x^*)$.

We define the sequence $\{x_k\}_{k = 1, 2, \ldots}$ (ideallic for now) as follows
\[ x_k \eqdef \argmin \{f(x) | x \in x_0 + \mathcal{L}_k\} \]
where
\[ \mathcal{L}_k = \Lin \{A(x_0-x^*), \ldots, A^k(x_0-x^*)\}. \]

We can see that for any $k \geq 1$, $\mathcal{L}_k = \Lin\{f'(x_0), \cdot, f'(x_{k-1})\}$~\cite[Lemma~1.3.1]{nesterov2004introductory}.

Removing the constant terms of $f(x)$, we see that
\[ x_k = \argmin \{\|x-x^*\|_A | x \in x_0 + \mathcal{L}_k\}. \]
We now see that $(x_k-x^*)$ is $A$-orthogonal to $\mathcal{L}_k$%
\footnote{otherwise, if $y \in \mathcal{L}_k$ is not orthogonal, let $z = \pm y$ so that the scalar product is positive,
$\|x_k-x^*\|_A = \|x_k-z-x^*\|_A+\|z\|_A+2\langle x_k-z-x^*,z \rangle_A = \|x_k-z-x^*\|_A+2\langle x_k-x^*,z \rangle_A > \|x_k-z-x^*\|_A$}.
In particular, for $i < k$,
\[ \langle f'(x_k), f'(x_i) \rangle = \langle A(x_k-x^*), f'(x_i) \rangle = \langle (x_k-x^*), f'(x_i) \rangle_A = 0. \]
We see that the gradients are conjugate.
A simple consequence of this is that $\mathbb{L}^n = \Rn$ so after $n$ steps, $x_n = \argmin\{ f(x) | x \in \Rn\} = x^*$.

Since $x_k$ minimize $\|x_k-x^*\|_A$, $x_k$ is the $A$-projection of $x^*$ in $\mathcal{L}_k$.
Therefore $x_{k+1}-x_k = (x_{k+1}-x^*) - (x_k-x^*)$ is the $A$-projection of $x^*$ in $\mathcal{L}_{k+1}$ from which we remove the
$A$-projection of $x^*$ in $\mathcal{L}_k$.
$x_{k+1}-x_k$ is therefore $A$-orthogonal to $\mathcal{L}_k$.

Let $\delta_k = x_{k+1}-x_k$.
Since $\Lin\{\delta_0, \ldots, \delta_{k-1}\} = \mathcal{L}_k$,
We see that the steps $\delta_k$ are $A$-conjugate.

We will see now that $\delta_k$ is only a linear combination of $f'(x_k)$ and $\delta_{k-1}$.
At first we can only say that there is $h_k, \lambda_0, \ldots, \lambda_{k-1}$ such that
\[ \delta_k = -h_kf'(x_k) + \sum_{j=0}^{k-1} \lambda_j \delta_j \]
but since the $\delta_k$ are $A$-conjugate and the gradients are conjugate,
\begin{align*}
  \langle \delta_i, \delta_k \rangle_A
  & = -h_k \langle A(x_{i+1} - x_i),f'(x_k) \rangle + \sum_{j=0}^{k-1} \lambda_j \langle \delta_i, \delta_j \rangle_A\\
  0 & = -h_k \langle f'(x_{i+1}-f'(x_i), f'(x_k) \rangle_A + \lambda_i \|\delta_i\|_A\\
\end{align*}
If $i < k-1$, we see that $\lambda_i = 0$ but for $i = k-1$, it gives the value of $\lambda_i$ so we have
\[ x_{k+1} = x_k - h_k\left(f'(x_k) - \frac{\|f'(x_k)\|^2}{\|\delta_{k-1}\|_A} \delta_{k-1}\right). \]
Using the properties we have seen we can rewrite this as
\[ x_{k+1} = x_k - h_kp_k \] % TODO check +- because nesterov do not say h_k > 0 but I give the right h_k
where
\[ p_k = f'(x_k) - \beta_k p_{k-1}. \]
where $\beta_k$ can be expressed as
\begin{description}
  \item[] $\|f'(x_{k})\|^2/\langle f'(x_{k}-f'(x_{-1}k),p_{k-1} \rangle$
  \item[Fletcher-Rieves] $-\|f'(x_{k})\|^2/\|f'(x_{k-1})\|^2$
  \item[Polak-Ribbière] $-\langle f'(x_{k}), f'(x_{k})-f'(x_{k-1}) \rangle/\|f'(x_{k-1})\|^2$
\end{description}
which are different expression of the same value in the quadratic case.
In the quadratic case, we have
\[ h_k = \frac{\|f'(x_k\|}{\|p_k\|_A}. \]

When $f$ is not quadratic, the three expressions for $\beta_k$ do not give the same results and $h_k$
is rather computed with a line search as with the gradient method.
The method do not necessarily terminates in $n$ steps but after $n$ iterations we need to restart
which consists in setting $\beta_k = 0$.
After $n$ steps, we can ensure that
\[ \|x_{n+1}-x^*\| \leq \constant \cdot \|x_0-x^*\|^2 \]
if $x_0$ is close enought to $x^*$.
This is slower than the variable metric but the iterations are cheaper.
As far as the global convergence is concerned it is still not better than the gradient method.

For a quadratic function $f$, we can compute the optimal $h_k$

For complementary information about the Conjugate gradient method, we refer the reader to \cite{shewchuk1994introduction} or \cite[\S2.3]{vandooren2011inma1170}.

\subsection{Constrained minimization}
Let us now see what we can do in constrained minimization
\begin{align*}
  \min_x f_0(x)\\
  f_i(x) & \leq 0
\end{align*}

Intuitively, a constrained problem is harder than an unconstrained problem so we won't try to find a faster scheme for constrained problem
and we will rely on the scheme we already have.
However, we will see in the next chapters that this is not always harder.

Two methods can be used to transform the problem to a sequence of unconstrained minimization problems.
\begin{multicols}{2}
  \subsubsection{Penalty Function Method}
  \begin{mydef}
    A \emph{penalty function} for a closed set $Q$ is a continuous function $\Phi(x)$ such that $\Phi(x) = 0$ for $x \in Q$ and
    $\Phi(x) > 0$ for $x \notin Q$.
  \end{mydef}
  If $\Phi_1(x)$ is a penalty function for $Q_1$ and $\Phi_2(x)$ is a penalty function for $Q_2$ then
  $\Phi_1(x) + \Phi_2(x)$ is a penalty function for the intersection $Q_1 \cap Q_2$.

  We choose $x_0 \in \Rn$ and a sequence of penalty coefficients $0 < t_k < t_{k+1}$, $t_k \to \infty$.
  And at the iteration $k$ we search for $x_{k+1} = \argmin_{x \in \Rn} \Psi_k(x) = f_0(x) + t_k \Phi(x)$ using $x_k$ as starting point.

  Let us assume that $x_{k+1}$ is a global minimum of $\Psi_k$.
  If there exists $\bar{t} > 0$ such that the set $S = \{x \in \Rn | f_0(x) + \bar{t}\Psi(x) \leq f^*\}$ is bounded,
  then
  \begin{align*}
    \lim_{k \to \infty} f_0(x_k) & = f_0^*\\ % TODO there wer no _0 in the book
    \lim_{k \to \infty} \Psi(x_k) & = 0.
  \end{align*}

  \subsubsection{Barrier Function Method}
  \begin{mydef}
    A \emph{barrier function} for a closed set $Q$ with nonempty interior is a continuous function $F(x)$ such that
    $F(x) \to \infty$ when $x$ approaches the boundary of the set $Q$.
  \end{mydef}
  If $F_1(x)$ is a penalty function for $Q_1$ and $F_2(x)$ is a penalty function for $Q_2$ then
  $F_1(x) + F_2(x)$ is a penalty function for the intersection $Q_1 \cap Q_2$.

  Since $Q$ must have a nonempty interior we mush satisfy the \emph{Slater condition}:
  $\exists \bar{x}$ such that $f_i(\bar{x}) < 0$ for all $i$.

  We choose $x_0 \in \inte Q$ and a sequence of penalty coefficients $0 < t_k < t_{k+1}$, $t_k \to \infty$.
  And at the iteration $k$ we search for $x_{k+1} = \argmin_{x \in \Rn} \Psi_k(x) = f_0(x) + t_k^{-1} F(x)$ using $x_k$ as starting point.

  Let us assume that $x_{k+1}$ is a global minimum of $\Psi_k$.
  If the barrier $F(x)$ is below bounded then
  \[ \lim_{k \to \infty} \min_{x \in Q} \Psi_k(x) = f^*. \]
\end{multicols}

There are still many questions that needs to be answered but in the framework on nonlinear programming,
we cannot answer them.

These questions are answered in smaller classes of problem such as convex optimization.

\section{Smooth Convex Programming}
\subsection{Smooth Convex functions}
What assumption can we make so as to make our problem more tractable ?
The reason of our trouble is the fact that $f'(x) = 0$ is not sufficient to be a local minimum, only to be a stationary point.

We would like to show that we need the function to be convex, which means that it is above all its tangent hyperplanes.
Let us impose that $f'(x) = 0$ must be a sufficient condition to be a global minimum.
If $x_0$ is such that $f'(x_0) = 0$, geometrically, $f$ must be above the ``flat'' hyperplane passing through $(x_0, f'(x_0))$.
Luckily, this hyperplane is actually the tangent at $x^*$.

\begin{multicols}{2}
  If $f'(x_0) \neq 0$, this reasoning does not work.
  The idea is to say that if we flatten the tangent at $x_0$, the resulting fucntion is still in our class.
  Let us therefore impose that if $f$ is in our class then $f + \langle a, x \rangle$ is also in our class for all $a$.
  Hence the function $\bar{f}(x) = f(x) + \langle -f'(x_0), (x-x_0) \rangle$ is in our class.
  However $\bar{f}'(x_0) = 0$ so $\bar{f}$ is above its hyperplane at $x_0$.
  Consequently, $f$ too.

  \begin{tikzpicture}[x=3cm,y=0.8cm]
    \draw[thick, domain=-.3:1.8] plot
    (\x, {(\x)^2});
    \draw[thick, domain=1.07:1.28] plot
    (\x, {-1+(10*(\x-1.1))^2/10});
    \draw (1.31,-0.9) node {$-$};
    \draw[color=blue, thick, domain=-.3:1.8] plot
    (\x, {1+2*((\x)-1)});
    \draw[thick, color=blue, domain=1.37:1.58] plot
    (\x, {-1+(0.01+2*((\x-1.4)-0.01))});
    \draw (1.65,-0.9) node {$=$};
    \draw[dashed, thick, domain=-.3:1.8] plot
    (\x, {(\x)^2 - (1+2*((\x)-1))});
    \draw[dashed, thick, domain=1.73:1.98] plot
    (\x, {-1.9+10*((\x)-1.8)^2 - (1+2*((\x-1.8)-1))});
    \draw[dashed, color=blue, thick, domain=-.3:1.8] plot
    (\x, {0});
    \draw[->, color=green] (1,0.8) to (1,0.2);
  \end{tikzpicture}
\end{multicols}

Let $\mathcal{F}^1$ be the class of \emph{continuously differentiable convex} function.
We have many equivalent definitions, we will be able to generalize some of them in the case of non-differentiable function in the next chapter.
\begin{mytheo}
  \label{theo:f1}
  Let $f$ be a \emph{continuously diffentiable} function.
  Then all the following conditions are equivalents:
  \begin{itemize}
    \item $f$ is \emph{convex}.
    \item for all $x,y \in \Rn$,
      \begin{equation}
        \label{eq:smoothconv1}
        f(y) \geq f(x) + \langle f'(x), y-x \rangle.
      \end{equation}
    \item for all $x,y \in \Rn$ and $\alpha \in [0,1]$,
      \begin{equation}
        \label{eq:smoothconv2}
        f(\alpha x + (1-\alpha) y) \leq \alpha f(x) + (1-\alpha)f(y).
      \end{equation}
    \item for all $x,y \in \Rn$,
      \begin{equation*}
        \langle f'(x)-f'(y), x-y \rangle \geq 0.
      \end{equation*}
  \end{itemize}
  If $f$ is twice continuously differentiable they are also equivalent to the following condition
  \begin{itemize}
    \item $f''(x) \geq 0$, i.e. $f''(x)$ is positive semidefinite.
  \end{itemize}
\end{mytheo}
$\langle f'(x)-f'(y), x-y \rangle$ is not easy to interpret geometrically.
Notice that if $f$ is twice differentiable and $f''(x)$ is constant or for $y \to x$, $f'(x) - f'(y) = f''(x) (x-y)$.
That gives $\langle f'(x)-f'(y), x-y \rangle/2 \approx \langle f''(x) (x-y), (x-y) \rangle/2$ which is the second order term of the Taylor serie of $f$ around $x$.

See \cite[Example~2.1.1]{nesterov2004introductory} for examples of \emph{continuously differentiable}
\footnote{it may be surprising that $|x|^p$ is continuously differentiable for $p > 1$}
convex function.

We have seen that $f$ has to be convex to ensure that $f'(x^*)=0$ is a sufficient condition for $x^*$ to be a global minimum.
We can also prove that if $f$ is convex, the condition is sufficient.
\begin{mytheo}
  If $f \in \mathcal{F}^1$ and $f'(x^*) = 0$ then $x^*$ is a global minimum of $f(x)$ on $\Rn$.
\end{mytheo}

We have useful invariant properties.
\begin{mylem}
  If $f_1, f_2 \in \mathcal{F}^1(\Rn)$, $\alpha,\beta \geq 0$, $b \in \Rn$ and $A \in \R^m \to \Rn$ then
  \begin{align*}
    \alpha f_1 + \beta f_2 & \in \mathcal{F}^1(\Rn),\\
    f(Ax+b) & \in \mathcal{F}^1(\Rn).
  \end{align*}
\end{mylem}

As with general nonlinear functions, we need further assumptions to guarantee some special topological properties.
We use the same notation for $\mathcal{F}$ as with $\mathcal{C}$.

\begin{mytheo}
  Let $f : \Rn \to \Rn$ be a \emph{continuously diffentiable} function.
  Then all the following conditions are equivalents:
  \begin{itemize}
    \item $f \in \mathcal{F}_L^{1,1}(\Rn)$.
    \item for all $x,y \in \Rn$,
      \begin{equation}
        \label{eq:convlip11}
        0 \leq f(y) - f(x) - \langle f'(x), y-x \rangle \leq \frac{L}{2}\|x-y\|^2.
      \end{equation}
    \item for all $x,y \in \Rn$,
      \begin{equation*}
        f(y) \geq f(x) + \langle f'(x), y-x \rangle + \frac{1}{2L} \|f'(x)-f'(y)\|^2.
      \end{equation*}
    \item for all $x,y \in \Rn$,
      \begin{equation}
        \label{eq:convlip13}
        \langle f'(x)-f'(y), x-y \rangle \geq \frac{1}{L}\|f'(x) - f'(y)\|^2.
      \end{equation}
  \end{itemize}
\end{mytheo}

\subsubsection{Lower complexity bound}
We can obtain a lower complexity bound for first order Lipschitz function and first order oracle.
It even works if we assume that $f$ is infinitelly many times continuously differentiable, i.e. $f \in \mathcal{F}_L^{\infty,1}$.

For any first order (because first order oracle) method $\mathcal{M}$ such that $x_k \in x_0 + \Lin\{f'(x_0), \ldots, f'(x_{k-1})\}$
for all $k \geq 1$, and for any $k$, $1 \leq k \leq (n-1)/2$ and $x_0 \in \Rn$ there exists a function $f \in \mathcal{F}_L^{\infty,1}(\Rn)$
such that
\begin{align*}
  f(x_k) - f^* & \geq \frac{L}{8(k+1)^2}\|x_0-x^*\|^2,\\
  \|x_k - x^*\|^2 & \geq \beta\|x_0-x^*\|^2,
\end{align*}
where $x^*$ is the minimum of $f(x)$, $f^* = f(x^*)$ and $\beta$ can be \emph{arbitrary} close to one.
Not that the lower bound on $f(x_k) - f^*$ is \emph{exact}~\cite[p.~60]{nesterov2004introductory}.

The bound is only valid for $k \leq (n-1)/2$, this type of bound is called \emph{uniform} in the dimension.
It shows the potential performance of numerical methods on the initial stage of the minimization process and warn us that without
a direct use of finite-dimensional arguments, we cannot get a better complexity estimate for any numerical method.

We have to accept that the convergence to the optimal point $\|x_k - x^*\|$ can be arbitary slow.
We will fix this problem by choosing a problem class in which the situation is better, the Strongly convex functions.

\subsection{Smooth Strongly Convex functions}
We have seen in the previous chapter with Second order Lipschitz that we needed $x_0$ to be close enough to $x^*$ to ensure convergence.
With the Newton method, we needed to say that $r_0 \leq l/M$ to ensure that $f''(x_0) > 0$.
Here, if $f$ is twice continuously differentiable, we know that $f''(x) \geq 0$ but not that $f''(x) > \mu I$ for some $\mu > 0$.
That is the assumption we will add.

%TODO use bar{x} instead of x_0 in the previous section too
In the previous section, we simply required that $f(x) \geq f(\bar{x})$ when $f'(\bar{x}) = 0$ and not for any $\bar{x}$ and that was enough thanks to our class invariants.
We will do the same here, we only assume that $f(x) \geq f(\bar{x}) + \mu\|\|x-\bar{x}\|^2/2$ when $f'(\bar{x}) = 0$ and this is enough to prove that we must have
\[ f(y) \geq f(x) + \langle f'(x), y-x \rangle + \frac{1}{2}\mu\|y-x\|^2. \]
for all $x,y \in \Rn$.

Let $\mathcal{S}_{\mu}^k$ be the class of $k$ times \emph{continuously differentiable strongly convex} function with parameter $\mu$. % FIXME parameter or constant ?
We have many equivalent definitions.
\begin{mytheo}
  \label{theo:s1}
  Let $f$ be a \emph{continuously diffentiable} function.
  Then all the following conditions are equivalents:
  \begin{itemize}
    \item $f \in \mathcal{S}_{\mu}^1$ is \emph{convex}.
    \item for all $x,y \in \Rn$,
      \begin{equation*}
        f(y) \geq f(x) + \langle f'(x), y-x \rangle + \frac{\mu}{2}\|y-x\|^2.
      \end{equation*}
    \item for all $x,y \in \Rn$ and $\alpha \in [0,1]$,
      \begin{equation*}
        f(\alpha x + (1-\alpha) y) \leq \alpha f(x) + (1-\alpha)f(y) - \alpha(1-\alpha)\frac{\mu}{2}\|x-y\|^2.
      \end{equation*}
    \item for all $x,y \in \Rn$,
      \begin{equation}
        \label{eq:s13}
        \langle f'(x)-f'(y), x-y \rangle \geq \mu\|x-y\|^2.
      \end{equation}
  \end{itemize}
  If $f$ is twice continuously differentiable they are also equivalent to the following condition
  \begin{itemize}
    \item $f''(x) \geq \mu I$.
  \end{itemize}
\end{mytheo}

See \cite[Example~2.1.2]{nesterov2004introductory} for examples of \emph{continuously differentiable}
strongly convex function.

Again, as for the previous section,
we have seen that $f$ has to be stronly convex under another assumption.
We can also prove the converse: if $f$ is strongly convex, the assumption is respected.
\begin{mytheo}
  If $f \in \mathcal{S}_\mu^1$ and $f'(x^*) = 0$ then $x^*$ is a global minimum of $f(x)$ on $\Rn$ but also
  \[ f(x) \geq f(x^*) + \frac{1}{2}\mu\|x-x^*\|^2 \]
  for all $x \in \Rn$.
\end{mytheo}

We have useful invariant properties.
Also remember that $\mathcal{F}_L^{k,l} \equiv \mathcal{S}_{0,L}^{k,l}$.
\begin{mylem}
  If $f_1 \in \mathcal{S}_{\mu_1}^1(\Rn)$ and $f_2 \in \mathcal{S}_{\mu_2}^1(\Rn)$ and $\alpha,\beta \geq 0$ then
  \begin{align*}
    \alpha f_1 + \beta f_2 & \in \mathcal{F}_{\alpha\mu_1+\beta\mu_2}^1(\Rn)
  \end{align*}
\end{mylem}

Using Cauchy-Schwarz on \eqref{eq:s13} and the first order Lipschitz condition we have
\[ \mu\|x-y\| \leq \|f'(x)-f'(y)\| \leq L\|x-y\| \]
if $f$ is twice continuously differentiable that means that the eigenvalues of $f''$ are between $\mu$ and $L$ at any point.
Therefore we defien the \emph{condition number} of $f$ as $Q_f \eqdef L/\mu$, note that $Q_f \geq 1$.

\eqref{eq:s13} can also be strenghten using the first order Lipschitz condition.
That gives the following theorem.
Indeed, if $L \to \infty$, \eqref{eq:s13L} becomes back \eqref{eq:s13}.

\begin{mytheo}
  Let $f \in \mathcal{S}_{\mu,L}^{1,1}(\Rn)$.
  Then for any $x,y \in \Rn$, we have
  \begin{equation}
    \label{eq:s13L}
    \langle f'(x)-f'(y), x-y \rangle \geq \frac{\mu L}{\mu+L}\|x-y\|^2 + \frac{1}{\mu+L} \|f'(x)-f'(y)\|^2.
  \end{equation}
\end{mytheo}

\subsubsection{Lower complexity bound}
We can obtain a much better lower complexity bound for first order Lipschitz function and first order oracle than with general smooth convex functions.
It does not depend on $n$ anymore so formally, we also include infinite-dimensional problems.

For any first order (because first order oracle) method $\mathcal{M}$ such that $x_k \in x_0 + \Lin\{f'(x_0), \ldots, f'(x_{k-1})\}$
for all $k \geq 1$, and for any $k$, $\mu > 0$ and $Q_f > 1$ and $x_0 \in \Rn$ there exists a function $f \in \mathcal{S}_{\mu,\mu Q_f}^{\infty,1}(\R^{\infty})$
such that
\begin{align*}
  f(x_k) - f^*    & \geq \frac{\mu}{2}\left(\frac{\sqrt{Q_f}-1}{\sqrt{Q_f}+1}\right)^{2k} \|x_0-x^*\|^2,\\
  \|x_k - x^*\|^2 & \geq \left(\frac{\sqrt{Q_f}-1}{\sqrt{Q_f}+1}\right)^{2k} \|x_0-x^*\|^2,
\end{align*}
where $x^*$ is the minimum of $f(x)$ and $f^* = f(x^*)$.
The lower bound on $f(x_k) - f^*$ is \emph{exact} since we will give a \emph{optimal} method, a method that reach this bound.

To see the number of iterations that we need for a precision $\epsilon$, we use the fact that $1+x \leq \exp(x)$
which implies that $(1+x)^{-2k} \geq \exp(-2kx)$ which we use for $x = 2/(\sqrt{Q_f}-1)$.
Hence we have
\[ f(x_k) - f^* \leq \frac{\mu}{2} \exp\Bigg(\frac{-4k}{\sqrt{Q_f}-1}\Bigg) r_0^2 \]
so
\[ k \geq \frac{\sqrt{Q_f}-1}{4k}\Big[\ln\frac{1}{\epsilon} + \ln \frac{\mu}{2} + 2 \ln r_0\Big]. \]

\subsection{Gradient method}
Let's analyse the efficiency of the gradient method with constant step-size $h_k = h > 0$.
With other step-size rule, the rate of convergence is similar
and the standard unconstrained minimization methods (variable metric and conjugage gradient)
have the similar efficiency estimates for smooth convex functions (strongly or not).

\subsubsection{Smooth convex functions}
Let $f \in \mathcal{F}_L^{1,1}(\Rn)$.
Since we are in a particular case of general smooth functions,
for $0 < h < \frac{2}{L}$, the gradient method still generates a relaxation sequence $\{f(x_k)\}$.
However, this time we do not only have guarantees on the convergence of $f'(x_k)$ to 0.
We know that~\cite[Theorem~2.1.13]{nesterov2004introductory}
\[ f(x_k) - f^* \leq \frac{2(f(x_0)-f^*)\|x_0-x^*\|^2}{2\|x_0-x^*\|^2+(f(x_0)-f^*)h(2-Lh)k} \]
and with the optimal step-size $h^* = 1/L$,
\[ f(x_k) - f^* \leq \frac{2L(f(x_0)-f^*)\|x_0-x^*\|^2}{2L\|x_0-x^*\|^2+(f(x_0)-f^*)k} \leq \frac{2L}{k+4}\|x_0-x^*\| \]

\subsubsection{Smooth strongly convex functions}
Let $f \in \mathcal{S}_{\mu,L}^{1,1}(\Rn)$.
For $0 \leq h \leq \frac{2}{\mu+L}$, the gradient method generates a sequence $\{x_k\}$ such that
\[ \|x_k-x^*\|^2 \leq \left(1 - \frac{2\mu L}{\mu+L}\right)^k \|x_0-x^*\|^2 \]
and with the optimal step-size $h^* = 2/(\mu+L)$ (note that we do not get the optimal step-size of general smooth convex functions $1/L$ with $\mu=0$),
\begin{align*}
  \|x_k-x^*\|  & \leq \left(\frac{Q_f-1}{Q_f+1}\right)^k \|x_0-x^*\|,\\
  f(x_k) - f^* & \leq \frac{L}{2}\left(\frac{Q_f-1}{Q_f+1}\right)^{2k} \|x_0-x^*\|^2.
\end{align*}

To see the number of iterations that we need for a precision $\epsilon$, we use the fact that $1+x \leq \exp(x)$
which implies that $(1-x)^{2k} \leq \exp(-2kx)$ which we use for $x = 2/(Q_f+1)$.
Hence we have
\[ f(x_k) - f^* \leq \frac{L}{2} \exp\Bigg(\frac{-4k}{Q_f+1}\Bigg) r_0^2 \]
so
\[ k = \bigg\lceil \frac{Q_f+1}{4}\Big[\ln\frac{1}{\epsilon} + \ln \frac{L}{2} + 2 \ln r_0\Big] \bigg\rceil. \]
We cannot conclude that our method is optimal because the main term in this estimate is $\frac{Q_f}{4} \ln\frac{1}{\epsilon}$ which is not proportional to the main term $\frac{\sqrt{Q_f}}{4}\ln\frac{1}{\epsilon}$ of the lower complexity bound.
However it could mean that our estimate is not sharp enough or that the lower complexity bound is could be improved.
We will see in the next section that it is not the second option.

\subsection{Optimal Methods}
The optimal methods will not try to be a relaxation sequence anymore for two reasons.
First, for some problem classes, it is too expensive for optimality.
Second, the schemes and the efficiency of the optimal methods are derived from \emph{global} properties of convex functions
while relaxation is a too ``microscopic'' to be useful.

Let $f \in \mathcal{S}_{\mu,L}^{1,1}$.
The idea of the method is to generate a sequence of quadratic functions $\phi_k(x) = \phi_k^* + \frac{\gamma_k}{2}\|x-v_k\|^2$
that somewhat ``approximate'' the function $f$, we will be more precise later.

To build this sequence of approximation, we have a sequence of points $y_k$ and an initial approximation $\phi_0(x) = \phi_0^* + \frac{\gamma_k}{2}\|x-v_k\|^2$.
Since $f$ is strictly convex with parameter $\mu$,
we know that $f$ is above $f(y_k) + \langle f'(y_k), x - y_k \rangle + \frac{\mu}{2} \|x-y_k\|^2$,
we will see that this is important.
We mix $\phi_k$ with this using a convex combination to get $\phi_{k+1}$.
\[ \phi_{k+1}(x) = (1-\alpha_k)\phi_k(x) + \alpha_k (f(y_k) + \langle f'(y_k), x - y_k \rangle + \frac{\mu}{2} \|x-y_k\|^2). \]

In summary, $\phi_k$ is a convex combination of $\phi_0(x)$ and all these quadratic lower approximation of $f$
\[ \phi_k(x) = \phi_0(x) \prod_{i=0}^{k-1} (1-\alpha_i) + \sum_{i=0}^{k-1} (f(y_k) + \langle f'(y_k), x - y_k \rangle + \frac{\mu}{2} \|x-y_k\|^2) \alpha_i \prod_{j=i+1}^{k-1} (1-\alpha_j) \]
Let $\lambda_k = \prod_{i=0}^{k-1} (1-\alpha_i)$ and $\lambda_0 = 1$,
we have
\[ \phi_k(x) = (1-\lambda_k)(\ldots) + \lambda_k \phi_0(x) \leq (1-\lambda_k)f(x) + \lambda_k \phi_0(x) \]
where $(\ldots)$ is a convex combination of lower approximation of $f$ so it is also a lower approximation of $f$.
Consequently,
\[ \phi_k(x) \leq (1-\lambda_k)f(x) + \lambda_k \phi_0(x). \]
If $\lambda_k \to 0$, such pair of sequence $\{\phi_k(x)\}_{k=0}^\infty$ and $\{\lambda_k\}_{k=0}^\infty$ is called an \emph{estimate sequence} of $f(x)$.

When $k \to \infty$, $\lambda_k \to 0$ so there is no more anything left of $\phi_0$ in $\lim_{k\to\infty}\phi_k$.
Therefore $\lim_{k\to\infty}\phi_k \leq f$ but now what ?
The key idea is to maintain in parallel a sequence $\{x_k\}$ such that $f(x_k) \leq \phi_k^*$ for all $k$.
Therefore, at the end $\lim_{k \to \infty} f(x_k) \leq \lim_{k \to \infty} \phi_k^* \leq \lim_{k \to \infty} \phi_k(x) \leq f(x)$ for all $x$ so $\lim_{k \to \infty} x_k = x^*$.
Necessarily, since $\lim_{k\to\infty} \phi_k \leq f$ and $\lim_{k\to\infty}f(x_k) \leq \lim_{k \to \infty}\phi_k^*$ we should have $\lim_{k \to \infty} x_k = \lim_{k \to \infty} v_k$ but that is only be true at infinity, taking $x_k = v_k$ for all $k$ doesn't work.

The tricky part is to maintain $f(x_k) \leq \phi_k^*$.
At the next step, $\phi_{k+1}^*$ is smaller than $\phi_k^*$ so $f(x_{k+1})$ needs to follow.
We can actually ensure it by choosing $x_{k+1}$ such that
\[ f(x_{k+1}) \leq f(y_k) - \frac{\omega}{2} \|f'(y_k)\| \]
and $\alpha_k$ such that
\[ \frac{\alpha_k^2}{\omega} = (1-\alpha_k)\gamma_k + \alpha_k\mu (=\gamma_{k+1}) \]
for some positive $\omega$.

Note that now we can forget about $\phi_k^*$.
We can just assume that we take $\phi_0^* = f(x_0)$ and since we have chosen the sequence $x_k$ and $\alpha_k$
so as to maintain $f(x_k) \leq \phi_k^*$ we do not care about the value of $\phi_k^*$.

We have a remaining freedom for $y_k$.
However, we have the inequality $\phi_{k+1}^* \geq f(x+1) + \ldots$ where $\ldots$ is positive.
If we use our remaining freedom to set $\ldots$ to 0, the inequality is sharper and we can hope faster convergence.

It remains to see how to find $x_{k+1}$ such that $f(x_{k+1}) \leq f(y_k) - \frac{\omega}{2} \|f'(y_k)\|$.
If we use $\omega = 1/L$, we can just use a gradient step
\[ x_{k+1} = y_k - \frac{1}{L}f'(y_k). \]

With this equality, we can simplify our scheme and get rid of $\{v_k\}$ and $\{\gamma_k\}$.
The simplified scheme is given by Algorithm~\ref{algo:optimalmethod}
\begin{algorithm}
  \caption{Optimal method for a smooth strongly convex $f \in S_{\mu,L}^{1,1}$ for $\omega = 1/L$.}
  \label{algo:optimalmethod}
  \begin{algorithmic}
    \STATE Choose $x_0 \in \Rn$ and $\alpha_0 \in (0,1)$.
      Set $y_0 = x_0$, $q = \mu/L$ (note that $q = 1/Q_f$).
    \FOR{$k = 0, 1, 2, 3, \ldots$}
      \STATE Compute $f(y_k)$ and $f'(y_k)$.
        Set $x_{k+1} = y_k - \frac{1}{L}f'(y_k)$.
      \STATE Compute $\alpha_{k+1} \in (0,1)$ from the equation $\alpha_{k+1}^2 = (1-\alpha_{k+1})\alpha_k^2+q\alpha_{k+1}$,
        and set
        \begin{align*}
          \beta_k & = \frac{\alpha_k(1-\alpha_k)}{\alpha_k^2+\alpha_{k+1}}\\
          y_{k+1} & = x_{k+1} + \beta_k(x_{k+1} - x_k).
        \end{align*}
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

We still have as degree of freedom $x_0$ which is usual but also $\alpha_0$.
At each step we need to solve a second order equation to find $\alpha_{k+1}$.
However, if we set $\alpha_0 = \sqrt{\frac{\mu}{L}} = \sqrt{q}$, we see by induction that $\alpha_k = \sqrt{q}$ for all $k \geq 0$.
Note however that this value of $\alpha_0$ does not work for $\mu = 0$. % say that we need >= and > for mu = 0
Actually, this value of $\alpha_0$ corresponds to taking $\gamma_0 = 0$ so also $\gamma_k = 0$ for all $k$.
that means that we approximate the smooth convex function by linear functions.
However, $f$ is not linear, its curvature is between $0 = \mu$ and $L$.
If we take $\gamma_0 = L$, it is much safer.
Actually we have $\gamma_k = \lambda_k\gamma_0 + (1-\lambda_k)\mu$ so $\lim_{k\to\infty} \gamma_k = 0$ anyway but it is not zero before infinity and that makes all the difference.

\subsubsection{Complexity}
We can prove that the optimal method generates a sequence $\{x_k\}$ such that
\begin{equation}
  \label{eq:optimalcomplexity1}
  f(x_k) - f^* \leq \lambda_k \left[f(x_0) - f^* + \frac{\gamma_0}{2} \|x_0-x^*\|^2\right].
\end{equation}
Using \eqref{eq:convlip11}, this gives
\begin{equation}
  \label{eq:optimalcomplexity2}
  f(x_k) - f^* \leq \lambda_k \left[\langle f'(x^*), x_0-x^* \rangle + \frac{\gamma_0+L}{2} \|x_0-x^*\|^2\right].
\end{equation}
However, since $f'(x^*) = 0$ (this won't always be true in the constrained case),
\begin{equation}
  \label{eq:optimalcomplexity3}
  f(x_k) - f^* \leq \lambda_k \frac{\gamma_0+L}{2} \|x_0-x^*\|^2.
\end{equation}

and if we choose $\gamma_0 \geq \mu$ and $w = 1/L$ then
\begin{equation}
  \label{eq:optimalcomplexitylambda}
  \lambda_k \leq \min\left\{\left(1-\sqrt{\frac{\mu}{L}}\right)^k, \frac{4L}{(2\sqrt{L}+k\sqrt{\gamma_0})^2}\right\}.
\end{equation}
As we have seen, $\gamma_0 = \mu$ gives an easy constant expression for $\alpha_k$ but $\gamma_0 = L$ is safer.
For $\gamma_0 = L$, we have
\begin{equation}
  \label{eq:optimalcomplexitygamma0eqL}
  f(x_k) - f^* \leq L \min\left\{\left(1-\sqrt{\frac{\mu}{L}}\right)^k, \frac{4}{(k+2)^2}\right\}\|x_0-x^*\|^2.
\end{equation}

To see the number of iterations that we need for a precision $\epsilon$, we use the fact that $1+x \leq \exp(x)$
which implies that $(1-x)^k \leq \exp(-kx)$ which we use for $x = 1/\sqrt{Q_f}$.
Hence we have
\[ f(x_k) - f^* \leq L \exp\Bigg(\frac{-k}{\sqrt{Q_f}}\Bigg) r_0^2 \]
so
\[ k = \bigg\lceil \sqrt{Q_f}\Big[\ln\frac{1}{\epsilon} + \ln L + 2 \ln r_0\Big] \bigg\rceil. \]
We see that we are optimal because the main term in this estimate is $\sqrt{Q_f} \ln\frac{1}{\epsilon}$ which is proportional to the main term $\frac{\sqrt{Q_f}}{4}\ln\frac{1}{\epsilon}$ of the lower complexity bound.
That proves 2 things. First that our method is optimal, and second that our lower bound is the best we can obtain.

Note that we only give the convergence of $f(x_k) - f^*$ because by strong convexity
\[ \frac{\mu}{2} \|x_k-x^*\|^2 \leq f(x_k)-f(x^*)-\langle f'(x^*),x_k-x^* \rangle = f(x_k)-f(x^*) \]
because $f'(x^*) = 0$.
This shows that the convergence of $f$ is harder to obtain than the convergence of $\|x_k-x^*\|$ for smooth strongly convex functions.

\subsection{Constrained optimization}
\label{sec:constrained}
We now look at the problem
\[ \min_{x \in Q}f(x), f \in \mathcal{F}^1(\Rn) \]
where $Q$ is convex (see \cite[p.~76--78]{nesterov2004introductory} for an introduction to convex sets).
As for Section~\ref{sec:nlp}, our condition $f'(x) = 0$ does not work in constrained optimization.
However it can be remplaced by the following theorem.
\begin{mytheo}
  \label{theo:constrainedoptcond}
  Let $f \in \mathcal{F}^1(\Rn)$ and $Q$ be a closed convex set.
  The point $x^*$ minimized $f$ on $Q$ if and only if
  \[ \langle f'(x^*), x - x^* \rangle \geq 0 \]
  for all $x \in Q$.
\end{mytheo}

We can also show that if $f$ is smooth strongly convex with $\mu > 0$ then the solution \emph{exists} and is \emph{unique}~\cite[Theorem~2.2.6]{nesterov2004introductory}.

\subsubsection{Gradient Mapping}
Remember our interpretetion of the gradient method at Section~\ref{sec:varmet}.
The gradient method with step $h$ can be seen as an exact minimization of the quadratic approximation at point $\bar{x}$
\[ f(\bar{x}) + \langle f'(\bar{x}), x - \bar{x} \rangle + \frac{1}{2} \langle I/h (x - \bar{x}), x - \bar{x} \rangle. \]
With $\gamma = 1/h$, it gives
\[ f(\bar{x}) + \langle f'(\bar{x}), x - \bar{x} \rangle + \frac{\gamma}{2} \|x - \bar{x}\|. \]

As we have seen in Section~\ref{sec:varmet}, with unconstrained minimization, the minimum is given by $\bar{x} - \frac{1}{L}f'(\bar{x})$.
With constraint minimization however, it is not as simple but if $\gamma > 0$, the quadratic approximation is strongly convex so
as we have seen in Section~\ref{sec:constrained} we know there is a unique
\[ x_Q(\bar{x};\gamma) = \argmin_{x \in Q} f(\bar{x}) + \langle f'(\bar{x}), x - \bar{x} \rangle + \frac{\gamma}{2} \|x - \bar{x}\|. \]
We know that the objective is quadratic and that if $Q = \Rn$ the minimum is $(\bar{x} - \frac{1}{\gamma}f'(\bar{x}))$
so without surprise we can see that the objective can be rewritten as
\[ f(\bar{x}) - \frac{1}{2\gamma}\|f'(\bar{x})\|^2 + \frac{\gamma}{2} \Big\| x - \Big(\bar{x} - \frac{1}{\gamma}f'(\bar{x})\Big) \Big\|^2 \]
so in practice we compute $x_Q(\bar{x};\gamma)$ as
\[ x_Q(\bar{x};\gamma) = \argmin_{x \in Q} \Big\| x - \Big(\bar{x} - \frac{1}{\gamma}f'(\bar{x})\Big) \Big\|. \]
In other words, $x_Q$ is the projection of $\bar{x} - \frac{1}{\gamma}f'(\bar{x})$ onto the set $Q$
\[ x_Q(\bar{x};\gamma) = \pi_Q\Big(\bar{x} - \frac{1}{\gamma}f'(\bar{x})\Big) \]
where the projection $\pi_Q$ is defined as
\[ \pi_Q = \argmin_{x \in Q} \|x-x_0\|. \]

We want to see $x_Q(\bar{x},\gamma)$ as being obtained by a gradient step with a generalized gradient that we call \emph{gradient mapping} and a step-size $\gamma$.
So we define the gradient mapping of $f$ on $Q$
\[ g_Q(\bar{x};\gamma) = \gamma(\bar{x} - x_Q(\bar{x},\gamma)). \]

One can prove \cite[Theorem~2.2.7]{nesterov2004introductory}.
\begin{mytheo}
  \label{eq:constrainedlowerapprox}
  Let $f \in \mathcal{S}_{\mu,L}^{1,1}(\Rn)$, $\gamma \geq L$ and $\bar{x} \in \Rn$.
  Then for any $x \in Q$ we have
  \[ f(x) \geq f(x_Q(\bar{x};\gamma)) + \langle g_Q(\bar{x};\gamma), x - \bar{x} \rangle + \frac{1}{2\gamma}\|g_Q(\bar{x};\gamma)\|^2 + \frac{\mu}{2}\|x - \bar{x}\|^2. \]
\end{mytheo}
That gives us
\begin{align}
  \label{eq:constrainedgradientguarantee}
  f\Big(\bar{x} - \frac{1}{\gamma}g_Q(\bar{x};\gamma)\Big) & \leq f(\bar{x}) - \frac{1}{2\gamma} \|g_Q(\bar{x};\gamma)\|^2,\\
  \notag
  \langle g_Q(\bar{x};\gamma), \bar{x} - x^* \rangle & \geq \frac{1}{2\gamma} \|g_Q(\bar{x};\gamma)\|^2 + \frac{\mu}{2}\|x-x^*\|^2,
\end{align}
which need to be compared to the guarantees we have for the unconstrained gradient step \eqref{eq:gradientguarantee} and first order Lipschitz condition for smooth convex functions \eqref{eq:convlip13} used with $y = x^*$ and $f'(y) = 0$
\begin{align*}
  f\Big(\bar{x} - \frac{1}{L}f'(\bar{x})\Big) & \leq f(\bar{x}) - \frac{1}{2L} \|f'(\bar{x})\|^2,\\
  \langle f'(\bar{x}), \bar{x} - x^* \rangle  & \geq \frac{1}{L}\|f'(\bar{x})\|^2.
\end{align*}

\subsection{Constrained gradient method}
The constrained gradient step is
\[ x_{k+1} = x_k - h g_Q(x_k;\gamma). \]
Since $x_k \in q$ and $x_{k+1} \in Q$ for $h = 1/\gamma$ by the definition of $g_Q(x_k;\gamma)$,
by the convexity of $Q$ we know that $x_{k+1} \in Q$ for all $0 \leq h \leq 1/\gamma$.

If we choose $\gamma = L$ and $h = 1/\gamma = 1/L$, we can ensure that
\[ \|x_k - x^*\|^2 \leq \left(1 - \frac{1}{Q_f}\right)^k \|x_0-x^*\|^2 \]
For unconstrained gradient method we used \eqref{eq:convlip11}
\[ f(x_k) - f(x^*) - \langle f'(x^*), x_k-x^* \rangle \leq \frac{L}{2}\|x^*-x_k\|^2. \]
and the fact that $f'(x^*) = 0$ to deduce the rate of convergence of $f(x_k) - f'(x^*)$.
However, here the only thing we can say is (see Theorem~\ref{theo:constrainedoptcond}) that  $\langle f'(x^*), x_k-x^* \rangle \geq 0$.

We can use two approach for this
\begin{itemize}
  \item
    However, using Cauchy-Schwarz, we can say that
    \[ f(x_k) - f(x^*) \leq \bigg(\|f'(x^*)\|^2 + \frac{L}{2}\bigg) \|x^*-x_k\|^2 \]
    so
    \[ f(x_k) - f(x^*) \leq \bigg(\|f'(x^*)\|^2 + \frac{L}{2}\bigg) \left(1 - \frac{1}{Q_f}\right)^k \|x_0-x^*\|^2. \]

    What we can say is that if $x^*$ is not at the border of the domain $Q$, which means that it does not activate any constraint,
    $f'(x^*) = 0$ so the term $\|f'(x^*)\|$ can be removed but we still do not have the same rate than in the unconstrained case
    because the exponent is $k$ and not $2k$.
    Intuitively, the more constraints are activated, the more $\|f'(x^*)\|$ will grow and the slower the gradient method will be for $f(x_k) - f(x^*)$.

    If we use the fact that $1-x \leq \exp(-x)$, we can see that
    \[ f(x_k) - f(x^*) \leq \bigg(\|f'(x^*)\|^2 + \frac{L}{2}\bigg)\exp\bigg(\frac{-k}{Q_f}\bigg) \|x_0-x^*\|^2 \]
    so we can ensure that $f(x_k) - f(x^*) \leq \epsilon$, with
    \[ k = \Bigg\lceil Q_f \bigg[\ln\frac{1}{\varepsilon} + \ln\bigg(\|f'(x^*)\|^2 + \frac{L}{2}\bigg) + 2\ln\|x_0-x^*\| \bigg] \Bigg\rceil \]
    iterations.
  \item
    We cannot guarantee that $f(x_k) - f(x^*) \leq  L/2 \|x_k - x^*\|^2$ since $f'(x^*) \neq  0$.
    However, we can guarantee it for $x_{k+1} = \arg\min\limits_{x \in Q} [ f(x_k)+ \la f'(x_k), x - x_k \ra + L/2 \| x - x_k \|^2]$:
    indeed,
    \begin{align*}
      f(x_{k+1}) & = \min\limits_{x \in Q} [ f(x_k) + \la f'(x_k), x - x_k \ra + \frac{L}{2} \| x - x_k \|^2]\\
                 & \leq f(x_k) + \la f'(x_k), x^* - x_k \ra +  \frac{L}{2} \|x^* - x_k\|^2\\
                 & \leq f(x^*) +  \frac{L}{2} \|x^* - x_k\|^2.
    \end{align*}
    For the last inequality, we used the convexity of $f$. %\eqref{eq:smoothconv1}.
    We can see that picking $x_{k+1} = \arg\min\limits_{x \in Q} [ f(x_k)+ \la f'(x_k), x - x_k \ra + L/2 \| x - x_k \|^2]$
    is exactly the same as doing a new gradient iteration $x_{k+1} = x_Q(x_k;L)$.
    Hence we have
    \[ f(x_k) - f(x^*) \leq \frac{L}{2}\left(1 - \frac{1}{Q_f}\right)^{k-1} \|x_0-x^*\|^2. \]

    Since it is just a gradient iteration, we can interpret it as if we do it at the first iteration.
    In some sense, we eliminate the ``constant part'' of the gradient at the first iteration.
\end{itemize}

\subsection{Constrained optimal methods}
Remember that the key part is to maintain a sequence $\{x_k\}$ such that $f(x_k) \leq \phi_k^*$
and that it is $x_k$ that tends to our target $x^*$.
Therefore of course, we must have $x_k \in Q$.
We won't however constraint $y_k$ to be feasible.

We have seen when comparing \eqref{eq:constrainedgradientguarantee} which \eqref{eq:gradientguarantee}
that the gradient mapping has the same decrease guarantee than the gradient step.
However, since \eqref{eq:constrainedgradientguarantee} gives this guarantee in term of the gradient mapping instead of the gradient,
we should change the lower approximation at $y_k$ we use to build $\phi_{k+1}$ from $\phi_k$.

In the unconstrained case, we used the fact that
\[ f(y_k) + \langle f'(y_k), x - y_k \rangle + \frac{\mu}{2} \|x-y_k\|^2 \]
is below $f(x)$.
Here we will not take this lower approximation but the one given in \label{eq:constrainedlowerapprox}.

However, it only changes $v_k$ and $\phi_k^*$.
Since we do not use them Algorithm~\ref{algo:optimalmethod}, the only thing we have to change is the computation of $x_{k+1}$,
we use $g_Q(y_k;L)$ instead of $f'(y_k)$.
We set
\[ x_{k+1} = x_Q(y_k;L) = y_k - \frac{1}{L}g_Q(y_k;L). \]
Note that since we do \emph{not} know if $y_k \in Q$, we \emph{cannot} say that $y_k - hg_Q(y_k;L)$ is feasible for all $0 \leq h \leq 1/L$
as we did with the gradient method.
We have to do a full step $h = 1/L$.
Another change we have to do in Algorithm~\ref{algo:optimalmethod} is to replace $x_0 \in \Rn$ by $x_0 \in Q$.
If this is not possible, just add $x'_0 = x_Q(x_0;\gamma)$ as a first step of the algorithm and use $x'_0$ instead of $x_0$.

For the complexity results, the state is somewhat better that for the gradient method because the bound
\eqref{eq:optimalcomplexity1} is for $f(x_k) - f(x^*)$ directly and not for $\|x_k - x^*\|$.
However, we cannot get rid of $f(x_0) - f(x^*)$ easily.
The distance $\|x_0 - x^*\|$ is no the only metric we need to estimate $k$,
we can see in \eqref{eq:optimalcomplexity2} that the bigger $f'(x^*)$ is the bigger our error in the estimate will be if we only take $\|x_0-x^*\|$ into account.

As with the gradient method
What we can also say is that if $x^*$ is not at the border of the domain $Q$, which means that it does not activate any constraint,
$f'(x^*) = 0$ so we have \eqref{eq:optimalcomplexity3}.
Again, intuitively, the more constraints are activated, the more $f'(x^*)$ will grow.
Equation~\eqref{eq:optimalcomplexitylambda} is still valid but \eqref{eq:optimalcomplexitygamma0eqL} is only valid for $\gamma_0 = L$ as before and $f'(x^*) = 0$ \eqref{eq:optimalcomplexity3}.

Using Cauchy-Schwarz, \eqref{eq:optimalcomplexity2} gives
\begin{equation*}
  f(x_k) - f^* \leq \lambda_k \Big(\|f'(x^*)\|^2 + \frac{\gamma_0+L}{2}\Big) \|x_0-x^*\|^2.
\end{equation*}
So we have
\[ k = \bigg\lceil \sqrt{Q_f}\Big[\ln\frac{1}{\epsilon} + \ln\Big(\|f'(x^*)\|^2 + \frac{\gamma_0+L}{2}\Big) + 2\ln (r_0)\Big] \bigg\rceil. \]
even if using \eqref{eq:optimalcomplexity1} directly,
since we use neither \eqref{eq:convlip11} nor Cauchy-Schwarz, should gives a sharper bound
\[ k = \bigg\lceil \sqrt{Q_f}\Big[\ln\frac{1}{\epsilon} + \ln\Big(f(x_0) - f(x^*) + \frac{\gamma_0}{2}r_0^2\Big)\Big] \bigg\rceil. \]

Note that again we only give the convergence of $f(x_k) - f^*$ because by strong convexity
\[ \frac{\mu}{2} \|x_k-x^*\|^2 \leq f(x_k)-f(x^*)-\langle f'(x^*),x_k-x^* \rangle \leq f(x_k)-f(x^*) \]
because $\langle f'(x^*),x_k-x^* \rangle$ is positive since $x_k$ is feasible for all $k \geq 0$.
The convergence of $f$ is harder to obtain than the convergence of $\|x_k-x^*\|$ for smooth strongly convex functions even in the constrained case.

\section{Nonsmooth Convex Programming}
\subsection{Properties of General Convex Functions}
In \cite[Section~2.3]{nesterov2004introductory}, objective of the form
\[ f(x) = \max_{1 \leq j \leq p} \phi_j(x) \]
where $\phi_j \in \mathcal{F}^1$ are considered.
It is treaten using gradient mapping however the gradient mapping becomes very expensive when $p$ is very large.
We could also treat $f$ as a general nonsmooth function.

In many applications the objective of the functional constraints are given implicitly as a solution of an auxiliary problem.
Such functions are called functions with \emph{implicit} structure and very often they appear to be nonsmooth.

We define the domain, the epigraph and the sublevel sets of a function $f$ respectively as
\begin{align*}
  \dom f               & = \{\, x \in \Rn \mid |f(x)| < \infty \,\},\\
  \epi(f)              & = \{\, (x,t) \in \dom f \times \R \mid f(x) \leq t \,\},\\
  \mathcal{L}_f(\beta) & = \{\, x \in \dom f \mid f(x) \leq \beta \,\}.
\end{align*}

\begin{mytheo}
  \label{theo:f1}
  Let $f : \Rn \to \R$ be a function with a convex domain $\dom f$.
  Then all the following conditions are equivalents:
  \begin{itemize}
    \item $f$ is \emph{convex}.
    \item for all $x,y \in \dom f$ and $\alpha \in [0,1]$,
      \begin{equation}
        \label{eq:nonsmoothconv2}
        f(\alpha x + (1-\alpha) y) \leq \alpha f(x) + (1-\alpha)f(y).
      \end{equation}
    \item for all $x,y \in \dom f$ and $\beta \geq 0 $,
      \begin{equation*}
        f(y + \beta(y-x)) \geq f(y) + \beta(f(y) - f(x)).
      \end{equation*}
    \item $\epi(f)$ is a convex set.
  \end{itemize}
\end{mytheo}
We can see comparing \eqref{eq:smoothconv2} and \eqref{eq:nonsmoothconv2} that this condition does not need any modification to be applied to the nonsmooth case.

\begin{mylem}[Jensen inequality]
  Let $f$ be a convex function,
  $x_1, \ldots, x_m \in \dom f$ and
  $\alpha_1, \ldots, \alpha_m \geq 0$ such that $\sum_{i=1}^m \alpha_i = 0$,
  we have
  \[
    f\Big( \sum_{i=1}^m \alpha_i x_i \Big) \leq \sum_{i=1}^m \alpha_i f(x_i).
  \]
\end{mylem}

The behavior of convex functions at the points of the boundary of its domain can be rather disappointing (see \cite[Example~3.1.1(6),~3.1.2(5)]{nesterov2004introductory}).
Fortunately, this is the only bad news about convex functions, we will see that the structure of the convex functions in the interior of its domain is very simple.

A convex function $f$ is called \emph{closed} if its epigraph $\epi(f)$ is a closed set.
If $f$ is a closed convex function then $\mathcal{L}_f(\beta)$ is closed or empty for all $\beta$.
If $f$ is convex, continuous and $\dom f$ is closed then $f$ is closed but

We also see that if $f$ is convex and $x_0 \in \inte(\dom f)$.
Then $f$ is locally upper bounded at $x_0$ \cite[Lemma~3.1.2]{nesterov2004introductory} which means that $f$ is locally Lischitz continuous at the interior of its domain%
\footnote{Remember that for a function with a convex domain, locally Lipschitz implies continuity}.
However, even if $f$ is also closed and convex we do not have the continuity of at the boundary.

Let $x \in \dom f$.
We call $f$ \emph{differentiable in the direction $p$} at the point $x$ if the limit
\[ f'(x;p) = \lim_{\alpha \downarrow 0} \frac{1}{\alpha} [f(x + \alpha p) - f(x)] \]
exists.

Again at the interior, all is well.
A convex fonction is differentiable in any direction $p$ at any point of the interior of its domain.
Since we do not have the continuity at the boundary we won't ask for the differentiability in any direction.

Remember that for a differentiable function, we have $f'(x;p) = \langle f'(x), p \rangle$.
This gives a hint for a possible generalization of \eqref{eq:smoothconv1}.
If $f$ is a convex function and $x \in \inte (\dom f)$ then $f'(x;p)$ is a convex homogeneous (of degree 1) function of $p$.
and for any $y \in \dom f$ we have:
\begin{align*}
  f(y) & \geq f(x) + f'(x;y-x).
\end{align*}

We have defined directional derivative which is a scalar derivative.
In the case of differentiability, this scalar is given by the gradient and the derivative.
What replaces the gradient for nonsmooth convex functions ?

\begin{mydef}
  Let $f$ be a convex function.
  A vector $g$ is called the \emph{subgradient} of $f$ at $x_0 \in \dom f$ if for any $x \in \dom f$ we have
  \begin{align*}
    f(x) \geq f(x_0) + \langle g, x - x_0 \rangle.
  \end{align*}
\end{mydef}
The set of all subgradients of $f$ at $x_0$, $\partial f(x_0)$, is called the \emph{subdifferential}
of function $f$ at the point $x_0$.
This looks like a better candidate for the generalization of \eqref{eq:smoothconv1}.

For $|x|$, we see that $\partial f(x) = \{f'(x)\}$ for $x \neq 0$ and $\partial f(0) = [-1,1]$.
In fact, if $f$ is differentiable on $\dom f$, then $\partial f(x) = \{f'(x)\}$ for all $x \in \inte(\dom f)$.

Note that we \emph{almost} have the beautiful generalization of \eqref{eq:smoothconv1}:
A function $f$ is convex if and only if for all $y$ there exists $g$ such that for all $y$
\[ f(y) \geq f(x) + \langle g, y-x \rangle. \]
In other words, $\partial f(x)$ is nonempty.

However, even if $\Leftarrow$ works, $\Rightarrow$ doesn't work.
If $f$ is closed convex, we can say that $\partial f(x)$ is nonempty and bounded for any $x \in \inte(\dom f)$
but we cannot say that $\partial f(x) \neq \emptyset$ for $x$ at the boundary.
Consider for example $f(x) = -\sqrt{x}$, $\partial f(0) = \emptyset$ while $f$ is closed convex.
Indeed, at 0 we would need $g = (1,-\infty)$ which is not valid.

If $\partial f(x) \neq \emptyset$,
we can generalize the formula $f'(x;p) = \langle f'(x), p \rangle$.
If the convex function $f$ is closed,
for a point $x \in \inte(\dom f)$
we have
\[ f'(x;p) = \max_{g \in \partial f(x)} \langle g, p \rangle. \]
Indeed if $f$ is differentiable on $\dom f$, $\partial f(x) = \{f'(x)\}$ for all $x \in \inte(\dom f)$ so it is a generalization.

\subsection{Separation theorems}
\begin{mydef}
  Let $Q$ be a convex set.
  We say that the hyperplane
  \[ \mathcal{H}(g,\gamma) = \{, x \in \Rn \mid \langle g, x \rangle = \gamma \}, \quad g \neq 0 \]
  is \emph{supporting} $Q$ if any $x \in Q$ satisfies the inequality $\langle g, x \rangle \leq \gamma$.

  We say that the hyperplane $\mathcal{H}(g,\gamma)$ separates a point $x_0$ from $Q$ if
  \[ \langle g, x \rangle \leq \gamma \leq \langle g, x_0 \rangle \]
  for all $x \in Q$.
  If the second inequality is strict, we call the separation \emph{strict}.
\end{mydef}

We define the projection
\[ \pi_Q(x_0) = \argmin_{x \in Q} \|x-x_0\| \]
which exists and is unique if $Q$ is a closed convex set.

\begin{multicols}{2}
  If $Q$ is a closed convex set, we have
  \begin{align*}
    \langle x_Q(x_0) - x_0, x - x_Q(x_0) \rangle & \geq 0
  \end{align*}
  so
  \begin{align*}
    \|x - x_0\|^2 & = \|x_Q(x_0) - x_0\| + \|x - x_Q(x_0)\|\\
                  & \quad {} + 2\langle x_Q(x_0) - x_0, x - x_Q(x_0) \rangle\\
                  & \geq \|x_Q(x_0) - x_0\| + \|x - x_Q(x_0)\|.
  \end{align*}

  \begin{tikzpicture}[x=.5cm,y=.5cm]
    \begin{scope}
      \path[clip] (0,0) ellipse (4 and 4);
      \draw[thick] (-2,0) ellipse (4 and 3.5);
    \end{scope}
    \node[left] at  (0,-2) {$x$};
    \node[left] at  (2, 0) {$x_Q(x_0)$};
    \node[right] at (4, 0) {$x_0$};
    \draw[thick,->] (2,0) to (0,-2);
    \draw[thick,->] (4,0) to (2, 0);
    \draw[thick,->] (4,0) to (0,-2);
  \end{tikzpicture}
\end{multicols}

\begin{multicols}{2}
  If $Q$ is a closed convex set and $x_0 \notin Q$,
  then there exists an hyperplane $\mathcal{H}(g,\gamma)$
  separating $x_0$ from $Q$.
  Namely we can take
  \begin{align*}
    g      & = x_0 - \pi_Q(x_0),\\
    \gamma & = \langle g, \pi_Q(x_0) \rangle.
  \end{align*}

  \begin{tikzpicture}[x=.5cm,y=.35cm]
    \begin{scope}
      \path[clip] (0,0) ellipse (4 and 4);
      \draw[thick] (-2,0) ellipse (4 and 3.5);
    \end{scope}
    \node[left] at  (2, 0) {$x_Q(x_0)$};
    \node[right] at (4, 0) {$x_0$};
    \draw[thick,->] (2,0) to (4, 0);
    \node[above] at (3, 0) {$g$};
    \draw[red, thick] (2,4) to (2,-4);
  \end{tikzpicture}
\end{multicols}

If we make $x_0$ tend to $\pi_Q(x_0)$, we get the following theorem:

Let $Q$ be a closed convex set and $x_0 \in \partial Q$.
Then there exists an hyperplane $\mathcal{H}(g,\gamma)$,
supporting to $Q$ and passing through $x_0$.

\subsection{Optimization for nonsmooth convex functions}
Let's analyse the consequences of the definition of the subgradient
\[ f(\bar{x}) \geq f(x) + \langle g, \bar{x} - x \rangle. \]
\begin{multicols}{2}
  Let's say that $g \neq 0$.
  In the direction $g$, i.e. for $\bar{x}$ such that $\langle g, \bar{x} - x \rangle > 0$,
  we know that $f(\bar{x}) > f(x)$.
  For the hyperplane defined by $\langle g, y - x \rangle > 0$, we can just say that $f(y) \geq f(x)$,
  and for $\langle g, \bar{x} - x \rangle < 0$ we do not know.

  However, we can conlude that all the points $\bar{x}$ such that $f(\bar{x}) \leq f(x)$ are such that
  $\langle g, \bar{x} - x \rangle < 0$.
  This means that all subgradient $g \in \partial f(x)$ supported for the sublevel set $\mathcal{L}_f(f(x))$.
  Practically, this mean that $x^*$ is in the ``side $-g$'' of the hyperplane.

  \begin{tikzpicture}[x=.5cm,y=.5cm]
    \draw[blue, thick, ->] (0,0) to (.8,1.2);
    \draw[red, thick] (-6,4) to (6,-4);
    \node[below] at (0, 0) {$x$};
    \filldraw (0,0) circle (.05);
    \node[below] at (-4.5, 3) {$y$};
    \filldraw (-4.5,3) circle (.05);
    \node[right] at (-4.5, 3.3) {$f(y) \geq f(x)$};
    \node[below] at (2, 2) {$\bar{x}$};
    \filldraw (2,2) circle (.05);
    \node[right] at (2, 2.3) {$f(\bar{x}) > f(x)$};
  \end{tikzpicture}
\end{multicols}

\begin{multicols}{2}
  From that we can see that $x^*$ is a minimum of $f$ if and only if
  $0 \in \partial f(x)$.

  \begin{tikzpicture}[x=3cm,y=0.6cm]
    \draw[thick, domain=-.5:.5] plot
    (\x, {(\x)^2-2*(\x)+3/4});
    \draw[blue, thick, domain=-.5:.5] plot
    (\x, {-(\x)+1/2});
    \draw[thick, domain=.5:1.1] plot
    (\x, {(\x)^2+2*(\x)-5/4});
    \draw[blue, thick, domain=.5:1.2] plot
    (\x, {3*(\x)-6/4});
    \draw[red, thick, domain=-.5:1.2] plot
    (\x, {0});
  \end{tikzpicture}
\end{multicols}
This result gives a very nice proof for the KKT conditions (see \cite[Theorem~3.1.17]{nesterov2004introductory}).

In this section we rewrite $\langle g(x), \bar{x} - x \rangle$ as $-\langle g(x), x-\bar{x} \rangle$.
We have seen that $\langle g(x), x - x^* \rangle > 0$ if $x$ does not minimize $f$.
This is simple but it has 2 consequences:
\begin{itemize}
  \item The direction $-g(x)$ decreases the distance between $x$ and $x^*$.
  \item The hyperplane defined by $\{\, y \in \Rn \mid \langle g(x), x - y \rangle = 0 \,\}$ cuts $\Rn$ on two half spaces.
    Only one of them constains $x^*$.
\end{itemize}

In order to develop nonsmooth minimization we have to forget about relaxation and approximation.
We use another concept called \emph{localization}.
We will now derive some technique to allow us to estimate a quality of a current point as an approximate solution.

\begin{multicols}{2}
  We define $v_f(\bar{x}, x)$ which represents the ``signed'' distance from $\bar{x}$ to the hyperplane orthogonal to $g(x)$.
  \[ v_f(\bar{x},x) = \frac{1}{\|g(x)\|}\langle g(x), x - \bar{x} \rangle. \]
  It is negative in the ``$+g$ side'' of the hyperplane, the uninteresting side,
  and positive on the ``$-g$ side'' where $x^*$ is.

  \begin{tikzpicture}[x=.5cm,y=.5cm]
    \draw[blue, thick, ->] (3,-2) to (3.8,-0.8);
    \draw[red, thick] (-6,4) to (6,-4);
    \node[below] at (3, -2) {$x$};
    \filldraw (3,-2) circle (.05);
    \node[below] at (0, 0) {$y$};
    \filldraw (0,0) circle (.05);
    \node[right] at (0, 0.3) {$v_f(y,x) = 0$};
    \node[below] at (1, 3) {$\bar{x}$};
    \filldraw (1,3) circle (.05);
    \node[right] at (1, 3.3) {$v_f(\bar{x},x) < 0$};
    \node[below] at (-2, -3) {$\bar{x}$};
    \filldraw (-2,-3) circle (.05);
    \draw (-2,-3) -- (0,0);
    \node[left] at (-1, -1.5) {$v_f(\bar{x},x)$};
  \end{tikzpicture}
\end{multicols}

We now introduce a measure of the variance of $f$ with respect to $x$
\[ \omega_f(\bar{x},t)= \max_{\|x-\bar{x}\| \leq t} f(x) - f(\bar{x}). \]
If $t < 0$ we set $\omega_f(\bar{x},t) = 0$.

We can see that $w_f(\bar{x},t) = 0$ for $t \leq 0$ and is non-decreasing in $t$.
If $f$ is zero-order Lipschitz with a constant $M$, we also see that
\[ \omega_f(\bar{x},t) \leq M t_+ \]
where $t_+ = t$ if $t \geq 0$ and $0$ otherwise.

Let $\bar{x}$ be a point on the ``$-g$ side'' of the hyperplane
and $y$ be its projection on the hyperplane.
\begin{multicols}{2}
  Since $f(y) \geq f(x)$, we see that
  \begin{align*}
    f(x) - f(\bar{x})
    & = f(y) - f(\bar{x})\\
    & \leq w_f(\bar{x}, v_f(\bar{x};x))\\
    & \leq M v_f(\bar{x};x)\\
  \end{align*}
  For $\bar{x}$ on the other side, $\bar{x}$, $f(x) - \bar{x} \leq 0$
  so for a general $\bar{x}$,
  \[ f(x) - f(\bar{x}) \leq M (v_f(\bar{x};x))_+. \]

  \begin{tikzpicture}[x=.5cm,y=.5cm]
    \draw[blue, thick, ->] (3,-2) to (3.8,-0.8);
    \draw[red, thick] (-6,4) to (6,-4);
    \node[below] at (3, -2) {$x$};
    \filldraw (3,-2) circle (.05);
    \node[above] at (0, 0) {$y$};
    \filldraw (0,0) circle (.05);
    \node[below] at (-2, -3) {$\bar{x}$};
    \filldraw (-2,-3) circle (.05);
    \draw (-2,-3) -- (0,0);
    \draw (-2,-3) circle (3.6);
  \end{tikzpicture}
\end{multicols}

We can now formalize our idea of localization set.
Let $\{x_i\}_{i=0}^\infty$ be a sequence in $Q$.
Define
\[ S_k = \{\, x \in Q \mid \langle g(x_i), x_i - x \rangle \geq 0, i = 0, \ldots, k \,\}. \]
We call this set the \emph{localization set} generated by the sequence $\{x_i\}_{i=0}^\infty$.

Let
\begin{align*}
  v_k^* & = \max\{\, r | B_2(x^*, r) \subseteq S_k \,\},\\
  f_k^* & = \min_{0 \leq i \leq k} f(x_i).
\end{align*}
Then
\[ f_k^* - f^* \leq \omega_f(x^*, v_k^*) \leq M v_k^*. \]

\subsection{Infinite dimensional approach}
\subsubsection{Unconstrained Lower complexity bound}
Let us consider the class of function $f$, such that $x^*$ exists, $r_0 \leq R$ for some $R$ and
$f$ is Lipschitz continuous on $B_2(x_0,R)$, with the constant $M$, i.e. $f_{B_2(x_0,R)} \in \mathcal{F}_M^{0,0}$.

For any $k$, $0 \leq k \leq n-1$, there exists a function of this class such that
\begin{equation}
  \label{eq:unconstrainednonsmoothbound}
  f(x_k) - f^* \geq \frac{MR}{2(1+\sqrt{k+1})}.
\end{equation}
for a method generating a sequence $\{x_k\}$ such that
\[ x_k \in x_0 + \Lin\{g(x_0), \ldots, g(x_{k-1})\}. \]
As for the bound of smooth convex function, this lower bound is uniform.
However, as expected, it is much slower.
In Section~\cite[Section~2.3]{nesterov2004introductory},
methods are given for nonsmooth problems that are faster than this bound.
This is because they are allowed to use the specific structure of the objective.

\subsection{Subgradient methods}
The subgradient methods is simply the gradient method with the normalized subgradient instead of the gradient.
Indeed for non-smooth functions, the norm of the subgradient $\|g(x)\|$ is not very informative
so we use $g(x)/\|g(x)\|$ instead of $g(x)$.

Let us first analyse the subgradient method for the problem
\[ \min_{x \in Q} f(x) \]
where $Q$ is a simple closed convex set (simple means that we can compute the projection in $Q$ easily).

\begin{algorithm}
  \caption{Gradient method for a nonsmooth convex function $f$ and simple constraints represented by a simple closed convex set $Q$.}
  \label{algo:gradientmethodnonsmooth}
  \begin{algorithmic}
    \STATE Choose $x_0 \in \Rn$ and a sequence $\{h_k\}_{k=0}^\infty$ and $h_k \to 0$.
    \FOR{$k = 0, 1, 2, 3, \ldots$}
      \STATE Compute $f(y_k)$ and $g(y_k)$.
        Set
        \[ x_{k+1} = \pi_Q\Big(x_k - h_k\frac{g(x_k)}{\|g(x_k)\|}\Big). \]
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

If $r_0 \leq R$ and $f_{B_2(x^*,R)} \in \mathcal{F}_M^{0,0}$, then
\[ f_k^* - f^* \leq M\frac{R^2 + \sum_{i=0}^k h_i^2}{2 \sum_{i=0}^k h_i}. \]

If we know a priori that we want to do a fixed number of steps $N$,
we know that we need to take a constant step-size
\[ h_i = \frac{R}{\sqrt{N+1}}, \quad i = 0, \ldots, N \]
to minimize the bound.
This gives
\[ f_N - f^* \leq \frac{MR}{\sqrt{N+1}}. \]
Comparing this with the lower bound~\eqref{eq:unconstrainednonsmoothbound},
we see that the subgradient is optimal uniformly in the dimension $n$.

If we know the desired accuracy $\epsilon$, we can fix $N$.
However, we often converge a lot faster than be bound so it is inefficient in practice to do all the $N$ steps.
In practice we rather check for the accuracy $f_k - f^*$ using $f_k - f_{k+1} \approx f_k - f^*$ and stop
when it is smaller than $\epsilon$.
Also in practice, since we do not know $R$, we cannot use it in the expression for $h_k$.
If we use
\[ h_i = \frac{r}{\sqrt{i+1}}, \quad i = 0, \ldots \]
we have
\[ f_k^* - f^* \leq M\frac{R^2+r\ln(k+1)}{2r\sqrt{k+1}}. \]
This rate of convergence can be classified as \emph{sub-optimal} rate.

If the constrained are more complicated,
we split them in a simple bounded closed convex set $Q$ and $m$ more complicated constraints $f_j$
\[ \min_{x \in Q} \{\, f(x) \mid f_j(x) \leq 0, j = 1, \ldots, m  \,\}. \]
Let
\[ \bar{f}(x) = \big(\max_{1 \leq j \leq m} f_j(x) \big)_+. \]
We can see that at the solution $x^*$, we have $\bar{f}(x) = 0$. % FIXME why ?
% TODO give the intuition
\begin{algorithm}
  \caption{Gradient method for constrained optimization with a nonsmooth convex function $f$.}
  \label{algo:gradientmethodconstrainednonsmooth}
  \begin{algorithmic}
    \STATE Choose $x_0 \in \Rn$ and a sequence $\{h_k\}_{k=0}^\infty : h_k = \frac{R}{\sqrt{k+0.5}}$.
    \FOR{$k = 0, 1, 2, 3, \ldots$}
      \STATE Compute $f(x_k)$, $g(x_k)$, $\bar{f}(x_k)$ and $\bar{g}(x_k)$ and set
        \[
          p_k =
          \begin{cases}
                 g (x_k), & \text{if }\bar{f}(x_k) < \|\bar{g}(x_k)\|h_k,\\
            \bar{g}(x_k), & \text{if }\bar{f}(x_k) \geq \|\bar{g}(x_k)\|h_k.
          \end{cases}
        \]
        Set
        \[ x_{k+1} = \pi_Q\Big(x_k - h_k\frac{p_k}{\|p_k\|}\Big). \]
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

If $r_0 \leq R$ and $f_{B_2(x^*,R)} \in \mathcal{F}_{M_1}^{0,0}$ and
\[ M_2 = \max_{1 \leq j \leq m} \{\|g\|: g \in \partial f_j(x), x \in B_2(x^*,R)\}, \]
then for any $k \geq 3$ there exists a number $i'$, $0 \leq i' \leq k$ such that
\begin{align*}
  f(x_{i'}) - f^* \leq \frac{\sqrt{3}M_1R}{\sqrt{k-1.5}},\\
  \bar{f}(x_{i'}) \leq \frac{\sqrt{3}M_2R}{\sqrt{k-1.5}}.
\end{align*}
Note that here we do not use the notation $f_k^*$ instead of $f(x_{i'})$ to show that
the bound is for the same $i'$ for $f$ and $\bar{f}$.

We can see that in the constrained and unconstrained case we are optimal even if the lower complexity bound
was obtained in the unconstrained case.
This means that from the viewpoint of analytical complexity, the general unconstrained minimization problems are not easier than the constrained ones.

The simplest method we have tried is already optimal.
In general, that indicates that the problems of our class are too complicated to be solved efficiently.
However, our conclusion is valid uniformly in the dimension of the problem.

The analytical complexity of the gradient method does not depend on $n$ but the complexity of one iteration depends on $n$.
Obviously, the gradient method does not work on infinite dimension,
however it does not make use of the fact there is finite number of dimension.
If we take the dimension factor into account in a proper way, we get more efficient schemes.

\subsection{Finite dimensional approach}
Let's suppose that we want to solve the problem
\[ \min_{x \in Q} f(x) \]
where here we do not assume that $Q$ is simple.
We have $Q = \{\, x \in \Rn \mid f_j (x) \leq 0, j = 1, \ldots, m \,\} = \{\, x \in \Rn \mid \bar{f} (x) = 0 \,\}$
where $\bar{f}$ is the max of the $f_j$ as defined in the previous section.

The key idea is very simple.
Let's say we have a point $x \in \Rn$
\begin{itemize}
  \item If $x \notin Q$, $\bar{f}(x) > 0$.
    Let $\bar{g}(x)$ be a subgradient of $\bar{f}$ at $x$.
    We have seen that the part of $\Rn$ in the ``$+g$ side'' of the hyperplane orthogonal to $\bar{g}(x)$ and passing through $x$ only contains points for which
    $\bar{f}(\bar{x}) \geq \bar{f}(x)$.
    Therefore we know that $Q$ (and $x^* \in Q$) is in the ``$-g$ side''.
    In other words, $\mathcal{H}(-g, \la -g, x \ra)$ is supporting for $Q$.
  \item If $x \in Q$, let $g(x)$ be a subgradient of $f$ at $x$.
    If $g = 0$, that means that $x = x^*$.
    Otherwise, we have seen that $x^*$ is in the ``$-g$ side'' of the hyperplanes orthogonal to $g(x)$ and passing through $x$.
\end{itemize}
In both case, we can use the hyperplane $\mathcal{H}(-g, \la -g, x \ra)$ to split search space in 2.

\subsubsection{Lower complexity bound}
Let's consider the simpler problem of finding a point in $Q$ and a new oracle that for a test point $x \in \Rn$
\begin{itemize}
  \item either reports that $x \in Q$
  \item or returns a vector $\bar{g}$, separating $x$ from $Q$:
    \[ \la \bar{g}, x - \bar{x} \ra \quad \forall \bar{x} \in Q. \]
\end{itemize}

If there exists a point $x^* \in Q$ such that $B_2(x^*, \epsilon) \subseteq Q$ for some $\epsilon > 0$ (looks like the Slater condition),
and we start with the localization set $B_\infty(0,R)$ ($Q \subseteq B_\infty(0,R)$),
the lower analytical complexity bound for finding a point $x^* \in Q$ is $n \ln \frac{R}{2\epsilon}$ call to the oracle
and the lower analytical complexity bound for finding the point $x^*$ minimizing a function $f \in \mathcal{F}_M^{0,0}(B_\infty(0,R))$
in $Q$ is $n \ln \frac{MR}{8\epsilon}$ call of the oracle.

\subsubsection{Cutting Plane Schemes}
Again we assume that $\int Q \neq \emptyset$ and we denote its diameter by $D < \infty$.

We will works with two sequences of sets $\{E_k\}_{k=0}^\infty$ and $\{S_k\}_{k=0}^\infty$.
The set $E_k$ is the localization set obtained with the following algorithm
\begin{algorithm}
  \caption{Cutting Plane Scheme.}
  \label{algo:cuttingplane}
  \begin{algorithmic}
    \STATE Choose a bounded set $E_0 \supseteq Q$.
    \FOR{$k = 0, 1, 2, 3, \ldots$}
      \STATE Choose $y_k \in E_k$
      \IF{$y_k \in Q$}
      \STATE Compute $f(y_k)$ and $g(y_k)$.
      \STATE Set $g_k = g(y_k)$.
      \ELSE
      \STATE Compute $\bar{g}(y_k)$ which separates $y_k$ from $Q$.
      \STATE Set $g_k = \bar{g}(y_k)$.
      \ENDIF
      \STATE Choose $E_{k+1} \supseteq \{\, x \in E_k \mid \la g_k, y_k - x \ra \geq 0 \,\}$.
    \ENDFOR
  \end{algorithmic}
\end{algorithm}
We are never sure that $E_k \subseteq Q$ so we can always have $y_k \notin Q$.
Note that if $y_k \notin Q$ we do not compute $f(y_k)$ because it is not feasible.
Also, if $y_k \notin Q$, the hyperplane is supporting for $Q$ so it does not reduce the part of $Q$ in $E_k$.
Let $\{x_i\}$ be the sub-sequence of $\{y_k\}$ with the points in $Q$.
We can define the sequence $\{S_i\}$ with $S_0 = Q$ and
\[ S_{i+1} = \{\, x \in S_i \mid \la g(x_i), x_i - x \ra \geq 0 \,\}. \]
We can see that $S_{i(k)} = E_k \cap Q$ where $i(k)$ is the number of points in $Q$ in the sequence $\{y_j\}_{j=0}^k$.
The sequence $\{S_i\}$ is useful to analyse the rate of convergence of the sequence $\{f(x_i)\}$ to $f^*$.
Denote $v_i^* = v_f(x^*;x_i)$ and $v_i^* = \min_{0\leq j \leq i} v_j$.
We have, for any $k$ such that $i(k) > 0$,
\[ v_{i(k)}^* \leq D \Big[\frac{\vol_n S_{i(k)}}{\vol_n Q}\Big]^{\frac{1}{n}} \leq \Big[\frac{\vol_n E_k}{\vol_n Q}\Big]^{\frac{1}{n}}. \]

The problem of this result is the following:
How do we ensure that $i(k) > 0$ ? What prevents $i(k)$ from being zero ?
Actually, $i(k) = 0$ means we that we do not have cut in $Q$ yet so
$\vol_n E_k < \vol_n Q$ implies that $i(k) > 0$.
Therefore if we ensure that $\vol_n E_k \to 0$ we know that we will have an $y_k \in Q$.

Historically, the first nonsmooth minimization method implementing the idea of cutting planes was the \emph{center of gravity} method.
The idea is to take $y_k$ as the center of gravity of $E_k$.
It turns out that if we do so, whatever $g$ is, the volume of $E_k$ is multiplied by $1-1/e$ at each iteration and
if $f_{B_2(x^*,D)} \in \mathcal{F}_M^{0,0}$ then for any $k \geq 0$, % FIXME why not impose that i(k) > 0 ?
\[ f_k^* - f^* \leq MD\Big(1 - \frac{1}{e}\Big)^{\frac{-k}{n}}. \]
Remember that $f_k^*$ is computed only among the points $y_k \in Q$.
Comparing this with the lower complexity bound, we see that the center of gravity method is optimal in finite dimension.
However, we are comparing the analytical complexity.
Computing the center of gravity in multi-dimensional space is actually harder than the initial problem.

There are two common alternatives.
\paragraph{Ellipsoid Method}
The idea of the Ellipsoid method is to use Ellipsoidal $E_k$ because it is easy to find the center of gravity of Ellipsoids.
\begin{multicols}{2}
  It is also easy to find the smallest Ellipsoid (volume-wise) that contains the part of the previous Ellispoid at the ``$-g$ side''
  of the hyperplane.
  \begin{align*}
    E_k     & = \{\, x \in \Rn \mid \la H_k^{-1}(x-y_k), x-y_k \ra \leq 1 \,\},\\
    y_{k+1} & = y_k - \frac{1}{n+1} \cdot \frac{H_kg_k}{\la H_kg_k, g_k \ra^{1/2}},\\
    H_{k+1} & = \frac{n^2}{n^2-1} \Big(H_k - \frac{2}{n+1} \cdot \frac{H_kg_kg_k^TH_k}{\la H_kg_k, g_k \ra}\Big).
  \end{align*}

  \begin{tikzpicture}[x=1.5cm,y=1.5cm]
    \draw[thick]         ( 0.5963, 0.1491) ellipse (2 and 1);
    \draw[blue,thick,->] ( 0.5963, 0.1491) to (1.5963,1.1491);
    \draw[red,thick]     (-0.9037, 1.6491) to (2.0963,-1.3509);
    \draw[dashed,thick,rotate=-23.4238]
                         ( 0,      0) ellipse (1.6724 and 0.9206);
    \filldraw            ( 0.5963, 0.1491) circle (.02);
    \node[below] at      ( 0.5963, 0.1491) {$y_k$};
    \node[above] at      ( 2.2,    0.8) {$H_k$};
    \filldraw            ( 0,      0) circle (.02);
    \node[below] at      ( 0,      0) {$y_{k+1}$};
    \node[above] at      (-1.2,    1) {$H_{k+1}$};
  \end{tikzpicture}
\end{multicols}

If $f_{B_2(x^*,R)} \in \mathcal{F}_M^{0,0}$ then for any $k$ such that $i(k) > 0$, % FIXME why R not D ?
\[ f_{i(k)}^* - f^* \leq MD\Big(1 - \frac{1}{(n+1)^2}\Big)^{\frac{k}{2}} \Big[\frac{\vol_n B_0(x_0,R)}{\vol_n Q}\Big]^{\frac{1}{n}}. \]

Here to guarantee that $i(k) > 0$ for some $k$, we need an additional assumption.
We need to assume that there exists some $\rho > 0$ and $\bar{x} \in Q$ such that
\[ B_2(\bar{x},\rho) \subseteq Q. \]
With functionnal constraints, it is enough to respect the Slater condition.

With that further assumption, we have
\[ f_{i(k)}^* - f^* \leq \frac{1}{\rho} M R^2 \exp\Big(-\frac{k}{2(n+1)^2}\Big). \]

Each iteration only take $\bigoh(n^2)$ arithmetic operations and the method needs
\[ 2(n+1)^2 \ln \frac{MR^2}{\rho \epsilon} \]
calls of the oracle to generate an $\epsilon$ solution.
It is not optimal but it has polynomial dependence on $\ln 1/\epsilon$ and the class parameters $M,R,\rho$.
For problem classes for which the oracle has polynomial complexity,
such algorithms are called weakly polynomial.

\paragraph{Inscribed Ellipsoid Method}
The point $y_k$ is chosen as the center of the maximal inscribed ellipsoid $W_k \subseteq E_k$

\paragraph{Analytic Center Method}
The point $y_k$ is chosen as the minimum of the \emph{analytical barrier}
\[ F_k(x) = -\sum_{j=1}^{m_k} \ln(b_k-\la a_j,x \ra). \]

\paragraph{Volumetric Center Method}
The point $y_k$ is chosen as the minimum of the \emph{volumetric barrier}
\[ V_k(x) = \ln \det F_k''(x). \]
where $F_k$ is the analytical barrier defined above.

The three last methods are polynomial with the complexity
\[ n \Big(\ln \frac{1}{\epsilon}\Big)^p \]
where $p$ is eighter 1 or 2.
However, at each iteration we need to run an Interior Point Method to find the $y_k$ which takes $n^3$--$n^4$ arithmetic operations.

\section{Structural Programming}
%In Structural programming, we do not consider constraints as black blox.
%We use the specificity in their structure to remplace them by barriers in the objective.
%The remaining objective is solved using the Newton['s]? scheme.

\biblio
\end{document}
