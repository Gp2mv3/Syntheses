\documentclass[en]{../../../eplsummary}

\usepackage{multicol}
\usepackage{pgfplots}

\DeclareMathOperator{\Lin}{Lin}
\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\cl}{cl}

\usepackage{algorithm}
\usepackage{algorithmic}

\hypertitle{Nonlinear programming}{8}{INMA}{2460}
{BenoÃ®t Legat}
{Yurii Nesterov}

\begin{center}
  ``The main fact, which should be known to any person dealing with optimization models,
  is that, in general, \emph{the optimization problems are unsolvable}.''
  \hfill\cite[p.~5]{nesterov1998introductory}
\end{center}

\section{The World of Nonlinear Optimization}
\label{sec:nlp}
We are interested in the problem
\begin{align*}
  \min & f_0(x)\\
  f_j(x) & \leq 0 & j = 1, \ldots, m\\
  x & \in S.
\end{align*}
where $f_0$ is the \emph{objective} function,
the vector function $f(x) = (f_1(x), \ldots,f_m(x))$ is called the \emph{functional constraint},
the set $S$ is called the \emph{basic feasible set} and the set
\[ Q = \{\, x \in S \mid f_j(x) \leq 0, j = 1, \ldots, m \, \} \]
is called the \emph{feasible set} of the optimization problem.

$S$ stands for \emph{structural} constraints, like non-negativity or boundedness of some variables.
% TODO link that to chapter 4

\begin{itemize}
  \item It is called \emph{unconstrained} if $Q \equiv S \equiv \Rn$ and \emph{constrained} otherwise.
  \item It is called \emph{smooth} if all the $f_i$ are differentiable and \emph{nonsmooth} otherwise.
\end{itemize}

A \emph{method} $\mathcal{M}$ for a \emph{class} $\mathcal{F}$ is given as input a problem $\mathcal{P}$ of $\mathcal{F}$.
Since $\mathcal{M}$ deals with the whole class $\mathcal{F}$,
it does not have the complete description of the problem $\mathcal{P}$ but rather the description of the class $\mathcal{F}$.
In addition it has access to an oracle $\mathcal{O}$ to collect specific information about $\mathcal{P}$.

The pair $(\mathcal{F}, \mathcal{O})$ defines a \emph{model}.
The \emph{peformance} of a method $\mathcal{M}$ on a model $(\mathcal{F}, \mathcal{O})$ is its performance on the \emph{worst} problem $\mathcal{P}_w$ of $\mathcal{F}$ for $\mathcal{M}$.

Once we define what ``a solution $x$ with accuracy $\epsilon$'' means (e.g. $\|x - x^*\| \leq \epsilon$, $|f(x)-f(x^*)| \leq \epsilon$, ...) % TODO do a list
we can define the complexity of the problem $\mathcal{P}$ for the method $\mathcal{M}$
\begin{description}
  \item[Analytical complexity]
    The number of calls to the oracle required for an accuracy $\epsilon$.
  \item[Arithmetical complexity]
    The number of arithmetic operations required for an accuracy $\epsilon$,
    including operations done by the oracle and by the method.
\end{description}

We will analyse three type of oracle of input $x \in \mathbb{R}^n$%TODO on les analyse vraiment tous ?
\begin{description}
  \item[Zero-order oracle] outputs the value $f(x)$.
  \item[First-order oracle] outputs the value $f(x)$ and the gradient $f'(x)$.
  \item[Second-order oracle] outputs the value $f(x)$, the gradient $f'(x)$ and the Hessian $f''(x)$.
\end{description}

\subsection{Global optimization}
The purpose of this section is to show that global optimization is impossible and we should work on smaller classes of problems.

Clearly, if we do not make any continuity assumption on the objective, the oracle won't give us a lot of information.
Also, since we do not consider that $f$ is convex, we cannot search for the \emph{global} optimal solution in an infinite domain.

Let % TODO the objective is f or f_0 ? f is (f_1, ..., f_m)...
\[ Q = B_n = \{\, x \in \Rn \mid 0 \leq x_i \leq 1, i = 1, \ldots, n \,\} \]
and $f_0$ be $L$-Lipschitz continuous
\begin{align*}
  |f(x) - f(y)| & \leq L\|x - y\| & \forall x,y \in B_n.
\end{align*}
We consider here a Zero-order oracle but the lower complexity bound will be the same for a smooth $f_0$ of for higher order oracle~\cite[p.~18]{nesterov1998introductory}.

The grid method $\mathcal{G}(p)$ consists in calling the oracle at all the $(p+1)^n$ points
of the set $\{0,1/p,2/p,\ldots,1\}^n$ and return the point of the grid giving the minimum value for the objective $f_0$.

\begin{table}
  \centering
  \begin{tabular}{lll}
    $L$-Lipschitz with norm & $\mathcal{G}$ & Lower\\
    2
    & $\big(\lfloor \frac{L\sqrt{n}}{2\epsilon} \rfloor + 2\big)^n$
    & $\big(\lfloor \frac{L}{2\epsilon} \rfloor\big)^n$\\
    $\infty$
    & $\big(\lfloor \frac{L}{2\epsilon} \rfloor + 2\big)^n$
    & $\big(\lfloor \frac{L}{2\epsilon} \rfloor\big)^n$
  \end{tabular}
  \caption{Efficiency of the box and lower bound $f \in \mathcal{C}_L^{0,0}$.
  See \cite[Corollary~1.1.1, Theorem~1.1.2]{nesterov1998introductory} for the norm 2 and \cite[Corollary~1.1.1, Theorem~1.1.2]{nesterov2004introductory} for the norm $\infty$.}
\end{table}

\subsection{Nonlinear optimization}
\subsubsection{Function classes}
If $f$ is differentiable at $\bar{x}$ then for $y \in \Rn$ we have
\[ f(x) = f(\bar{x}) + \langle f'(\bar{x}), y-\bar{x}\rangle + o(\|y-\bar{x}\|) \]
where $o(r)$ is some function of $r > 0$ such that $\lim_{r \downarrow 0} \frac{1}{r}o(r) = 0$ and $o(0) = 0$.

\begin{mynota}
  We denote by $C^{k,p}_L(Q)$ the class of functions $f$ that are $k$ times continuously differentiable on $Q$
  and its $p$th derivative is Lipschitz continuous on $Q$ with the constant $L$:
  \[ \| f^{(p)}(x) - f^{(p)}(y) \| \leq L \| x - y \| \]
  for all $x,y \in Q$.
\end{mynota}

We have seen that we cannot win the game of optimization in General or with a Lipschitz continuous fonction.
Any function can be approximated as close as possible with a smooth function so with need First or Second order Lipschitz.
\paragraph{First order Lipschitz}
\begin{mylem}
  A function $f \in C^2$ belongs to $C^{2,1}_L(\Rn) \subseteq C^{1,1}_L(\Rn)$ if and only if
  \[ \| f''(x) \| \leq L \]
  for all $x \in \Rn$.
\end{mylem}

\begin{multicols}{2}
  \begin{myprop}
    Let $f \in C^{1,1}_L$. Then for any $x,y \in \Rn$ we have
    \begin{equation}
      \label{eq:lipschitzfirst}
      | f(y) - f(x) - \langle f'(x), y-x \rangle| \leq \frac{L}{2} \|y - x\|^2.
    \end{equation}
  \end{myprop}

  \begin{tikzpicture}[x=3cm,y=0.6cm]
    \draw[color=red, thick, domain=-1:1] plot
    (\x, {-\x/2+(\x)^2});
    \draw[color=blue, thick, domain=-1:1] plot
    (\x, {-\x/2});
    \draw[thick, domain=-1:1] plot
    (\x, {-(\x)^3/2 - \x*\x*sin(60-\x*420)/6-\x/2});
    \draw[color=red, thick, domain=-1:1] plot
    (\x, {-\x/2-(\x)^2});
  \end{tikzpicture}
\end{multicols}

\paragraph{Second order Lipschitz}
\label{sec:secondorderLipschitz}
We have the exact same property thant for First order Lipschitz with $f'$ since $f'$ is first order Lipschitz
\begin{myprop}
  Let $f \in C^{2,2}_M$. Then for any $x,y \in \Rn$ we have
  \[ | f'(y) - f'(x) - \langle f''(x), y-x \rangle| \leq \frac{M}{2} \|y - x\|^2. \]
\end{myprop}
\begin{mycorr}
  Let $f \in C^{2,2}_M$. Then for any $x,y \in \Rn$ we have
  \begin{equation}
    \label{eq:lipschitzsecondeig}
    f''(x) - MrI_n \preceq f''(y) \preceq f''(x) + MrI_n.
  \end{equation}
  where $r = \|y-x\|$.
  Remember that $A \preceq B$ means that $B-A$ is positive semidefinite.
\end{mycorr}

If $f \in C_M^{2,2}$ and there is a local minimum $x^*$ such that $f'(x^*) = 0$ and $f''(x^*)$ is positive definite.
Let $0 < l \leq L < \infty$ such that the eigenvalues of the hessian $f''(x^*)$ are in the interval $[l,L]$.

Since $f'(x^*)=0$, we can see that $f'(x) = G(x)(x-x^*)$ where
$G(x) = \int_0^1 f''(x^* + \tau(x-x^*)) \dif \tau$.
If we use \eqref{eq:lipschitzsecondeig} and integrate,
we see that the eigenvalues of $G(x)$ are between $l-\frac{r}{2}M$ and $L+\frac{r}{2}M$
where $r = \|x - x^*\|$:
\begin{equation}
  \label{eq:lipschitzsecongG}
  \Big(l-\frac{r}{2}M\Big)I \preceq G(x) \preceq \Big(L-\frac{r}{2}M\Big)I.
\end{equation}

What that means is that from a point $x$,
the directional derivative in the direction of $x^*$ $p = \frac{x^*-x}{\|x^*-x\|}$ which is
$p^T f'(x) = -(x-x^*)^TG(x)(x-x^*)/\|x^*-x\|$ is between $-(L+\frac{r}{2}M) r$ and $-(l-\frac{r}{2}M) r$.
\begin{multicols}{2}
  We see that this direction is always a decrease for $x \in B[x^*, \bar{r}]$ where $\bar{r} \eqdef \frac{2l}{M}$.
  Geometrically, that means that there is no hill between $x$ and the target, the path is monotone to $x^*$.
  Also the decrease is bounded from both sides.
  However, this direction is ideal since we do not know $x^*$.

  \begin{tikzpicture}[x=3cm,y=0.6cm]
    % derivative should be between xl - 1/2*x^2M and xL + 1/2*x^2M
    % I fix l = 1, L = 2 and M = 1 so bar{r} = 2
    % I want the derivative to be 1.5*x + sin(x) * x^2
    % So the function is 1.5x^2/2 - cos(x) * x^2 + 2sin(x)*x + 2cos(x)
    % Or                 1.5x^2/2 - (k^2*cos(k*x) * x^2 + k*2*sin(k*x)*x + 2cos(k*x)) / k^3
    % I set k to 10 to increase frequency of noise
    \draw[color=red, thick, domain=-2:0] plot
    (\x, {(\x)^2 + abs(\x)^3/6}); % 2/2x^2 + 1/6x^3
    \draw[color=red, thick, domain=-2:0] plot
    (\x, {(\x)^2/2 - abs(\x)^3/6}); % 1/2x^2 + 1/6x^3
    \draw[thick, domain=-2:0] plot
    (\x, {1.5*(\x)^2/2 + (-100*cos(10*180/pi*(\x))*(\x)^2 + 10*2*sin(10*180/pi*(\x))*(\x) + 2*cos(10*180/pi*(\x))-2)/2/1000}); % 1/2x^2 + 1/6x^3
    \draw[thick, domain=-2:0] plot
    (\x, {1.5*(\x)^2/2 + (-100*cos(10*180/pi*(\x))*(\x)^2 + 10*2*sin(10*180/pi*(\x))*(\x) + 2*cos(10*180/pi*(\x))-2)/2/1000}); % 1/2x^2 + 1/6x^3
    \node[above] at (-2, 3) {$x$};
    \node[below] at (0, 0) {$x^*$};
    \draw (-2, 0) -- (0,0);
    \draw (-2, 0.1) -- (-2,-0.1);
    \node[below] at (-2, 0) {$\bar{r}$};
  \end{tikzpicture}
\end{multicols}


\subsection{Local methods in unconstrained minimization}
\begin{mydef}
  We call a sequence $\{a_k\}_{k=1}^\infty$ a \emph{relaxation sequence} if $a_{k+1} \leq a_k$ for all $k \geq 0$.
\end{mydef}
Remember that every non-increasing sequence bounded from below is Cauchy.
If we generate a relaxation sequence $\{f(x_k)\}_{k=0}^\infty$ and $f$ is bounded from below it converges.

\subsubsection{Gradient method}
The gradient method iteration is
\begin{equation}
  \label{eq:graditer}
  x_{k+1} = x_k - h_kf'(x_k).
\end{equation}
If follows the steepest descent $f'(x_k)$.
$h_k$ can be fixed a priori to constant or a function of $k$, e.g. $h/\sqrt{k+1}$ for some constant $h$.
But it can also be computed at each iteration.
Theoritically, we would like to pick
\[ h_k = \argmin_{h \geq 0} f(x_k - hf'(x_k)). \]
That is called a \emph{full relaxation}.

In practice we rather use the Goldstein-Armijo rule
\[ \alpha\langle f'(x_k), x_k - x_{k+1}\rangle \leq f(x_k) - f(x_{k+1}) \leq \beta\langle f'(x_k), x_k - x_{k+1}\rangle \]
\begin{multicols}{2}
  \noindent
  which in the case of the gradient method \eqref{eq:graditer} is
  \[ \alpha h_k\|f'(x_k)\|^2 \leq f(x_k) - f(x_{k+1}) \leq \beta h_k\|f'(x_k)\|^2 \]
  since $x_k - x_{k+1} = -h_kf'(x_k)$.

  \begin{tikzpicture}[x=3cm,y=0.6cm]
    \draw[color=red, thick, domain=0:2] plot
    (\x, {-0.2*4*\x+1.75});
    \draw[color=red, thick, domain=0:1.3] plot
    (\x, {-0.55*4*\x+1.75});
    \draw[color=blue, thick, domain=0:0.6] plot
    (\x, {-4*\x+1.75});
    \draw[thick, domain=0:0.26] plot
    (\x, {2*(\x-1)^2 - cos(\x*420)/4});
    \draw[green, domain=0.27:0.56] plot
    (\x, {2*(\x-1)^2 - cos(\x*420)/4});
    \draw[thick, domain=0.57:0.89] plot
    (\x, {2*(\x-1)^2 - cos(\x*420)/4});
    \draw[green, domain=0.90:1.54] plot
    (\x, {2*(\x-1)^2 - cos(\x*420)/4});
    \draw[thick, domain=1.55:2] plot
    (\x, {2*(\x-1)^2 - cos(\x*420)/4});
  \end{tikzpicture}
\end{multicols}

\paragraph{First order Lipschitz}
If $f \in C^{1,1}_L$, \eqref{eq:lipschitzfirst} with \eqref{eq:graditer} becomes
\[ f(x_{k+1}) \leq f(x_k) - h_k\Big(1 - \frac{h_k}{2}L\Big) \|f'(x_k)\|^2. \]

We see that for $0 \leq h_k \leq 2/L$, $\{f(x_k)\}_{k=0}^\infty$ is a relaxation sequence.
We decrease at least as much as
\begin{equation}
  \label{eq:gradientguarantee}
  f(x_{k+1}) \leq f(x_k) - \frac{w}{L} \|f'(x_k)\|^2
\end{equation}
where $w = 1/2$ for a constant $h_k = 1/L$ or for a full relaxation and $w = 2\alpha(1-\beta)$ for Goldstein-Armijo.

Summing for $k = 0, \ldots, N-1$ we get
\[ \frac{w}{L} \sum_{k=0}^N \| f'(x_k) \|^2 \leq f(x_0) - f(x_N) \leq f(x_0) - f^*. \]
If we define $g_N^* = \min_{0 \leq k \leq N} g_k$ we have
\[ g_N^* \leq \frac{1}{\sqrt{N+1}} \Big[ \frac{L}{w} (f(x_0) - f^*) \Big]^2. \]

Note that we cannot say anything about the rate of convergence of the sequence $\{f(x_k)\}$ or $\{x_k\}$
and without additional very strict assumption we cannot guarantee the convergence to a minimum.
The fact that $\|f'(x_k)\| \to 0$ as $k \to \infty$ only ensures us that we converge to a stationary point.
%TODO lower bound are not known + cite

\paragraph{Second order Lipschitz}
If $f \in C^{2,2}$ and there is a local minimum $x^*$ such that $f'(x^*) = 0$ and $f''(x^*)$ positive definite.
Let $0 < l \leq L < \infty$ such that the eigenvalues of the hessian $f''(x^*)$ are in the interval $[l,L]$. % TODO Same assumption than for Newton, stop restating it

Using $f'(x_k) = G_k(x_k-x^*)$, \eqref{eq:graditer} gives
\[ r_{k+1} = (I-h_kG_k)r_k \]
and we have seen that the eigenvalues of $I-h_kG_k$
are between $1 - h_k(L+\frac{r_k}{2}M)$ and $1 - h_k(l-\frac{r_k}{2}M)$.

If $x_0$ is close enough to a local minimum $x^*$, i.e. $r_0 < \bar{r}$ where $r_k = \|x_k - x^*\|$,
and for all our steps we choose $h_k$ such that $0 \leq h_k \leq \frac{2}{L+\frac{r_k}{2}M}$ then
for $k > 0$, $x_k$ is also be in the ball $B(x^*,\bar{r})$ and
$\{r_k\}$ is a relaxation sequence.

With the step size $h_k = 2/(L+l)$ we can ensure a linear rate of convergence~\cite[Theorem~1.2.4]{nesterov1998introductory}
\[ \|x_k - x^*\| \leq \frac{\bar{r}r_0}{\bar{r}-r_0} \Big(1 - \frac{l}{L+l}\Big)^k. \]
This rate of convergence is called \emph{linear}.

\subsubsection{Newton method}
Initially, the Newton method was proposed for finding a root of a function of one variable $\phi(t)$, $t \in \R$, $\phi(t^*) = 0$.
Using a linear approximation $\phi(t+\Delta t) \approx \phi(t) + \phi'(t)\Delta t$ which gives $t_{k+1} = t_k - \phi(t_k)/\phi'(t_k)$.
It can be generalized for a function $F : \Rn \to \Rn$ where the scheme becomes $x_{k+1} = x_k - [F'(x_k)]^{-1}F(x_k)$.

In \emph{unconstrained} minimization, it is necessary that $f'(x) = 0$ for $x$ to be a local minimum%
\footnote{In constrainted minimization we need to use KKT condition instead}.
Since $f' : \Rn \to \Rn$, we can apply
\begin{equation}
  \label{eq:newton}
  x_{k+1} = x_k - [f''(x_k)]^{-1}f'(x_k).
\end{equation}

This method has two drawbacks.
First, it can break down if $f''(x_k)$ is degenerate.
Second, the Newton process can diverge (see \cite[Example~1.2.4]{nesterov1998introductory}).
To escape from the possible divergence, we can apply a \emph{damped Newton method}
\[ x_{k+1} = x_k - h_k[f''(x_k)]^{-1}f'(x_k) \]
where $h_k > 0$ is the step-size parameter.
At the initial stage when we are far from $x^*$ we can use the same step-size strategies than for the gradient method.
At the final stage it is reasonable to choose $h_k = 1$.

From \eqref{eq:newton}, we have
\[ x_{k+1} - x^* = [f''(x_k)]^{-1}[f''(x_k)](x_k - x^*) - [f''(x_k)]^{-1}f'(x_k) \]
so
\[ x_{k+1} - x^* = [f''(x_k)]^{-1}[f''(x_k)-G_k](x_k - x^*). \]
Let $G'_k = f''(x_k)-G_k$, we can see (\cite[p.~36]{nesterov1998introductory} or \cite[p.~35]{nesterov2004introductory}) that $\|G'_k\| \leq \frac{r_k}{2}M$.

\paragraph{Second order Lipschitz}
If $f \in C_M^{2,2}(\Rn)$ and there exists a local minimum $x^*$ of $f$ with positive definite Hessian and let $l>0$ be a lower bound for the eigenvalues of $f''(x^*)$.
From \eqref{eq:lipschitzsecondeig}, we know that the eigenvalues of $f''(x_k)$ are greater than $l-Mr_k$.
If $r_k < l/M = \bar{r}/2$, $f''(x_k)$ is positive definite and the eigenvalues of $f''(x_k)$ are smaller than $(l-Mr_k)^{-1}$ so $\|[f''(x_k)]^{-1}\| \leq (l-Mr_k)^{-1}$.
We have
\[ r_{k+1} \leq \frac{Mr_k^2}{2(l-Mr_k)}. \]
One can verify from this equation that if $r_k < 2l/(3M) = \bar{r}/3$, $\{r_k\}$ is a relaxation sequence.
The rate of convergence of this type is called \emph{quadratic}.

\subsubsection{Variable metric method (or quasi-Newton)}
\label{sec:varmet}
Note that the gradient $f'(x)$ of a nonlinear function $f(x)$ is defined with
respect to the standard Euclidean inner product $\langle x, y\rangle = y^Tx$ on $\Rn$.
If we define $f'_A = A^{-1}f'(x)$ as the gradient and Hessian with respect to $\langle Ax, y\rangle = y^TAx$,
we have the linear approximation
\begin{align*}
  f(y) & = f(x) + \langle f'(x), y-x \rangle + o(\|y-x\|)\\
       & = f(x) + \langle A^{-1}f'(x), y-x \rangle_A + o(\|y-x\|)\\
       & = f(x) + \langle f'_A(x), y-x \rangle_A + o(\|y-x\|)
\end{align*}
The direction $A^{-1}$ is therefore simply the gradient with respect to a different norm.

The scheme can be seen as a gradient method with $h=1$ but with a different norm.

One other way to see the scheme is the minimization at each step of a the quadratic approximation
\begin{align*}
  f(y) & \approx \phi(x) = f(x) + \langle f'(x), y-x \rangle + \langle A(y-x), y-x \rangle
\end{align*}
The minimization gives the step $A^{-1}f'(x)$.
With $A = f''(x)$ we have the classical approximation corresponding to the second order Taylor expansion and the scheme is the Newton method.

\begin{multicols}{2}
  If $f \in \mathcal{C}_L^{1,1}$,
  with $A = I/h$ and $0 < h \leq 1/L$, we see with \eqref{eq:lipschitzfirst} that $\phi$ is an upper approximation of $f(y)$.
  The minimum of $\phi$ has a lower value of $\phi$ than $\phi(x) = f(x)$, since it is an upper approximation, its value of $f$ is even lower.
  Therefore, we are sure to have a relaxation sequence.

  \begin{tikzpicture}[x=6cm]
    \draw[color=red, thick, domain=0:1.2] plot
    (\x, {-\x/2+(\x)^2});
    \draw[color=blue, thick, domain=0:1.2] plot
    (\x, {-\x/2});
    \draw[thick, domain=0:1.2] plot
    (\x, {-(\x)^3/2 - \x*\x*sin(60-\x*420)/6-\x/2});
    \draw[dashed, thick, domain=0:0.7] plot
    (\x, {-\x/2+4.2*(\x)^2});
    \draw[dashed, thick, domain=0:0.95] plot
    (\x, {-\x/2+2.6*(\x)^2});
    \draw[dashed, thick, domain=0:1.1] plot
    (\x, {-\x/2+1.8*(\x)^2});
    \draw[dashed, thick, domain=0:1.2] plot
    (\x, {-\x/2+1.4*(\x)^2});
    \draw[dashed, thick, domain=0:1.2] plot
    (\x, {-\x/2+1.2*(\x)^2});
    \draw[dashed, thick, domain=0:1.2] plot
    (\x, {-\x/2+1.1*(\x)^2});
  \end{tikzpicture}
\end{multicols}
This is the Gradient method.
Actually as we have seen we are also sure to have a relaxation sequence if $\frac{1}{L} < h < \frac{2}{L}$ but it is no more an upper approximation.

The variable metric methods consist in using different matrices $A$ at each iteration.
Those matrices are built only from the zero and first order oracle, not from the Hessian.
Let $H_k = A_k^{-1}$.
We do the iteration
\[ x_{k+1} = x_k - h_kH_kf'(x_k) \]
where $h_k$ is chosen the same way as with the gradient method (e.g. Goldstein-Armijo).

$H_{k+1}$ is computed using $H_k$, $f(x_k),f(x_{k+1})$ and $f'(x_k),f'(x_{k+1})$.
In the quadratic case, $f(x) = \alpha + \langle a, x \rangle + \langle Ax, x \rangle/2$, $f'(x) = Ax + a$ so
$f'(x_{k+1}) - f'(x_k) = A(x_{k+1}-x_k)$.
From this we have the \emph{quasi-Newton rule}:
\[ H_{k+1}(f'(x_{k+1})-f'(x_k)) = x_{k+1} - x_k. \]

There are several example of variable metric schemes, see \cite[Example~1.2.3]{nesterov1998introductory}.
BFGS is considered as the most stable scheme.

For quadratic functions, the variable metric usually terminates in $n$ iterations.
For general functions,
in the neighborhood of a strict minimum they have a \emph{superlinear} rate of convergence:
For $x_0 \in \Rn$,  there exists $N$ such that for all $k \geq N$ we have
\[ \|x_{k+1} - x^*\| \leq \constant \cdot \|x_k - x^*\| \cdot \|x_{k-n} - x^*\|. \]
but in theory they do not have better global convergence than the gradient method.

\subsubsection{Conjugate gradient method}
The idea of the conjugate gradient is to base the scheme on the quadratic function
\begin{align*}
  f(x)
  & = \alpha + \langle a, x \rangle + \frac{1}{2} \langle Ax, x \rangle\\
  & = \alpha - \frac{1}{2} \langle Ax^*, x^* \rangle + \frac{1}{2} \langle A(x-x^*), x-x^* \rangle\\
  & = \alpha - \frac{1}{2} \langle Ax^*, x^* \rangle + \frac{1}{2} \|x-x^*\|_A
\end{align*}
We see that $f'(x) = A(x-x^*)$.

We define the sequence $\{x_k\}_{k = 1, 2, \ldots}$ (ideallic for now) as follows
\[ x_k \eqdef \argmin \{f(x) | x \in x_0 + \mathcal{L}_k\} \]
where
\[ \mathcal{L}_k = \Lin \{A(x_0-x^*), \ldots, A^k(x_0-x^*)\}. \]

We can see that for any $k \geq 1$, $\mathcal{L}_k = \Lin\{f'(x_0), \cdot, f'(x_{k-1})\}$~\cite[Lemma~1.3.1]{nesterov1998introductory}.

Removing the constant terms of $f(x)$, we see that
\[ x_k = \argmin \{\|x-x^*\|_A | x \in x_0 + \mathcal{L}_k\}. \]
We now see that $(x_k-x^*)$ is $A$-orthogonal to $\mathcal{L}_k$%
\footnote{otherwise, if $y \in \mathcal{L}_k$ is not orthogonal, let $z = \pm y$ so that the scalar product is positive,
$\|x_k-x^*\|_A = \|x_k-z-x^*\|_A+\|z\|_A+2\langle x_k-z-x^*,z \rangle_A = \|x_k-z-x^*\|_A+2\langle x_k-x^*,z \rangle_A > \|x_k-z-x^*\|_A$}.
In particular, for $i < k$,
\[ \langle f'(x_k), f'(x_i) \rangle = \langle A(x_k-x^*), f'(x_i) \rangle = \langle (x_k-x^*), f'(x_i) \rangle_A = 0. \]
We see that the gradients are conjugate.
A simple consequence of this is that $\mathcal{L}_n = \Rn$ so after $n$ steps, $x_n = \argmin\{ f(x) | x \in \Rn\} = x^*$.

Since $x_k$ minimize $\|x_k-x^*\|_A$, $x_k$ is the $A$-projection of $x^*$ in $\mathcal{L}_k$.
Therefore $x_{k+1}-x_k = (x_{k+1}-x^*) - (x_k-x^*)$ is the $A$-projection of $x^*$ in $\mathcal{L}_{k+1}$ from which we remove the
$A$-projection of $x^*$ in $\mathcal{L}_k$.
$x_{k+1}-x_k$ is therefore $A$-orthogonal to $\mathcal{L}_k$.

Let $\delta_k = x_{k+1}-x_k$.
Since $\Lin\{\delta_0, \ldots, \delta_{k-1}\} = \mathcal{L}_k$,
we see that the steps $\delta_k$ are $A$-conjugate.

We will see now that $\delta_k$ is only a linear combination of $f'(x_k)$ and $\delta_{k-1}$.
At first we can only say that there is $h_k, \lambda_0, \ldots, \lambda_{k-1}$ such that
\[ \delta_k = -h_kf'(x_k) + \sum_{j=0}^{k-1} \lambda_j \delta_j \]
but since the $\delta_k$ are $A$-conjugate and the gradients are conjugate,
\begin{align*}
  \langle \delta_i, \delta_k \rangle_A
  & = -h_k \langle A(x_{i+1} - x_i),f'(x_k) \rangle + \sum_{j=0}^{k-1} \lambda_j \langle \delta_i, \delta_j \rangle_A\\
  0 & = -h_k \langle f'(x_{i+1}-f'(x_i), f'(x_k) \rangle_A + \lambda_i \|\delta_i\|_A\\
\end{align*}
If $i < k-1$, we see that $\lambda_i = 0$ but for $i = k-1$, it gives the value of $\lambda_i$ so we have
\[ x_{k+1} = x_k - h_k\left(f'(x_k) - \frac{\|f'(x_k)\|^2}{\|\delta_{k-1}\|_A} \delta_{k-1}\right). \]
Using the properties we have seen we can rewrite this as
\[ x_{k+1} = x_k - h_kp_k \] % TODO check +- because nesterov do not say h_k > 0 but I give the right h_k
where
\[ p_k = f'(x_k) - \beta_k p_{k-1}. \]
where $\beta_k$ can be expressed as
\begin{description}
  \item[] $\|f'(x_{k})\|^2/\langle f'(x_{k+1}-f'(x_k),p_{k-1} \rangle$
  \item[Fletcher-Rieves] $-\|f'(x_{k})\|^2/\|f'(x_{k-1})\|^2$
  \item[Polak-RibbiÃ¨re] $-\langle f'(x_{k}), f'(x_{k})-f'(x_{k-1}) \rangle/\|f'(x_{k-1})\|^2$
\end{description}
which are different expression of the same value in the quadratic case.
In the quadratic case, we have
\[ h_k = \frac{\|f'(x_k)\|}{\|p_k\|_A}. \]

When $f$ is not quadratic, the three expressions for $\beta_k$ do not give the same results and $h_k$
is rather computed with a line search as with the gradient method.
The method do not necessarily terminates in $n$ steps but after $n$ iterations we need to restart
which consists in setting $\beta_k = 0$.
After $n$ steps, we can ensure that
\[ \|x_{n+1}-x^*\| \leq \constant \cdot \|x_0-x^*\|^2 \]
if $x_0$ is close enought to $x^*$.
This is slower than the variable metric but the iterations are cheaper.
As far as the global convergence is concerned it is still not better than the gradient method.

For a quadratic function $f$, we can compute the optimal $h_k$

For complementary information about the Conjugate gradient method, we refer the reader to \cite{shewchuk1994introduction} or \cite[\S2.3]{vandooren2011inma1170}.

\subsection{Constrained minimization}
Let us now see what we can do in constrained minimization
\begin{align*}
  \min_x f_0(x)\\
  f_i(x) & \leq 0
\end{align*}

Intuitively, a constrained problem is harder than an unconstrained problem so we won't try to find a faster scheme for constrained problem
and we will rely on the scheme we already have.
However, we will see in the next chapters that this is not always harder.

\clearpage
Two methods can be used to transform the problem to a sequence of unconstrained minimization problems.
\begin{multicols}{2}
  \subsubsection{Penalty Function Method}
  \begin{mydef}
    A \emph{penalty function} for a closed set $Q$ is a continuous function $\Phi(x)$ such that $\Phi(x) = 0$ for $x \in Q$ and
    $\Phi(x) > 0$ for $x \notin Q$.
  \end{mydef}
  If $\Phi_1(x)$ is a penalty function for $Q_1$ and $\Phi_2(x)$ is a penalty function for $Q_2$ then
  $\Phi_1(x) + \Phi_2(x)$ is a penalty function for the intersection $Q_1 \cap Q_2$.

  We choose $x_0 \in \Rn$ and a sequence of penalty coefficients $0 < t_k < t_{k+1}$, $t_k \to \infty$.
  And at the iteration $k$ we search for $x_{k+1} = \argmin_{x \in \Rn} \Psi_k(x) = f_0(x) + t_k \Phi(x)$ using $x_k$ as starting point.

  Let us assume that $x_{k+1}$ is a global minimum of $\Psi_k$.
  If there exists $\bar{t} > 0$ such that the set $S = \{x \in \Rn | f_0(x) + \bar{t}\Psi(x) \leq f^*\}$ is bounded,
  then
  \begin{align*}
    \lim_{k \to \infty} f_0(x_k) & = f_0^*\\ % TODO there wer no _0 in the book
    \lim_{k \to \infty} \Psi(x_k) & = 0.
  \end{align*}

  \begin{proof}
    Let $\Phi_k^* = \Phi_k(x_{k+1})$.
    \begin{itemize}
      \item
        Since $\Psi_k^*$ is the minimum, it is above bounded by $\Psi_k^* \leq \Psi_k(x)$ for all $x$.
        In particular, we have $\Psi_k^* \leq \Psi_k(x^*) = f_0(x^*)$.
        Since $\Psi_k(x)$ increases with $k$, so do $\Psi_k^*$.
        We have just seen that it is above bounded so it converges.
        Let $\Psi^*$ be the limit, it is still below the bound so $\Psi^* \leq f_0^*$.
      \item
        We only know that $x^*$ is the minimum inside $Q$ so we \emph{cannot} say that $f_0^* \leq f_0(x_{k+1}) \leq \Psi_k^*$.
        Consider the set
        \[ S_k = \{\, x \in \Rn \mid \Psi_k(x) \leq f_0^* \,\}. \]
        Since $\Psi_k$ increases with $k$ we see that $S_{k+1} \subseteq S_k$ and by assumption,
        for $t_k \geq \bar{t}$, $S_k$ is bounded.
        Since $x_k$ is the minimum of $\Psi_k$, it must be in that set. So $\{x_k\}$ has a limit point $x_*$.
        Since $t_k \to \infty$, the limit of $S_k$ do not have any point outside $Q$ so $\Phi(x_*) = 0$ thus $\Psi^* = f_0(x_*) \geq f_0^*$.
    \end{itemize}
    In short, we have
    \[ f_0^* \leq \Psi^* \leq f_0^*. \]
    We see that $\Psi^* = f_0^*$.
  \end{proof}

  \subsubsection{Barrier Function Method}
  \begin{mydef}
    A \emph{barrier function} for a closed set $Q$ with nonempty interior is a continuous function $F(x)$ such that
    $F(x) \to \infty$ when $x$ approaches the boundary of the set $Q$.
  \end{mydef}
  If $F_1(x)$ is a penalty function for $Q_1$ and $F_2(x)$ is a penalty function for $Q_2$ then
  $F_1(x) + F_2(x)$ is a penalty function for the intersection $Q_1 \cap Q_2$.

  Since $Q$ must have a nonempty interior we mush satisfy the \emph{Slater condition}:
  $\exists \bar{x}$ such that $f_i(\bar{x}) < 0$ for all $i$.

  We choose $x_0 \in \inte Q$ and a sequence of penalty coefficients $0 < t_k < t_{k+1}$, $t_k \to \infty$.
  And at the iteration $k$ we search for $x_{k+1} = \argmin_{x \in \Rn} \Psi_k(x) = f_0(x) + t_k^{-1} F(x)$ using $x_k$ as starting point.

  Let us assume that $x_{k+1}$ is a global minimum of $\Psi_k$.
  If the barrier $F(x)$ is below bounded then
  \[ \lim_{k \to \infty} \min_{x \in Q} \Psi_k(x) = f_0^*. \]

  \begin{proof}
    Let $\Psi_k^* = \Psi_k(x_{k+1})$.
    \begin{itemize}
      \item
        Since $F(x)$ can be negative and positive, we cannot say that $t_k^{-1} F(x)$ decreases with $k$ so we cannot say that $\Psi_k^*$ is decreasing.
        However, for a fixed $\bar{x}$, it is either increasing or decreasing so $\Psi_k(\bar{x})$ converges and since $t_k^{-1} \to 0$, it converges to $f_0(x^*)$.
        Since $\Psi_k^*$ is the minimum it is above bounded by $\Psi_k(\bar{x})$ and in particular by $\Psi_k(x^*)$.
      \item
        The fact that $F$ is below bounded by some $F^*$ allows us to
        say that $\Psi_k(x)$ (and in particular $\Psi_k^*$) is below bounded by $f_0(x) + t_k^{-1} F^*$ which is below bounded by $f^* + t_k^{-1} F^* \to f_0^*$.
    \end{itemize}
    In short, we have
    \[ f_0^* \leftarrow f_0^* + t_k^{-1}F^* \leq \Psi_k^* \leq \Psi_k(x^*) \to f_0^*. \]
    We see that $\Psi_k^* \to f_0^*$ since it is ``sandwiched''.
  \end{proof}
\end{multicols}

There are still many questions that needs to be answered but in the framework on nonlinear programming,
we cannot answer them.

These questions are answered in smaller classes of problem such as convex optimization.

\section{Smooth Convex Programming}
\subsection{Smooth Convex functions}
What assumption can we make so as to make our problem more tractable ?
The reason of our trouble is the fact that $f'(x) = 0$ is not sufficient to be a local minimum, only to be a stationary point.

We would like to show that we need the function to be convex, which means that it is above all its tangent hyperplanes.
Let us impose that $f'(x) = 0$ must be a sufficient condition to be a global minimum.
If $x_0$ is such that $f'(x_0) = 0$, geometrically, $f$ must be above the ``flat'' hyperplane passing through $(x_0, f'(x_0))$.
Luckily, this hyperplane is actually the tangent at $x^*$.

\begin{multicols}{2}
  If $f'(x_0) \neq 0$, this reasoning does not work.
  The idea is to say that if we flatten the tangent at $x_0$, the resulting fucntion is still in our class.
  Let us therefore impose that if $f$ is in our class then $f + \langle a, x \rangle$ is also in our class for all $a$.
  Hence the function $\bar{f}(x) = f(x) + \langle -f'(x_0), (x-x_0) \rangle$ is in our class.
  However $\bar{f}'(x_0) = 0$ so $\bar{f}$ is above its hyperplane at $x_0$.
  Consequently, $f$ too.

  \begin{tikzpicture}[x=3cm,y=0.8cm]
    \draw[thick, domain=-.3:1.8] plot
    (\x, {(\x)^2});
    \draw[thick, domain=1.07:1.28] plot
    (\x, {-1+(10*(\x-1.1))^2/10});
    \draw (1.31,-0.9) node {$-$};
    \draw[color=blue, thick, domain=-.3:1.8] plot
    (\x, {1+2*((\x)-1)});
    \draw[thick, color=blue, domain=1.37:1.58] plot
    (\x, {-1+(0.01+2*((\x-1.4)-0.01))});
    \draw (1.65,-0.9) node {$=$};
    \draw[dashed, thick, domain=-.3:1.8] plot
    (\x, {(\x)^2 - (1+2*((\x)-1))});
    \draw[dashed, thick, domain=1.73:1.98] plot
    (\x, {-1.9+10*((\x)-1.8)^2 - (1+2*((\x-1.8)-1))});
    \draw[dashed, color=blue, thick, domain=-.3:1.8] plot
    (\x, {0});
    \draw[->, color=green] (1,0.8) to (1,0.2);
  \end{tikzpicture}
\end{multicols}

Let $\mathcal{F}^1$ be the class of \emph{continuously differentiable convex} function.
We have many equivalent definitions, we will be able to generalize some of them in the case of non-differentiable function in the next chapter.
\begin{mytheo}
  \label{theo:f1}
  Let $f$ be a \emph{continuously diffentiable} function.
  Then all the following conditions are equivalents:
  \begin{itemize}
    \item $f$ is \emph{convex}.
    \item for all $x,y \in \Rn$,
      \begin{equation}
        \label{eq:smoothconv1}
        f(y) \geq f(x) + \langle f'(x), y-x \rangle.
      \end{equation}
    \item for all $x,y \in \Rn$ and $\alpha \in [0,1]$,
      \begin{equation}
        \label{eq:smoothconv2}
        f(\alpha x + (1-\alpha) y) \leq \alpha f(x) + (1-\alpha)f(y).
      \end{equation}
    \item for all $x,y \in \Rn$,
      \begin{equation*}
        \langle f'(x)-f'(y), x-y \rangle \geq 0.
      \end{equation*}
  \end{itemize}
  If $f$ is twice continuously differentiable they are also equivalent to the following condition
  \begin{itemize}
    \item $f''(x) \geq 0$, i.e. $f''(x)$ is positive semidefinite.
  \end{itemize}
\end{mytheo}
$\langle f'(x)-f'(y), x-y \rangle$ is not easy to interpret geometrically.
Notice that if $f$ is twice differentiable and $f''(x)$ is constant or for $y \to x$, $f'(x) - f'(y) = f''(x) (x-y)$.
That gives $\langle f'(x)-f'(y), x-y \rangle/2 \approx \langle f''(x) (x-y), (x-y) \rangle/2$ which is the second order term of the Taylor serie of $f$ around $x$.

See \cite[Example~2.1.1]{nesterov1998introductory} for examples of \emph{continuously differentiable}
\footnote{it may be surprising that $|x|^p$ is continuously differentiable for $p > 1$}
convex function.

We have seen that $f$ has to be convex to ensure that $f'(x^*)=0$ is a sufficient condition for $x^*$ to be a global minimum.
We can also prove that if $f$ is convex, the condition is sufficient.
\begin{mytheo}
  If $f \in \mathcal{F}^1$ and $f'(x^*) = 0$ then $x^*$ is a global minimum of $f(x)$ on $\Rn$.
\end{mytheo}

We have useful invariant properties.
\begin{mylem}
  If $f_1, f_2 \in \mathcal{F}^1(\Rn)$, $\alpha,\beta \geq 0$, $b \in \Rn$ and $A \in \R^m \to \Rn$ then
  \begin{align*}
    \alpha f_1 + \beta f_2 & \in \mathcal{F}^1(\Rn),\\
    f(Ax+b) & \in \mathcal{F}^1(\Rn).
  \end{align*}
\end{mylem}

As with general nonlinear functions, we need further assumptions to guarantee some special topological properties.
We use the same notation for $\mathcal{F}$ as with $\mathcal{C}$.

\begin{mytheo}
  Let $f : \Rn \to \Rn$ be a \emph{continuously diffentiable} function.
  Then all the following conditions are equivalents:
  \begin{itemize}
    \item $f \in \mathcal{F}_L^{1,1}(\Rn)$.
    \item for all $x,y \in \Rn$,
      \begin{equation}
        \label{eq:convlip11}
        0 \leq f(y) - f(x) - \langle f'(x), y-x \rangle \leq \frac{L}{2}\|x-y\|^2.
      \end{equation}
    \item for all $x,y \in \Rn$,
      \begin{equation*}
        f(y) \geq f(x) + \langle f'(x), y-x \rangle + \frac{1}{2L} \|f'(x)-f'(y)\|^2.
      \end{equation*}
    \item for all $x,y \in \Rn$,
      \begin{equation}
        \label{eq:convlip13}
        \langle f'(x)-f'(y), x-y \rangle \geq \frac{1}{L}\|f'(x) - f'(y)\|^2.
      \end{equation}
  \end{itemize}
\end{mytheo}

\subsubsection{Lower complexity bound}
We can obtain a lower complexity bound for first order Lipschitz function and first order oracle.
It even works if we assume that $f$ is infinitelly many times continuously differentiable, i.e. $f \in \mathcal{F}_L^{\infty,1}$.

For any first order (because first order oracle) method $\mathcal{M}$ such that $x_k \in x_0 + \Lin\{f'(x_0), \ldots, f'(x_{k-1})\}$
for all $k \geq 1$, and for any $k$, $1 \leq k \leq (n-1)/2$ and $x_0 \in \Rn$ there exists a function $f \in \mathcal{F}_L^{\infty,1}(\Rn)$
such that
\begin{align*}
  f(x_k) - f^* & \geq \frac{L}{8(k+1)^2}\|x_0-x^*\|^2,\\
  \|x_k - x^*\|^2 & \geq \beta\|x_0-x^*\|^2,
\end{align*}
where $x^*$ is the minimum of $f(x)$, $f^* = f(x^*)$ and $\beta$ can be \emph{arbitrary} close to one.
Not that the lower bound on $f(x_k) - f^*$ is \emph{exact}~\cite[p.~60]{nesterov1998introductory}.

The bound is only valid for $k \leq (n-1)/2$, this type of bound is called \emph{uniform} in the dimension.
It shows the potential performance of numerical methods on the initial stage of the minimization process and warn us that without
a direct use of finite-dimensional arguments, we cannot get a better complexity estimate for any numerical method.

We have to accept that the convergence to the optimal point $\|x_k - x^*\|$ can be arbitary slow.
We will fix this problem by choosing a problem class in which the situation is better, the Strongly convex functions.

\subsection{Smooth Strongly Convex functions}
We have seen in the previous chapter with Second order Lipschitz that we needed $x_0$ to be close enough to $x^*$ to ensure convergence.
With the Newton method, we needed to say that $r_0 \leq l/M$ to ensure that $f''(x_0) > 0$.
Here, if $f$ is twice continuously differentiable, we know that $f''(x) \geq 0$ but not that $f''(x) > \mu I$ for some $\mu > 0$.
That is the assumption we will add.

%TODO use bar{x} instead of x_0 in the previous section too
In the previous section, we simply required that $f(x) \geq f(\bar{x})$ when $f'(\bar{x}) = 0$ and not for any $\bar{x}$ and that was enough thanks to our class invariants.
We will do the same here, we only assume that $f(x) \geq f(\bar{x}) + \mu\|\|x-\bar{x}\|^2/2$ when $f'(\bar{x}) = 0$ and this is enough to prove that we must have
\[ f(y) \geq f(x) + \langle f'(x), y-x \rangle + \frac{1}{2}\mu\|y-x\|^2. \]
for all $x,y \in \Rn$.

Let $\mathcal{S}_{\mu}^k$ be the class of $k$ times \emph{continuously differentiable strongly convex} function with parameter $\mu$. % FIXME parameter or constant ?
We have many equivalent definitions.
\begin{mytheo}
  \label{theo:s1}
  Let $f$ be a \emph{continuously diffentiable} function.
  Then all the following conditions are equivalents:
  \begin{itemize}
    \item $f \in \mathcal{S}_{\mu}^1$ is \emph{convex}.
    \item for all $x,y \in \Rn$,
      \begin{equation*}
        f(y) \geq f(x) + \langle f'(x), y-x \rangle + \frac{\mu}{2}\|y-x\|^2.
      \end{equation*}
    \item for all $x,y \in \Rn$ and $\alpha \in [0,1]$,
      \begin{equation*}
        f(\alpha x + (1-\alpha) y) \leq \alpha f(x) + (1-\alpha)f(y) - \alpha(1-\alpha)\frac{\mu}{2}\|x-y\|^2.
      \end{equation*}
    \item for all $x,y \in \Rn$,
      \begin{equation}
        \label{eq:s13}
        \langle f'(x)-f'(y), x-y \rangle \geq \mu\|x-y\|^2.
      \end{equation}
  \end{itemize}
  If $f$ is twice continuously differentiable they are also equivalent to the following condition
  \begin{itemize}
    \item $f''(x) \geq \mu I$.
  \end{itemize}
\end{mytheo}

See \cite[Example~2.1.2]{nesterov1998introductory} for examples of \emph{continuously differentiable}
strongly convex function.

Again, as for the previous section,
we have seen that $f$ has to be stronly convex under another assumption.
We can also prove the converse: if $f$ is strongly convex, the assumption is respected.
\begin{mytheo}
  If $f \in \mathcal{S}_\mu^1$ and $f'(x^*) = 0$ then $x^*$ is a global minimum of $f(x)$ on $\Rn$ but also
  \[ f(x) \geq f(x^*) + \frac{1}{2}\mu\|x-x^*\|^2 \]
  for all $x \in \Rn$.
\end{mytheo}

We have useful invariant properties.
Also remember that $\mathcal{F}_L^{k,l} \equiv \mathcal{S}_{0,L}^{k,l}$.
\begin{mylem}
  If $f_1 \in \mathcal{S}_{\mu_1}^1(\Rn)$ and $f_2 \in \mathcal{S}_{\mu_2}^1(\Rn)$ and $\alpha,\beta \geq 0$ then
  \begin{align*}
    \alpha f_1 + \beta f_2 & \in \mathcal{F}_{\alpha\mu_1+\beta\mu_2}^1(\Rn)
  \end{align*}
\end{mylem}

Using Cauchy-Schwarz on \eqref{eq:s13} and the first order Lipschitz condition we have
\[ \mu\|x-y\| \leq \|f'(x)-f'(y)\| \leq L\|x-y\| \]
if $f$ is twice continuously differentiable that means that the eigenvalues of $f''$ are between $\mu$ and $L$ at any point.
Therefore we defien the \emph{condition number} of $f$ as $Q_f \eqdef L/\mu$, note that $Q_f \geq 1$.

\eqref{eq:s13} can also be strenghten using the first order Lipschitz condition.
That gives the following theorem.
Indeed, if $L \to \infty$, \eqref{eq:s13L} becomes back \eqref{eq:s13}.

\begin{mytheo}
  Let $f \in \mathcal{S}_{\mu,L}^{1,1}(\Rn)$.
  Then for any $x,y \in \Rn$, we have
  \begin{equation}
    \label{eq:s13L}
    \langle f'(x)-f'(y), x-y \rangle \geq \frac{\mu L}{\mu+L}\|x-y\|^2 + \frac{1}{\mu+L} \|f'(x)-f'(y)\|^2.
  \end{equation}
\end{mytheo}

\subsubsection{Lower complexity bound}
We can obtain a much better lower complexity bound for first order Lipschitz function and first order oracle than with general smooth convex functions.
It does not depend on $n$ anymore so formally, we also include infinite-dimensional problems.

For any first order (because first order oracle) method $\mathcal{M}$ such that $x_k \in x_0 + \Lin\{f'(x_0), \ldots, f'(x_{k-1})\}$
for all $k \geq 1$, and for any $k$, $\mu > 0$ and $Q_f > 1$ and $x_0 \in \Rn$ there exists a function $f \in \mathcal{S}_{\mu,\mu Q_f}^{\infty,1}(\R^{\infty})$
such that
\begin{align*}
  f(x_k) - f^*    & \geq \frac{\mu}{2}\left(\frac{\sqrt{Q_f}-1}{\sqrt{Q_f}+1}\right)^{2k} \|x_0-x^*\|^2,\\
  \|x_k - x^*\|^2 & \geq \left(\frac{\sqrt{Q_f}-1}{\sqrt{Q_f}+1}\right)^{2k} \|x_0-x^*\|^2,
\end{align*}
where $x^*$ is the minimum of $f(x)$ and $f^* = f(x^*)$.
The lower bound on $f(x_k) - f^*$ is \emph{exact} since we will give a \emph{optimal} method, a method that reach this bound.

To see the number of iterations that we need for a precision $\epsilon$, we use the fact that $1+x \leq \exp(x)$
which implies that $(1+x)^{-2k} \geq \exp(-2kx)$ which we use for $x = 2/(\sqrt{Q_f}-1)$.
Hence we have
\[ f(x_k) - f^* \leq \frac{\mu}{2} \exp\Bigg(\frac{-4k}{\sqrt{Q_f}-1}\Bigg) r_0^2 \]
so
\[ k \geq \frac{\sqrt{Q_f}-1}{4k}\Big[\ln\frac{1}{\epsilon} + \ln \frac{\mu}{2} + 2 \ln r_0\Big]. \]

\subsection{Gradient method}
Let's analyse the efficiency of the gradient method with constant step-size $h_k = h > 0$.
With other step-size rule, the rate of convergence is similar
and the standard unconstrained minimization methods (variable metric and conjugage gradient)
have the similar efficiency estimates for smooth convex functions (strongly or not).

\subsubsection{Smooth convex functions}
Let $f \in \mathcal{F}_L^{1,1}(\Rn)$.
Since we are in a particular case of general smooth functions,
for $0 < h < \frac{2}{L}$, the gradient method still generates a relaxation sequence $\{f(x_k)\}$.
However, this time we do not only have guarantees on the convergence of $f'(x_k)$ to 0.
We know that~\cite[Theorem~2.1.13]{nesterov1998introductory}
\[ f(x_k) - f^* \leq \frac{2(f(x_0)-f^*)\|x_0-x^*\|^2}{2\|x_0-x^*\|^2+(f(x_0)-f^*)h(2-Lh)k} \]
and with the optimal step-size $h^* = 1/L$,
\[ f(x_k) - f^* \leq \frac{2L(f(x_0)-f^*)\|x_0-x^*\|^2}{2L\|x_0-x^*\|^2+(f(x_0)-f^*)k} \leq \frac{2L}{k+4}\|x_0-x^*\| \]

\subsubsection{Smooth strongly convex functions}
Let $f \in \mathcal{S}_{\mu,L}^{1,1}(\Rn)$.
For $0 \leq h \leq \frac{2}{\mu+L}$, the gradient method generates a sequence $\{x_k\}$ such that
\[ \|x_k-x^*\|^2 \leq \left(1 - \frac{2\mu L}{\mu+L}\right)^k \|x_0-x^*\|^2 \]
and with the optimal step-size $h^* = 2/(\mu+L)$ (note that we do not get the optimal step-size of general smooth convex functions $1/L$ with $\mu=0$),
\begin{align*}
  \|x_k-x^*\|  & \leq \left(\frac{Q_f-1}{Q_f+1}\right)^k \|x_0-x^*\|,\\
  f(x_k) - f^* & \leq \frac{L}{2}\left(\frac{Q_f-1}{Q_f+1}\right)^{2k} \|x_0-x^*\|^2.
\end{align*}

To see the number of iterations that we need for a precision $\epsilon$, we use the fact that $1+x \leq \exp(x)$
which implies that $(1-x)^{2k} \leq \exp(-2kx)$ which we use for $x = 2/(Q_f+1)$.
Hence we have
\[ f(x_k) - f^* \leq \frac{L}{2} \exp\Bigg(\frac{-4k}{Q_f+1}\Bigg) r_0^2 \]
so
\[ k = \bigg\lceil \frac{Q_f+1}{4}\Big[\ln\frac{1}{\epsilon} + \ln \frac{L}{2} + 2 \ln r_0\Big] \bigg\rceil. \]
We cannot conclude that our method is optimal because the main term in this estimate is $\frac{Q_f}{4} \ln\frac{1}{\epsilon}$ which is not proportional to the main term $\frac{\sqrt{Q_f}}{4}\ln\frac{1}{\epsilon}$ of the lower complexity bound.
However it could mean that our estimate is not sharp enough or that the lower complexity bound is could be improved.
We will see in the next section that it is not the second option.

\subsection{Optimal Methods}
The optimal methods will not try to be a relaxation sequence anymore for two reasons.
First, for some problem classes, it is too expensive for optimality.
Second, the schemes and the efficiency of the optimal methods are derived from \emph{global} properties of convex functions
while relaxation is a too ``microscopic'' to be useful.

Let $f \in \mathcal{S}_{\mu,L}^{1,1}$.
The idea of the method is to generate a sequence of quadratic functions $\phi_k(x) = \phi_k^* + \frac{\gamma_k}{2}\|x-v_k\|^2$
that somewhat ``approximate'' the function $f$, we will be more precise later.

To build this sequence of approximation, we have a sequence of points $y_k$ and an initial approximation $\phi_0(x) = \phi_0^* + \frac{\gamma_k}{2}\|x-v_k\|^2$.
Since $f$ is strictly convex with parameter $\mu$,
we know that $f$ is above $f(y_k) + \langle f'(y_k), x - y_k \rangle + \frac{\mu}{2} \|x-y_k\|^2$,
we will see that this is important.
We mix $\phi_k$ with this using a convex combination to get $\phi_{k+1}$.
\[ \phi_{k+1}(x) = (1-\alpha_k)\phi_k(x) + \alpha_k (f(y_k) + \langle f'(y_k), x - y_k \rangle + \frac{\mu}{2} \|x-y_k\|^2). \]

In summary, $\phi_k$ is a convex combination of $\phi_0(x)$ and all these quadratic lower approximation of $f$
\[ \phi_k(x) = \phi_0(x) \prod_{i=0}^{k-1} (1-\alpha_i) + \sum_{i=0}^{k-1} (f(y_k) + \langle f'(y_k), x - y_k \rangle + \frac{\mu}{2} \|x-y_k\|^2) \alpha_i \prod_{j=i+1}^{k-1} (1-\alpha_j) \]
Let $\lambda_k = \prod_{i=0}^{k-1} (1-\alpha_i)$ and $\lambda_0 = 1$,
we have
\[ \phi_k(x) = (1-\lambda_k)(\ldots) + \lambda_k \phi_0(x) \leq (1-\lambda_k)f(x) + \lambda_k \phi_0(x) \]
where $(\ldots)$ is a convex combination of lower approximation of $f$ so it is also a lower approximation of $f$.
Consequently,
\[ \phi_k(x) \leq (1-\lambda_k)f(x) + \lambda_k \phi_0(x). \]
If $\lambda_k \to 0$, such pair of sequence $\{\phi_k(x)\}_{k=0}^\infty$ and $\{\lambda_k\}_{k=0}^\infty$ is called an \emph{estimate sequence} of $f(x)$.

When $k \to \infty$, $\lambda_k \to 0$ so there is no more anything left of $\phi_0$ in $\lim_{k\to\infty}\phi_k$.
Therefore $\lim_{k\to\infty}\phi_k \leq f$ but now what ?
The key idea is to maintain in parallel a sequence $\{x_k\}$ such that $f(x_k) \leq \phi_k^*$ for all $k$.
Therefore, at the end $\lim_{k \to \infty} f(x_k) \leq \lim_{k \to \infty} \phi_k^* \leq \lim_{k \to \infty} \phi_k(x) \leq f(x)$ for all $x$ so $\lim_{k \to \infty} x_k = x^*$.
Necessarily, since $\lim_{k\to\infty} \phi_k \leq f$ and $\lim_{k\to\infty}f(x_k) \leq \lim_{k \to \infty}\phi_k^*$ we should have $\lim_{k \to \infty} x_k = \lim_{k \to \infty} v_k$ but that is only be true at infinity, taking $x_k = v_k$ for all $k$ doesn't work.

The tricky part is to maintain $f(x_k) \leq \phi_k^*$.
At the next step, $\phi_{k+1}^*$ is smaller than $\phi_k^*$ so $f(x_{k+1})$ needs to follow.
We can actually ensure it by choosing $x_{k+1}$ such that
\[ f(x_{k+1}) \leq f(y_k) - \frac{\omega}{2} \|f'(y_k)\| \]
and $\alpha_k$ such that
\[ \frac{\alpha_k^2}{\omega} = (1-\alpha_k)\gamma_k + \alpha_k\mu (=\gamma_{k+1}) \]
for some positive $\omega$.

Note that now we can forget about $\phi_k^*$.
We can just assume that we take $\phi_0^* = f(x_0)$ and since we have chosen the sequence $x_k$ and $\alpha_k$
so as to maintain $f(x_k) \leq \phi_k^*$ we do not care about the value of $\phi_k^*$.

We have a remaining freedom for $y_k$.
However, we have the inequality $\phi_{k+1}^* \geq f(x+1) + \ldots$ where $\ldots$ is positive.
If we use our remaining freedom to set $\ldots$ to 0, the inequality is sharper and we can hope faster convergence.

It remains to see how to find $x_{k+1}$ such that $f(x_{k+1}) \leq f(y_k) - \frac{\omega}{2} \|f'(y_k)\|$.
If we use $\omega = 1/L$, we can just use a gradient step
\[ x_{k+1} = y_k - \frac{1}{L}f'(y_k). \]

With this equality, we can simplify our scheme and get rid of $\{v_k\}$ and $\{\gamma_k\}$.
The simplified scheme is given by Algorithm~\ref{algo:optimalmethod}
\begin{algorithm}
  \caption{Optimal method for a smooth strongly convex $f \in S_{\mu,L}^{1,1}$ for $\omega = 1/L$.}
  \label{algo:optimalmethod}
  \begin{algorithmic}
    \STATE Choose $x_0 \in \Rn$ and $\alpha_0 \in (0,1)$.
      Set $y_0 = x_0$, $q = \mu/L$ (note that $q = 1/Q_f$).
    \FOR{$k = 0, 1, 2, 3, \ldots$}
      \STATE Compute $f(y_k)$ and $f'(y_k)$.
        Set $x_{k+1} = y_k - \frac{1}{L}f'(y_k)$.
      \STATE Compute $\alpha_{k+1} \in (0,1)$ from the equation $\alpha_{k+1}^2 = (1-\alpha_{k+1})\alpha_k^2+q\alpha_{k+1}$,
        and set
        \begin{align*}
          \beta_k & = \frac{\alpha_k(1-\alpha_k)}{\alpha_k^2+\alpha_{k+1}}\\
          y_{k+1} & = x_{k+1} + \beta_k(x_{k+1} - x_k).
        \end{align*}
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

We still have as degree of freedom $x_0$ which is usual but also $\alpha_0$.
At each step we need to solve a second order equation to find $\alpha_{k+1}$.
However, if we set $\alpha_0 = \sqrt{\frac{\mu}{L}} = \sqrt{q}$, we see by induction that $\alpha_k = \sqrt{q}$ for all $k \geq 0$.
Note however that this value of $\alpha_0$ does not work for $\mu = 0$. % say that we need >= and > for mu = 0
Actually, this value of $\alpha_0$ corresponds to taking $\gamma_0 = 0$ so also $\gamma_k = 0$ for all $k$.
that means that we approximate the smooth convex function by linear functions.
However, $f$ is not linear, its curvature is between $0 = \mu$ and $L$.
If we take $\gamma_0 = L$, it is much safer.
Actually we have $\gamma_k = \lambda_k\gamma_0 + (1-\lambda_k)\mu$ so $\lim_{k\to\infty} \gamma_k = 0$ anyway but it is not zero before infinity and that makes all the difference.

\subsubsection{Complexity}
We can prove that the optimal method generates a sequence $\{x_k\}$ such that
\begin{equation}
  \label{eq:optimalcomplexity1}
  f(x_k) - f^* \leq \lambda_k \left[f(x_0) - f^* + \frac{\gamma_0}{2} \|x_0-x^*\|^2\right].
\end{equation}
Using \eqref{eq:convlip11}, this gives
\begin{equation}
  \label{eq:optimalcomplexity2}
  f(x_k) - f^* \leq \lambda_k \left[\langle f'(x^*), x_0-x^* \rangle + \frac{\gamma_0+L}{2} \|x_0-x^*\|^2\right].
\end{equation}
However, since $f'(x^*) = 0$ (this won't always be true in the constrained case),
\begin{equation}
  \label{eq:optimalcomplexity3}
  f(x_k) - f^* \leq \lambda_k \frac{\gamma_0+L}{2} \|x_0-x^*\|^2.
\end{equation}

and if we choose $\gamma_0 \geq \mu$ and $w = 1/L$ then
\begin{equation}
  \label{eq:optimalcomplexitylambda}
  \lambda_k \leq \min\left\{\left(1-\sqrt{\frac{\mu}{L}}\right)^k, \frac{4L}{(2\sqrt{L}+k\sqrt{\gamma_0})^2}\right\}.
\end{equation}
As we have seen, $\gamma_0 = \mu$ gives an easy constant expression for $\alpha_k$ but $\gamma_0 = L$ is safer.
For $\gamma_0 = L$, we have
\begin{equation}
  \label{eq:optimalcomplexitygamma0eqL}
  f(x_k) - f^* \leq L \min\left\{\left(1-\sqrt{\frac{\mu}{L}}\right)^k, \frac{4}{(k+2)^2}\right\}\|x_0-x^*\|^2.
\end{equation}

To see the number of iterations that we need for a precision $\epsilon$, we use the fact that $1+x \leq \exp(x)$
which implies that $(1-x)^k \leq \exp(-kx)$ which we use for $x = 1/\sqrt{Q_f}$.
Hence we have
\[ f(x_k) - f^* \leq L \exp\Bigg(\frac{-k}{\sqrt{Q_f}}\Bigg) r_0^2 \]
so
\[ k = \bigg\lceil \sqrt{Q_f}\Big[\ln\frac{1}{\epsilon} + \ln L + 2 \ln r_0\Big] \bigg\rceil. \]
We see that we are optimal because the main term in this estimate is $\sqrt{Q_f} \ln\frac{1}{\epsilon}$ which is proportional to the main term $\frac{\sqrt{Q_f}}{4}\ln\frac{1}{\epsilon}$ of the lower complexity bound.
That proves 2 things. First that our method is optimal, and second that our lower bound is the best we can obtain.

Note that we only give the convergence of $f(x_k) - f^*$ because by strong convexity
\[ \frac{\mu}{2} \|x_k-x^*\|^2 \leq f(x_k)-f(x^*)-\langle f'(x^*),x_k-x^* \rangle = f(x_k)-f(x^*) \]
because $f'(x^*) = 0$.
This shows that the convergence of $f$ is harder to obtain than the convergence of $\|x_k-x^*\|$ for smooth strongly convex functions.

\subsection{Constrained optimization}
\label{sec:constrained}
We now look at the problem
\[ \min_{x \in Q}f(x), f \in \mathcal{F}^1(\Rn) \]
where $Q$ is convex (see \cite[p.~76--78]{nesterov1998introductory} for an introduction to convex sets).
As for Section~\ref{sec:nlp}, our condition $f'(x) = 0$ does not work in constrained optimization.
However it can be remplaced by the following theorem.
\begin{mytheo}
  \label{theo:constrainedoptcond}
  Let $f \in \mathcal{F}^1(\Rn)$ and $Q$ be a closed convex set.
  The point $x^*$ minimized $f$ on $Q$ if and only if
  \[ \langle f'(x^*), x - x^* \rangle \geq 0 \]
  for all $x \in Q$.
\end{mytheo}

We can also show that if $f$ is smooth strongly convex with $\mu > 0$ then the solution \emph{exists} and is \emph{unique}~\cite[Theorem~2.2.6]{nesterov1998introductory}.

\subsubsection{Gradient Mapping}
Remember our interpretetion of the gradient method at Section~\ref{sec:varmet}.
The gradient method with step $h$ can be seen as an exact minimization of the quadratic approximation at point $\bar{x}$
\[ f(\bar{x}) + \langle f'(\bar{x}), x - \bar{x} \rangle + \frac{1}{2} \langle I/h (x - \bar{x}), x - \bar{x} \rangle. \]
With $\gamma = 1/h$, it gives
\[ f(\bar{x}) + \langle f'(\bar{x}), x - \bar{x} \rangle + \frac{\gamma}{2} \|x - \bar{x}\|. \]

As we have seen in Section~\ref{sec:varmet}, with unconstrained minimization, the minimum is given by $\bar{x} - \frac{1}{L}f'(\bar{x})$.
With constraint minimization however, it is not as simple but if $\gamma > 0$, the quadratic approximation is strongly convex so
as we have seen in Section~\ref{sec:constrained} we know there is a unique
\[ x_Q(\bar{x};\gamma) = \argmin_{x \in Q} f(\bar{x}) + \langle f'(\bar{x}), x - \bar{x} \rangle + \frac{\gamma}{2} \|x - \bar{x}\|. \]
We know that the objective is quadratic and that if $Q = \Rn$ the minimum is $(\bar{x} - \frac{1}{\gamma}f'(\bar{x}))$
so without surprise we can see that the objective can be rewritten as
\[ f(\bar{x}) - \frac{1}{2\gamma}\|f'(\bar{x})\|^2 + \frac{\gamma}{2} \Big\| x - \Big(\bar{x} - \frac{1}{\gamma}f'(\bar{x})\Big) \Big\|^2 \]
so in practice we compute $x_Q(\bar{x};\gamma)$ as
\[ x_Q(\bar{x};\gamma) = \argmin_{x \in Q} \Big\| x - \Big(\bar{x} - \frac{1}{\gamma}f'(\bar{x})\Big) \Big\|. \]
In other words, $x_Q$ is the projection of $\bar{x} - \frac{1}{\gamma}f'(\bar{x})$ onto the set $Q$
\[ x_Q(\bar{x};\gamma) = \pi_Q\Big(\bar{x} - \frac{1}{\gamma}f'(\bar{x})\Big) \]
where the projection $\pi_Q$ is defined as
\[ \pi_Q = \argmin_{x \in Q} \|x-x_0\|. \]

We want to see $x_Q(\bar{x},\gamma)$ as being obtained by a gradient step with a generalized gradient that we call \emph{gradient mapping} and a step-size $\gamma$.
So we define the gradient mapping of $f$ on $Q$
\[ g_Q(\bar{x};\gamma) = \gamma(\bar{x} - x_Q(\bar{x},\gamma)). \]

One can prove \cite[Theorem~2.2.7]{nesterov1998introductory}.
\begin{mytheo}
  \label{eq:constrainedlowerapprox}
  Let $f \in \mathcal{S}_{\mu,L}^{1,1}(\Rn)$, $\gamma \geq L$ and $\bar{x} \in \Rn$.
  Then for any $x \in Q$ we have
  \[ f(x) \geq f(x_Q(\bar{x};\gamma)) + \langle g_Q(\bar{x};\gamma), x - \bar{x} \rangle + \frac{1}{2\gamma}\|g_Q(\bar{x};\gamma)\|^2 + \frac{\mu}{2}\|x - \bar{x}\|^2. \]
\end{mytheo}
That gives us
\begin{align}
  \label{eq:constrainedgradientguarantee}
  f\Big(\bar{x} - \frac{1}{\gamma}g_Q(\bar{x};\gamma)\Big) & \leq f(\bar{x}) - \frac{1}{2\gamma} \|g_Q(\bar{x};\gamma)\|^2,\\
  \notag
  \langle g_Q(\bar{x};\gamma), \bar{x} - x^* \rangle & \geq \frac{1}{2\gamma} \|g_Q(\bar{x};\gamma)\|^2 + \frac{\mu}{2}\|x-x^*\|^2,
\end{align}
which need to be compared to the guarantees we have for the unconstrained gradient step \eqref{eq:gradientguarantee} and first order Lipschitz condition for smooth convex functions \eqref{eq:convlip13} used with $y = x^*$ and $f'(y) = 0$
\begin{align*}
  f\Big(\bar{x} - \frac{1}{L}f'(\bar{x})\Big) & \leq f(\bar{x}) - \frac{1}{2L} \|f'(\bar{x})\|^2,\\
  \langle f'(\bar{x}), \bar{x} - x^* \rangle  & \geq \frac{1}{L}\|f'(\bar{x})\|^2.
\end{align*}

\subsection{Constrained gradient method}
The constrained gradient step is
\[ x_{k+1} = x_k - h g_Q(x_k;\gamma). \]
Since $x_k \in q$ and $x_{k+1} \in Q$ for $h = 1/\gamma$ by the definition of $g_Q(x_k;\gamma)$,
by the convexity of $Q$ we know that $x_{k+1} \in Q$ for all $0 \leq h \leq 1/\gamma$.

If we choose $\gamma = L$ and $h = 1/\gamma = 1/L$, we can ensure that
\[ \|x_k - x^*\|^2 \leq \left(1 - \frac{1}{Q_f}\right)^k \|x_0-x^*\|^2 \]
For unconstrained gradient method we used \eqref{eq:convlip11}
\[ f(x_k) - f(x^*) - \langle f'(x^*), x_k-x^* \rangle \leq \frac{L}{2}\|x^*-x_k\|^2. \]
and the fact that $f'(x^*) = 0$ to deduce the rate of convergence of $f(x_k) - f'(x^*)$.
However, here the only thing we can say is (see Theorem~\ref{theo:constrainedoptcond}) that  $\langle f'(x^*), x_k-x^* \rangle \geq 0$.

We can use two approach for this
\begin{itemize}
  \item
    However, using Cauchy-Schwarz, we can say that
    \[ f(x_k) - f(x^*) \leq \bigg(\|f'(x^*)\|^2 + \frac{L}{2}\bigg) \|x^*-x_k\|^2 \]
    so
    \[ f(x_k) - f(x^*) \leq \bigg(\|f'(x^*)\|^2 + \frac{L}{2}\bigg) \left(1 - \frac{1}{Q_f}\right)^k \|x_0-x^*\|^2. \]

    What we can say is that if $x^*$ is not at the border of the domain $Q$, which means that it does not activate any constraint,
    $f'(x^*) = 0$ so the term $\|f'(x^*)\|$ can be removed but we still do not have the same rate than in the unconstrained case
    because the exponent is $k$ and not $2k$.
    Intuitively, the more constraints are activated, the more $\|f'(x^*)\|$ will grow and the slower the gradient method will be for $f(x_k) - f(x^*)$.

    If we use the fact that $1-x \leq \exp(-x)$, we can see that
    \[ f(x_k) - f(x^*) \leq \bigg(\|f'(x^*)\|^2 + \frac{L}{2}\bigg)\exp\bigg(\frac{-k}{Q_f}\bigg) \|x_0-x^*\|^2 \]
    so we can ensure that $f(x_k) - f(x^*) \leq \epsilon$, with
    \[ k = \Bigg\lceil Q_f \bigg[\ln\frac{1}{\varepsilon} + \ln\bigg(\|f'(x^*)\|^2 + \frac{L}{2}\bigg) + 2\ln\|x_0-x^*\| \bigg] \Bigg\rceil \]
    iterations.
  \item
    We cannot guarantee that $f(x_k) - f(x^*) \leq  L/2 \|x_k - x^*\|^2$ since $f'(x^*) \neq  0$.
    However, we can guarantee it for $x_{k+1} = \arg\min\limits_{x \in Q} [ f(x_k)+ \la f'(x_k), x - x_k \ra + L/2 \| x - x_k \|^2]$:
    indeed,
    \begin{align*}
      f(x_{k+1}) & = \min\limits_{x \in Q} [ f(x_k) + \la f'(x_k), x - x_k \ra + \frac{L}{2} \| x - x_k \|^2]\\
                 & \leq f(x_k) + \la f'(x_k), x^* - x_k \ra +  \frac{L}{2} \|x^* - x_k\|^2\\
                 & \leq f(x^*) +  \frac{L}{2} \|x^* - x_k\|^2.
    \end{align*}
    For the last inequality, we used the convexity of $f$. %\eqref{eq:smoothconv1}.
    We can see that picking $x_{k+1} = \arg\min\limits_{x \in Q} [ f(x_k)+ \la f'(x_k), x - x_k \ra + L/2 \| x - x_k \|^2]$
    is exactly the same as doing a new gradient iteration $x_{k+1} = x_Q(x_k;L)$.
    Hence we have
    \[ f(x_k) - f(x^*) \leq \frac{L}{2}\left(1 - \frac{1}{Q_f}\right)^{k-1} \|x_0-x^*\|^2. \]

    Since it is just a gradient iteration, we can interpret it as if we do it at the first iteration.
    In some sense, we eliminate the ``constant part'' of the gradient at the first iteration.
\end{itemize}

\subsection{Constrained optimal methods}
Remember that the key part is to maintain a sequence $\{x_k\}$ such that $f(x_k) \leq \phi_k^*$
and that it is $x_k$ that tends to our target $x^*$.
Therefore of course, we must have $x_k \in Q$.
We won't however constraint $y_k$ to be feasible.

We have seen when comparing \eqref{eq:constrainedgradientguarantee} which \eqref{eq:gradientguarantee}
that the gradient mapping has the same decrease guarantee than the gradient step.
However, since \eqref{eq:constrainedgradientguarantee} gives this guarantee in term of the gradient mapping instead of the gradient,
we should change the lower approximation at $y_k$ we use to build $\phi_{k+1}$ from $\phi_k$.

In the unconstrained case, we used the fact that
\[ f(y_k) + \langle f'(y_k), x - y_k \rangle + \frac{\mu}{2} \|x-y_k\|^2 \]
is below $f(x)$.
Here we will not take this lower approximation but the one given in \label{eq:constrainedlowerapprox}.

However, it only changes $v_k$ and $\phi_k^*$.
Since we do not use them Algorithm~\ref{algo:optimalmethod}, the only thing we have to change is the computation of $x_{k+1}$,
we use $g_Q(y_k;L)$ instead of $f'(y_k)$.
We set
\[ x_{k+1} = x_Q(y_k;L) = y_k - \frac{1}{L}g_Q(y_k;L). \]
Note that since we do \emph{not} know if $y_k \in Q$, we \emph{cannot} say that $y_k - hg_Q(y_k;L)$ is feasible for all $0 \leq h \leq 1/L$
as we did with the gradient method.
We have to do a full step $h = 1/L$.
Another change we have to do in Algorithm~\ref{algo:optimalmethod} is to replace $x_0 \in \Rn$ by $x_0 \in Q$.
If this is not possible, just add $x'_0 = x_Q(x_0;\gamma)$ as a first step of the algorithm and use $x'_0$ instead of $x_0$.

For the complexity results, the state is somewhat better that for the gradient method because the bound
\eqref{eq:optimalcomplexity1} is for $f(x_k) - f(x^*)$ directly and not for $\|x_k - x^*\|$.
However, we cannot get rid of $f(x_0) - f(x^*)$ easily.
The distance $\|x_0 - x^*\|$ is no the only metric we need to estimate $k$,
we can see in \eqref{eq:optimalcomplexity2} that the bigger $f'(x^*)$ is the bigger our error in the estimate will be if we only take $\|x_0-x^*\|$ into account.

As with the gradient method
What we can also say is that if $x^*$ is not at the border of the domain $Q$, which means that it does not activate any constraint,
$f'(x^*) = 0$ so we have \eqref{eq:optimalcomplexity3}.
Again, intuitively, the more constraints are activated, the more $f'(x^*)$ will grow.
Equation~\eqref{eq:optimalcomplexitylambda} is still valid but \eqref{eq:optimalcomplexitygamma0eqL} is only valid for $\gamma_0 = L$ as before and $f'(x^*) = 0$ \eqref{eq:optimalcomplexity3}.

Using Cauchy-Schwarz, \eqref{eq:optimalcomplexity2} gives
\begin{equation*}
  f(x_k) - f^* \leq \lambda_k \Big(\|f'(x^*)\|^2 + \frac{\gamma_0+L}{2}\Big) \|x_0-x^*\|^2.
\end{equation*}
So we have
\[ k = \bigg\lceil \sqrt{Q_f}\Big[\ln\frac{1}{\epsilon} + \ln\Big(\|f'(x^*)\|^2 + \frac{\gamma_0+L}{2}\Big) + 2\ln (r_0)\Big] \bigg\rceil. \]
even if using \eqref{eq:optimalcomplexity1} directly,
since we use neither \eqref{eq:convlip11} nor Cauchy-Schwarz, should gives a sharper bound
\[ k = \bigg\lceil \sqrt{Q_f}\Big[\ln\frac{1}{\epsilon} + \ln\Big(f(x_0) - f(x^*) + \frac{\gamma_0}{2}r_0^2\Big)\Big] \bigg\rceil. \]

Note that again we only give the convergence of $f(x_k) - f^*$ because by strong convexity
\[ \frac{\mu}{2} \|x_k-x^*\|^2 \leq f(x_k)-f(x^*)-\langle f'(x^*),x_k-x^* \rangle \leq f(x_k)-f(x^*) \]
because $\langle f'(x^*),x_k-x^* \rangle$ is positive since $x_k$ is feasible for all $k \geq 0$.
The convergence of $f$ is harder to obtain than the convergence of $\|x_k-x^*\|$ for smooth strongly convex functions even in the constrained case.

\section{Nonsmooth Convex Programming}
\subsection{Properties of General Convex Functions}
In \cite[Section~2.3]{nesterov1998introductory}, objective of the form
\[ f(x) = \max_{1 \leq j \leq p} \phi_j(x) \]
where $\phi_j \in \mathcal{F}^1$ are considered.
It is treaten using gradient mapping however the gradient mapping becomes very expensive when $p$ is very large.
We could also treat $f$ as a general nonsmooth function.

In many applications the objective of the functional constraints are given implicitly as a solution of an auxiliary problem.
Such functions are called functions with \emph{implicit} structure and very often they appear to be nonsmooth.

We define the domain, the epigraph and the sublevel sets of a function $f$ respectively as
\begin{align*}
  \dom f               & = \{\, x \in \Rn \mid |f(x)| < \infty \,\},\\
  \epi(f)              & = \{\, (x,t) \in \dom f \times \R \mid f(x) \leq t \,\},\\
  \mathcal{L}_f(\beta) & = \{\, x \in \dom f \mid f(x) \leq \beta \,\}.
\end{align*}

\begin{mytheo}
  \label{theo:f1}
  Let $f : \Rn \to \R$ be a function with a convex domain $\dom f$.
  Then all the following conditions are equivalents:
  \begin{itemize}
    \item $f$ is \emph{convex}.
    \item for all $x,y \in \dom f$ and $\alpha \in [0,1]$,
      \begin{equation}
        \label{eq:nonsmoothconv2}
        f(\alpha x + (1-\alpha) y) \leq \alpha f(x) + (1-\alpha)f(y).
      \end{equation}
    \item for all $x,y \in \dom f$ and $\beta \geq 0 $,
      \begin{equation*}
        f(y + \beta(y-x)) \geq f(y) + \beta(f(y) - f(x)).
      \end{equation*}
    \item $\epi(f)$ is a convex set.
  \end{itemize}
\end{mytheo}
We can see comparing \eqref{eq:smoothconv2} and \eqref{eq:nonsmoothconv2} that this condition does not need any modification to be applied to the nonsmooth case.

\begin{mylem}[Jensen inequality]
  Let $f$ be a convex function,
  $x_1, \ldots, x_m \in \dom f$ and
  $\alpha_1, \ldots, \alpha_m \geq 0$ such that $\sum_{i=1}^m \alpha_i = 0$,
  we have
  \[
    f\Big( \sum_{i=1}^m \alpha_i x_i \Big) \leq \sum_{i=1}^m \alpha_i f(x_i).
  \]
\end{mylem}

The behavior of convex functions at the points of the boundary of its domain can be rather disappointing (see \cite[Example~3.1.1(6),~3.1.2(5)]{nesterov1998introductory}).
Fortunately, this is the only bad news about convex functions, we will see that the structure of the convex functions in the interior of its domain is very simple.

A convex function $f$ is called \emph{closed} if its epigraph $\epi(f)$ is a closed set.
If $f$ is a closed convex function then $\mathcal{L}_f(\beta)$ is closed or empty for all $\beta$.
If $f$ is convex, continuous and $\dom f$ is closed then $f$ is closed.

We also see that if $f$ is convex and $x_0 \in \inte(\dom f)$.
Then $f$ is locally upper bounded at $x_0$ \cite[Lemma~3.1.2]{nesterov1998introductory} which means that $f$ is locally Lischitz continuous at the interior of its domain%
\footnote{Remember that for a function with a convex domain, locally Lipschitz implies continuity}.
However, even if $f$ is also closed and convex we do not have the continuity of at the boundary.

Let $x \in \dom f$.
We call $f$ \emph{differentiable in the direction $p$} at the point $x$ if the limit
\[ f'(x;p) = \lim_{\alpha \downarrow 0} \frac{1}{\alpha} [f(x + \alpha p) - f(x)] \]
exists.

Again at the interior, all is well.
A convex fonction is differentiable in any direction $p$ at any point of the interior of its domain.
Since we do not have the continuity at the boundary we won't ask for the differentiability in any direction.

Remember that for a differentiable function, we have $f'(x;p) = \langle f'(x), p \rangle$.
This gives a hint for a possible generalization of \eqref{eq:smoothconv1}.
If $f$ is a convex function and $x \in \inte (\dom f)$ then $f'(x;p)$ is a convex homogeneous (of degree 1) function of $p$.
and for any $y \in \dom f$ we have:
\begin{align*}
  f(y) & \geq f(x) + f'(x;y-x).
\end{align*}

We have defined directional derivative which is a scalar derivative.
In the case of differentiability, this scalar is given by the gradient and the derivative.
What replaces the gradient for nonsmooth convex functions ?

\begin{mydef}
  Let $f$ be a convex function.
  A vector $g$ is called the \emph{subgradient} of $f$ at $x_0 \in \dom f$ if for any $x \in \dom f$ we have
  \begin{align*}
    f(x) \geq f(x_0) + \langle g, x - x_0 \rangle.
  \end{align*}
\end{mydef}
The set of all subgradients of $f$ at $x_0$, $\partial f(x_0)$, is called the \emph{subdifferential}
of function $f$ at the point $x_0$.
This looks like a better candidate for the generalization of \eqref{eq:smoothconv1}.

For $|x|$, we see that $\partial f(x) = \{f'(x)\}$ for $x \neq 0$ and $\partial f(0) = [-1,1]$.
In fact, if $f$ is differentiable on $\dom f$, then $\partial f(x) = \{f'(x)\}$ for all $x \in \inte(\dom f)$.

Note that we \emph{almost} have the beautiful generalization of \eqref{eq:smoothconv1}:
A function $f$ is convex if and only if for all $y$ there exists $g$ such that for all $y$
\[ f(y) \geq f(x) + \langle g, y-x \rangle. \]
In other words, $\partial f(x)$ is nonempty.

However, even if $\Leftarrow$ works, $\Rightarrow$ doesn't work.
If $f$ is closed convex, we can say that $\partial f(x)$ is nonempty and bounded for any $x \in \inte(\dom f)$
but we cannot say that $\partial f(x) \neq \emptyset$ for $x$ at the boundary.
Consider for example $f(x) = -\sqrt{x}$, $\partial f(0) = \emptyset$ while $f$ is closed convex.
Indeed, at 0 we would need $g = (1,-\infty)$ which is not valid.

If $\partial f(x) \neq \emptyset$,
we can generalize the formula $f'(x;p) = \langle f'(x), p \rangle$.
If the convex function $f$ is closed,
for a point $x \in \inte(\dom f)$
we have
\[ f'(x;p) = \max_{g \in \partial f(x)} \langle g, p \rangle. \]
Indeed if $f$ is differentiable on $\dom f$, $\partial f(x) = \{f'(x)\}$ for all $x \in \inte(\dom f)$ so it is a generalization.

\subsection{Separation theorems}
\begin{mydef}
  Let $Q$ be a convex set.
  We say that the hyperplane
  \[ \mathcal{H}(g,\gamma) = \{\, x \in \Rn \mid \langle g, x \rangle = \gamma \,\}, \quad g \neq 0 \]
  is \emph{supporting} $Q$ if any $x \in Q$ satisfies the inequality $\langle g, x \rangle \leq \gamma$.

  We say that the hyperplane $\mathcal{H}(g,\gamma)$ separates a point $x_0$ from $Q$ if
  \[ \langle g, x \rangle \leq \gamma \leq \langle g, x_0 \rangle \]
  for all $x \in Q$.
  If the second inequality is strict, we call the separation \emph{strict}.
\end{mydef}

We define the projection
\[ \pi_Q(x_0) = \argmin_{x \in Q} \|x-x_0\| \]
which exists and is unique if $Q$ is a closed convex set.

\begin{multicols}{2}
  If $Q$ is a closed convex set, we have
  \begin{align*}
    \langle x_Q(x_0) - x_0, x - x_Q(x_0) \rangle & \geq 0
  \end{align*}
  so
  \begin{align}
    \notag
    \|x - x_0\|^2 & = \|x_Q(x_0) - x_0\|^2 + \|x - x_Q(x_0)\|^2\\
    \notag
                  & \quad {} + 2\langle x_Q(x_0) - x_0, x - x_Q(x_0) \rangle\\
    \label{eq:projsqr}
                  & \geq \|x_Q(x_0) - x_0\|^2 + \|x - x_Q(x_0)\|^2.
  \end{align}

  \begin{tikzpicture}[x=.5cm,y=.5cm]
    \begin{scope}
      \path[clip] (0,0) ellipse (4 and 4);
      \draw[thick] (-2,0) ellipse (4 and 3.5);
    \end{scope}
    \node[left] at  (0,-2) {$x$};
    \node[left] at  (2, 0) {$x_Q(x_0)$};
    \node[right] at (4, 0) {$x_0$};
    \draw[thick,->] (2,0) to (0,-2);
    \draw[thick,->] (4,0) to (2, 0);
    \draw[thick,->] (4,0) to (0,-2);
  \end{tikzpicture}
\end{multicols}

\begin{multicols}{2}
  If $Q$ is a closed convex set and $x_0 \notin Q$,
  then there exists an hyperplane $\mathcal{H}(g,\gamma)$
  separating $x_0$ from $Q$.
  Namely we can take
  \begin{align*}
    g      & = x_0 - \pi_Q(x_0),\\
    \gamma & = \langle g, \pi_Q(x_0) \rangle.
  \end{align*}

  \begin{tikzpicture}[x=.5cm,y=.35cm]
    \begin{scope}
      \path[clip] (0,0) ellipse (4 and 4);
      \draw[thick] (-2,0) ellipse (4 and 3.5);
    \end{scope}
    \node[left] at  (2, 0) {$x_Q(x_0)$};
    \node[right] at (4, 0) {$x_0$};
    \draw[thick,->] (2,0) to (4, 0);
    \node[above] at (3, 0) {$g$};
    \draw[red, thick] (2,4) to (2,-4);
  \end{tikzpicture}
\end{multicols}

If we make $x_0$ tend to $\pi_Q(x_0)$, we get the following theorem:

Let $Q$ be a closed convex set and $x_0 \in \partial Q$.
Then there exists an hyperplane $\mathcal{H}(g,\gamma)$,
supporting to $Q$ and passing through $x_0$.

\subsection{Optimization for nonsmooth convex functions}
Let's analyse the consequences of the definition of the subgradient
\[ f(\bar{x}) \geq f(x) + \langle g, \bar{x} - x \rangle. \]
\begin{multicols}{2}
  Let's say that $g \neq 0$.
  In the direction $g$, i.e. for $\bar{x}$ such that $\langle g, \bar{x} - x \rangle > 0$,
  we know that $f(\bar{x}) > f(x)$.
  For the hyperplane defined by $\langle g, y - x \rangle > 0$, we can just say that $f(y) \geq f(x)$,
  and for $\langle g, \bar{x} - x \rangle < 0$ we do not know.

  However, we can conlude that all the points $\bar{x}$ such that $f(\bar{x}) \leq f(x)$ are such that
  $\langle g, \bar{x} - x \rangle < 0$.
  This means that all subgradient $g \in \partial f(x)$ supported for the sublevel set $\mathcal{L}_f(f(x))$.
  Practically, this mean that $x^*$ is in the ``side $-g$'' of the hyperplane.

  \begin{tikzpicture}[x=.5cm,y=.5cm]
    \draw[blue, thick, ->] (0,0) to (.8,1.2);
    \draw[red, thick] (-6,4) to (6,-4);
    \node[below] at (0, 0) {$x$};
    \filldraw (0,0) circle (.05);
    \node[below] at (-4.5, 3) {$y$};
    \filldraw (-4.5,3) circle (.05);
    \node[right] at (-4.5, 3.3) {$f(y) \geq f(x)$};
    \node[below] at (2, 2) {$\bar{x}$};
    \filldraw (2,2) circle (.05);
    \node[right] at (2, 2.3) {$f(\bar{x}) > f(x)$};
  \end{tikzpicture}
\end{multicols}

\begin{multicols}{2}
  From that we can see that $x^*$ is a minimum of $f$ if and only if
  $0 \in \partial f(x)$.

  \begin{tikzpicture}[x=3cm,y=0.6cm]
    \draw[thick, domain=-.5:.5] plot
    (\x, {(\x)^2-2*(\x)+3/4});
    \draw[blue, thick, domain=-.5:.5] plot
    (\x, {-(\x)+1/2});
    \draw[thick, domain=.5:1.1] plot
    (\x, {(\x)^2+2*(\x)-5/4});
    \draw[blue, thick, domain=.5:1.2] plot
    (\x, {3*(\x)-6/4});
    \draw[red, thick, domain=-.5:1.2] plot
    (\x, {0});
  \end{tikzpicture}
\end{multicols}
This result gives a very nice proof for the KKT conditions (see \cite[Theorem~3.1.17]{nesterov1998introductory}).

In this section we rewrite $\langle g(x), \bar{x} - x \rangle$ as $-\langle g(x), x-\bar{x} \rangle$.
We have seen that $\langle g(x), x - x^* \rangle > 0$ if $x$ does not minimize $f$.
This is simple but it has 2 consequences:
\begin{itemize}
  \item The direction $-g(x)$ decreases the distance between $x$ and $x^*$.
  \item The hyperplane defined by $\{\, y \in \Rn \mid \langle g(x), x - y \rangle = 0 \,\}$ cuts $\Rn$ on two half spaces.
    Only one of them constains $x^*$.
\end{itemize}

In order to develop nonsmooth minimization we have to forget about relaxation and approximation.
We use another concept called \emph{localization}.
We will now derive some technique to allow us to estimate a quality of a current point as an approximate solution.

\begin{multicols}{2}
  We define $v_f(\bar{x}, x)$ which represents the ``signed'' distance from $\bar{x}$ to the hyperplane orthogonal to $g(x)$.
  \[ v_f(\bar{x},x) = \frac{1}{\|g(x)\|}\langle g(x), x - \bar{x} \rangle. \]
  It is negative in the ``$+g$ side'' of the hyperplane, the uninteresting side,
  and positive on the ``$-g$ side'' where $x^*$ is.

  \begin{tikzpicture}[x=.5cm,y=.5cm]
    \draw[blue, thick, ->] (3,-2) to (3.8,-0.8);
    \draw[red, thick] (-6,4) to (6,-4);
    \node[below] at (3, -2) {$x$};
    \filldraw (3,-2) circle (.05);
    \node[below] at (0, 0) {$y$};
    \filldraw (0,0) circle (.05);
    \node[right] at (0, 0.3) {$v_f(y,x) = 0$};
    \node[below] at (1, 3) {$\bar{x}$};
    \filldraw (1,3) circle (.05);
    \node[right] at (1, 3.3) {$v_f(\bar{x},x) < 0$};
    \node[below] at (-2, -3) {$\bar{x}$};
    \filldraw (-2,-3) circle (.05);
    \draw (-2,-3) -- (0,0);
    \node[left] at (-1, -1.5) {$v_f(\bar{x},x)$};
  \end{tikzpicture}
\end{multicols}

We now introduce a measure of the variance of $f$ with respect to $x$
\[ \omega_f(\bar{x},t)= \max_{\|x-\bar{x}\| \leq t} f(x) - f(\bar{x}). \]
If $t < 0$ we set $\omega_f(\bar{x},t) = 0$.

We can see that $w_f(\bar{x},t) = 0$ for $t \leq 0$ and is non-decreasing in $t$.
If $f$ is zero-order Lipschitz with a constant $M$, we also see that
\[ \omega_f(\bar{x},t) \leq M t_+ \]
where $t_+ = t$ if $t \geq 0$ and $0$ otherwise.

Let $\bar{x}$ be a point on the ``$-g$ side'' of the hyperplane
and $y$ be its projection on the hyperplane.
\begin{multicols}{2}
  Since $f(y) \geq f(x)$, we see that
  \begin{align*}
    f(x) - f(\bar{x})
    & \leq f(y) - f(\bar{x})\\
    & \leq w_f(\bar{x}, v_f(\bar{x};x))\\
    & \leq M v_f(\bar{x};x)\\
  \end{align*}
  For $\bar{x}$ on the other side, $f(x) - f(\bar{x}) \leq 0$
  so for a general $\bar{x}$,
  \[ f(x) - f(\bar{x}) \leq M (v_f(\bar{x};x))_+. \]

  \begin{tikzpicture}[x=.5cm,y=.5cm]
    \draw[blue, thick, ->] (3,-2) to (3.8,-0.8);
    \draw[red, thick] (-6,4) to (6,-4);
    \node[below] at (3, -2) {$x$};
    \filldraw (3,-2) circle (.05);
    \node[above] at (0, 0) {$y$};
    \filldraw (0,0) circle (.05);
    \node[below] at (-2, -3) {$\bar{x}$};
    \filldraw (-2,-3) circle (.05);
    \draw (-2,-3) -- (0,0);
    \draw (-2,-3) circle (3.6);
  \end{tikzpicture}
\end{multicols}

We can now formalize our idea of localization set.
Let $\{x_i\}_{i=0}^\infty$ be a sequence in $Q$.
Define
\[ S_k = \{\, x \in Q \mid \langle g(x_i), x_i - x \rangle \geq 0, i = 0, \ldots, k \,\}. \]
We call this set the \emph{localization set} generated by the sequence $\{x_i\}_{i=0}^\infty$.

Let
\begin{align*}
  v_i & = v_f(x^*; x_i)\\
  v_k^* & = \min_{0 \leq i \leq k} v_i = \max\{\, r | B_2(x^*, r) \subseteq S_k \,\},\\
  f_k^* & = \min_{0 \leq i \leq k} f(x_i).
\end{align*}
Then
\[ f_k^* - f^* \leq \omega_f(x^*, v_k^*) \leq M v_k^*. \]

% TODO v_i = v_f(x^*;x_i)

\subsection{Infinite dimensional approach}
\subsubsection{Unconstrained Lower complexity bound}
Let us consider the class of function $f$, such that $x^*$ exists, $r_0 \leq R$ for some $R$ and
$f$ is Lipschitz continuous on $B_2(x_0,R)$, with the constant $M$, i.e. $f_{B_2(x_0,R)} \in \mathcal{F}_M^{0,0}$.

For any $k$, $0 \leq k \leq n-1$, there exists a function of this class such that
\begin{equation}
  \label{eq:unconstrainednonsmoothbound}
  f(x_k) - f^* \geq \frac{MR}{2(1+\sqrt{k+1})}.
\end{equation}
for a method generating a sequence $\{x_k\}$ such that
\[ x_k \in x_0 + \Lin\{g(x_0), \ldots, g(x_{k-1})\}. \]
As for the bound of smooth convex function, this lower bound is uniform.
However, as expected, it is much slower.
In Section~\cite[Section~2.3]{nesterov1998introductory},
methods are given for nonsmooth problems that are faster than this bound.
This is because they are allowed to use the specific structure of the objective.

\subsection{Subgradient methods}
The subgradient methods is simply the gradient method with the normalized subgradient instead of the gradient.
Indeed for non-smooth functions, the norm of the subgradient $\|g(x)\|$ is not very informative
so we use $g(x)/\|g(x)\|$ instead of $g(x)$.

Let us first analyse the subgradient method for the problem
\[ \min_{x \in Q} f(x) \]
where $Q$ is a simple closed convex set (simple means that we can compute the projection in $Q$ easily).

\begin{algorithm}
  \caption{Gradient method for a nonsmooth convex function $f$ and simple constraints represented by a simple closed convex set $Q$.}
  \label{algo:gradientmethodnonsmooth}
  \begin{algorithmic}
    \STATE Choose $x_0 \in \Rn$ and a sequence $\{h_k\}_{k=0}^\infty$ such that $h_k \to 0$.
    \FOR{$k = 0, 1, 2, 3, \ldots$}
      \STATE Compute $f(y_k)$ and $g(y_k)$.
        Set
        \[ x_{k+1} = \pi_Q\Big(x_k - h_k\frac{g(x_k)}{\|g(x_k)\|}\Big). \]
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

If $r_0 \leq R$ and $f_{B_2(x^*,R)} \in \mathcal{F}_M^{0,0}$, then
we have at each iteration
\begin{equation}
  \label{eq:subgraddec}
  r_{i+1}^2 \leq r_i^2 - 2h_iv_i + h_i^2.
\end{equation}

Indeed
\begin{align*}
  r_{i+1}^2
  & = \Big\| \pi_Q\Big(x_i - h_i\frac{g(x_i)}{\|g(x_i)\|}\Big) - x^* \Big\|^2\\
  & \stackrel{\eqref{eq:projsqr}}{\leq} \Big\| x_i - h_i\frac{g(x_i)}{\|g(x_i)\|} - x^* \Big\|^2\\
  & = \la x_i - x^* - h_i\frac{g(x_i)}{\|g(x_i)\|}, x_i - x^* - h_i\frac{g(x_i)}{\|g(x_i)\|} \ra\\
  & = \la x_i - x^*, x_i - x^* \ra - 2 \la x_i - x^*, h_i\frac{g(x_i)}{\|g(x_i)\|} \ra + \frac{h_i^2}{\|g(x_i)\|} \la g(x_i), g(x_i) \ra\\
  & = r_i^2 - 2h_iv_i + h_i^2.
\end{align*}

Summing \eqref{eq:subgraddec} for all $i$ gives
\begin{equation}
  \label{eq:subgradconv}
  f_k^* - f^* \leq M\frac{R^2 + \sum_{i=0}^k h_i^2}{2 \sum_{i=0}^k h_i}.
\end{equation}

If we know a priori that we want to do a fixed number of steps $N$,
we know that we need to take a constant step-size
\[ h_i = \frac{R}{\sqrt{N+1}}, \quad i = 0, \ldots, N \]
to minimize the bound.
This gives
\[ f_N - f^* \leq \frac{MR}{\sqrt{N+1}}. \]
Comparing this with the lower bound~\eqref{eq:unconstrainednonsmoothbound},
we see that the subgradient is optimal uniformly in the dimension $n$.

\subsubsection{Realistic step-size}
If we know the desired accuracy $\epsilon$, we can fix $N$.
However, we often converge a lot faster than be bound so it is inefficient in practice to do all the $N$ steps.
In practice we rather check for the accuracy $f_k - f^*$ using $f_k - f_{k+1} \approx f_k - f^*$ and stop
when it is smaller than $\epsilon$.
Also in practice, since we do not know $R$, we cannot use it in the expression for $h_k$.
If we use
\[ h_i = \frac{r}{\sqrt{i+1}}, \quad i = 0, \ldots \]
we can
upper bound the sum $\sum_{i=0}^{k} \frac{1}{i+1}$ by $1+\int_0^{k} \frac{1}{x+1} \dif x = 1+\ln(k+1)$ and
lower bound the sum $\sum_{i=0}^{k} \frac{1}{\sqrt{i+1}}$ by $\int_0^{k+1} \frac{1}{\sqrt{x+1}} \dif x = 2(\sqrt{k+2}-1)$.
Therefore we have
\[ f_k^* - f^* \leq M\frac{R^2+r^2+r^2\ln(k+1)}{4r(\sqrt{k+2}-1)}. \]
This rate of convergence can be classified as \emph{sub-optimal} rate.

\subsubsection{Square summable but not summable step-size}
When we analyse \eqref{eq:subgradconv} and the step-size $h_i = \frac{r}{\sqrt{i+1}}$ we can
see that as $k \to \infty$, both the numerator and the denominator tends to infinity because
$\sum_{i=0}^k h_i^2$ and $\sum_{i=0}^k h_i$ both diverge.
We say that the sequence $\{h_i\}$ is neither square summable nor summable.

However, if we take $h_i = \frac{r}{i+1}$, we can see that $\sum_{i=0}^k h_i^2$ converges.
As $k \to \infty$, it is therefore obvious that the fraction tends to 0.
We say that the sequence $\{h_i\}$ is square summable but not summable.

Again, we can
upper bound the sum $\sum_{i=0}^k \frac{1}{(i+1)^2}$ by $1+\int_0^k \frac{1}{(x+1)^2} \dif x = 2 - \frac{1}{k+1}$ and
lower bound the sum $\sum_{i=0}^{k} \frac{1}{i+1}$ by $\int_0^{k+1} \frac{1}{x+1} \dif x = \ln(k+2)$.
Therefore we have
\[ f_k^* - f^* \leq M\frac{R^2+2r^2-\frac{r^2}{k+1}}{2r\ln(k+2)}. \]


\subsubsection{Polyak step-size}
When $f^*$ is known, we can do better.
First notice that by definition of the subgradient, if $y$ is the projection of $x^*$ on the hyperplane, we have
\[ f(x^*) - f(y) \geq \la g(x_k), x^* - y \ra \]
so
\begin{equation}
  \label{eq:polyakdist}
  v_k = v_f(x^*,x_k) = v_f(x^*,y) = \frac{1}{\|g(x_k)\|}\la g(x_k), y - x^* \ra \geq \frac{f(y) - f(x^*)}{\|g(x_k)\|} \geq \frac{f(x_k) - f(x^*)}{\|g(x_k)\|}.
\end{equation}

From \eqref{eq:subgraddec}, we deduce that
\[ r_{k+1}^2 \leq r_k^2 - 2 \frac{h_k}{\|g(x_k)\|}(f(x_k) - f(x^*)) + h_k^2. \]
If we minimize this expression in $h_k$ we find
\[ h_k = \frac{f(y) - f(x^*)}{\|g(x_k)\|}. \]
Note that we have already normalized $g(x_k)$ but we divide again by its norm.

With this value of $h_k$ we have
\[ r_{k+1}^2 \leq r_k^2 - \frac{(f(x_k) - f(x^*))^2}{\|g(x_k)\|^2}. \]
If we sum, we get
\[ \sum_{i=0}^k \frac{(f(x_k) - f^*)^2}{\|g(x_k)\|^2} \leq R^2. \]
Since we know that $\|g(x_k)\| \leq M$, we have
\[ \sum_{i=0}^k (f(x_k) - f^*)^2 \leq M^2R^2. \]
Since the sum is bounded, of course, we should have $f(x_k) \to f^*$.
However we see that we also have
\[ f_k^* - f^* \leq \frac{MR}{\sqrt{k+1}}. \]

\begin{multicols}{2}
  With \eqref{eq:polyakdist}, we have seen that
  $v_k \geq \frac{f(x_k) - f(x^*)}{\|g(x_k)\|}$.
  We take exactly $h_k = \frac{f(x_k) - f(x^*)}{\|g(x_k)\|}$,
  what does that mean ?
  What if we take $h < h_k$ ?
  In that case, we have $x_{k+1} = x_k - h g/\|g(x_k)\|$
  and the definition of the subgradient gives
  $f(x_{k+1}) > f(x^*)$.

  \begin{center}
    \begin{tikzpicture}[x=8cm,y=4cm]
      \draw[thick, domain=-.2:.5] plot
      (\x, {(\x)^2-1/4});
      \draw[blue, thick, ->] (.5,0) to (.6,.2);
      \draw[thick, domain=.5:.7] plot
      (\x, {3*(\x)-1.5});
      \filldraw (.5,-.25) ellipse (.005 and .01);
      \filldraw (3/8,-.25) ellipse (.005 and .01);
      \node[below] at (3/8,-.25) {$x_{k+1}$};
      \node[right] at (.5,-.25) {$x_k$};
      \draw[thick,->] (.5,-.25) to (3/8,-.25);
      \draw[dashed] (0,-.25) to (.5,-.25);
      \draw[dashed, blue] (.3,-.4) to (.7,.4);
    \end{tikzpicture}
  \end{center}
\end{multicols}

\begin{multicols}{2}
  We know that $x^*$ is not in the ``$+g$ side'' of the hyperplane passing through $x_k$.
  Once we now $f(x^*)$, we have just see that it is also not in the ``$+g$ side''
  of the hyperplane passing through $x_k - h_k g/\|g(x_k)\|$.
  The Polyak step simply consists in following $-g$ until the border of the half space
  containing $x^*$.

  \begin{center}
    \begin{tikzpicture}[x=.5cm,y=.5cm]
      \draw[blue, thick, ->] (0,0) to (.8,1.2);
      \draw[dashed, blue] (-3,-4.5) to (2,3);
      \draw[red, thick] (-3,2) to (3,-2);
      \draw[red, thick] (-5,-1) to (1,-5);
      \filldraw (0,0) circle (.05);
      \node[left] at (0, 0) {$x_k$};
      \filldraw (-2,-3) circle (.05);
      \node[right] at (-2,-3) {$x_{k+1}$};
      \node[below] at (-1,-.8) {$f(x_{k+1}) > f(x^*)$};
      \node[below] at (-4,-3) {$x^*$};
      \filldraw (-4,-3) circle (.05);
      \node[below] at (.7, 2.5) {$f(x_{k+1}) > f(x_k)$};
    \end{tikzpicture}
  \end{center}
\end{multicols}

\subsubsection{Not simple constraints}
If the constrained are more complicated,
we split them in a simple bounded closed convex set $Q$ and $m$ more complicated constraints $f_j$
\[ \min_{x \in Q} \{\, f(x) \mid f_j(x) \leq 0, j = 1, \ldots, m  \,\}. \]
Let
\[ \bar{f}(x) = \big(\max_{1 \leq j \leq m} f_j(x) \big)_+. \]
We can see that at the solution $x^*$, we have $\bar{f}(x) = 0$. % FIXME why ?
% TODO give the intuition
\begin{algorithm}
  \caption{Gradient method for constrained optimization with a nonsmooth convex function $f$.}
  \label{algo:gradientmethodconstrainednonsmooth}
  \begin{algorithmic}
    \STATE Choose $x_0 \in \Rn$ and a sequence $\{h_k\}_{k=0}^\infty : h_k = \frac{R}{\sqrt{k+0.5}}$.
    \FOR{$k = 0, 1, 2, 3, \ldots$}
      \STATE Compute $f(x_k)$, $g(x_k)$, $\bar{f}(x_k)$ and $\bar{g}(x_k)$ and set
        \[
          p_k =
          \begin{cases}
                 g (x_k), & \text{if }\bar{f}(x_k) < \|\bar{g}(x_k)\|h_k,\\
            \bar{g}(x_k), & \text{if }\bar{f}(x_k) \geq \|\bar{g}(x_k)\|h_k.
          \end{cases}
        \]
        Set
        \[ x_{k+1} = \pi_Q\Big(x_k - h_k\frac{p_k}{\|p_k\|}\Big). \]
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

If $r_0 \leq R$ and $f_{B_2(x^*,R)} \in \mathcal{F}_{M_1}^{0,0}$ and
\[ M_2 = \max_{1 \leq j \leq m} \{\|g\|: g \in \partial f_j(x), x \in B_2(x^*,R)\}, \]
then for any $k \geq 3$ there exists a number $i'$, $0 \leq i' \leq k$ such that
\begin{align*}
  f(x_{i'}) - f^* \leq \frac{\sqrt{3}M_1R}{\sqrt{k-1.5}},\\
  \bar{f}(x_{i'}) \leq \frac{\sqrt{3}M_2R}{\sqrt{k-1.5}}.
\end{align*}
Note that here we do not use the notation $f_k^*$ instead of $f(x_{i'})$ to show that
the bound is for the same $i'$ for $f$ and $\bar{f}$.

We can see that in the constrained and unconstrained case we are optimal even if the lower complexity bound
was obtained in the unconstrained case.
This means that from the viewpoint of analytical complexity, the general unconstrained minimization problems are not easier than the constrained ones.

The simplest method we have tried is already optimal.
In general, that indicates that the problems of our class are too complicated to be solved efficiently.
However, our conclusion is valid uniformly in the dimension of the problem.

The analytical complexity of the gradient method does not depend on $n$ but the complexity of one iteration depends on $n$.
Obviously, the gradient method does not work on infinite dimension,
however it does not make use of the fact there is finite number of dimension.
If we take the dimension factor into account in a proper way, we get more efficient schemes.

\subsection{Finite dimensional approach}
Let's suppose that we want to solve the problem
\[ \min_{x \in Q} f(x) \]
where here we do not assume that $Q$ is simple.
We have $Q = \{\, x \in \Rn \mid f_j (x) \leq 0, j = 1, \ldots, m \,\} = \{\, x \in \Rn \mid \bar{f} (x) = 0 \,\}$
where $\bar{f}$ is the max of the $f_j$ as defined in the previous section.

The key idea is very simple.
Let's say we have a point $x \in \Rn$
\begin{itemize}
  \item If $x \notin Q$, $\bar{f}(x) > 0$.
    Let $\bar{g}(x)$ be a subgradient of $\bar{f}$ at $x$.
    We have seen that the part of $\Rn$ in the ``$+g$ side'' of the hyperplane orthogonal to $\bar{g}(x)$ and passing through $x$ only contains points for which
    $\bar{f}(\bar{x}) \geq \bar{f}(x)$.
    Therefore we know that $Q$ (and $x^* \in Q$) is in the ``$-g$ side''.
    In other words, $\mathcal{H}(-g, \la -g, x \ra)$ is supporting for $Q$.
  \item If $x \in Q$, let $g(x)$ be a subgradient of $f$ at $x$.
    If $g = 0$, that means that $x = x^*$.
    Otherwise, we have seen that $x^*$ is in the ``$-g$ side'' of the hyperplanes orthogonal to $g(x)$ and passing through $x$.
\end{itemize}
In both case, we can use the hyperplane $\mathcal{H}(-g, \la -g, x \ra)$ to split search space in 2.

\subsubsection{Lower complexity bound}
Let's consider the simpler problem of finding a point in $Q$ and a new oracle that for a test point $x \in \Rn$
\begin{itemize}
  \item either reports that $x \in Q$
  \item or returns a vector $\bar{g}$, separating $x$ from $Q$:
    \[ \la \bar{g}, x - \bar{x} \ra \quad \forall \bar{x} \in Q. \]
\end{itemize}

If there exists a point $x^* \in Q$ such that $B_2(x^*, \epsilon) \subseteq Q$ for some $\epsilon > 0$ (looks like the Slater condition),
and we start with the localization set $B_\infty(0,R)$ ($Q \subseteq B_\infty(0,R)$),
the lower analytical complexity bound for finding a point $x^* \in Q$ is $n \ln \frac{R}{2\epsilon}$ call to the oracle
and the lower analytical complexity bound for finding the point $x^*$ minimizing a function $f \in \mathcal{F}_M^{0,0}(B_\infty(0,R))$
in $Q$ is $n \ln \frac{MR}{8\epsilon}$ call of the oracle.

\subsubsection{Cutting Plane Schemes}
Again we assume that $\int Q \neq \emptyset$ and we denote its diameter by $D < \infty$.

We will works with two sequences of sets $\{E_k\}_{k=0}^\infty$ and $\{S_k\}_{k=0}^\infty$.
The set $E_k$ is the localization set obtained with the following algorithm.
\begin{algorithm}
  \caption{Cutting Plane Scheme.}
  \label{algo:cuttingplane}
  \begin{algorithmic}
    \STATE Choose a bounded set $E_0 \supseteq Q$.
    \FOR{$k = 0, 1, 2, 3, \ldots$}
      \STATE Choose $y_k \in E_k$
      \IF{$y_k \in Q$}
      \STATE Compute $f(y_k)$ and $g(y_k)$.
      \STATE Set $g_k = g(y_k)$.
      \ELSE
      \STATE Compute $\bar{g}(y_k)$ which separates $y_k$ from $Q$.
      \STATE Set $g_k = \bar{g}(y_k)$.
      \ENDIF
      \STATE Choose $E_{k+1} \supseteq \{\, x \in E_k \mid \la g_k, y_k - x \ra \geq 0 \,\}$.
    \ENDFOR
  \end{algorithmic}
\end{algorithm}
We are never sure that $E_k \subseteq Q$ so we can always have $y_k \notin Q$.
Note that if $y_k \notin Q$ we do not compute $f(y_k)$ because it is not feasible.
Also, if $y_k \notin Q$, the hyperplane is supporting for $Q$ so it does not reduce the part of $Q$ in $E_k$.
Let $\{x_i\}$ be the sub-sequence of $\{y_k\}$ with the points in $Q$.
We can define the sequence $\{S_i\}$ with $S_0 = Q$ and
\[ S_{i+1} = \{\, x \in S_i \mid \la g(x_i), x_i - x \ra \geq 0 \,\}. \]
We can see that $S_{i(k)} = E_k \cap Q$ where $i(k)$ is the number of points in $Q$ in the sequence $\{y_j\}_{j=0}^k$.
The sequence $\{S_i\}$ is useful to analyse the rate of convergence of the sequence $\{f(x_i)\}$ to $f^*$.
Denote $v_i = v_f(x^*;x_i)$ and $v_i^* = \min_{0\leq j \leq i} v_j$.
We have, for any $k$ such that $i(k) > 0$,
\[ v_{i(k)}^* \leq D \Big[\frac{\vol_n S_{i(k)}}{\vol_n Q}\Big]^{\frac{1}{n}} \leq \Big[\frac{\vol_n E_k}{\vol_n Q}\Big]^{\frac{1}{n}}. \]

The problem of this result is the following:
How do we ensure that $i(k) > 0$ ? What prevents $i(k)$ from being zero ?
Actually, $i(k) = 0$ means we that we do not have cut in $Q$ yet so
$\vol_n E_k < \vol_n Q$ implies that $i(k) > 0$.
Therefore if we ensure that $\vol_n E_k \to 0$ and that $\vol_n Q > 0$ we know that we will have an $y_k \in Q$.
To ensure that $\vol_n Q > 0$, we need to assume that there exists some $\rho > 0$ and $\bar{x} \in Q$ such that
\[ B_2(\bar{x},\rho) \subseteq Q. \]
With functionnal constraints, it is enough to respect the Slater condition.


Historically, the first nonsmooth minimization method implementing the idea of cutting planes was the \emph{center of gravity} method.
The idea is to take $y_k$ as the center of gravity of $E_k$.
It turns out that if we do so, whatever $g$ is, the volume of $E_k$ is multiplied by $1-1/e$ at each iteration and
if $f_{B_2(x^*,D)} \in \mathcal{F}_M^{0,0}$ then for any $k \geq 0$, % FIXME why not impose that i(k) > 0 ?
\[ f_k^* - f^* \leq MD\Big(1 - \frac{1}{e}\Big)^{\frac{-k}{n}}. \]
Remember that $f_k^*$ is computed only among the points $y_k \in Q$.
Comparing this with the lower complexity bound, we see that the center of gravity method is optimal in finite dimension.
However, we are comparing the analytical complexity.
Computing the center of gravity in multi-dimensional space is actually harder than the initial problem.

There are two common alternatives.
\paragraph{Ellipsoid Method}
The idea of the Ellipsoid method is to use Ellipsoidal $E_k$ because it is easy to find the center of gravity of Ellipsoids.
\begin{multicols}{2}
  It is also easy to find the smallest Ellipsoid (volume-wise) that contains the part of the previous Ellispoid at the ``$-g$ side''
  of the hyperplane.
  \begin{align*}
    E_k     & = \{\, x \in \Rn \mid \la H_k^{-1}(x-y_k), x-y_k \ra \leq 1 \,\},\\
    y_{k+1} & = y_k - \frac{1}{n+1} \cdot \frac{H_kg_k}{\la H_kg_k, g_k \ra^{1/2}},\\
    H_{k+1} & = \frac{n^2}{n^2-1} \Big(H_k - \frac{2}{n+1} \cdot \frac{H_kg_kg_k^TH_k}{\la H_kg_k, g_k \ra}\Big).
  \end{align*}

  \begin{tikzpicture}[x=1.5cm,y=1.5cm]
    \draw[thick]         ( 0.5963, 0.1491) ellipse (2 and 1);
    \draw[blue,thick,->] ( 0.5963, 0.1491) to (1.0963,0.6491);
    \draw[red,thick]     (-0.9037, 1.6491) to (2.0963,-1.3509);
    \draw[dashed,thick,rotate=-23.4238]
                         ( 0,      0) ellipse (1.6724 and 0.9206);
    \filldraw            ( 0.5963, 0.1491) circle (.02);
    \node[below] at      ( 0.5963, 0.1491) {$y_k$};
    \node[above] at      ( 2.2,    0.8) {$H_k$};
    \filldraw            ( 0,      0) circle (.02);
    \node[below] at      ( 0,      0) {$y_{k+1}$};
    \node[above] at      (-1.2,    1) {$H_{k+1}$};
  \end{tikzpicture}
\end{multicols}

If $f_{B_2(x^*,R)} \in \mathcal{F}_M^{0,0}$ then for any $k$ such that $i(k) > 0$, % FIXME why R not D ?
\[ f_{i(k)}^* - f^* \leq MR\Big(1 - \frac{1}{(n+1)^2}\Big)^{\frac{k}{2}} \Big[\frac{\vol_n B_2(x_0,R)}{\vol_n Q}\Big]^{\frac{1}{n}}. \]

As we have seen, to ensure that $i(k) > 0$ for some $k$, we need to ensure that $\vol_n Q > 0$.
If there exists some $\rho > 0$ and $\bar{x} \in Q$ such that
\( B_2(\bar{x},\rho) \subseteq Q, \)
we have $\vol_n Q \geq \rho^n$.
Since $\vol_n B_2(x_0,R) = R^n$, we have
\[ f_{i(k)}^* - f^* \leq \frac{1}{\rho} M R^2 \Big(1 - \frac{1}{(n+1)^2}\Big)^{\frac{k}{2}}. \]
As usual, using the fact that $1-x \leq \exp(-x)$, we have
\[ f_{i(k)}^* - f^* \leq \frac{1}{\rho} M R^2 \exp\Big(-\frac{k}{2(n+1)^2}\Big). \]
Note that for unconstrained optimization,
since we need for the reasoning to work that $Q \subseteq E_0$,
and $E_0 = B_2(x_0,R)$ it is as if $Q = B_2(x_0,R)$ so we take $\rho = R$.

Each iteration only take $\bigoh(n^2)$ arithmetic operations and the method needs
\[ 2(n+1)^2 \ln \frac{MR^2}{\rho \epsilon} \]
calls of the oracle to generate an $\epsilon$ solution for $r_0 = R$ and $Q \supseteq B_2(\bar{x},\rho)$ for some $\bar{x} \in Q$ or
\[ 2(n+1)^2 \ln \frac{MR}{\epsilon} \]
for $Q = \Rn$.

It is not optimal but it has polynomial dependence on $\ln 1/\epsilon$ and the class parameters $M,R,\rho$.
For problem classes for which the oracle has polynomial complexity,
such algorithms are called weakly polynomial.

\paragraph{Inscribed Ellipsoid Method}
The point $y_k$ is chosen as the center of the maximal inscribed ellipsoid $W_k \subseteq E_k$

\paragraph{Analytic Center Method}
The point $y_k$ is chosen as the minimum of the \emph{analytical barrier}
\[ F_k(x) = -\sum_{j=1}^{m_k} \ln(b_k-\la a_j,x \ra). \]

\paragraph{Volumetric Center Method}
The point $y_k$ is chosen as the minimum of the \emph{volumetric barrier}
\[ V_k(x) = \ln \det F_k''(x). \]
where $F_k$ is the analytical barrier defined above.

The three last methods are polynomial with the complexity
\[ n \Big(\ln \frac{1}{\epsilon}\Big)^p \]
where $p$ is eighter 1 or 2.
However, at each iteration we need to run an Interior Point Method to find the $y_k$ which takes $n^3$--$n^4$ arithmetic operations.

\section{Structural Programming}
%In Structural programming, we do not consider constraints as black blox.
%We use the specificity in their structure to remplace them by barriers in the objective.
%The remaining objective is solved using the Newton['s]? scheme.
We are interested in the problem
\begin{align*}
  \min & f(x)\\
  f_j(x) & \leq 0, \quad j = 1, \ldots, m.
\end{align*}
where $f$ and $f_j$ are convex for $j = 1, \ldots, m$.

The convex programming schemes for solving this problem are based on the Black Box concept.
These black box provide information of the functions $f$ and $f_j$ at a point $x$.
These black boxes are local and they are the only information available for the numerical method.

However, we know that those functions are convex.
Since checking convexity is harder than minimizing if we do not use the structure of the functions.
Indeed what we usually do is check that the functions are convex using their structure,
e.g. they are the sum of convex functions
and then give to the method only a black box.
Why don't we allow the method to see the structure too ?

The idea to enable the method to have a better view of the problem is through a mediator.
We select a set of problems for which the method is very efficient and the standard theory poorly describe this efficiency.
In the case of the Newton's method, those are the self-concordant functions.
We then try to see how to solve problem using it.
In this case, we can construct self-concordant barriers from some functional constraints.

Another good example is Cholesky factorization.
The set of problmes for which the the method is very efficient is triangular matrices.
Then we look at the problems that can be reduced to triangular matrices.

We can see that the performance Newton method is poorly described in the standard theory for self-concordant functions.
First we the norm us use depends on the coordinate system while the Newton method do not depend ont he coordinate system.
If we apply it on $\phi(y) = f(Ay)$ and we take $x_0$ to solve $f$ and $y_0 = A^{-1}x_0$ to solve $\phi$ then
we generate the sequences $x_k$ and $y_k$ such that $y_k = A^{-1}x_k$.
We say that the Newton method is \emph{affine invariant}.
The problem is that in our definition of second order Lipschitz
\[ \|f''(x) - f''(y)\| \leq M \|x-y\|, \]
which can be rewritten as
\[ \la f'''(x)[u]v,v \ra \leq M \|u\| \cdot \|v\|^2 \]
if $f \in C^3(\Rn)$,
the right-hand side is not not affine invariant.

If we fix the definition of second order Lipschitz continuitity, we get the definition of self-concordant functions.

\subsection{Self-concordant functions}
Let
\begin{align*}
  \|u\|_x      & = \la  f''(x)      u    , u     \ra^{1/2},\\
  \|u\|_x^*    & = \la [f''(x)]^{-1}u    , u     \ra^{1/2},\\
  \lambda_f(x) & = \la [f''(x)]^{-1}f'(x), f'(x) \ra^{1/2}.
\end{align*}
The norm $\|u\|_x$ is called the \emph{local norm} of direction $u$ with respect to $x$
and $\lambda_f(x)$ is called the \emph{local norm of the gradient $f'(x)$}
We have the trivial properties
\begin{align*}
  |\la v, u \ra| & \leq \|v\|_x^* \cdot \|u\|_x,\\
   \lambda_f(x)  & = \|f'(x)\|_x^*.
\end{align*}

We have the following definition of self-concordant functions.
\begin{mydef}
  Let $f \in C^3(\dom f)$ be a closed convex function with open domain.
  Then all the following conditions are equivalents:
  \begin{itemize}
    \item $f$ is \emph{self-concordant} with constant $M_f \geq 0$.
    \item The inequality
      \[ D^3f(x)[u,u,u] \leq M_f \|u\|_x^3 \]
      holds for any $x \in \dom f$ and $u \in \Rn$.
    \item The inequality
      \[ |D^3f(x)[u_1,u_2,u_3]| \leq M_f \|u_1\|_x \|u_2\|_x \|u_3\|_x \]
      holds for any $x \in \dom f$ and $u_1,u_2,u_3 \in \Rn$.
  \end{itemize}
\end{mydef}

We have the following properties
\begin{myprop}
  Let the function $f_1,f_2$ is self concordant with constant $M_1,M_2$ respectively.
  \begin{itemize}
    \item If $\phi(x) = f_1(\mathcal{A}(x))$ where $\mathcal{A}(x) = Ax + b$ then
      $\phi$ is self-concordant with constant $M_\phi = M_1$
    \item If $\phi(x) = \alpha f_1(x) + \beta f_2(x)$ then
      $\phi$ is self-concordant with constant
      \[ M_\phi = \max\Big\{ \frac{1}{\sqrt{\alpha}}M_1, \frac{1}{\sqrt{\beta}}M_2 \Big\}. \]
  \end{itemize}
\end{myprop}

Since $f \in C^3(\dom f)$ is convex we know that $f''(x) \succeq 0$ but we will be able to ensure that $f''(x) \succ 0$ if $\dom f$ contains no straight line (even half-line) thanks to the following theorem.
\begin{mytheo}
  Let $f$ be self-concordant.
  If $\dom f$ contains no straight line,
  then the Hessian $f''(x) \succ 0$ is nondegenerate at any $x \in \dom f$.
\end{mytheo}

We can also see that for any self-concordant function (not just the self-concordant barriers that we will construct),
the value of the function goes to infinity at the border of its domain thanks to \cite[Theorem~4.1.4]{nesterov2004introductory}.

\subsubsection{Standard self-concordant functions}
Let us define the \emph{Dikin} ellipsoid of function $f$ at $x$:
\begin{align*}
  W^0(x;r)     & = \{\, y \in \Rn \mid \|y-x\| < r \,\},\\
  W^0(x;r)     & = \{\, y \in \Rn \mid \|y-x\| \leq r \,\} = \cl(W^0(x;r)).
\end{align*}

Consider the function
\[ \phi(t) = \frac{1}{\|u\|_{x+tu}}. \]
We can see that $\phi'(t) \leq M_f/2$ so $\phi(t) \geq \phi(0) - M_f/2|t|$.
If $\phi(t) > 0$, $\|u\|_{x+tu} < \infty$ so $x + tu \in \dom f$.
Therefore for
\[ |t| < \frac{2}{M_f} \phi(0) = \frac{2}{M_f\|u\|x} \]
we know that $x + tu \in \dom f$.
Since $\|tu\|_x = |t| \cdot \|u\|_x$, we see that $x+tu \in \dom f$ for $\|tu\| \leq 2/M_f$.

In conclusion, $W^0(x;2/M_f) \subseteq \dom f$.
It is the first most important fact that forms the basis for almost all consequent results.
The second one is the following.
For all $x \in \dom f$ and $y \in W(x;r)$ with $r < M_f/2$,
we have second most important fact, it is the ``new \eqref{eq:lipschitzsecondeig}'':
\[ (1-r)^{M_f}f''(x) \preceq f''(y) \preceq \frac{1}{(1-r)^{M_f}}f''(x) \]

We have seen that we change change $M_f$ at will just by scaling the output of $f$.
To make our life easier we would like $M_f = 2$ so that the result becomes
$W^0(x;1) \subseteq \dom f$.
We call them standard self-concordant functions.

For $M_f = 2$, we get the ``new \eqref{eq:lipschitzsecongG}'':
\[ \Big(1-r+\frac{r^3}{3}\Big) f''(x) \preceq G(x,y) \preceq \frac{1}{1-r}f''(x) \]
where
\[ G(x,y) = \int_0^1 f''(x + \tau(y-x)) \dif \tau. \]

Let us define the convex functions $\omega$ and $\omega_*$
\begin{multicols}{2}
  \begin{align*}
    \omega  (t) & =  t - \ln(1+t),\\
    \omega_*(t) & = -t - \ln(1-t)  =  \omega(-t),
  \end{align*}
  we have
  \begin{align*}
    \omega'  (t) & = \frac{t}{1+t},\\
    \omega_*'(t) & = \frac{t}{1-t} = -\omega'(-t).
  \end{align*}
  We see on the graph that $\omega'$ and $\omega_*'$ are the symmetric with respect to the identity
  so they are inverse to each other.
  For $t \geq 0$ and $0 \leq \tau < 1$,
  \begin{align*}
    \omega'(\omega_*'(\tau)) & = \tau, & \omega_*'(\omega'(t)) & = t.
  \end{align*}

  \begin{tikzpicture}[x=1.5cm,y=1.5cm]
    \draw[thick, domain=-.96:3] plot
    (\x, {(\x) - ln(1 + (\x))});
    \node[above] at ( 2,1) {$w$};
    \draw[thick, domain=-3:.96] plot
    (\x, {-(\x) - ln(1 - (\x))});
    \node[above] at (-2,1) {$w_*$};
    \draw[blue, thick, domain=-.7:3] plot
    (\x, {(\x)/(1+(\x))});
    \node[above] at (-2,-.7) {$w_*'$};
    \draw[blue, thick, domain=-3:.7] plot
    (\x, {(\x)/(1-(\x))});
    \node[below] at (2,.7) {$w'$};
    \draw[dashed, ->] (-3,0) to (3,0);
    \draw[dashed, ->] (0,-2) to (0,3);
  \end{tikzpicture}
\end{multicols}

We also have
\[ \omega(t) + \omega_*(\tau) \geq \tau t, \]
and
\begin{align*}
  \omega  (t   ) & = \max_{0 \leq \xi < 1}[\xi t    - \omega_*(\xi)], &
  \omega_*(\tau) & = \max_{0 \leq \xi < 1}[\xi \tau - \omega  (\xi)],\\
  \omega  (t   ) & = t    \omega  '(t   ) - \omega_*(\omega  '(t   )), &
  \omega_*(\tau) & = \tau \omega_*'(\tau) - \omega  (\omega_*'(\tau)),\\
\end{align*} % TODO conjugate

We have the following definition of standard self-concordant functions.
\begin{mydef}
  Let $f \in C^3(\dom f)$ be a closed convex function with open domain.
  Then all the following conditions are equivalents:
  \begin{itemize}
    \item $f$ is \emph{a standard self-concordant} function.
    \item $f$ is self-concordant with constant $M_f = 2$.
    \item The inequality
      \[ \omega'(\|y-x\|_x) \leq \|y-x\|_y \]
      holds for any $x,y \in \dom f$.
    \item The inequality
      \[ \|y-x\|_y \leq \omega_*'(\|y-x\|_x) \]
      holds for any $x \in \dom f$ and $y \in W^0(x;1)$.
    \item The inequality
      \[ \frac{\|y-x\|_x^2}{1+\|y-x\|_x} \leq \la f'(y)-f'(x), y-x \ra \]
      holds for any $x,y \in \dom f$.
    \item The inequality
      \[ \la f'(y)-f'(x), y-x \ra \leq \frac{\|y-x\|_x^2}{1-\|y-x\|_x} \]
      holds for any $x \in \dom f$ and $y \in W_0(x;1)$.
    \item The inequality
      \[ f(x) + \la f'(x), y-x \ra + \omega(\|y-x\|_x) \leq f(y) \]
      holds for any $x,y \in \dom f$.
    \item The inequality
      \[ f(y) \leq f(x) + \la f'(x), y-x \ra + \omega_*(\|y-x\|_x) \]
      holds for any $x \dom f$ and $y \in W_0(x;1)$.
  \end{itemize}
\end{mydef}

We also have the inequalites % TODO is it an equivalent definition ?
\begin{itemize}
  \item The inequality
    \[ f(x) + \la f'(x), y-x \ra + \omega(\|f'(y)-f'(x)\|_y^*) \leq f(y) \]
    holds for any $x,y \in \dom f$.
  \item The inequality
    \[ f(y) \leq f(x) + \la f'(x), y-x \ra + \omega_*(\|f'(y)-f'(x)\|_y^*) \]
    holds for any $x \dom f$ and $y \in W_0(x;1)$.
\end{itemize}

\subsection{Minimizing the self-concordant function}
We will see now how to minimize self-concordant function when $\dom f$ contains no straight line.
\[ \min_{x \in \dom f} f(x). \]

If $\lambda_f(x) < 1$ for some $x \in \dom f$, then the solution of this problem (i.e. global minimum of $f$) exists and is unique.
This condition cannot be strengthened, $\lambda_f(x) = 0$ is not enough (see \cite[Example~4.1.2]{nesterov2004introductory}).

\subsubsection{Standard Newton Method}
For the standard Newton method
\[ x_{k+1} = x_k - [f''(x_k)]^{-1}f'(x_k), \]
we could use four ways to measure the convergence:
$f(x_k) - f(x_f^*)$, $\lambda_f(x_k)$, $\|x_k - x_f^*\|_{x_k}$ or $r_*(x_k) = \|x_k - x_f^*\|_{x_k^*}$.
We can see that they are all equivalents if $\lambda_f(x_k) < 1$ and $r_*(x) < 1$~\cite[Theorem~4.1.13]{nesterov2004introductory} so we will use $\lambda_f(x_k)$.

If $\lambda_f(x_k) < 1$ then
\[ \lambda_f(x_{k+1}) \leq [w_*(\lambda_f(x_k))]^2. \]
We can see that $\{\lambda_f(x_k)\}$ forms a relaxation sequence if $[w_*(\lambda_f(x_k))]^2 \leq \lambda_f(x_k)$,
or equivalently (since $\lambda_f(x_k) < 1$) $\lambda_f(x_k) \leq \bar{\lambda} \eqdef 2 - \varphi$ where $\varphi = \frac{1+\sqrt{5}}{2}$ is the golden ratio.
Since it forms a relaxation sequence, it converges.
We can see that if $\lambda < \bar{\lambda}$,
\[ \lambda_f(x_{k+1}) \leq \left(\frac{\lambda_f(x_k)}{1-\lambda_f(x_k)}\right)^2 \leq \frac{1}{(1-\bar{\lambda})^2}\lambda_f(x_k)^2, \]
so it converges quadratically.
Note that if we just know that $\lambda_f(x_k) < 1$, we cannot say that it has quadratic convergence (even if we can isolate a constant as we have just done) because we are not sure that it converges because we are not sure that if forms a relaxation sequence.
For example, if $\lambda_f(x_k) = 0.9$, the inequality just says $\lambda_f(x_{k+1} \leq 81$.

For $\lambda_f(x_k) \leq 2 - \varphi$, we have
\[ \lambda_f(x_{k+1}) \leq \frac{1}{(\varphi-1)^2} [\lambda_f(x_k)]^2 \leq 2.62 [\lambda_f(x_k)]^2 \]
and for $\lambda_f(x_k) \leq 1 - 1/\sqrt{2}$, we have
\[ \lambda_f(x_{k+1}) \leq 2 [\lambda_f(x_k)]^2. \]
When $\lambda_f(x_k)$ gets close to 0, the constant in front of $[\lambda_f(x_k)]^2$ tends to 1.

However we can see that the convergence we have guaranteed is very local so we need another method.

\subsubsection{Damped Newton Method}
For the damped Newton method
\[ x_{k+1} = x_k - \frac{1}{1+\lambda_f(x_k)}[f''(x_k)]^{-1}f'(x_k). \]

Here we have 2 convergence results.
One very useful globally
\[ f(x_{k+1}) \leq f(x_k) - \omega(\lambda_f(x_k)) \]
and another that show that the rate of convergence is quadratic when $\lambda_f(x_k) < 1$
\[ \lambda_f(x_{k+1}) \leq 2 [\lambda_f(x_k)]^2. \]

\subsubsection{Combining the two}
We have seen that the Damped Newton Method is also quadratic but
for $\lambda_f(x_k) \leq 2-\varphi$, the standard Newton method starts to be interesting and
from $\lambda_f(x_k) \leq 1 - 1/\sqrt{2}$, it is faster than the Damped Newton Method.

The optimization should therefore be done in two stages
\begin{description}
  \item[First stage]
    $\lambda_k(x_k) \geq \beta$, where $\beta \leq \bar{\lambda}$, we apply the damped Newton Method.
    $\{f(x_k)\}$ forms a relaxation sequence:
    \[ f(x_{k+1}) \leq f(x_k) - \omega(\beta) \]
    and the number of steps is bounded by
    \[ N \leq \frac{1}{\omega(\beta)}[f(x_0) - f(x_f^*)]. \]
  \item[Second stage]
    When $\lambda_k(x_k) < \beta$, we apply the standard Newton Method.
    $\{\lambda_k(x_k)\}$ forms a relaxation sequence and converge quadratically to 0.
\end{description}

\biblio
\end{document}
