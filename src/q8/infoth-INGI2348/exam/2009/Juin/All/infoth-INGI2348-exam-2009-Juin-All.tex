\documentclass[en]{../../../../../../eplexam}

%\DeclareMathOperator{\gcd}{gcd}

\hypertitle{Information theory and coding}{8}{INGI}{2348}{2009}{Juin}
{John de Wasseige \and Beno\^it Legat}
{Benoît Macq, Jérôme Louveaux and Olivier Pereira}

\paragraph{Discussion link}
\url{http://www.forum-epl.be/viewtopic.php?t=6731}

\section{BM-1}
\begin{solution}
  Tout d'abord on décrit l'allure de $p(x)$.
  On dira que $p(x)$ vaut $\alpha$ entre $0$ et $D/2$
  et $2\alpha/5$ entre $-D/2$ et $0$.
  \[
    \int_{\frac{-D}{2}}^{\frac{D}{2}} p(x) \dif x
    = \left(\frac{2\alpha}{5} + \alpha \right) \frac{D}{2} = 1
  \]
  et donc 
  \[
    \alpha = \frac{10}{7D}
  \]
  Ensuite, on calcule $\sigma_x$ et $\sigma_\epsilon$.
  \[
    \mu_x = \int_{\frac{-D}{2}}^{\frac{D}{2}} x p(x) \dif x = \frac{3D}{28}
  \]
  et donc
  % integrate ( (x-m)^2*(4/(7*D)) dx) from -D/2 to 0; m=3*D/28 
  % integrate ( (x-m)^2*(10/(7*D)) dx) from 0 to D/2; m=3*D/28
  \[
    \sigma_x^2 = \int_{\frac{-D}{2}}^{\frac{D}{2}} (x - \mu_x)^2 p(x) \dif x
    = \frac{349 D^2}{8232} + \frac{485 D^2}{16464} = \frac{169 D^2}{2352}
  \]
  
  Même raisonnement pour l'erreur de quantization
  % integrate (x+q)*(4/(7*D))*dx from -D/2 to -D/6; q = D/3
  % integrate (x)*(4/(7*D))*dx from -D/6 to 0; q = D/3
  % integrate (x)*(10/(7*D))*dx from 0 to D/6; q = D/3
  % integrate (x-q)*(10/(7*D))*dx from D/6 to D/2; q = D/3
  \begin{align*}
    \mu_\epsilon
    =& \int_{\frac{-D}{2}}^{\frac{-D}{6}} (x + q) \frac{4}{7D} \dif x
    + \int_{\frac{-D}{6}}^{0} x \frac{4}{7D} \dif x \\
    &+ \int_0^{\frac{D}{6}} x \frac{10}{7D} \dif x    
    + \int_{\frac{D}{6}}^{\frac{D}{2}} (x - q) \frac{10}{7D} \dif x \\
    =&~0 + \frac{-D}{126} + \frac{5 D}{252} + 0 = \frac{D}{84}
  \end{align*}
  et
  % integrate (x+q - m)^2*(4/(7*D))*dx from -D/2 to -D/6; q = D/3; m = D/84
  % integrate (x - m)^2*(4/(7*D))*dx from -D/6 to 0; q = D/3; m = D/84
  % integrate (x - m)^2*(10/(7*D))*dx from 0 to D/6; q = D/3; m = D/84
  % integrate (x-q - m)^2*(10/(7*D))*dx from D/6 to D/2; q = D/3; m = D/84
  \begin{align*}
    \sigma_\epsilon^2 =& 
    \int_{\frac{-D}{2}}^{\frac{-D}{6}} (x + q - \mu_\epsilon)^2 \frac{4}{7D} \dif x
    + \int_{\frac{-D}{6}}^{0} (x - \mu_\epsilon)^2 \frac{4}{7D} \dif x \\
    &+ \int_0^{\frac{D}{6}} (x - \mu_\epsilon)^2 \frac{10}{7D} \dif x    
    + \int_{\frac{D}{6}}^{\frac{D}{2}} (x - q - \mu_\epsilon)^2 \frac{10}{7D} \dif x \\
    =&~\frac{199 D^2}{111132} + \frac{241 D^2}{222264}
    + \frac{785 D^2}{444 528} + \frac{995 D^2}{222264}
    = \frac{7743599 D^2}{723691584}
  \end{align*}
  
  Il reste à calculer les $\tilde{p}$.
  \begin{align*}
    \tilde{p}(-1) &= \int_{\frac{-D}{2}}^{\frac{-D}{6}} \frac{4}{7D} \dif x 
    = \frac{4 D^2}{21} \\
    \tilde{p}(0) &= \int_{\frac{-D}{6}}^{0} \frac{4}{7D} \dif x 
    + \int_{0}^{\frac{D}{6}} \frac{10}{7D} \dif x 
    = \frac{7 D^2}{21} \\
    \tilde{p}(1) &= \int_{\frac{D}{6}}^{\frac{D}{2}} \frac{10}{7D} \dif x
    = \frac{10 D^2}{21} 
  \end{align*}
  
  Finalement,
  \[
    H(X) = -\sum_{i=-1}^1 p(i)\log_2(p(i))
  \]
  ce qui nous donne en substituant un point $(H, \sigma_\epsilon^2/\sigma_x^2)$
  de la courbe débit-distorsion.
\end{solution}

\section{BM-2}
\begin{solution}
  \begin{enumerate}
    \item
      We have
      \[
        \begin{array}{lccc}
          & 1 & 2 & 3\\
          \Pr[X = \cdot] & 1/4 & 1/2 & 1/4\\
          \Pr[Y = \cdot] & 3/8 & 3/8 & 1/4
        \end{array}
      \]
      so
      \begin{align*}
        H(X)
        & = \log_2(4) - 1/2 \log_2(2)\\
        & = 3/2\\
        H(Y)
        & = \log_2(8) - 6/8 \log_2(3) - 1/4\log_2(2)\\
        & = 11/4 - 3/4 \log_2(3)
      \end{align*}

      We have
      \[ H(X,Y) = 3/4 \log_2(4) + 1/4 \log_2(8) = \frac{9}{4}. \]
      so
      \begin{align*}
        H(X|Y) & = H(X,Y) - H(Y)\\
               & = 3/4 \log_2(3) - 1/2\\
        H(Y|X) & = H(X,Y) - H(X)\\
               & = 4/3\\
        I(X;Y) & = H(X) - H(X|Y)\\
               & = 2 - \frac{3}{4} \log_2(3).
      \end{align*}
    \item We can design an Huffman code.
      \[
        \begin{array}{ll}
          (1,2) & 00\\
          (2,1) & 01\\
          (2,3) & 10\\
          (3,1) & 110\\
          (3,2) & 111\\
        \end{array}
      \]
      We can see that each time one bit does a choice one half/one half so the rate is optimal $R = H(X,Y) = 9/4$.
  \end{enumerate}
\end{solution}

\section{JL-1}
\begin{solution}
  \begin{enumerate}
    \item $k=1$ and $n = 2$ so $R = 1/2$.
    \item A code is systematic if the $k$-bits input is
      represented in clear in $k$ bits of the output.
      This is not the case here since we do not have $x_{i,t} = u_t$ for $i = 1$ or $2$.
    \item
      \[ G(D) =
        \begin{pmatrix}
          1 + D^3 &
          1 + D^4
        \end{pmatrix}.
      \]
    \item Using the Euclide algorithm, we have
      \begin{align*}
        \gcd(1+D^3, 1+D^4)
        & = \gcd(1+D, 1+D^3)\\
        & = \gcd(1+D^2, 1+D)\\
        & = 1+D
      \end{align*}
      since $1+D^3 = (1+D)(1+D+D^2)$ and $1+D^4 = (1+D)(1+D+D^2+D^3)$.
      Since $1+D$ is not a power of $D$,
      the code is catastrophic.

      Another way to justify it is simply to see that with $u(D) = \frac{1}{1+D} = 1 + D + D^2 + D^3 + \cdots$,
      \[ \underline{x}(D) = u(D)G(D) =
        \begin{pmatrix}
          1+D+D^2 &
          1+D+D^2+D^3
        \end{pmatrix}
      \]
      $w(u(D)) = \infty$ and $w(\underline{x}(D)) = 7$.
    \item
      Two encoders $G(D)$ and $G'(D)$ are \emph{equivalent}
      if they generate the same code.

      We know that they are equivalent iff thers is
      a $k \times k$ nonsingular matrix $Q(D)$ of fractions of polynomials such that
      \[ G'(D) = Q(D) G(D). \]
      Here $k = 1$ so $Q(D)$ is $1 \times 1$.
      If we take $Q(D) = 1/(1+D)$ we get
      \[ G'(D) =
        \begin{pmatrix}
          1+D+D^2 &
          1+D+D^2+D^3
        \end{pmatrix}.
      \]
      and
      \begin{align*}
        \gcd(1+D+D^2, 1+D+D^2+D^3)
        & = \gcd(1, 1+D+D^2)\\
        & = 1.
      \end{align*}
      Since $1$ is a power of $D$ it is non-catastrophic.
      Actually since we get 1, it is a \emph{basic} encoder.
    \item Which one is he talking of ? $G$ or $G'$ ?
      Since the free distance is the minimum of the weight of
      the possible input, if we find one of weight 4 it proves that the minimum is smaller than 4.

      For $G$ it is obvious with the input $u(D) = 1$,
      we have the output
      \(
      \underline{x}(D) =
      \begin{pmatrix}
        1 + D^3 &
        1 + D^4
      \end{pmatrix}
      \)
      which has a weight 4.
      For $G'$, we use the input $u(D) = 1+D$ to get
      the same output.
  \end{enumerate}
\end{solution}

\section{JL-2}
\begin{solution}
  \begin{enumerate}
    \item C'est équiprobable donc $H(U) = \log_2(4) = 2$.
    \item
      We have for $f$ then $g$
      \[
        \begin{array}{l|cccccccc}
          & 0 & \alpha^7 & \alpha  & \alpha^3  & \alpha^2  & \alpha^6  & \alpha^4  & \alpha^5\\
          & 0 & 1 & \alpha  & \alpha+1  & \alpha^2  & \alpha^2+1  & \alpha^2+\alpha  & \alpha^2+\alpha+1\\
          \hline
          0 & 0.9 & 0.1\\
          1 & 0.1 & 0.8 & 0.1\\
          a & &  & 0.8 & 0.1 & 0.1\\
          b & &  &  & 0.1 & 0.8 & 0.1\\
          \hline
          0 & 0.9 & 0.1\\
          1 & &  & 0.8 & 0.1 & 0.1\\
          a & &  & & & & & 0.8 & 0.2\\
          b & & 0.1 & &  & 0.1 & 0.8\\
        \end{array}
      \]

      \begin{itemize}
        \item
          For $f$ we get
          the probabilities $1/4,9/40,9/40,2/40,9/40,9/40,1/40$ for $Y$
          so
          % \begin{align*}
          %   H(Y)
          %   & = \log_2(40) - \frac{1}{4}\log_2(10) - \frac{9}{10} \log_2(9)\\
          %   & = \frac{11}{4} + \frac{3}{4}\log_2(5) - \frac{9}{5} \log_2(3) \approx 1.64.
          % \end{align*}
          \[
            H(Y) = 
            - \frac{10}{40} \log_2(\frac{10}{40}) 
            - 3 \frac{9}{40} \log_2(\frac{9}{40}) 
            - \frac{2}{40} \log_2(\frac{2}{40}) 
            - \frac{1}{40} \log_2(\frac{1}{40})
            \approx 2.3
          \]
          For $H(Y|U)$ we get
          \begin{align*}
            H(Y|U)
            & = \frac{1}{4}(\log_2(10) - \frac{9}{10} \log_2(9))
              + \frac{3}{4}(\log_2(10) - \frac{8}{10} \log_2(8))\\
              & = 1 + \log_2(5) - \frac{9}{5} - \frac{9}{20} \log_2(3)\\
              & = \frac{-4}{5} + \log_2(5) - \frac{9}{20} \log_2(3) \approx 0.81.
          \end{align*}
          Therefore we have
          \[ H(U,Y) = H(U) + H(Y|U) = \frac{6}{5} + \log_2(5) - \frac{9}{20} \log_2(3) \approx 2.81 \]
          Alternatively, to avoid computing $H(Y|U)$, one can immediately compute 
          \[ H(U,Y) = 
          - \frac{9}{40} \log_2(\frac{9}{40}) 
          - 7 \frac{1}{40} \log_2(\frac{1}{40}) 
          - 3 \frac{8}{40} \log_2(\frac{8}{40})
          \approx 2.81 \]
          Then
          % \[ H(U|Y) = H(U,Y) - H(Y) = \frac{-31}{20} + \frac{1}{4} \log_2(5) + \frac{27}{20} \log_2(3) \approx 1.17 \]
          \[ H(U|Y) = H(U,Y) - H(Y) \approx 0.51 \]
          so
          % \[ I(U;Y) = H(U) - H(U|Y) = \frac{71}{20} - \frac{1}{4} \log_2(5) - \frac{27}{20} \log_2(3) \approx 0.83. \]
          \[ I(U;Y) = H(U) - H(U|Y) \approx 1.49 \]
        \item
          First
          \[
            H(Y) = 
            - \frac{9}{40} \log_2(\frac{9}{40}) 
            - 3 \frac{2}{40} \log_2(\frac{2}{40}) 
            - 3 \frac{8}{40} \log_2(\frac{8}{40}) 
            - \frac{1}{40} \log_2(\frac{1}{40})
            \approx 2.659
          \]
          then
          \[ H(U,Y) = 
          - \frac{9}{40} \log_2(\frac{9}{40}) 
          - 5 \frac{1}{40} \log_2(\frac{1}{40}) 
          - 3 \frac{8}{40} \log_2(\frac{8}{40})
          - \frac{2}{40} \log_2(\frac{2}{40})
          \approx 2.759 \]
          Therefore
          \[ H(U|Y) = H(U,Y) - H(Y) \approx 0.1 \]
          and finally
          \[ I(U;Y) = H(U) - H(U|Y) \approx 1.9 \]
      \end{itemize}
      The best one is the one with the maximal value of $I(U;Y)$ because if the information about $U$ that $Y$ got.
      Equivalently we can pick the one with the minimal value of $H(U|Y)$ because $I(U;Y) = H(U) - H(U|Y)$ and $H(U)$ is the same for both.
      
      The best one is thus the \emph{second} one, 
      intuitively confirmed by the fact that we allow
      more values in the Galois field to be taken.
    \item Because it has no memory, $Y_i$ is independent of $U_j$ for $j \neq i$ so
      $H(V|X) = 4 \cdot H(U|Y) \approx 4 \cdot 1.17 = 4.68$.
  \end{enumerate}
\end{solution}

\section{OP-1}
\nosolution


\end{document}
