\clearpage{}
\section{Describe and compare different strategies for integration testing,
and the need for stubs and drivers. Discuss the case of testing
object-oriented designs. Cite software tools that can be used for program
testing and analysis. Describe fault estimation using fault seeding and by
independent test groups.}

\subsection{Drivers and stubs}

How can you test a function that depends on other functions? 
$\rightarrow$ Use test drivers and stubs.

\begin{tabular}{m{12cm}m{5cm}}
\begin{description}
    \item[Driver] a program that calls a component (passes a test case to it).
    \item[Stub] a program that is called by a component (simulates the activity of a missing component), i.e.\ function which simulates the behaviour of the called function\ldots
\end{description}
&
\begin{tikzpicture}[node distance=0.5cm]
    \node[draw, rectangle] (d) {Driver};
    \node[draw, rectangle] (t) [below=of d] {Tested component};
    \node[draw, rectangle] (s) [below=of t] {Stub};

    \draw (d) edge[->] node[right] {calls} (t);
    \draw (t) edge[->] node[right] {calls} (s);
    \end{tikzpicture}
    \end{tabular}

\textbf{Never modify a component to support testing!} Drivers and stubs are separate programs.

\subsection{Strategies for integration testing}

Consider the component hierarchy
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{strategies_integration_testing.png}
\end{figure}

\subsubsection{Bottom-up}

Useful if many general-purpose utility routines, reused components at lowest level. \newline
Needs drivers.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{bottom_up.png}
\end{figure}


\begin{itemize}
        \proitem{} Suitable for object-oriented programs
    \consitem The top-level components are tested last (most important, may reveal design bugs)
\end{itemize}

\subsubsection{Top-down}
Test the top-level, controlling component first.
Needs stubs.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{top_down.png}
\end{figure}

\begin{itemize}
        \proitem{} Allows test by external function
    \proitem{} Major design faults or issues revealed early
    \proitem{} Drivers not needed
        \consitem{} Stubs can be difficult to develop, affects validity of the test
\end{itemize}

\subsubsection{Modified top-down}
Test components individually before integrating components

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{modified_top_down.png}
\end{figure}

\begin{itemize}
        \proitem{} Avoids large number of stubs
        \consitem{} Needs stubs and drivers
\end{itemize}

\subsubsection{Bing-bang}
Everything integrated in one shot.
For small systems only.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{bing_bang.png}
\end{figure}

\begin{itemize}
        \consitem{} Needs drivers and stubs
        \consitem{} Faults are hard to localize
        \consitem{} Interface faults are hard to distinguish
\end{itemize}

\subsubsection{Sandwich}
Combine bottom-up and top-down, converge to target middle layer.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.35\linewidth]{sandwich.png}
\end{figure}

\begin{itemize}
        \proitem{} Early test of top layer
        \consitem{} No stubs for utility components in bottom layer
\end{itemize}

\subsubsection{Modified sandwich}
Test upper-level components before merging.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.35\linewidth]{modified_sandwich.png}
\end{figure}

\subsubsection{Comparison}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{comparison_stategies_integration_testing.png}
\end{figure}

All except bottom-up need stubs!
All except top-down need drivers!
\FloatBarrier

\subsection{Testing object-oriented designs}
Unit testing is easier but integration testing is more extensive. \newline

Ex: Dynamic binding, complex interface, polymorphism, inheritance are aspect related to OO which cause difficulties during test.

(Inheritance need to retest all new subclass with new data)

\subsection{Software tools for program testing and analysis}

\subsubsection{Automated testing tools}

\begin{itemize}
\item \textbf{Test execution}: planning and running of the tests
\begin{itemize}
    \item Capture and replay (keystrokes)
    \item Generating stubs and drivers
    \item Automated testing environments (database, measurement, analysis, simulation,\ldots)
\end{itemize}

\item \textbf{Test case generators}:
\begin{itemize}
    \item Structural (structure of source code)
    \item Functional (specifications)
    \item Random
\end{itemize}
\end{itemize}

\subsubsection{Code analysis tools}

\begin{itemize}
\item \textbf{Static analysis}: analyse the source code
\begin{itemize}
    \item Code analyser
    \item Structure checker
    \item Data analyser
    \item Sequence checker
    \item Measurements: paths, fan-in/fan-out, decision points,\ldots
    \item Program provers
\end{itemize}

\item \textbf{Dynamic analysis}: execute the program
\begin{itemize}
    \item Program monitoring (coverage)
    \item Instrumentation, breakpoints
\end{itemize}
\end{itemize}

\subsection{Fault estimation}

How to estimate how many faults remain? (\textbf{Help us to know WHEN to stop tests}) 

\begin{itemize}
        \item \textbf{Fault seeding}: Intentionally insert faults in the program:
\begin{itemize}
    \item A known number $S$
    \item Same kind and complexity as actual faults
\end{itemize}

Use the number of discovered seeded faults s and the number of discovered actual faults $n$ to estimate the number of actual faults $N$.

Hypothesis: same effectiveness $\rightarrow n / N = s / S \rightarrow N = S \times n / s$


\item \textbf{Independent Test Groups}: Split tests in two groups E1,E2
    (E1, E2 with same effectiveness on faults detection).

    Used if we don't know typical faults to insert.

\begin{itemize}
	\item n1 faults detected by E1
	\item n2 faults detected by E2
	\item n12 faults detected by both E1 and E2
	\item N faults in total
\end{itemize} 

Hypothesis : effectiveness of E1 is the same on all faults as on faults
detected by E2
$$ n1/N=n12/n2 \rightarrow N = n1\times n2/n12$$
\end{itemize}

