\clearpage{}
\section{Discuss issues with testing safety-critical systems. Describe the
principles of safety cases and FMEA analysis. Define and discuss the merits
of software diversity and formal verification.}


\subsection{Issues with testing safety-critical systems}

A safety-critical system is a system in which a failure can harm or kill
people (e.g.\ airplane, nuclear plant, medical device, etc.). \newline

Safety-critical systems demand \textbf{very high reliability}:
${10}^{-9}$ failures per hour = once per 100 000 years. 
$\Rightarrow$ It cannot be tested directly!

\begin{itemize}
    \item Difficult to build it:
        \begin{itemize}
            \item Formal verification techniques: proof are hard and
                may have errors
            \item Design diversity (same system with different teams
                and designs, voting scheme): still common faults due
                to common techniques and approaches
        \end{itemize}

    \item Six-sigma quality requirement does not apply for software (ok
        for hardware): 
        \begin{itemize}
            \item Software process has uncontrollable variability 
            \item no tolerances, interaction faults
            \item Defect rate are irrelevant for copies of the
                same software
        \end{itemize}
\end{itemize}


\subsection{Principles of safety cases and FMEA analysis}

\subsubsection{Safety case}

We can decompose the safety goals and assign failure rates or
constraints to each component of the design, so that satisfying each
lower-level goal will “roll up” to allow us to meet safety goals for the
entire system. In this way, we make a safety case for the system, making
explicit the ways in which our software meets performance goals for
safety-critical systems. \newline

\textbf{In the slide:} A structured argument, supported by evidence, intended to justify that a system is 
acceptably safe. \newline

\subsubsection{Failure Modes and Effects Analysis (FMEA)}

A failure mode, is a situation which contains risks. FMEA, analyse the
failures mode (e.g.\ by using a fault tree), to look after some unknown
system effect. A system effect is a hazardous situation which cans lead
to a crash. By default (because we don’t know) we considers this
situation as a crash. \newline

FMEA is highly labour-intensive and based on the experience of the
analysts. It usually involves our performing an initial analysis of the
software design, abstracting modes that might lead to failures. Then we
look at how combinations of the basic failure modes might lead to actual
failures. \newline

Summary: Identify failure modes, analyse system effects.

\subsection{Merits of software diversity and formal verification}

\subsubsection{Design diversity}

The same system is built according to the same requirements
specification but in several independent ways, each according to a
different design. Each system runs in parallel with the others, and a
voting scheme coordinates actions when one system’s results differ from
the others’. \newline

The underlying assumption is that it is unlikely that at least three of
the five groups of developers will write incorrect software for a given
requirement. \newline

However, there is empirical evidence suggesting that independently
developed software versions will not fail independently; diverse designs
do not always offer reliability higher than of a single version.

\subsubsection{Formal verification}

Idea: usage of formal verification techniques with our requirements,
designs, and code. \newline 

But formal evaluation of natural language is impossible, and important
information may be lost if we translate natural language to mathematical
symbols. Even formal proofs of specification and design are not
infallible, because mistakes are sometimes made in the proofs.
