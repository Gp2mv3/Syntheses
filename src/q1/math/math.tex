\input{../../lib.tex}

\usepackage[hideerrors]{xcolor}
\usepackage{array}
\usepackage{fancybox}
\usepackage{float}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{qtree}
\usepackage{tensor}
\usepackage{multirow}

\DeclareMathOperator{\Sur}{Sur}
\DeclareMathOperator{\In}{In}
\DeclareMathOperator{\newnull}{null}
\DeclareMathOperator{\newim}{Im}
\DeclareMathOperator{\newker}{Ker}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\newrang}{rang}
\DeclareMathOperator{\sev}{sev}

\hypertitle{Mathématique}{1}{1101}{Guillaume François et Benoît Legat}
{Guillaume François \and Benoît Legat}

\part{Analyse}
% y^
%  |    LYSE
%  | ANA
%  +--------->x
\section{Fondements}
\subsection{Démonstrations}
\begin{itemize}
  \item Implication
  \item Contraposition
  \item Equivalence
  \item Contradiction (absurde)
  \item Récurrence (induction)
\end{itemize}
\subsection{Relations}
\begin{itemize}
  \item Réflexive ($xRx$)
  \item Symétrique ($xRy \Rightarrow yRx$)
  \item Transitive
    \footnote{$\land$ est le ``et'' logique}
    ($xRy \land yRz \Rightarrow xRz$)
  \item Antisymétrique ($xRy \land yRx \Rightarrow x = y$)
\end{itemize}
$ $\\
\begin{itemize}
  \item Equivalence $\Leftrightarrow$ Symétrique, réflexive, transitive
  \item Ordre partiel $\Leftrightarrow$ Antisymétrique, réflexive, transitive
  \item Orde total $\Leftrightarrow$ Ordre partiel et $\forall x, y \in A$
    on a $xRy$ OU $yRx$
\end{itemize}

\section{Limite et continuité}
\subsection{Limite $L$ au point $a$}
Soit une fonction $f$ définie sur un intervalle centré en $a$.
\[ \lim_{x \to a} f(x) = L \]
si et seulement si
$\forall \varepsilon > 0, \exists \delta > 0$ tel que
$\forall x \in A$, si $|x - a| \leq \delta$ alors
$|f(x) - L| \leq \varepsilon$.

\subsection{Continuité au point $a$}
Une fonction $f$ est continue au point $a$ si et seulement si
\[ \lim_{x \to a} f(x) = f(a) \]

\subsection{Dérivabilité au point $a$}
$f : A \to \mathbb{R}$ est dérivable en $a$ si et seulement si
\[ \lim_{x \to a} \frac{f(x) - f(a)}{x - a} \]
existe.

\subsection{Théorème des valeurs intermédiaires}
Soit $f : [a,b] \to \mathbb{R}$ continue.

Si $f(a) \leq y \leq f(b)$ ou $f(b) \leq y \leq f(a)$
$\exists c \in [a,b]$ tel que $y = f(c)$.

\subsection{Théorème des bornes atteintes}
Une fonction continue sur un intervalle fermé, bornée atteint ses bornes.
C'est à dire qu'il existe $q, p \in [a,b]$ tel que
$f(q) = \sup f$ sur $[a,b]$
et que $f(p) = \inf f$ sur $[a,b]$.

\subsection{Théorème de Rolle}
Soit $f$ continue sur $[a,b] \to \mathbb{R}$ et dérivable sur $]a,b[$.

Si $f(a) = f(b)$ alors il existe un $c \in ]a,b[$ tel que $f'(c) = 0$.

\subsection{Théorème des accroissements finis}
Soit $f$ continue sur $ [a,b] \rightarrow \mathbb{R}$ et dérivable sur $]a,b[$.

Alors il existe un $c \in ]a,b[$ tel que $f'(c) = \frac{f(b) - f(a)}{b - a}$.

\subsection{Théorème de la valeur constante}
Soit $f$ continue sur $ [a,b] \rightarrow \mathbb{R}$.

Si $f$ est dérivable sur $]a,b[$ et que
$f'(c) = 0$ $\forall x \in ]a,b[$ alors $f$ est constante.

\section{Polynôme de Taylor}
\subsection{Définition}
Etant donné une fonction $f : A \rightarrow \mathbb{R}$,
un naturel $n \geq 1$ et un point $a$ appartenant à \emph{l'intérieur}
de $A$ en lequel $f$ est $n$ fois dérivable,
le polynôme de Taylor d'ordre $n$ de $f$ autour du point $a$ est:
\[ T_n^{f,a}(x) = \sum_{k = 0}^n \frac{f^{(k)}(a)}{k!}(x - a)^k \]

\subsection{Théorème de Taylor}
Soit une fonction $f : A \rightarrow \mathbb{R}$,
un point $a$ appartenant à \emph{l'intérieur} de $A$,
et un naturel $n \geq 1$.

Si $f$ est $n$ fois dérivable en $a$, alors:
\[ \lim_{\substack{x \rightarrow a}}
\frac{f(x) - T_n^{f,a}(x)}{(x - a)^n} = 0 \]

\subsection{Réciproque de Taylor}
Soit une fonction $f : A \rightarrow \mathbb{R}$, un point $a$ appartenant
à \emph{l'intérieur} de $A$, et un naturel $n \geq 1$.

Si $f$ est $n$ fois dérivable en $a$, et si
$P_n(x)$ est un polynôme de degré inférieur ou égal à $n$, vérifiant
\[ \lim_{\substack{x \rightarrow a}} \frac{f(x) - P_n(x)}{(x - a)^n} = 0 \]
alors
\[ P_n(x) = T_n^{f,a}(x) \]

\subsection{Théorème du reste}
Soit une fonction $f : A \rightarrow \mathbb{R}$,
un point appartenant à \emph{l'intérieur} de $A$,
un intervalle ouvert $I$ tel que $a \in I$ et $I\subset A$
et un naturel $n \geq 1$.

Si $f$ est $n + 1$ fois dérivable sur $I$,
alors pour $\forall x \in I \backslash \{a\}$,
il existe un point $c$ compris strictement entre $a$ et $x$ tel que
\[ f(x) = T_n^{f,a}(x) + \frac{f^{n+1}(c)(x - a)^{n+1}}{(n + 1)!} \]

\subsection{Dérivée de Taylor}
\[ \left(T_n^{f,a}\right)'(x) =
\sum_{k = 0}^n \frac{f^{(k+1)}(a)(x - a)^{k}}{k!} \]

\section{Intégration}
\subsection{Implications}
\begin{itemize}
  \item Si $f$ est intégrable alors $f$ est bornée;
  \item Si $f$ est continue alors elle est intégrable.
\end{itemize}

\subsection{Théorème de la moyenne}
Soit une fonction continue donc intégrable de $[a,b] \rightarrow \mathbb{R}$.
Alors $\exists c \in [a,b]$ tel que $f(c) = \mu{(f)}$ c'est-à-dire
\[ \int_a^b f(x) dx = (b - a)f(c) \]

\subsection{Théorème fondamental (1)}
Soit une fonction continue sur$[a,b] \to \mathbb{R}$ et $p \in [a,b]$,
alors la fonction $F : [a,b] \to \mathbb{R} : x \mapsto F(x)$
telle que
\[ F(x) = \int_p^x f(t) dt \]
est une primitive de $f$.

\subsection{Théorème fondamental (2)}
Soit $f$ continue $f : I \to \mathbb{R}$,
si $F$ est une primitive de $f$ sur $I$, alors
\[ \int_p^q f(t) dt = F(q) - F(p) \]
avec $p, q \in I$.

\subsection{Corollaire du théorème fondamental}
Soient $I$ et $J$, 2 intervalles,
$f$ une fonction continue de $I \to \mathbb{R}$ et
2 fonctions $U : J \to \mathbb{R}$ et
$V : J \to \mathbb{R}$ dérivables,
telles que $U(J) \subset I$ et $V(J) \subset I$.

La fonction
\[ H : J \to \mathbb{R} :
x \mapsto H(x) = \int_{U(x)}^{V(x)} f(t) dt \]
est dérivable et sa dérivée est donnée par
\[ H'(x) = f(V(x)).V'(x) - f(U(x)).U'(x) \]

En effet, par le théorème fondamental,
\[ H(x) = F(V(x)) - F(U(x)) \]
où $F$ est une primitive de $f$.

\section{Suites et séries}
\subsection{Convergence d'une série}
On définit la somme partielle $S_n$ des $a_n$ jusque $n$ comme suit
\[ S_n \eqdef \sum_{k = 0}^n a_k \]

On remarque que
\[ \sum_{n = 0}^{\infty} a_n = \lim_{n \to \infty} S_n \]
Dès lors, on dit que
\begin{itemize}
  \item Si la suite $S_n$ diverge, alors la série $\sum a_n$ diverge;
  \item Si la suite $S_n$ converge vers un valeur,
    alors la série $\sum a_n$ converge vers cette même valeur.
\end{itemize}

Seulement, pour appliquer cette définition, il nous faut calculer $S_n$.
Et c'est seulement possible dans des cas particuliers comme les
séries géométriques et les séries téléscopantes.

C'est pourquoi on a d'autres tests qui nous permettent de déterminer si
$\sum a_n$ converge ou pas même s'ils ne nous permettent pas de savoir
vers quel valeur elle converge.

\subsection{Séries téléscopantes}
Une série est dite \emph{téléscopante} lorsque
ses sommes partielles se simplifient entre elles.

Par exemple, si on arrive à trouver une suite $b_n$ telle que
$a_n = b_n - b_{n+1}$, on a
$S_n = b_1 - b_{n+1}$.

Exemple à retenir:
\[ \sum_{n = 1}^{\infty} \frac{1}{n(n + 1)} =
\sum_{n =1}^{\infty} \left(\frac{1}{n} - \frac{1}{n + 1}\right) \]

\subsection{Séries géométriques}
Une suite géométrique est une suite telle que $\exists a, r \in \mathbb{R}$
tels que
\[ a_n = ar^{n-1} \]
$r$ est appelé la \emph{raison} de la suite.
L'avantage est qu'on connait la somme d'une suite géométrique, elle vaut
Une somme infinie de suite géométrique est comme suit
\[ S_n = a \frac{r^n - 1}{r - 1} \]

Dès lors, soit la série suivante
\[ \sum_{n = 1}^{\infty} ar^{n-1} \]
\begin{itemize}
  \item Si $a = 0$,
    \begin{itemize}
      \item La série converge vers 0.
    \end{itemize}
  \item Sinon,
    \begin{itemize}
      \item Si $|r| < 1 \rightarrow$ La série converge vers $\frac{a}{1 - r}$;
      \item Si $r \geq 1 \rightarrow$ La série converge vers $\pm \infty$
        en fonction du signe de $a$;
      \item Si $r \leq -1 \rightarrow$ La série diverge.
    \end{itemize}
\end{itemize}

\subsection{Combinaisons linéaires}
Si deux séries sont convergentes alors
toutes leurs combinaisons linéaires sont convergentes.
\[ \sum_{n=0}^{\infty} (\alpha a_n + \beta b_n)
= \alpha\sum_{n=0}^{\infty}a_n + \beta\sum_{n=0}^{\infty}b_n \]

\subsection{Convergence absolue}
\begin{itemize}
  \item
    La série $\sum a_n$ est absolument convergente
    lorsque $\sum |a_n|$ converge;
  \item
    $\sum a_n$ est absolument convergente
    $\Rightarrow$ $\sum a_n$ est convergente;
  \item
    La réciproque n'est pas vraie !
\end{itemize}

\subsection{Test de divergence}
Si $\lim_{n \to \infty}a_n \neq 0$ où n'existe pas,
alors la série $\sum_{n = 1}^{\infty}a_n$ diverge.

\subsection{Test de la série alternante}
Si $\exists N_0 \in \mathbb{R}_{> 0}$ tel que
ces trois conditions soient respectées
$\forall n \geq N_0$
\begin{itemize}
  \item $a_na_{n+1} < 0$
  \item $|a_{n+1}| \leq |a_{n}|$
  \item $\lim_{n\to\infty}a_n = 0$
\end{itemize}
alors $\sum a_n$ converge.

\subsection{Test de l'intégrale}
Soit $(a_n)$ une suite à termes positifs.
On suppose que $(a_n) = f(n)$, où $f$ est une fonction continue,
positive, décroissante $\forall x > N_0$.

Alors la série $\sum_{n =N_0}^{\infty} a_n$ et
$\int_{N_0}^{\infty} f(x) dx$ converge ou diverge en même temps.

\subsection{Test de comparaison}
Si $\exists K, N_0 \in \mathbb{R}_{> 0}$ tels que
$0 \leq a_n \leq Kb_n$ $\forall n \geq N_0$.
\begin{center}
  \begin{tabular}{ll}
    Si $\sum a_n$ diverge & alors $\sum b_n$ diverge\\
    Si $\sum b_n$ converge & alors $\sum a_n$ converge
  \end{tabular}
\end{center}

\subsection{Test de comparaison de la limite}
Soit
\[ L \eqdef \lim_{n \to \infty} \frac{a_n}{b_n} \]
\begin{tabular}{ll}
  Si $L < \infty$ et $\sum b_n$ converge &
  alors $\sum a_n$ converge\\
  Si $L > 0$ et $\sum b_n$ diverge à l'infini &
  alors $\sum a_n$ diverge à l'infini
\end{tabular}

\subsection{Test du quotient}
Soit
\[ \rho \eqdef \lim_{n \to \infty} \frac{|a_{n+1}|}{|a_n|} \]
\begin{itemize}
  \item Si $0 \leq \rho < 1$, alors la série converge;
  \item Si $1 < \rho \leq \infty$, la série diverge.
\end{itemize}

\subsection{Test de la racine}
Soit
\[ \sigma \eqdef \lim_{n \to \infty} \sqrt[n]{|a_n|} \]
\begin{itemize}
  \item Si $0 \leq \sigma < 1$, alors la série converge;
  \item Si $1 < \sigma \leq \infty$, la série diverge.
\end{itemize}

\subsection{Série harmonique}
\[ \sum_{n = 1}^{\infty} \frac{1}{n} \]
\begin{itemize}
  \item La série harmonique diverge;
  \item C'est un cas particulier des P-séries pour $p = 1$;
  \item L'Annexe~\ref{ann:prime_harm} présente
    une démonstration assez intéressante utilisant
    le fait que la série harmonique diverge.
\end{itemize}

\subsection{P-séries}
\[ \sum_{n = 1}^{\infty} \frac{1}{n^p} \]
\begin{itemize}
  \item La série converge pour $p > 1$;
  \item La série diverge pour $p \leq 1$.
\end{itemize}

\subsection{Séries entières (Power series)}
\[ \sum_{n =0}^{\infty}  a_n(x - c)^n \]
\begin{itemize}
  \item $a_n$ est une suite de réels appelés coefficients de la série entière;
  \item $c$ est un réel appelé centre de convergence de la série.
\end{itemize}

L'ensemble des valeurs de $x$ pour lesquelles la série converge est
un intervalle centré en $x = c$ appelé intervalle de convergence,
 et est égal à $[c - R, c + R]$, avec
\[ \frac{1}{R} = \lim_{n \to \infty} \left| \frac{a_{n + 1}}{a_n} \right|
  = \lim_{n \to \infty} \sqrt[n]{|a_n|} \]
$R$ peut être soit $[$, soit $]$,
les cas $x = c - R$ et $x = c + R$ sont à traiter séparément.


\section{Equations différentielles}
\subsection{Classification}
\resizebox{\textwidth}{!}{
  \Tree [.{Equations différentielles}
    [.{Ordre > 1} {\color{violet}...} ]
    [.{Ordre 1}
      [.Linéaire
        [.Homogène {\color{violet}$y' + a(x)y = 0$} ]
        [.Non-homogène {\color{violet}$y' + a(x)y = b(x)$} ]
      ]
      [.Non-linéaire
        [.{Variables\\séparables} {\color{violet}$y' = a(x)b(y)$} ]
        [.{Variables\\non-séparables} {\color{violet}...} ]
      ]
    ]
  ]
}

\subsection{Définition}
Une équation différentielle est une équation qui a pour inconnue une
\emph{fonction} dont une ou plusieurs
\emph{dérivées} apparaissent dans l'équation.
\subsection[Linéaire homogène de premier ordre]
{Equation différentielle linéaire homogène de premier ordre}
\begin{eqnarray*}
  y' +a(x)y &=& 0\\
  \frac{1}{y}\frac{dy}{dx} &=& -a(x)\\
  \int{ \frac{1}{y} dy}&=& - \int{a(x) dx}\\
  \ln{|y|} &=& - \int_k^x{a(x) dx} + C\\
  y(x) &=& ke^{-\int{a(x) dx}}
\end{eqnarray*}
Avec $k$ une valeur réelle.

\subsection[Linéaire non-homogène de premier ordre]
{Equation différentielle linéaire non-homogène de premier ordre}
\subsubsection{Première méthode}
La solution générale de ce genre d'équation est égale à la somme de la solution
générale de l'équation homogène associée ($y_h$)
et d'une solution particulière de l'équation non-homogène ($y_p$).
\[ y = y_h + y_p \]
\begin{center}
  \begin{tabular}{p{6cm}|p{6cm}}
    \strong{Forme de $b(x)$}&\strong{Forme de $y_p$}\\
    \hline\\
    Polynôme de degré $n$ & Polynôme de degré $n$ si $a(x) \neq 0 \qquad$
    Polynôme de degré $n+1$ si $a(x) = 0$\\
    &\\
    $k_1\cos{\theta}x + k_2\sin{\theta}x$ &
    $l_1\cos{\theta}x + l_2\cos{\theta}x$\\
    &\\
    $e^{\lambda{x}}P(x)$&$e^{\lambda{x}}Q(x)$\\
    $P(x)$ un polynôme de degré $n$ et $\lambda$ un réel ou un complexe. &
    $Q(x)$ un polynôme de degré $n$ ou $n + 1$\\
    &\\
    Constante, de même que $a(x)$ & $B/A$
  \end{tabular}
\end{center}

\subsubsection{Deuxième méthode}
Résoudre l'équation homogène associée.
\[ y_h = ke^{-\int{a(x) dx}} \]
Supposer que si k est une fonction de $x$,
$y_h$ est une solution de l'équation $y' +a(x)y =b(x)$.
\[ (k(x)e^{-\int{a(x) dx}})' + a(x) k(x)e^{-\int{a(x) dx}} = b(x) \]
L'expression se simplifie alors pour donner une expression de $k(x)$.
\subsection[Non-linéaire de premier ordre à variables séparables]
{Equation différentielle non-linéaire de premier ordre à variables séparables}
Se résoud de la même manière que les équations différentielles
linéaires homogènes de premier ordre.
\begin{align*}
  y' & = a(x)b(y)\\
  \frac{dy}{dx} & =a(x) b(y)\\
  \int{ \frac{1}{b(y)} dy} & = \int{a(x) dx}
\end{align*}
\subsection{Problème de Cauchy}
Le problème formé par l'équation différentielle et
la condition initiale est appelée \emph{Problème de Cauchy}.
\[
\left\{
  \begin{array}{r c l}
    y'(x) &=& f(x,y)\\
    y(x_0) &=& y_0
  \end{array}
\right.
 \]

\part{Algèbre}
%     _________
% \  / Algebra |
%  \/
\section{Espaces vectoriels}
\subsection{Définition}
$E$ est un espace vectoriel sur $K$ si
$\forall x,y \in E$ et $\forall \alpha,\beta \in K$
\begin{itemize}
  \item $x + y = y + x$;
  \item $(\alpha + \beta)x = \alpha{x} + \beta{x}$;
  \item $\alpha{(x + y)} = \alpha{x} + \alpha{y}$;
  \item $x(\alpha{\beta}) = (x\alpha{})\beta{}$;
  \item $x\cdot1 = x$.
\end{itemize}

\subsection{Sous-espaces vectoriels}
La partie $V$ de l'espace vectoriel $E$ sur un corps $K$ est
un sous espace vectoriel si,
elle est une partie non vide de $E$ stable par combinaison linéaire.
\[ \forall{x, y} \in V, \alpha, \beta \in K \qquad \alpha x + \beta y \in V \]

\subsection{Somme directe}
Tout vecteur $x$ de $V_1 \oplus V_2 \oplus \cdots \oplus V_n$ s'écrit de manière
unique comme une somme de vecteurs appartenant à $V_1, V_2, \cdots , V_n$.
Ce qui est équivalent à, $\forall v_1 \in V_1, \ldots, v_n \in V_n$
\[ \sum_{i = 1}^{n} v_i = 0\quad\Rightarrow\quad v_i =
0\quad\forall i\in\{1, \ldots, n\} \]

\subsection{SEV engendré}
C'est le plus petit sous-espace vectoriel contenant $v_1, v_2, \cdots ,v_n$.
On le note
\[ \sev< v_1, v_2, \cdots, v_n> \]

\subsection{Libre, Génératrice, Base}
$(e_1, e_2, \cdots , e_n)$ est une
\begin{description}
  \item[suite génératrice] de $E$ si $sev<v_1, v_2, \cdots , v_n> = E$;
  \item[suite libre] si $\sum_{i = 1}^{n}\alpha_ie_i = 0
    \Rightarrow \alpha_1 = \alpha_2 = \cdots = \alpha_n = 0$;
  \item[base] de $E$ si elle est à la fois libre et génératrice de $E$.
\end{description}
Si $(e_1, e_2, \cdots , e_n)$ est une base de $E$, tout vecteur de $E$ s'écrit
de manière unique comme combinaison linéaire de cette suite.

\subsection{Changement de base}
Soit un sous espace vectoriel $V$ de dimension $n$ et $e, f$ deux de ces bases.
$\exists P \in K^{n \times n}$ tel que $\forall x \in V$
\[ \tensor*[_f]{(x)}{} = P\tensor*[_e]{(x)}{} \]
On a d'ailleurs $P = \tensor*[_f]{(I)}{_e}$.
La matrice $P$ est régulière (possède une inverse).

\subsection{Dimension}
Toutes les bases d'un espace vectoriel finement engendré ont le même nombre
d'éléments. Ce nombre est appelé \emph{dimension de l'espace vectoriel}.
\[ \dim(V + W)  + \dim(V \cap W) = \dim V + \dim W \]

\subsection{Rang d'une matrice}
$A \in K^{m\times{n}}$\\
\begin{itemize}
  \item $\mathcal{L}(A)$ = SEV des lignes $\subset K^n$;
  \item $\mathcal{C}(A)$ = SEV des colonnes $\subset K^m$.
\end{itemize}
Théorème:
\[ \dim\mathcal{L}(A) = \dim\mathcal{C}(A) = \newrang(A) \]
Si A = BC alors:
\begin{itemize}
  \item $\mathcal{L}(A) \subset \mathcal{L}(C)$
  \item $\mathcal{C}(A) \subset \mathcal{C}(B)$
\end{itemize}
D'où
\[ \dim A \leq \min(\dim B, \dim C) \]

\subsection{Théorème de Rouché}
$Ax = b$ admet une solution \emph{ssi} $\newrang(A) = \newrang(A | b)$.

\section{Systèmes linéaires \& calcul matriciel}

\subsection{Linéarité}
$\forall x,y \in{E}, \alpha, \beta \in{K}$,
\[ A(\alpha{x} + \beta{y}) = \alpha{(Ax)} + \beta{(Ay)} \]

\subsection{Opérations élémentaires}
\begin{description}
  \item[Type I] $L_i \rightarrow L_i + \lambda{L_j}$
  \item[Type II] $L_i \leftrightarrow L_j$
  \item[Type II] $L_i \rightarrow \lambda{L_i} \qquad{(\lambda \neq 0)}$
\end{description}
\subsection{Opérations par blocs}
\[
  \left(
  \begin{array}{cc|c}
    a_{11} & a_{12} & a_{13} \\
    \hline
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33}
  \end{array}
  \right)
  = \begin{pmatrix}
    A&B \\
    C&D
  \end{pmatrix}
\]
Attention à ce que les blocs soient compatibles lors d'opérations.

\subsection{Transposée}
\[ (a_{ij})^t = (a_{ji}) \]

\subsection{Inverse}
Soit $A\in K^{m \times n}$:\\
\begin{tabular}{ll}
  Inverse à gauche $BA = I$ & $\Leftrightarrow \newrang A  = n$\\
  Inverse à droite $AC = I$ & $\Leftrightarrow \newrang A  = m$
\end{tabular}\\
Une matrice est inversible, régulière, non-singulière
si elle possède un inverse à gauche et une inverse à droite.
\[ BA = AB = I \]
La matrice inverse est unique et $\newrang A = n = m$.
\[ (A|I) \xrightarrow[\textrm{élémentaires}]{\textrm{opérations}} (I|A^{-1}) \]

\subsubsection{Propriétés}
Si $A$ est régulière,
\begin{itemize}
  \item $(AB)^{-1} = B^{-1}A^{-1}$
  \item $(AB)^t = B^tA^t$
  \item $(A^{-1})^t = (A^t)^{-1}$
  \item $\det(A^{-1}) = \frac{1}{\det(A)}$
  \item $\det(A) \neq 0$
  \item $\newrang(A) = n$
\end{itemize}

\subsection{Déterminant}
\begin{itemize}
  \item Si une matrice est \emph{singulière} son déterminant est nul.
  \item $\det(AB) = \det(A) \det(B)$
  \item $\det(A^t) = \det(A)$
  \item \[ \det(A) = \sum_{l = 1}^{n}(-1)^{l+k}a_{lk}\det(A_{lk}) \]
\end{itemize}

\subsection{Matrice des cofacteurs}
\[ \cof(A) = \left((-1)^{i + j}\det(A_{i,j})\right)_{i,j} \]
\[ A^{-1} = (\det(A))^{-1} . (\cof(A))^t \]


\section{Applications linéaires}

\subsection{Notion d'application linéaire}
Soient E et F, des espaces vectoriels sur K.
Une application $A : E \rightarrow F$ est dite linéaire
si les conditions suivantes sont vérifiées:
\[ \forall a,y \in E, \alpha ,\beta \in\mathbb{K} :
A(\alpha{x} + \beta{y}) = \alpha{A(x)} + \beta{A(y)} \]
Une application linéaire bijective est appelées isomorphisme.

\subsection{Noyau et image}
Le noyau de l'application linéaire
$A: E \rightarrow F$ est un sev de $E$ tel que:
\[ \newker A = \{x \in E| A(x) = \vec{0}\} \]
L'espace image de A est un sev de $F$ tel que:
\[ \newim A = \{y \in F | \exists x \in E, A(x) = y\} \]
On note
\[ \newnull A \eqdef \dim \newker A \]
et
\[ \newrang A \eqdef \dim \newim A \]

\subsection{Propriétés (1)}
\begin{itemize}
  \item L'ensemble des solutions pour l'équation linéaire de type $A(x) = b$
    est égal à la somme d'une solution particulière et du noyau de $A$:
    \[ u + \newker A = \{u + v | v \in \newker A\} \]
  \item A est inversible à gauche si il existe $B : F \rightarrow E$ tel que:
    \[ B\circ A = I_E \]
  \item A est inversible à droite si il existe $B : F \rightarrow E$ tel que:
    \[ A\circ B = I_F \]
\end{itemize}

\subsection{Propriétés (2)}
Soit $A : E \rightarrow F$.
\[ \newnull A + \newrang A = \dim E \]
\begin{center}
  \begin{tabular}{|c c|}
    \hline
    &\\
    A injective & A surjective\\
    $\Updownarrow$ & $\Updownarrow$\\
    $\newker A = \{0\}$ & $\newim A = F$\\
    $\Updownarrow$ & $\Updownarrow$\\
    $\newnull A = 0$ & $\newnull A = \dim E - \dim F$\\
    $\Updownarrow$ & $\Updownarrow$\\
    $\newrang A = \dim E$ & $\newrang A = \dim F$\\
    $\Updownarrow$ & $\Updownarrow$\\
    A inversible à gauche&A inversible à droite\\
    &\\
    \hline
  \end{tabular}
\end{center}

\subsection{Représentation matricielle}
Une application linéaire $A : E \rightarrow F$ peut être représentée par
une matrice $_f(A)_e$ dont chaque colonne est formée par l'image d'un vecteur
de la base de $E$, exprimé dans la base de $F$.

\part{Maths discrètes}
% _________________
% \ Maths discrètes \
%  \_________________\

\section{Ensembles}

\subsection{Définitions}
\begin{itemize}
  \item \emph{Equipotence}: A et B sont équipotents, noté $A\approx B$,
    si il existe une bijection de A vers B.
  \item \emph{Ensemble fini}:
    Si $A \approx \{1, \cdots , n\}$ pour $n \in \mathbb{N}$.
    $n$ est le \emph{cardinal} de A, noté $|A|$.
  \item \emph{Ensemble infini}:
    $A$ est infini dénombrable si $A \approx \mathbb{N}$.
\end{itemize}

\subsection{\emph{Power set}}
\paragraph{Définition}
Pour tout ensemble non-vide A,
l'ensemble $P(A)$ est \emph{équipotents} à l'ensemble
$\{0, 1\}^{A}$ des fonctions de A vers $\{0, 1\}^{A}$.
\begin{proof}
  À tout sous-ensemble $B$ de $A$,
  on associe la fonction $f_B : A \rightarrow \{0, 1\}$ définie comme suit:
  \[
    \left \{
      \begin{array}{r c l}
        f_B(x) &=& 1 \qquad{\qquad} \text{si } x \in B\\
        f_B(x) &=& 0 \qquad{\qquad} \text{si } x \in A \backslash B
      \end{array}
    \right.
   \]
\end{proof}
La fonction $f_B$ est appelée la fonction caractéristique
de $B$ comme sous-ensemble de $A$.

\subsection{Principe des tiroirs}
\paragraph{Informel}
Si $m$ objets sont rangés dans $n$ tiroirs et si $m > n$,
alors il y au moins un tiroir qui contient plus d'un objet.

\paragraph{Formel}
Soient $A$ et $B$, des ensembles finis non-vides tels que $|A| > |B|$.
Alors il n'existe pas de fonction injective de $A$ dans $B$.

\subsection{Principe d'induction}
Conditions:
\begin{description}
  \item[(1)] $P(n_0)$ est vrai;
  \item[(2)] Avec $k \in \mathbb{N}, \forall k \geq n_0$,
    si $P(k)$ est vrai, alors $P(k+1)$ est vrai.
\end{description}
Alors $P(n)$ est vrai pour tout naturel $n \geq n_0$.

\subsection{Principe d'inclusion et d'exclusion}
\paragraph{Formules}
\begin{itemize}
\item $|A \cup B| = |A| + |B| - |A \cap B|$;
\item $|A\backslash B| \geq |A| - |B|$;
\item $|A \Delta B| = |A| + |B| - 2|A \cap B|$.
\end{itemize}

\paragraph{Principe}
Soit $S$ un ensemble fini non-vide, et soient $S_1, S_2, \cdots, S_N$
des sous-ensembles de $S$ avec $n \geq 1$.
On s'intéresse au nombre d'éléments de $S$ qui n'appartiennent
à aucun des $S_i$, c'est-à-dire au nombre
\[ \sigma =|S| - \left | \bigcup_{i = 1}^n S_i \right | \]

\paragraph{Généralisation}
\begin{itemize}
  \item $S_I = \bigcap_{i \in I} S_i$;
  \item $S_{\emptyset} = S$;
  \item $R_n = \{1, \cdots , n\}$.
\end{itemize}
\[ \sigma = \sum_{r = 0}^n\left ( (-1)^r \sum_{|I| = r}|S_I| \right ) \]

\subsection{Règle de la somme}
Si $A \cap B = \emptyset$, alors $|A \cup B| = |A| + |B|$.

\subsection{Règle du produit}
\[ |A \times B| = |A||B| \]

\section{Dénombrement}
\subsection{Binôme de Newton}
\[ (x + y)^n = \sum_{k = 0}^n \begin{pmatrix}n\\ k \end{pmatrix} x^ky^{n-k} \]

\subsection{Les fonctions}
Une fonction de $A$ vers $B$ est un triple $(A,B,R)$
tel que $\forall a \in A$ il existe un unique $b \in B : aRb$.

On note $B^{A} = \{f | f : A \rightarrow B\}$.

\begin{itemize}
  \item Rangement des objets de $A$ dans les tiroirs de $B$;
  \item Mot de longueur $|A|$ pris dans l'alphabet $B$.
\end{itemize}
Soient A et B finis, $|A| = n , |B| = k$, le nombre de fonctions de A vers B est
\[ k^n \]

\subsection{Injections}
Une fonction $A \to B$ est injective si et seulement si
le mot qui la représente ne contient pas deux fois la même lettre.

Soient A et B finis, $|A| = k, |B| = n$,
\[ \In(n \leftarrow k) = [n]_k \]
\paragraph{Récurrence}
\[ \In(n \to k) = k\In(n-1 \to k-1) + \In(n-1 \to k) \]

\subsection{Surjections}
Une fonction $A \to B$ est surjective si et seulement si le mot
qui la représente contient au moins une fois fois chaque lettres de de $B$.
Si $|A| = n, |B| = k$,
\[ \Sur(n \rightarrow k) = \sum_{r = 0}^k (-1)^r
  \begin{pmatrix} k\\ r \end{pmatrix} (k - r)^n \]
On trouve ce résultat à l'aide du principe d'inclusion et d'exclusion.
\paragraph{Récurrence}
\[ \Sur(n \to k) = k\Sur(n-1 \to k-1) + k\Sur(n-1 \to k) \]

\subsection{Bijections}
Une fonction bijective ou bijection,
est une fonction à la fois injective et surjective.

Le nombre de bijections de $A$ vers $A$ est $n!$.

\subsection{Les dérangements}
Un dérangement sur $A$ est une bijection $f$ sur $A$ telle que
\[ \forall a \in A : f(a) \neq a \]
Le nombre de dérangements d'un n-ensembles est
\[ d_n = n! \sum_{r = 0}^{n} \frac{(-1)^r}{r!} \]

\subsection{Combinaisons}
\begin{center}
  \begin{tabular}{p{0.19\textwidth}|p{0.4\textwidth}|p{0.4\textwidth}}
    &\strong{Sans ordre}&\strong{Avec ordre}\\
    \hline
    &&\\
    \strong{Sans répétitions} &
    $B(n,k) =
    \begin{pmatrix} n\\ k \end{pmatrix} = C^k_n$ \[ \frac{n!}{k!(n - k)!} \] &
    $[n]_k = A^k_n = \In(n \leftarrow k)$\[ \frac{n!}{(n - k)!} \]\\
    \hline
    &&\\
    \strong{Avec répétions} &
    $B^*(n, k) = {n + k - 1 \choose k}$
    \[ |\bullet | \bullet \bullet | \, |\bullet | \bullet \bullet | \] &
    \[ n^k \]\\
  \end{tabular}
\end{center}
\paragraph{Récurrence}
\[ {n \choose k} = {n-1 \choose k-1} + {n-1 \choose k} \]

\subsection{Stirling}
Le nombre de \emph{k-partitions} d'un ensemble de cardinal $n$.
\[ S(n, k) = \frac{1}{k!}\Sur( n \rightarrow k) \]
\paragraph{Récurrence}
\[ S(n, k) = S(n - 1, k - 1) + kS(n - 1, k) \]

\subsection{Bell}
Le nombre total de \emph{partitions} d'un n-ensemble.
\[ b_n = \sum_{i = 0}^{i = n}S(n, i) \]
\paragraph{Récurrence}
\[ b_n = \sum_{i = 0}^{n - 1}\begin{pmatrix}  n - 1  \\ i \end{pmatrix}b_i \]

\subsection{Parallélisme}
\begin{center}
  \begin{tabular}{p{6cm}|p{6cm}}
    \strong{Pascal-Ensembles}&\strong{Stirling-Partitions}\\
    \hline
    &\\
    \[ B(n,k) = \frac{1}{k!}\In(n \leftarrow k) \] &
    \[ S(n,k) = \frac{1}{k!}\Sur(n \rightarrow k) \]\\
    \hline
    &\\
    $B(n,k) =$\[ B(n - 1, k - 1) +  B(n - 1, k) \] &
    $S(n, k) =$\[ S(n - 1, k - 1) + kS(n - 1, k) \]\\
    \hline
    &\\
    $\In(n \leftarrow k)=$\[ k\In(n-1 \leftarrow k-1) + \In(n-1 \leftarrow k) \]
    & $\Sur(n \to k) =$\[ k\Sur(n-1 \to k-1) + k\Sur(n-1 \to k) \]\\
    \hline
    &\\
    \[ \sum_{k = 0}^{n}\begin{pmatrix}  n \\ k \end{pmatrix} = 2^n \] &
    \[ \sum_{k = 0}^{n}S(n,k) = b_n \]\\
  \end{tabular}
\end{center}

\section{Equations de récurrence}

\subsection{Définition}
Soit $k$ un entier naturel. Une récurrence linéaire d'ordre $k$,
à coefficients constants, en la suite \emph{inconnue} $(v_n)_{n = 0}^{\infty}$,
est une équation de la forme
\[ a_0v_{n+k} + a_1v_{n+k-1} + \cdots + a_{k-1}v_{n+1} + a_kv_n =
b_n \]
avec $n \in \mathbb{N}$.
\begin{itemize}
  \item $(v_n)^{\infty}_{n = 0}$ est la suite \emph{inconnue};
  \item $a_0, \cdots , a_k$ sont des coefficients réels constants;
  \item $(b_n)_{n = 0}^{\infty}$ est une suite de réels donnée.
\end{itemize}
\begin{itemize}
  \item \emph{Homogène}: $\forall n \in \mathbb{N} : b_n = 0$;
  \item \emph{Affine}: cas général.
\end{itemize}
\subsection{Récurrences homogènes de degré 0 et 1}
\begin{itemize}
  \item \emph{Degré 0}: $v_n = 0 \qquad \forall n$;
  \item \emph{Degré 1}: $v_n = v_0\left (\frac{-a_1}{a_0} \right )^n$.
\end{itemize}

\subsection{Récurrences homogène de degré 2}
Équation générale:
\[ v_{n+2} + a_1v_{n+1} + a_2v_n = 0 \]
Équation caractéristique:
\[ r^2 + a_1r + a_2 = 0 \]

\subsection{Cas avec 2 racines réels distinctes}
Solution générale:
\[ v_n = c_1r_1^n + c_2r_2^n \]
Conditions initiales:
\[ c_1 + c_2 = v_0 \qquad{\qquad{\qquad{\qquad}}} c_1r_1 + c_2r_2 = v1 \]

\subsection{Cas avec 2 racines très proches}
Si les racines sont très proches l'une de l'autre,
$r_2 = r_1(1 + \delta )$, avec $|\delta | \ll 1$.

La solution générale sera de la forme approchée
\[
  \begin{array}{r c l}
    v_n &\approx &c_1r_1^n + c_2r_1^n(1 + n\delta)\\
    &=&(c_1 + c_2)r_1^n + (c_2r_1\delta )nr_1^{n-1}
  \end{array}
\]

\subsection{Cas avec 2 racines confondues}
Solution générale:
\[ v_n = d_0r_1^n + d_1nr_1^{n-1} \]
Conditions initiales:
\[ d_0 = v_0 \qquad{\qquad{\qquad{\qquad}}} d_0r_1 = d_1 = v_1 \]

\subsection{Cas avec 2 racines complexes conjuguées}
Solution générale:
\[ v_n = c_1\rho{^n}\cos{(n\theta)} + c_2\rho{^n}\sin{(n\theta)} \]
Conditions initiales:
\[ c_1 = v_0 \qquad{\qquad{\qquad{\qquad}}}
c_1\rho\cos{(\theta)} + c_2\rho\sin{(\theta)} = v_1 \]

\subsection{Cas général}
Soit la suite $(v_n)_{n \in \mathbb{N}}$ respectant
\[ \sum_{i = 0}^{k} a_i v_{n + i} = 0 \]
Soient $r_1, ..., r_s$, respectivement de multiplicité,
$m_1, ..., m_s$, les racines de l'équation caractéristique
\[ \sum_{i = 0}^{k} a_i x^{i} \]
On a donc bien évidemment $\sum_{i = 0}^{s} m_i = k$.
Et la solution générale de $v_n$ est
$$\sum_{i=0}^{s} P_i(n)r_{i}^{n}$$
Où $P_i$ est un polynôme avec
$\deg(P_i) \leq m_i$ $\forall i \in \{1, ..., s\}$.

\subsection{Certaines récurrences non homogènes}
Equation générale:
\[ a_0v_{n+k} + a_1v_{n+k-1} + \cdots + a_{k-1}v_{n+1} + a_kv_n = b_n \]
avec $n \in \mathbb{N}$
Supposons $b_n = bs^n$.
Résolution:
\begin{itemize}
  \item Somme d'une solution particulière et des solutions homogènes;
  \item \emph{Degré 0}: $v_n = bs^n \qquad \forall n$.
\end{itemize}

\subsection{Récurrences non homogènes de degré 1}
Equation générale:
\[ v_{n+1} - av_n = bs^n \]
Résolution, en cherchant $v_n = cs^n$:
\begin{itemize}
  \item $s \neq a$
    \begin{align*}
      v_n & = \frac{b}{s - a}s^n + da^n\\
      v_0 & = \frac{b}{s - a} + d
    \end{align*}
  \item $s = a$ \[ v_n = bna^{n-1} + v_0a^n \]
\end{itemize}


\section{Graphes}

\subsection{Définition}
Soit un $N$ ensemble fini non vide,
dont les éléments sont appelés des noeuds.
Soit un $R$ ensemble fini,
dont les éléments sont appelés des arêtes.
Soit $I$ une relation entre noeuds et arêtes,
c'est-à-dire un sous-ensemble de $N \times R$,
appelé relation d'\emph{incidence},
telle que le nombre de noeuds incidents à une arête soit égal à 1 ou à 2.
On dit alors que le triplet $(N, R, I)$ est un \emph{graphe} (non orienté).

\subsection{Vocabulaire}
\begin{itemize}
  \item $\alpha \in \mathbb{R}$ est une boucle si $|\{i|iI\alpha\}| = 1$;
  \item $|N|$ est l'\emph{ordre} du graphe;
  \item Le \emph{degré} de $n$, noté $\deg(n)$,
    est le nombre d'arêtes adjacentes au noeud $n$, les boucles comptant double;
  \item Un graphe est \emph{simple} si il n'a ni boucle,
    ni noeuds reliés par des arêtes multiples;
  \item $\alpha = \{i, j\}$ identifie l'unique arête
    telle que $iI\alpha$ et $jI\alpha$.
\end{itemize}

\subsection{Isomorphisme}
Deux graphes simples $G = (N,R)$ et $G' = (N',R')$ sont \emph{isomorphes} si:
\begin{itemize}
  \item Il existe une bijection $f : N \rightarrow N'$;
  \item $\{i,j\} \in R \Leftrightarrow \{f(i), f(j)\} \in R'$.
\end{itemize}

\subsection{Matrice d'incidence}
La matrice d'incidence M est de genre $|N|\times |R|$
\begin{itemize}
  \item $m_{i,\alpha} := 2 \Leftrightarrow \alpha$ est boucle sur $i$;
  \item $m_{i,\alpha} := 1 \Leftrightarrow \alpha$
    est une arête ordinaire incidente à $i$;
  \item $m_{i,\alpha} := 0 \Leftrightarrow \alpha$ n'est pas incidente à $i$.
\end{itemize}
Propriétés:
\begin{itemize}
  \item $\sum_{i \in N}m_{i,\alpha} = 2$;
  \item $\sum_{\alpha \in R}m_{i,\alpha} = \deg(i)$;
  \item $\sum_{i \in N}\deg(i) = 2|R|$;
  \item Le nombre de noeuds de degré impair d'un graphe est pair.
\end{itemize}

\subsection{Matrice d'adjacence}
La matrice d'adjacence $A$ est de genre $|N| \times |N|$
\begin{itemize}
  \item $a_{i,j} :=$ nombre d'arêtes reliant $i$ et $j$ si $i \neq j$;
  \item $a_{i,i} :=$ deux fois le nombre de boucles sur $i$.
\end{itemize}
Propriétés:
\begin{itemize}
  \item $\sum_{j \in N}a_{i,j} = \sum_{j \in N}a_{i,j} = \deg(i)$;
  \item $\sum_{(i,j) \in N^2}a_{i,j} = 2|R|$;
  \item $MM^t = A + D$ où D est la matrice diagonale des degrés.
\end{itemize}

\subsection{Voyages dans un graphe}
\begin{center}
  \begin{tabular}{p{4cm}|p{3.5cm}|p{3.5cm}}
    &\strong{Noeuds distincts}&\strong{Arêtes distinctes}\\
    \hline
    &&\\
    \strong{Parcours ouvert $\: i_0 \neq i_k$}&chemin&piste ouverte\\
    &&\\
    \hline
    &&\\
    \strong{Parcours fermé}&cycle&circuit (piste fermée)\\
    &&\\
  \end{tabular}
\end{center}
Si $G$ est un graphe simple, le nombre de parcours de longueur $k$
entre ses noeuds $i$ et $j$ est donné par $(A^k)_{i,j}$.

\subsection{Graphes bipartis}
Le graphe simple $G = (N, R)$ est \emph{biparti}
si il existe une partition $\{N_0, N_1\}$ de N telle que
\[ \{i, j\} \in R \Rightarrow i \in N_0\: et \:
j \in N_1\:(ou\:i \in N_1\: et \: j \in N_0) \]
Un graphe simple est biparti si et seulement si
il ne possède aucun cycle de longueur impaire.

\subsection{Connexité}
Un graphe est \emph{connexe}
si il existe un chemin reliant toute paire de noeuds.

Soit un graphe $G = (N, R)$.
On considère la partition $\{N_1, \cdots , N_m\}$ de N
et l'ensemble $\{R_1, \cdots R_m\}$ des sous-ensembles $R_l$ de R,
disjoints deux à deux, qui satisfont aux deux conditions suivantes:
\begin{itemize}
  \item $R_l$ est l'ensemble des arêtes incidentes aux noeuds dans $N_l$;
  \item Le graphe $G_l := (N_l, R_l)$ est connexe,
\end{itemize}
pour $l = 1, \cdots , m$. Alors les graphes $G_1, \cdots , G_m$
sont appelés les composantes connexes du graphe G.

Si il existe $N' \subset N$ non vide tel que aucune arête ne relie
un noeud de $N$ et un noeud de $N\backslash N'$, alors $G$ n'est pas connexe.

\paragraph{Test de connexité}
Soient:
\begin{itemize}
  \item $i_0 \in N$ un noeud quelconque de $G = (N,R)$;
  \item $N' = \{i_0\}$ et $R' = \emptyset$;
  \item $R_{reste} = R$.
\end{itemize}
Tant que $\exists \alpha \in R_{reste}$ et $i \in N'$ tels que $iI\alpha$
\begin{itemize}
  \item $R_{reste} := R_{reste}\backslash \{\alpha\},
    R' = R' \cup \{\alpha\}$;
  \item $N' := N' \cup \{j | jI\alpha\}$.
\end{itemize}
$G$ est connexe si et seulement si $N' = N$.

\paragraph{Corollaires}
\begin{itemize}
  \item Si $G = (N,R)$ est connexe alors $|R| \geq |N| - 1$;
  \item Si $G = (N,R)$ est connexe alors $|R| = |N| - 1$
    si et seulement si G est sans cycle.
\end{itemize}

\subsection{Arbres}
\begin{itemize}
  \item G est connexe et sans cycle;
  \item G est connexe et $|R| = |N| - 1$;
  \item G est sans cycle et $|R| = |N| - 1$;
  \item G est sans cycle et lui ajouter une arête crée un t un seul cycle;
  \item G est connexe et supprimer une arête quelconque le déconnecte;
  \item Deux noeuds distincts de G sont reliés par un et un seul chemin.
\end{itemize}

\subsection{Arbres sous-tendants}
L'arbre $G' = (N', R')$ est un arbre sous-tendant
$G = (N,R)$ si $N = N'$ et $R' \subseteq R$.

\begin{itemize}
  \item $G$ est connexe $\Leftrightarrow$ $G$ possède un arbre sous-tendant.
\end{itemize}

\subsection{Arbres sous-tendants de poids minimum}
Soient
\begin{itemize}
  \item $c : R \rightarrow \mathbb{R}$ associant un poids à chaque arête;
  \item $c(G) := \sum_{r \in R}c(r)$ est le poids du graphe $G$.
\end{itemize}
$A$ est un \emph{arbre sous-tendant de poids minimum} de $G$
si et seulement si $A$ sous-tend $G$ et tout arbre
$A'$ sous-tendant $G$ est tel que $c(A) \leq c(A')$.

\paragraph{Kruskal}
Soit pour un graphe connexe $G = (N,R)$:
\begin{itemize}
  \item $R_{ord} := trier(R)$;
  \item $R' := \emptyset$.
\end{itemize}
Tant que $|R'| < |N| - 1$:
\begin{itemize}
  \item $\alpha = Premier(R_{ord})$;
  \item $R_{ord} = R_{ord}\backslash\{\alpha\}$;
  \item Si $(N, R' \cup \{\alpha\})$ est sans cycle,
    alors $R' = R' \cup \{\alpha\}$.
\end{itemize}
$(N, R')$ est un arbre sous-tendant de poids minimum de $(N, R)$.

\subsection{Euler}
\begin{itemize}
  \item Une \emph{piste eulérienne} est une piste
    qui passe par toutes les arêtes du graphe;
  \item Un grahe connexe $G$ est eulérien $\Leftrightarrow$ $G$ possède un
    circuit eulérien $\Leftrightarrow$ Tous les noeuds de G sont de degré pair;
  \item Le graphe $G$ possède une piste eulérienne $\Leftrightarrow$ $G$ est
    connexe, et contient au maximum deux noeuds de degré impair.
\end{itemize}

\subsection{Hamilton}
\begin{itemize}
  \item un graphe simple possède un \emph{chemin hamiltonien} si il possède
    un chemin passant par chacun de ses noeuds une et une seule fois;
  \item Un graphe simple possède un \emph{cycle hamiltonien} si il possède
    un cycle passant par chacun de ses noeuds une et une seule fois.
    Un \emph{graphe hamiltonien} est
    un graphe simple possédant un cycle hamiltonien.
\end{itemize}

\subsection{Voyages complets dans un graphe}
\begin{center}
  \begin{tabular}{p{4cm}|p{3.5cm}|p{3.5cm}}
    & \strong{Par tous les noeuds une et une seule fois}
    & \strong{Par toutes les arêtes une et une seule fois}\\
    \hline
    \strong{Parcours ouvert $\: i_0 \neq i_k$}
    & chemin hamiltonien & piste eulérienne\\
    \hline
    \multirow{2}{*}{\strong{Parcours fermé}} & cycle hamiltonien
    & circuit eulérien\\
    & graphe hamiltonien & graphe eulérien\\
  \end{tabular}
\end{center}

\annexe
\section{Série harmonique et nombres premiers}
\label{ann:prime_harm}
Il est amusant, pour commencer, de remarquer que la fonction
\[ \zeta : \mathbb{C} \to \mathbb{C} :
s \mapsto \sum_{n=1}^\infty \frac{1}{n^s} \]
dont la recherche des racines est le problème le plus
important du millénaire pour les mathématiques (voir ``Hypothèse de Riemann''),
n'est autre qu'une p-série dans laquelle on a mis des complexes.

Le cas particulier $\sum \frac{1}{n}$ est la série harmonique, $\zeta(1)$.
Elle apparait dans une démonstration de $|\mathbb{P}| = \infty$
où $P$ est l'ensemble des nombres premiers.
\begin{proof}
  Procédons par l'absurde.
  Si $|\mathbb{P}|$ est fini,
  comme $\sum_{k = 0}^{\infty}\frac{1}{p^k}$ est convergente,
  $$\sum_{k = 1}^{\infty}\frac{1}{k} = \prod_{p\in\mathbb{P}}\sum_{k = 0}^{\infty}\frac{1}{p^k}$$
  serait convergente aussi, car ça serait un produit fini de facteurs finis.
  Ce qui est absurde car c'est une série harmonique.
\end{proof}

\end{document}
