\input{../../lib.tex}

\DeclareMathOperator{\lin}{Lin}
\DeclareMathOperator{\spec}{spec}
\newcommand{\sol}{\mathcal{S}}

\hypertitle{\'Equations diff\'erentielles ordinaires}{4}{MAT}{1223}
{}{}

\section{EDO à coefficients constants}
Nous allons traiter la résolution, l'existence et l'unicité des solutions
de l'équation différentielle ordinaire
\[ u'(t) = A[u(t)] \]
où $u:I\to\Rn$ est une fonction vectorielle, $I \subseteq \R $ un intervalle et $t \in I$ .
Dans cette section, $A$ sera une matrice à coefficients constants.

On remarque que si $u$ et donc $A$ sont scalaires, la solution est
\[ u(t) = e^{At}u(0) = \lim_{m\to\infty} \sum_{k=0}^m\frac{(At)^k}{k!}u(0) \]
d'où l'intuition qui semble parachutée de la solution pour $u(t)$ vectoriel
\[ u(t) = \lim_{m\to\infty} \sum_{k=0}^m\frac{(tA)^k[u(0)]}{k!}. \]

Seulement, il nous faut s'assurer de deux choses
\begin{itemize}
  \item La limite converge bien;
  \item C'est bien solution,
    c'est à dire que c'est dérivable et que $u(t)$ défini
    ainsi satisfait bien $u'(t) = A[u(t)]$.
\end{itemize}

On peut démontrer que c'est bien le cas.
On définit alors l'\emph{exponentielle d'opérateur}
\[ e^{A}[v] \eqdef \lim_{k\to\infty} \sum_{i=0}^k \frac{A^i[v]}{i!}. \]
Il faut faire attention à ne pas se laisser tromper par la notation.
Ce n'est pas un exponentielle comme les autres, on a pas toutes les propriétés
des exponentielles classiques.
Par exemple, en général, $e^{A + B} \neq e^A \circ e^{B}$.
On a par contre
\begin{align*}
  \fdif{}{t}(e^{tA}[v]) & = e^{tA} \circ A[v]\\
  A \circ e^A & = e^A \circ A.
\end{align*}

%faudrait enlever les s à propriétés en général t'en définis qu'une à la fois nan ? ;) 
\begin{myprop}[Existence, unicité et solution]
  Soit $A \in \lin(\R^n, \R^n)$ et $u_0 \in \R^n$,
  il \emph{existe} une solution
  $u:\R\to\R^n$ du problème
  \begin{align*}
    u'(t) & = A[u(t)]\\
    u(t_0) & = u_0
  \end{align*}
  cette solution est $u(t) = e^{(t-t_0)A}[u_0]$ et est \emph{unique}.
\end{myprop}
Pour $u'(t) = A[u(t)] + b(t)$, la solution devient
\[ u(t) = e^{(t-t_0)A}[u_0] + \int_{t_0}^t e^{(t-s)A}[b(s)]\dif s. \]

On remarque que l'exponentielle d'opérateurs se simplifie grandement
pour les valeurs propres généralisées de $A$.

En effet, si $\lambda$ est une valeur propre généralisée d'ordre $k$
\emph{et que $v$ est un vecteur propre généralisé de valeur propre $\lambda$},
on a
\[ e^{tA}[v] =
e^{\lambda t} \sum_{i=0}^{k-1}\frac{t^i}{i!}(A-\lambda I)^i[v]. \]

C'est un résultat assez intéressant.
En effet, comme l'EDO est linéaire, si on a une condition initiale
$v$ qui n'est pas une valeur propre, on pourra la décomposer en une somme
de vecteurs propres $v_i$ à l'aide de Gram-Schmidt
\footnote{Voir la synthèse de Mathématique Q2.}. % TODO biblio
On pourra alors tout simplement écrire
\[ e^{tA}[v] =
\sum_{i=1}^m e^{\lambda_i t}
\sum_{j=0}^{k_i-1}\frac{t^j}{j!}(A-\lambda_i I)^j[v_i] \]
où $\lambda_1, \ldots, \lambda_m$ sont les $m$ valeurs propres généralisées
de degré $k_i$ où $\sum_{i=1}^m k_i = n$.

La projection des $v$ en valeurs propres $v_i$ peut être modélisé
par un produit matriciel $v_i = P_i v$.
%Moi je te suis plus vraiment en tout cas entre v val propre et vect propre
%mais bon c'est la fin de journée la :p
On peut alors mettre en évidence $v$ dans la formule précédente et en posant
$B_{i,j} = (A-\lambda_i I)^j P_i$, on a
\[ e^{tA} =
\sum_{i=1}^m e^{\lambda_i t}
\sum_{j=0}^{k_i-1}\frac{t^j}{j!}B_{i,j}. \]
On pose $B_{i,j}$ ici pour mettre en évidence le fait que le plus important,
c'est qu'il existe une certaine matrice $B_{i,j}$,
sa valeur exacte n'est pas utilisée aussi souvent que le fait que $B_{i,j}$
existe.

En effet, un des corrolaires importants de cet identité est que
si $\sigma > \Re(\lambda)$ $\forall \lambda \in \spec A$,
il existe $C \geq 1$ tel que
\[ \|e^{tA}\| \leq Ce^{\sigma t}. \]
C'est une inégalité assez utile,
ça permet par exemple de dire que si
$\Re(\lambda) < 0$, $\forall \lambda \in \spec(A)$,
$\lim_{t\to 0} e^{tA} = 0$.
% FIXME ref stabilité ?

\section{EDO à coefficients variables}
Nous allons étudier l'EDO suivante.
Soit $I \subseteq \R$ un intervalle et $u:I\to\R^n$ dérivable tel que
\begin{align*}
  u'(t) & = A(t)[u(t)] + f(t) \forall t \in I\\
  u(t_0) & = u_0
\end{align*}
où $A : I \to \lin(\R^n)$ est continue.

On peut démontrer que cette EDO a toujours une solution et que cette
solution est unique.
C'est à dire que si on a deux fonctions $u_1$ et $u_2$ qui satisfont
l'EDO pour les même $t_0$ et $u_0$, on a en fait $u_1 = u_2$.

\subsection{Structure de la solution}
\label{sec:struct}
Analysons à présent, dans le cas où $f = 0$,
la structure de la solution avant d'y avoir appliqué la condition initiale
$u(t_0) = u_0$.
On a donc $u'(t) = A(t)[u(t)]$.
Notons $\sol$ l'ensemble des solutions.
Pour les éléments qui vont suivre, il est important que $I$ soit un \emph{intervalle}. 
Si ce n'est plus le cas, problème il y aura. 

On remarque alors que la solution forme un espace vectoriel
de dimension $n$.
On remarque aussi que la famille de solutions $u_1, \ldots, u_k \in \sol$
est une base de $\Rn$ si et seulement si
la famille $u_1(t_0), \ldots, u_k(t_0)$ est une base de $\Rn$.
On a même cette propriété séparément pour famille libre et pour
famille génératrice.

On peut alors trouver la base de la solution assez aisément.
Il nous suffit de prendre une base $v_i$ de $\Rn$
et puis de prendre la famille
formée par les solutions ayant comme conditions initiales $u(t_0) = v_i$.

\subsection{Résolvante}
La résolvante est une matrice qui permet de passer d'un vecteur d'un
temps donné à un autre temps en suivant l'EDO.
Sa définition est la suivante

\begin{mydef}[Résolvante]
  La résolvante $R: I \times I \to \lin(R^n)$ est l'unique fonction
  telle que, si
  \[ u'(t) = A(r)[u(r)] \]
  $\forall r \in I$, alors $\forall s, t \in I$
  \[ u(s) = R(s, t)[u(t)]. \]
\end{mydef}
Un moyen mnémotechnique pour savoir dans quel ordre on met $s$ et $t$ dans
les arguments de $R$ est que $s$ doit être proche de $u(s)$ et $t$ proche
de $u(t)$.

On peut peut montrer que $u(t)$ est solution de l'EDO si et seulement si
\[ u(t) = R(t, t_0)[u_0] + \int_{t_0}^t R(t,s)[f(s)] \dif s. \]
On a aussi que
\[ R(t, s) = R(t, r) \circ R(r, s). \]

Pour la calculer, on prend $n$ solutions $u_i$ linéairement indépendantes
de $u'(t) = A(t)[u(t)]$ et on prend
 
\[ R(s, t) =
  \begin{pmatrix}
    u_1(s) \ldots u_n(s)
  \end{pmatrix}
  \begin{pmatrix}
    u_1(t) \ldots u_n(t)
\end{pmatrix}^{-1}. \]
En effet, on a $u_i(t) = R(t, s) u_i(s)$ $\forall i$.

\subsection{Méthodes de résolution}
On va maintenant analyser comment résoudre l'EDO en pratique.
On doit pour cela utiliser 2 méthodes,
la première utilise une base qu'on sait calculer grâce à la deuxième.

\subsubsection{Méthode de la variation de la constante}
Comme vu à la section~\ref{sec:struct},
la solution de $u'(t) = A(t)[u(t)]$ $\forall t \in I$ est un espace vectoriel
de dimension $n$.

On commence par calculer une base $u_1,\ldots,u_n$ de cet espace vectoriel.
Toute solution de l'EDO pour $f = 0$ s'écrit alors comme une combinaison
linéaire des $u_i$.
L'astuce de cette méthode est de se dire qu'en ne prenant pas une combinaision
linéaire avec des coefficients constant $\alpha_i$ mais des coefficients
variables $\alpha_i(t)$, on va pouvoir trouver une solution avec $f \neq 0$.

On trouve en effet une solution en prenant $\alpha_i(t)$ tels que
\begin{align*}
  \sum_{i=1}^n \alpha_i'(t)u_i(t) & = f(t) \,\forall t \in I\\
  \sum_{i=1}^n \alpha_i(t_0)u_i(t_0) & = u_0.
\end{align*}
On trouve $\alpha_i'(t)$ par les équations ci-dessus et il
nous reste plus qu'à intégrer à l'aide des $\alpha_i(t_0)$ trouvés
pour trouver les $\alpha_i(t)$
et donc $u(t)$.

\subsubsection{Méthode de la réduction d'ordre}
Observons maintenant comment calculer cette base $u_1,\ldots,u_n$.
On ne va traiter ici que de la réduction d'ordre pour $n=2$.
On a donc à résoudre
\[ u'(t) = A(t)u(t). \]
On va avoir besoin d'une première solution $u_1$,
on sait qu'il en existe une mais
il faut avoir un peu d'intuition pour la trouver.

Il nous faut maintenant prendre un vecteur constant
qui soit linéairement indépendant de $u_1$.
Pour cela, si $u_1$ a sa première (resp. sa deuxième) composante nulle,
il suffit de prendre $(1, 0)$ (resp. $(0, 1)$).
S'il en a aucune, on peut prendre celui qu'on veut de $(0,1)$ et $(1,0)$.
En effet, si on prend par exemple $(0,1)$,
pour que $\alpha u_1$ ait sa première composante nulle,
il faut que $\alpha = 0$ car on a supposé que $u_1$ n'en avait aucune.
On aura alors $\alpha u_1 = (0,0) \neq (0,1)$
donc $(0,1)$ n'est pas un multiple de $u_1$.

On pose ensuite (sans perte de généralité, disons qu'on a pris $(0,1)$)
\[ u_2(t) = \alpha(t)u_1(t) + \beta(t)(0,1). \]
avec $\alpha,\beta$ des fonctions scalaires à déterminer.
On écrit alors $u_2'(t) = A[u_2(t)]$.
Il devrait apparaitre à gauche $\alpha(t) u_1'(t)$ et à droite
$\alpha(t) A[u_1(t)]$ qu'on peut simplifier.
Ça nous fait un système de deux équation différentielles du premier degré
en $\alpha(t)$ et $\beta(t)$ qu'il faut résoudre pour finalement trouver
$u_2(t)$.
Seulement, ce système est plus facile à résoudre que celui de départ
car dans la première équation, on a simplement
\[ \alpha'(t)u_{1;1}(t) = \beta(t)A_{1,2}. \]
où $u_{1;1}$ est la première composante de $u_1$ et $A_{1;2}$ est
la composante de $A$ sur la première ligne et la deuxième colonne.
D'où on tire directement $\beta(t)$ qu'on peut réinjecter dans la deuxième
où il n'y aura alors que des $\alpha$ qu'on peut alors trouver ce qui nous
donne $\beta$ en repassant à la première équation.

\section{EDO non linéaires}
On va étudier les équations du type
\[ u'(t) = f(t, u(t)). \]

\subsection{Existence et unicité}
Commençons par définir ce qu'est un champ de vecteur et
une courbe intégrale
\begin{mydef}[Champ de vecteur]
  Soit $\Omega \subseteq \R \times \Rn$,
  un \emph{champ de vecteurs} sur $\Omega$ est une fonction
  $f:\Omega \to \Rn$.
\end{mydef}
\begin{mydef}[Courbe intégrale]
  Soit $\Omega \subseteq \R \times \Rn$ et $f:\Omega\to\Rn$
  un champ de vecteur sur $\Omega$.
  Soit $I \subseteq \R$ un intervalle \emph{ouvert}.
  La fonction $u:I\to\Rn$ est une \emph{courbe intégrale} de $f$ si $u$
  est \emph{dérivable} et si pour chaque $t\in I$, $(t, u(t)) \in \Omega$ et
  \[ u'(t) = f(t, u(t)). \]
\end{mydef}
En gros, une courbe intégrale est une courbe qui respecte l'équation
$u'(t) = f(t, u(t))$ et qui reste dans le champ de vecteur $\Omega$
($(t, u(t)) \in \Omega$).

On peut s'assurer de l'existence d'une courbe intégrale dans un domaine
borné en s'assurant que $||f||$ ne dépasse pas une limite.
C'est comme quand la police cherche un prisonnier mais qu'ils savent
qu'il est à pied et donc qu'il ne va pas à plus de
% 12 km/h pfff petit joueur
\si{12}{\kilo\meter\per\hour}, elle peut dessiner un cercle de
\si{6}{\kilo\meter} de rayon
et elle est sûre que le prisonnier n'en sortira pas en
une demi-heure.
C'est ce qui est dit de manière plus formelle par
la proposition~\ref{prop:cauchy_peano}.
\begin{myprop}[Cauchy-Peano]
  \label{prop:cauchy_peano}
  Soit $t_0 \in \R$, $h > 0$, $r > 0$,
  $u_0 \in \Rn$ et $f:]t_0-h,t_0+h[\times B(u_0,r)\to\Rn$ un champ de vecteurs
  continu.
  Si pour chaque $(t,x) \in ]t_0-h,t_0+h[ \times B(u_0,r)$,
  \[ h\|f(t,x)\| \leq r, \]
  alors il existe $u:]t_0-h,t_0+h[ \to B(0,r)$
  qui soit une courbe intégrale de $f$.
\end{myprop}

Pour l'unicité, on va devoir utiliser le concept de champ lipschitzien.
\begin{mydef}[Champ de vecteurs lipschitzien en espace]
  Soit $\Omega \subseteq \R \times \Rn$.
  Le champ de vecteurs $f:\Omega\to\Rn$ est \emph{lipschitzien
  en espace}, s'il existe $L \geq 0$ tel que pour chaque
  $t\in\R$, $x,y\in\Rn$, si $(t,x) \in \Omega$ et $(t,y) \in \Omega$, alors
  \[ \|f(t,x) - f(t,y)\| \leq L \|x - y\|. \]
\end{mydef}
\begin{mydef}[Champ de vecteurs localement lipschitzien en espace]
  Soit $\Omega \subseteq \R \times \Rn$.
  Le champ de vecteurs $f:\Omega\to\Rn$ est \emph{localement lipschitzien
  en espace}, si pour chaque
  $(t,x) \in \Omega$,
  il existe $r > 0$ et $h > 0$ tels que
  $f$ est lipschitzien en espace sur
  $]t-h,t+h[ \times B(x,r) \subseteq \Omega$.
\end{mydef}
On remarque que si $f$ est lipschitzien, alors $f$ est localement lipschitzien
et que si $f$ est de classe $C_1$ en espace, alors $f$ est localement
lipschitzien en espace.

Essayons de comprendre tout cela avec des exemples.
\begin{myexem}
  \label{ex:ux}
  Prenons $\Omega = \R\times\R$ et $f:\Omega\to\R$ définit par
  \[
    f(t,x) =
    \begin{cases}
      0 & x < 0\\
      1 & x \geq 0.
    \end{cases}
  \]
  On remarque que $f$ est pas dérivable en $0$ mais est localement
  lipschitzien.
  Par contre, il n'est pas lipschitzien.
  En effet, si on prend $x < 0$ et $y > 0$, on prenant $x$ et $y$ aussi
  prochent de $0$ que l'on veut,
  on sait avoir $\frac{\|f(t,y)-f(t,x)\|}{\|y-x\|}$
  aussi grand que l'on veut.

  Il est localement lipschitzien car même en prenant $x$
  aussi proche de $0$ que l'on veut,
  il suffit de prendre $r \leq |x|$ on n'aura pas de problème avec $x = 0$
  car alors $0 \notin B(x,r)$ car c'est une boule ouverte.
\end{myexem}
\begin{myexem}
  Prenons $\Omega = \R\times\R_{>0}$ et
  $f:\Omega\to\R:f(t,x)\mapsto \frac{1}{x}$.
  $f$ est de classe $C_1$ sur $\Omega$ et est donc localement lipschitzien
  mais n'est pas lipschitzien.
  En effet, en prenant $x < y$, on a
  \[ \frac{\left\|\frac{1}{x}-\frac{1}{y}\right\|}{\|x-y\|} = \frac{1}{xy} \]
  qui peut être aussi grand que l'on veut en faisant tendre $x$ vers 0.
\end{myexem}
\begin{myexem}
  Prenons $\Omega = \R\times\R$ et $f:\Omega\to\R$ définit par
  \[
    f(t,x) =
    \begin{cases}
      0 & x \neq 0\\
      1 & x = 0.
    \end{cases}
  \]
  %Pq f est féminin mtn, plus un champ de vecteur ? 
  Ici $f$ n'est pas continue donc pas dérivable ni localement lipschitzienne
  donc pas lipschitzienne non plus.
  En effet, pour $x = 0$, prendre $r$ aussi petit que l'on veut ne change rien,
  en prenant $y$ suffisamment proche de $x$, on a $\|1\|/\|x-y\|$,
  aussi grand que l'on veut.
\end{myexem}
\begin{myexem}
  \label{ex:tabsx}
  Prenons $\Omega = \R\times\R$ et $f:\Omega\to\R$ définit par
  \[
    f(t,x) = t|x|.
  \]
  Ici, bien que $f$ ne soit pas dérivable si $t \neq 0$,
  $f$ est bien localement lipschitzienne
  mais n'est pas lischitzienne.
  En effet, en prenant sans perte de généralité $x < y$, on a,
  si $x$ et $y$ sont de même signe
  \[ \frac{\|t|x| - t|y|\|}{\|x-y\|} = |t| \]
  et s'il sont de signe opposé (alors $y \geq 0$ et $x \leq 0$ car $x < y$)
  \[ \frac{\|t|x| - t|y|\|}{\|x-y\|}
  = |t|\frac{|x|+|y|}{|x|+|y|} = |t| \]
  On voit que si on ne restreint pas $t$, on ne trouvera pas
  de $L$, $f$ n'est donc pas lipschitzien.
  Par contre, si on avait pris $\Omega = [-1;1]\times\R$,
  $f$ aurait été lipschitzien en espace car on aurait pu prendre $L = 1$
  car $|t| \leq 1$ $\forall t \in [-1;1]$.
\end{myexem}
\begin{myexem}
  \label{ex:sqrtx}
  Prenons $\Omega = \R\times\R_{\geq 0}$ et $f:\Omega\to\R$ définit par
  \[
    f(t,x) = \sqrt{x}.
  \]
  Ici, $f$ n'est pas dérivable en 0 ni localement lipschitzienne
  et donc pas lipschitzienne non plus.
  En effet, pour $x < y$, en prenant $x$ et $y$ de plus en plus petits,
  on a
  \[ \frac{\|\sqrt{x}-\sqrt{y}\|}{\|x-y\|} =
  \frac{\sqrt{y} - \sqrt{x}}{y-x} =
  \frac{\sqrt{y} - \sqrt{x}}{(\sqrt{y} - \sqrt{x})(\sqrt{y} + \sqrt{x})}
  = \frac{1}{\sqrt{x}+\sqrt{y}} \]
  qui devient aussi grand que l'on veut.
  On remarque qu'on aurait pu se simplifier la vie en prenant $x = 0$
  directement mais on voit mieux ici le lien entre être lipschitzien et avoir
  une dérivée qui tend vers $\infty$.
  On peut être non dérivable et être lipschitzien comme pour
  l'exemple~\ref{ex:tabsx} mais
  lorsque la dérivée tend vers $\infty$, c'est difficile d'être
  lipschitzien.

  $f$ n'est pas localement lipschitzienne à cause de $x=0$.
  En effet, prendre $r$ aussi petit que l'on veut ne changera rien,
  $\frac{1}{\sqrt{y}}$ tend vers $\infty$ quand $y$ tend vers $x = 0$.
  
  Par contre, si on avait pris $\Omega = \R \times \R_{>0}$, $f$
  aurait été \emph{dérivable} donc \emph{localement lipschitzienne} mais
  pas \emph{lipschitzienne}.
\end{myexem}
On remarque que continu ne veut pas dire localement lipschitzien avec
l'exemple~\ref{ex:sqrtx} et que localement lipschitzien ne veut pas
dire continue non plus avec l'exemple~\ref{ex:ux}.

On a alors deux théorèmes.
Un pour l'unicité locale et un pour l'unicité globale.
Le théorème de l'unicité globale est en fait strictement plus
fort que ceui de l'unicité locale.
En effet, pour l'unicité globale, on demande juste un $\Omega$ ouvert
alors que pour l'unicité locale, on demande un
$\Omega=]t_0-h,t_0+h[ \times B(u_0,r)$
pour un certain $h>0$ et un certain $r>0$.
Alors qu'on est plus restrictif pour l'unicité local,
on va demander que $f$ soit \emph{continue} et \emph{lipschitzien en espace}
alors que l'unicité globale ne demandera que d'être \emph{continu} et
\emph{localement lipschitzien en espace}.

En effet, il suffit dans l'unicité locale de n'être que localement
lipschitzien (et toujours continu bien entendu)
pour garantir l'unicité, ça se démontre avec l'unicité globale.
On pourrait donc se poser la question de l'intérêt de l'unicité
locale si la propriété est aussi inutile en pratique.
L'intérêt est qu'elle est bien plus simple à prouver et que l'unicité
globale l'utilise en fait dans sa preuve.
Elle est donc utile en théorie mais inutile en pratique.

\begin{myprop}[Unicité locale]
  Soit $t_0 \in \R$, $h > 0$, $r > 0$, $u_0 \in \Rn$ et
  $f:]t_0-h,t_0+h[ \times B(u_0,r) \to \Rn$ un champ de vecteurs
  \emph{continu} et \emph{lipschitzien en espace}.
  Si $u:]t_0-h,t_0+h[\to\Rn$ et $v:]t_0-h,t_0+h[\to\Rn$ sont des courbes
  intégrales de $f$ et si $u(t_0) = v(t_0)$, alors pour chaque
  $t \in ]t_0-h,t_0+h[$,
  \[ u(t) = v(t). \]
\end{myprop}
\begin{myprop}[Unicité globale]
  Soit $\Omega \subseteq \R\times\Rn$ un ouvert,
  $f: \Omega\to \Rn$ un champ de vecteurs \emph{continu},
  $I \subseteq \R$ un intervalle ouvert.
  Soient $u:I\to \Rn$ et $v:I \to \Rn$ deux courbes intégrales de $f$.
  Si $f$ est \emph{localement lipschitzienne} en espace
  et s'il existe $t_0 \in I$
  tel que $u(t_0) = v(t_0)$, alors pour chaque $t \in I$,
  \[ u(t) = v(t). \]
\end{myprop}

\subsection{Courbe intégrale maximale}
On va maintenant s'intéresser à l'intervalle
sur lequel on définit une solution.
En effet, lorsqu'on a une solution,
on pourrait très bien la tronquer et avoir toujours une solution.
On va donc définir une \emph{courbe intégrale maximale} comme une courbe
qu'on ne peut plus étendre.
Si par exemple on définit $u:I\to\R$ où $I = ]t_1,t_2[$,
soit $t_1 = -\infty$, soit $\lim_{t\to t_1} u(t) = \pm\infty$.
On remarque aussi que $I$ doit être ouvert, en effet, si $I$ est fermé en
$t_2$ par exemple, $u$ ne peut pas être maximale car si elle est définit en
$t_2$, c'est qu'on explose pas encore et donc qu'on peut agrandir $I$
à sa droite.

On a un théorème très fort qui nous garanti que si dans
un $\Omega \subseteq \R\times\Rn$, on a $f$ qui est \emph{continue}
et \emph{localement lipschitzienne},
alors si pour un certain $(t_0, u_0) \in \Omega$, on impose
$u(t_0) = u_0$, on est non seulement sûr de trouver un courbe intégrale
maximale mais en plus elle est unique.

Voyons maintenant comment déterminer si une fonction $u$ est une
courbe intégrale maximale.
Une fonction $u:I\to\R$ est une courbe intégrale maximale
si et seulement si
$I$ est un intervalle \emph{ouvert non vide} et que,
pour tout compact $K \subseteq \Omega$,
\begin{align*}
  \sup\{t \in I: (t, u(t)) \in K\} & \in I\\
  \inf\{t \in I: (t, u(t)) \in K\} & \in I.
\end{align*}

On remarque tout de suite l'importance d'imposer que $I$ soit ouvert
car s'il est fermé, les deux inclusions sont triviallement respectées
alors que $u$ ne peut pas être maximale avec un $I$ fermé.

Il faut comprendre ce théorème comme une manière et rigoureuse de dire que
$u$ explose aux deux bornes ouverte de $I$.
En effet, si $I$ explose à droite de $I$,
quelque soit le compact qu'on prend, on coupera toujours $u(t)$ pour un
$t < t_2$, en effet, on tend vers l'infini en $t_2$ et c'est impossible
pour un compact de ne pas intercepter cette asymptote.

Par contre, si on explose pas en $t_2$, un compact bien choisi
contiendra $(t_2, \lim_{t \to t_2} u(t))$ et le suprémum sera alors $t_2$
qui n'appartient pas à $I$ car $I$ est ouvert.

\section{Stabilité}
Dans ce chapitre, on s'intéressera uniquement aux
champ des vecteurs $f$ indépendants du temps.
On s'intéresse donc à la stabilité des solutions d'une EDO de la forme
\begin{align*}
  u'(t) & = f(u(t))\\
  u(0) & = x
\end{align*}
lorsqu'on modifie $x$.
C'est à dire que si on passe de $x = u_0$ à $x = u_0 + \delta$,
on va regarder si
la solution va être proche ou très différente de celle avec $u_0$.

On peut définir la stabilité de différentes manières.
On va en étudier deux.

\subsection{Équilibre stable}
Un équilibre est \emph{stable} (au sens de Lyapounov)
si on peut s'assurer que la
solution ne s'écarte pas de plus de $\epsilon>0$
en prenant un $x$ qui n'est pas
à une distance de $u_0$ de plus que $\delta>0$ avec un $\delta$
suffisamment petit.

Il existe une condition suffisante pour que $u_0$ soit un équilibre stable.
Il suffit que $f$ soit \emph{localement lipschitzien} et qu'il existe
$V \in C^1(\Omega;\R)$ tel que pour toute courbe intégrale $u:I\to\Rn$ de $f$,
$V \circ u$ soit \emph{décroissante} et
que pour chaque $x\in\Omega\setminus\{u_0\}$,
$V(y) \leq V(x)$.% FIXME slides... ?

\subsection{Équilibre asymptotiquement stable}
Un équilibre est asymptotiquement stable (au sens de Lyapounov)
s'il est \emph{stable}
et qu'on peut s'assurer qu'en prenant $x$ qui n'est pas à une distance
de $u_0$ de plus de $\delta>0$ avec un $\delta$ suffisamment petit,
la solution tendra vers $u_0$
\[ \lim_{t\to\infty}u(t) = u_0. \]
Il est important de remarquer qu'un équilibre \emph{asymptotiquement stable}
doit être un équilibre stable.
Si on ne l'avait pas demandé, on aurait juste
eu un équilibre \emph{attractif}.

On a à nouveau une condition suffisante
pour que $u_0$ soit un équilibre asymptotiquement stable.
C'est d'ailleurs la même que pour que ce soit un équilibre stable
sauf qu'on exige une décroissance stricte et une inégalité stricte.
Il suffit que $f$ soit \emph{localement lipschitzien} et qu'il existe
$V \in C^1(\Omega;\R)$ tel que pour toute courbe intégrale $u:I\to\Rn$ de $f$,
$V \circ u$ soit \emph{strictement décroissante} et
que pour chaque $x\in\Omega\setminus\{u_0\}$,
$V(y) < V(x)$.

\subsection{Analyse de la stabilité avec la jacobienne}
Si $f$ est \emph{localement lipschitzien},
que $f(u_0) = 0$ et que $f$ est dérivable
en $u_0$.
On peut analyser la stabilité de $u_0$ à l'aide de la Jacobienne de $f$
en $u_0$ $Df(u_0)$ (aussi notée $Jf(u_0)$).

Soient $\lambda_i$ les valeurs propres de $Df(u_0)$.
\begin{itemize}
  \item Si $\forall i$ $\Re(\lambda_i) < 0$, $u_0$
    est un équilibre asymptotiquement stable;
  \item Si $\exists i$ $\Re(\lambda_i) > 0$, $u_0$
    est un équilibre instable;
  \item Si $\forall i$ $\Re(\lambda_i) \leq 0$ et que $\forall i$ tel que
    $\Re(\lambda_i) = 0$, $m_g(\lambda_i) = m_a(\lambda_i)$,
    $u_0$ est un équilibre stable;
  \item Si $\forall i$ $\Re(\lambda_i) \leq 0$ et que $\exists i$ tel que
    $\Re(\lambda_i) = 0$, où $m_g(\lambda_i) < m_a(\lambda_i)$,
    $u_0$ est un équilibre instable.
\end{itemize}

\section{Problèmes aux limites}
Un problème aux limites est un problème dans lequel,
contrairement au problème de Cauchy,
on cherche une fonction $u$ qui satisfasse des conditions initiales
à deux points différents $a$ et $b$ et non à un seul point $t_0$.

C'est une EDO de la forme
\begin{align*}
  u'(t) & = A(t)[u(t)] + f(t) & \forall t \in [a,b]\\
  B_a[u(a)] & = B_b[u(b)]
\end{align*}
où $f:[a,b]\to\Rn$, $A \in C([a,b];\Rn)$ et $B_a,B_b \in \lin(\Rn,\R^p)$.

On va maintenant voir une condition nécessaire et suffisante pour
l'existance et l'unicité.
\subsection{Existence}
Pour qu'il existe une fonction $u$,
il faudra et il suffit d'imposer une condition sur $f$.
Il va falloir que
\[ \int_a^b (v(s)|f(s)) \dif s = 0. \]
pour tout $v$ tels que
\begin{align*}
  v'(t) & = -A(t)*[u(t)] + f(t)\\
  (\alpha|v(a)) & = (\beta|v(b))
\end{align*}
où $A(t)^*$ est la  matrice adjointe de $A$ c'est à dire la matrice telle
que $(A(t)^*x|y) = (x|A(t)y)$ pour tout $x,y \in \Rn$.

\subsection{Unicité}
Pour s'assurer qu'il existe au plus une fonction $u$,
comme notre problème est linéaire,
il faut et il suffit que la seule solution
du problème homogène (donc avec $f = 0$)
soit la solution nulle.

En effet,
on sait comme c'est linéaire que l'ensemble des solutions
est l'ensemble des solutions homogènes dont on ajoute
pour chacune une même solution particulière.
Il y a donc autant de solutions que de solutions homogènes.

\end{document}
