\documentclass[fr]{../../../eplsummary}

\usepackage[SIunits]{../../../eplunits}
\usepackage{../../../eplmath}

\DeclareMathOperator{\lin}{Lin}
\DeclareMathOperator{\spec}{spec}
\DeclareMathOperator{\newnull}{null}
\newcommand{\sol}{\mathcal{S}}

\hypertitle[']{\'Equations diff\'erentielles ordinaires}{4}{MAT}{1223}
{Beno\^it Legat}
{Jean Van Schaftingen}

\section{EDO à coefficients constants}
Nous allons traiter la résolution, l'existence et l'unicité des solutions
de l'équation différentielle ordinaire
\[ u'(t) = A[u(t)] \]
où $u\colon I\to\Rn$ est une fonction vectorielle,
$I \subseteq \R $ un intervalle et $t \in I$.
Dans cette section, $A$ sera une matrice à coefficients constants.

On remarque que si $u$ et donc $A$ sont scalaires, la solution est
\[ u(t) = \e^{At}u(0) = \lim_{m\to\infty} \sum_{k=0}^m\frac{(At)^k}{k!}u(0) \]
d'où l'intuition qui semble parachutée de la solution pour $u(t)$ vectoriel
\[ u(t) = \lim_{m\to\infty} \sum_{k=0}^m\frac{(tA)^k[u(0)]}{k!}. \]

Seulement, il nous faut s'assurer de deux choses :
\begin{itemize}
  \item la limite converge bien;
  \item c'est bien solution,
    c'est à dire que c'est dérivable et que $u(t)$ défini
    ainsi satisfait bien $u'(t) = A[u(t)]$.
\end{itemize}

On peut démontrer que c'est bien le cas.
On définit alors l'\emph{exponentielle d'opérateur}
\[ \e^{A}[v] \eqdef \lim_{k\to\infty} \sum_{i=0}^k \frac{A^i[v]}{i!}. \]
Il faut faire attention à ne pas se laisser tromper par la notation.
Ce n'est pas un exponentielle comme les autres, on n'a pas toutes les propriétés
des exponentielles classiques.
Par exemple, en général, $\e^{A + B} \neq \e^A \circ \e^{B}$.
On a par contre
\begin{align*}
  \fdif{}{t}(\e^{tA}[v]) & = \e^{tA} \circ A[v]\\
  A \circ \e^A & = \e^A \circ A.
\end{align*}

%faudrait enlever les s à propriétés en général
%t'en définis qu'une à la fois nan ? ;)
\begin{myprop}[Existence, unicité et solution]
  Soit $A \in \lin(\Rn, \Rn)$ et $u_0 \in \Rn$,
  il \emph{existe} une solution
  $u\colon \R\to\Rn$ du problème
  \begin{align*}
    u'(t) & = A[u(t)]\\
    u(t_0) & = u_0.
  \end{align*}
  Cette solution est $u(t) = \e^{(t-t_0)A}[u_0]$ et est \emph{unique}.
\end{myprop}
Pour $u'(t) = A[u(t)] + b(t)$, la solution devient
\[ u(t) = \e^{(t-t_0)A}[u_0] + \int_{t_0}^t \e^{(t-s)A}[b(s)]\dif s. \]

On remarque que l'exponentielle d'opérateurs se simplifie grandement
pour les valeurs propres généralisées (voir l'annexe~\ref{ann:vpg}) de $A$.

En effet, si $\lambda$ est une valeur propre généralisée d'ordre $k$
\emph{et que $v$ est un vecteur propre généralisé de valeur propre $\lambda$},
on a
\[ \e^{tA}[v] =
\e^{\lambda t} \sum_{i=0}^{k-1}\frac{t^i}{i!}(A-\lambda I)^i[v]. \]

C'est un résultat assez intéressant.
En effet, comme l'EDO est linéaire, si on a une condition initiale
$v$ qui n'est pas un vecteur propre, on pourra la décomposer en une somme
de vecteurs propres $v_i$ à l'aide de Gram-Schmidt
\footnote{Voir la synthèse de Mathématique Q2.}. % TODO biblio
On pourra alors tout simplement écrire
\[ \e^{tA}[v] =
\sum_{i=1}^m \e^{\lambda_i t}
\sum_{j=0}^{k_i-1}\frac{t^j}{j!}(A-\lambda_i I)^j[v_i] \]
où $\lambda_1, \ldots, \lambda_m$ sont les $m$ valeurs propres généralisées
de degré $k_i$ où $\sum_{i=1}^m k_i = n$.

La projection des $v$ en vecteurs propres $v_i$ peut être modélisée
par un produit matriciel $v_i = P_i v$.
On peut alors mettre en évidence $v$ dans la formule précédente et en posant
$B_{i,j} = (A-\lambda_i I)^j P_i$, on a
\[ \e^{tA} =
\sum_{i=1}^m \e^{\lambda_i t}
\sum_{j=0}^{k_i-1}\frac{t^j}{j!}B_{i,j}. \]
On pose $B_{i,j}$ ici pour mettre en évidence le fait que le plus important,
c'est qu'il existe une certaine matrice $B_{i,j}$,
sa valeur exacte n'est pas utilisée aussi souvent que le fait que $B_{i,j}$
existe.

En effet, un des corrolaires importants de cette identité est que
si $\sigma > \Re(\lambda)$ $\forall \lambda \in \spec A$,
il existe $C \geq 1$ tel que
\[ \norm{\e^{tA}} \leq C\e^{\sigma t}. \]
C'est une inégalité assez utile,
ça permet par exemple de dire que si
$\Re(\lambda) < 0$, $\forall \lambda \in \spec(A)$,
$\lim_{t\to 0} \e^{tA} = 0$.
% FIXME ref stabilité ?

\section{EDO à coefficients variables}
Nous allons étudier l'EDO suivante.
Soit $I \subseteq \R$ un intervalle et $u\colon I\to\Rn$ dérivable tel que
\begin{align*}
  u'(t) & = A(t)[u(t)] + f(t) & \forall t \in I\\
  u(t_0) & = u_0
\end{align*}
où $A \colon I \to \lin(\Rn)$ est continue.

On peut démontrer que cette EDO a toujours une solution et que cette
solution est unique.
C'est à dire que si on a deux fonctions $u_1$ et $u_2$ qui satisfont
l'EDO pour les mêmes $t_0$ et $u_0$, on a en fait
$u_1(t) = u_2(t)$ $\forall t \in I$.

\subsection{Structure de la solution}
\label{sec:struct}
Analysons à présent, dans le cas où $f = 0$,
la structure de la solution avant d'y avoir appliqué la condition initiale
$u(t_0) = v$.
On a donc $u'(t) = A(t)[u(t)]$.
Notons $\sol$ l'ensemble des solutions.
Pour les éléments qui vont suivre,
il est important que $I$ soit un \emph{intervalle}.
Si ce n'est plus le cas, problème il y aura.

On remarque alors que $\sol$ est un espace vectoriel
de dimension $n$.
On remarque aussi que
la famille $u_1(t_0), \ldots, u_k(t_0)$ est une base de $\sol$
si et seulement si
la famille de solutions $v_1, \ldots, v_k$
est une base de $\Rn$.
On a même cette propriété séparément pour famille libre et pour
famille génératrice.

On peut alors trouver la base de la solution assez aisément.
Il nous suffit de prendre une base $v_i$ de $\Rn$
et puis de prendre la famille
formée par les solutions ayant comme conditions initiales $u(t_0) = v_i$.
En pratique, on va plutôt utiliser la méthode de réduction d'ordre
abordée à la section~\ref{sec:redordre}.

\subsection{Résolvante}
La résolvante est une matrice qui permet de passer d'un vecteur d'un
temps donné à un autre temps en suivant l'EDO.
Sa définition est la suivante

\begin{mydef}[Résolvante]
  \label{def:res}
  La résolvante $R\colon I \times I \to \lin(R^n)$ est l'unique fonction
  telle que, si
  \[ u'(r) = A(r)[u(r)] \]
  $\forall r \in I$, alors $\forall s, t \in I$
  \[ u(s) = R(s, t)[u(t)]. \]
\end{mydef}
Un moyen mnémotechnique pour savoir dans quel ordre on met $s$ et $t$ dans
les arguments de $R$ est que $s$ doit être proche de $u(s)$ et $t$ proche
de $u(t)$.

\begin{myprop}[Linéarité de la résolvante]
  Soit $u_1,u_2 \in \sol$, pour tout $s,t \in I$,
  \[ R(s,t)[\alpha u_1(t) + \beta u_2(t)] = \alpha R(s,t)[u_1(t)]
  + \beta R(s,t)[u_2(t)] \]
  \begin{proof}
    Si $u_1$ et $u_2$ sont solutions, comme $\sol$ est un sous
    espace vectoriel, $\alpha u_1 + \beta u_2 \in \sol$.
    Du coup, par la définition~\ref{def:res} de la résolvante,
    $R(s,t)[\alpha u_1(t) + \beta u_2(t)] = \alpha u_1(s) + \beta u_2(s)$,
    $R(s,t)[u_1(t)] = u_1(s)$ et $R(s,t)[u_2(t)] = u_2(s)$.
    D'où l'égalité demandée.
  \end{proof}
\end{myprop}

On peut montrer que $u(t)$ est solution de l'EDO si et seulement si
\[ u(t) = R(t, t_0)[u_0] + \int_{t_0}^t R(t,s)[f(s)] \dif s. \]
On a aussi que
\[ R(t, s) = R(t, r) \circ R(r, s). \]

Pour la calculer, on prend $n$ solutions $u_i$ linéairement indépendantes
de $u'(t) = A(t)[u(t)]$, c'est-à-dire une base de $\sol$, et on prend

\[ R(s, t) =
  \begin{pmatrix}
    u_1(s) & \cdots & u_n(s)
  \end{pmatrix}
  \begin{pmatrix}
    u_1(t) & \cdots & u_n(t)
\end{pmatrix}^{-1}. \]
En effet, on a $R(s, t) u_i(t) = u_i(s)$ $\forall i$.

\subsection{Méthodes de résolution}
On va maintenant analyser comment résoudre l'EDO en pratique.
On doit pour cela utiliser 2 méthodes,
la première utilise une base de $\sol$ qu'on sait calculer grâce à la deuxième.

\subsubsection{Méthode de la variation de la constante}
Comme vu à la section~\ref{sec:struct},
la solution $\sol$ de
$u'(t) = A(t)[u(t)]$ $\forall t \in I$
est un espace vectoriel de dimension $n$.

On commence par calculer une base $u_1,\ldots,u_n$ de cet espace vectoriel,
ce qui peut se faire par la méthode de réduction d'ordre abordée
à la section~\ref{sec:redordre}.
Toute solution de l'EDO pour $f = 0$ s'écrit alors comme une combinaison
linéaire des $u_i$.
L'astuce de cette méthode est de se dire qu'en ne prenant pas une combinaision
linéaire avec des coefficients \emph{constants} $\alpha_i$ mais des coefficients
\emph{variables} $\alpha_i(t)$,
on va pouvoir trouver une solution avec $f \neq 0$.

On trouve en effet une solution en prenant $\alpha_i(t)$ tels que
\begin{align}
  \label{eq:dalpha}
  \sum_{i=1}^n \alpha_i'(t)u_i(t) & = f(t) & \forall t \in I\\
  \label{eq:cialpha}
  \sum_{i=1}^n \alpha_i(t_0)u_i(t_0) & = u_0.
\end{align}
On trouve $\alpha_i'(t)$ grâce aux
$n$ équations données par \eqref{eq:dalpha}
et il nous reste plus qu'à intégrer à l'aide des
$\alpha_i(t_0)$ trouvés grâce aux
$n$ équations données par \eqref{eq:cialpha}
pour trouver les $\alpha_i(t)$
et donc $u(t)$.

\paragraph{Équation du second ordre}
Pour une équation du second ordre
\[ w''(t) + \gamma w'(t) + \delta w(t) = f(t) \]
avec $w$ une fonction réelle,
on peut poser $u = (w,w')$ et faire la méthode de réduction d'ordre
classique mais on peut aussi se simplifier la vie dans ce cas en posant,
si on a deux solutions homogènes $w_1,w_2$,
$w(t) = \alpha(t) w_1(t) + \beta(t) w_2(t)$.

Seulement, quand on calcule
\[ w'(t) = \alpha'(t) w_1(t) + \beta'(t) w_2(t)
+ \alpha(t) w_1'(t) + \beta(t) w_2'(t), \]
il faut poser $\alpha'(t) w_1(t) + \beta'(t) w_2(t) = 0$.
On a alors
\[ w''(t) = \alpha'(t) w_1'(t) + \beta'(t) w_2'(t)
+ \alpha(t) w_1''(t) + \beta(t) w_2''(t). \]

Ce qui fait 2 équations différentielles pour $\alpha'$ et $\beta'$ ce qui
nous permettra de trouver $\alpha$ et $\beta$ à l'aide des conditions initiales
sur $w(t_0)$ et $w'(t_0)$.

\subsubsection{Méthode de la réduction d'ordre}
\label{sec:redordre}
Observons maintenant comment calculer cette base $u_1,\ldots,u_n$.
On ne va traiter ici que de la réduction d'ordre pour $n=2$.
On a donc à résoudre
\[ u'(t) = A(t)u(t). \]
On va avoir besoin d'une première solution $u_1 \neq 0$,
on sait qu'il en existe une mais
il faut avoir un peu d'intuition pour la trouver.

Il nous faut maintenant prendre un vecteur $e$ tel que $\la e,u_1 \ra$
ne s'annule jamais.
Il est toujours possible de trouver ce $e$ pour un certain intervalle
car $u_1$ ne s'annule jamais partout à moins que
$u_1$ soit la solution triviale nulle mais on a supposé que $u_1 \neq 0$ car
on ne sait pas construire de solution à partir de rien.

On pose ensuite (sans perte de généralité, disons qu'on ait pris $(0,1)$)
\[ u_2(t) = \alpha(t)u_1(t) + \beta(t)(0,1) \]
avec $\alpha,\beta$ des fonctions scalaires à déterminer.
On écrit alors $u_2'(t) = A[u_2(t)]$.
Il devrait apparaitre à gauche $\alpha(t) u_1'(t)$ et à droite
$\alpha(t) A[u_1(t)]$ qu'on peut simplifier.
Ça nous fait un système de deux équations différentielles du premier degré
en $\alpha(t)$ et $\beta(t)$ qu'il faut résoudre pour finalement trouver
$u_2(t)$.
Seulement, ce système est plus facile à résoudre que celui de départ
car dans la première équation, on a simplement
\[ \alpha'(t)u_{1;1}(t) = \beta(t)A_{1,2} \]
où $u_{1;1}$ est la première composante de $u_1$ et $A_{1,2}$ est
la composante de $A$ sur la première ligne et la deuxième colonne.
D'où on tire directement $\beta(t)$ qu'on peut réinjecter dans la deuxième
où il n'y aura alors que des $\alpha$ qu'on peut alors trouver ce qui nous
donne $\beta$ en repassant à la première équation.

\paragraph{Équation du second ordre}
Pour une équation du second ordre
\[ w''(t) + \beta w'(t) + \gamma w(t) = 0 \]
avec $w$ une fonction réelle,
on peut poser $u = (w,w')$ et faire la méthode de réduction d'ordre
classique mais on peut aussi se simplifier la vie dans ce cas en posant,
si on a une solution $w_1$,
$w_2(t) = \alpha(t) w_1(t)$.
Ça fonctionne toujours pour une équation du second ordre.

\section{EDO non linéaires}
On va étudier les équations du type
\[ u'(t) = f(t, u(t)). \]

\subsection{Existence et unicité}
Commençons par définir ce qu'est un champ de vecteurs et
une courbe intégrale
\begin{mydef}[Champ de vecteurs]
  Soit $\Omega \subseteq \R \times \Rn$,
  un \emph{champ de vecteurs} sur $\Omega$ est une fonction
  $f\colon \Omega \to \Rn$.
\end{mydef}
\begin{mydef}[Courbe intégrale]
  Soit $\Omega \subseteq \R \times \Rn$ et $f\colon \Omega\to\Rn$
  un champ de vecteurs sur $\Omega$.
  Soit $I \subseteq \R$ un intervalle \emph{ouvert}.
  La fonction $u\colon I\to\Rn$ est une \emph{courbe intégrale} de $f$ si $u$
  est \emph{dérivable} et si pour tout $t\in I$, $(t, u(t)) \in \Omega$ et
  \[ u'(t) = f(t, u(t)). \]
\end{mydef}
En gros, une courbe intégrale est une courbe qui respecte l'équation
$u'(t) = f(t, u(t))$ et qui reste dans le champ de vecteurs $\Omega$
($(t, u(t)) \in \Omega$).

On peut s'assurer de l'existence d'une courbe intégrale dans un domaine
borné en s'assurant que $\norm{f}$ ne dépasse pas une limite.
C'est comme quand la police cherche un prisonnier mais qu'elle sait
qu'il est à pied et donc qu'il ne va pas à plus de
% 12 km/h pfff petit joueur
\si{12}{\kilo\meter\per\hour}, elle peut dessiner un cercle de
\si{6}{\kilo\meter} de rayon
et elle est sûre que le prisonnier n'en sortira pas en moins
d'une demi-heure.
C'est ce qui est dit de manière plus formelle par
la propriété~\ref{prop:cauchy_peano}.
\begin{myprop}[Cauchy-Peano]
  \label{prop:cauchy_peano}
  Soit $t_0 \in \R$, $h > 0$, $r > 0$,
  $u_0 \in \Rn$ et $f\colon \intervaloo{t_0-h}{t_0+h} \times B(u_0,r)\to\Rn$ un
  champ de vecteurs continu.
  Si pour tout $(t,x) \in \intervaloo{t_0-h}{t_0+h} \times B(u_0,r)$,
  \[ h \norm{f(t,x)} \leq r, \]
  alors il existe $u\colon \intervaloo{t_0-h}{t_0+h} \to B(0,r)$
  qui soit une courbe intégrale de $f$.
\end{myprop}

Pour l'unicité, on va devoir utiliser le concept de champ lipschitzien.
\begin{mydef}[Champ de vecteurs lipschitzien en espace]
  Soit $\Omega \subseteq \R \times \Rn$.
  Le champ de vecteurs $f\colon \Omega\to\Rn$ est \emph{lipschitzien
  en espace}, s'il existe $L \geq 0$ tel que pour tout
  $t\in\R$, $x,y\in\Rn$, si $(t,x) \in \Omega$ et $(t,y) \in \Omega$, alors
  \[ \norm{f(t,x) - f(t,y)} \leq L \norm{x - y}. \]
\end{mydef}
\begin{mydef}[Champ de vecteurs localement lipschitzien en espace]
  Soit $\Omega \subseteq \R \times \Rn$.
  Le champ de vecteurs $f\colon \Omega\to\Rn$ est \emph{localement lipschitzien
  en espace}, si pour tout
  $(t,x) \in \Omega$,
  il existe $r > 0$ et $h > 0$ tels que
  $f$ est lipschitzien en espace sur
  $\intervaloo{t-h}{t+h} \times B(x,r) \subseteq \Omega$.
\end{mydef}
On remarque que si $f$ est lipschitzien, alors $f$ est localement lipschitzien
et que si $f$ est de classe $C_1$ en espace, alors $f$ est localement
lipschitzien en espace.

L'élément clef qui fait qu'il est plus facile d'être localement lipschitizen
que lipschitzien c'est que pour localement lipschitzien, $L$ peut changer
pour chaque $\intervaloo{t-h}{t+h} \times B(x,r)$.
C'est ce qui aide dans l'exemple~\ref{ex:tabsx} et l'exemple~\ref{ex:harm}.

Essayons de comprendre tout cela avec des exemples.
\begin{myexem}
  \label{ex:ux}
  Prenons $\Omega = \R\times\R\setminus\{0\}$ et $f\colon \Omega\to\R$ définit par
  \[
    f(t,x) =
    \begin{cases}
      0 & x < 0\\
      1 & x \geq 0.
    \end{cases}
  \]
  On remarque que $f$ est pas dérivable en $0$ mais est localement
  lipschitzien.
  Par contre, il n'est pas lipschitzien.
  En effet, si on prend $x < 0$ et $y > 0$, on prenant $x$ et $y$ aussi
  prochent de $0$ que l'on veut,
  on sait avoir $\frac{\norm{f(t,y)-f(t,x)}}{\norm{y-x}}$
  aussi grand que l'on veut.

  Il est localement lipschitzien car même en prenant $x$
  aussi proche de $0$ que l'on veut,
  il suffit de prendre $r \leq \abs{x}$.
  On n'aura pas de problème avec $x = 0$ car il n'est pas dans $\Omega$.
\end{myexem}
\begin{myexem}
  \label{ex:harm}
  Prenons $\Omega = \R\times\R_{>0}$ et
  $f\colon \Omega\to\R\colon f(t,x)\mapsto \frac{1}{x}$.
  $f$ est de classe $C_1$ sur $\Omega$ et est donc localement lipschitzien
  mais n'est pas lipschitzien.
  En effet, en prenant $x < y$, on a
  \[ \frac{\norm{\frac{1}{x}-\frac{1}{y}}}{\norm{x-y}} = \frac{1}{xy} \]
  qui peut être aussi grand que l'on veut en faisant tendre $x$ vers 0.

  Bien que le fait que $f$ soit de classe $C_1$ implique que $f$ est
  localement lipschitizien,
  ça peut paraitre bizarre si on oublie qu'on peut changer de $L$ pour
  chaque localité.
  En effet, pour $(t,x) \in \R\times\R_{>0}$,
  en prenant $r < x$
  on voit que $f$ est lipchitzien sur $\intervaloo{t-h}{t+h} \times B(x,r)$
  avec
  \[ L = \frac{1}{x(x-r)}. \]
\end{myexem}
\begin{myexem}
  Prenons $\Omega = \R\times\R$ et $f\colon \Omega\to\R$ défini par
  \[
    f(t,x) =
    \begin{cases}
      0 & x \neq 0\\
      1 & x = 0.
    \end{cases}
  \]
  Ici $f$ n'est pas continue donc pas dérivable ni localement lipschitzien
  donc pas lipschitzien non plus.
  En effet, pour $x = 0$, prendre $r$ aussi petit que l'on veut ne change rien,
  en prenant $y$ suffisamment proche de $x$, on a $\norm{1}/\norm{x-y}$,
  aussi grand que l'on veut.
\end{myexem}
\begin{myexem}
  \label{ex:tabsx}
  Prenons $\Omega = \R\times\R$ et $f\colon \Omega\to\R$ défini par
  \[
    f(t,x) = t\abs{x}.
  \]
  Ici, bien que $f$ ne soit pas dérivable si $t \neq 0$,
  $f$ est bien localement lipschitzien
  mais n'est pas lischitzienne.
  En effet, en prenant sans perte de généralité $x < y$, on a,
  si $x$ et $y$ sont de même signe
  \[ \frac{\norm{t\abs{x} - t\abs{y}}}{\norm{x-y}} = \abs{t} \]
  et s'il sont de signe opposé (alors $y \geq 0$ et $x \leq 0$ car $x < y$)
  \[ \frac{\norm{t\abs{x} - t\abs{y}}}{\norm{x-y}}
  = \abs{t}\frac{\abs{x}+\abs{y}}{\abs{x}+\abs{y}} = \abs{t} \]
  On voit que si on ne restreint pas $t$, on ne trouvera pas
  de $L$, $f$ n'est donc pas lipschitzien.
  Par contre, si on avait pris $\Omega = \intervalcc{-1}{1} \times\R$,
  $f$ aurait été lipschitzien en espace car on aurait pu prendre $L = 1$
  car $\abs{t} \leq 1$ $\forall t \in \intervalcc{-1}{1}$.

  $f$ est de toute façon localement lipschitzien car pour tout
  $(t,x) \in \R^2$, $f$ est lipschitzien sur $\intervaloo{t-h}{t+h} \times B(x,r)$ avec
  $L = \max(\abs{t-h},\abs{t+h})$.
\end{myexem}
\begin{myexem}
  \label{ex:sqrtx}
  Prenons $\Omega = \R\times\R_{\geq 0}$ et $f\colon \Omega\to\R$ défini par
  \[
    f(t,x) = \sqrt{x}.
  \]
  Ici, $f$ n'est pas dérivable en 0 ni localement lipschitzien
  et donc pas lipschitzien non plus.
  En effet, pour $x < y$, en prenant $x$ et $y$ de plus en plus petits,
  on a
  \[ \frac{\norm{\sqrt{x}-\sqrt{y}}}{\norm{x-y}} =
  \frac{\sqrt{y} - \sqrt{x}}{y-x} =
  \frac{\sqrt{y} - \sqrt{x}}{(\sqrt{y} - \sqrt{x})(\sqrt{y} + \sqrt{x})}
  = \frac{1}{\sqrt{x}+\sqrt{y}} \]
  qui devient aussi grand que l'on veut.
  On remarque qu'on aurait pu se simplifier la vie en prenant $x = 0$
  directement mais on voit mieux ici le lien entre être lipschitzien et avoir
  une dérivée qui tend vers $\infty$.
  On peut être non dérivable et être localement lipschitzien comme pour
  l'exemple~\ref{ex:tabsx} mais
  lorsque la dérivée tend vers $\infty$, c'est difficile d'être
  lipschitzien.

  $f$ n'est pas localement lipschitzien à cause de $x=0$.
  En effet, prendre $r$ aussi petit que l'on veut ne changera rien,
  $\frac{1}{\sqrt{y}}$ tend vers $\infty$ quand $y$ tend vers $x = 0$.

  Par contre, si on avait pris $\Omega = \R \times \R_{>0}$, $f$
  aurait été \emph{dérivable} donc \emph{localement lipschitzien} mais
  pas \emph{lipschitzien}.
\end{myexem}
On remarque que continu ne veut pas dire localement lipschitzien avec
l'exemple~\ref{ex:sqrtx} et que localement lipschitzien ne veut pas
dire continu non plus avec l'exemple~\ref{ex:ux}.
Seulement, l'exemple~\ref{ex:ux} est assez spécial car $\Omega$ n'est pas
convexe.
Si $\Omega$ est convexe, localement lipschitzien implique continu.

On a alors deux théorèmes.
Un pour l'unicité locale et un pour l'unicité globale.
Le théorème de l'unicité globale est en fait strictement plus
fort que celui de l'unicité locale.
En effet, pour l'unicité globale, on demande juste un $\Omega$ ouvert
alors que pour l'unicité locale, on demande un
$\Omega=\intervaloo{t_0-h}{t_0+h} \times B(u_0,r)$
pour un certain $h>0$ et un certain $r>0$.
Alors qu'on est plus restrictif pour l'unicité locale,
on va demander que $f$ soit \emph{continue} et \emph{lipschitzien en espace}
alors que l'unicité globale ne demandera que d'être \emph{continu} et
\emph{localement lipschitzien en espace}.

En effet, il suffit dans l'unicité locale de n'être que localement
lipschitzien (et toujours continu bien entendu)
pour garantir l'unicité, ça se démontre avec l'unicité globale.
On pourrait donc se poser la question de l'intérêt de l'unicité
locale si la propriété est aussi inutile en pratique.
L'intérêt est qu'elle est bien plus simple à prouver et que l'unicité
globale l'utilise en fait dans sa preuve.
Elle est donc utile en théorie mais inutile en pratique.

\begin{myprop}[Unicité locale]
  Soit $t_0 \in \R$, $h > 0$, $r > 0$, $u_0 \in \Rn$ et
  $f\colon \intervaloo{t_0-h}{t_0+h} \times B(u_0,r) \to \Rn$ un champ de vecteurs
  \emph{continu} et \emph{lipschitzien en espace}.
  Si $u\colon \intervaloo{t_0-h}{t_0+h} \to\Rn$ et $v\colon \intervaloo{t_0-h}{t_0+h} \to\Rn$ sont des courbes
  intégrales de $f$ et si $u(t_0) = v(t_0)$, alors pour tout
  $t \in \intervaloo{t_0-h}{t_0+h}$,
  \[ u(t) = v(t). \]
\end{myprop}
\begin{myprop}[Unicité globale]
  Soit $\Omega \subseteq \R\times\Rn$ un ouvert,
  $f\colon \Omega\to \Rn$ un champ de vecteurs \emph{continu},
  $I \subseteq \R$ un intervalle ouvert.
  Soient $u\colon I\to \Rn$ et $v\colon I \to \Rn$ deux courbes intégrales de $f$.
  Si $f$ est \emph{localement lipschitzien} en espace
  et s'il existe $t_0 \in I$
  tel que $u(t_0) = v(t_0)$, alors pour tout $t \in I$,
  \[ u(t) = v(t). \]
\end{myprop}

\subsection{Courbe intégrale maximale}
On va maintenant s'intéresser à l'intervalle
sur lequel on définit une solution.
En effet, lorsqu'on a une solution,
on pourrait très bien la tronquer et avoir toujours une solution.
On va donc définir une \emph{courbe intégrale maximale} comme une courbe
qu'on ne peut plus étendre.
Si par exemple on dit que $u\colon I\to\R$ où $I = \intervaloo{t_1}{t_2}$
est une courbe intégrale maximale,
soit $t_1 = -\infty$, soit $\lim_{t\to t_1} u(t) = \pm\infty$.
On remarque aussi que $I$ doit être ouvert, en effet, si $I$ est fermé en
$t_2$ par exemple, $u$ ne peut pas être maximale car si elle est définie en
$t_2$, c'est qu'on n'explose pas encore et donc qu'on peut agrandir $I$
à sa droite.

Il y a cependant une exception à ça quand $f$ n'est pas défini sur tout
$\R \times \Rn$.
En effet, soit $\Omega$ le domaine de définition de $f$, on peut dire
qu'aux extrémités de $I$, $(t,u(t))$ sort de $\Omega$, c'est-à-dire par
exemple si $\Omega = \intervaloo{0}{\infty} \times \Rn$ qu'à l'extrémité
gauche de $I$, $u$ n'est pas obligé d'exploser.
Si par contre $\Omega = \R \times \R_{\ge 0}$,
$u$ n'est pas obligé d'exploser non plus, il suffit qu'il devienne nul.

Un théorème nous garantit que si dans
un $\Omega \subseteq \R\times\Rn$, on a $f$ qui est \emph{continue}
et \emph{localement lipschitzienne},
alors si pour un certain $(t_0, u_0) \in \Omega$, on impose
$u(t_0) = u_0$, on est non seulement sûr de trouver un courbe intégrale
maximale mais en plus elle est unique.
Ce n'est pas la même unicité que celle garantie par l'unicité globale car
ici, on garantit aussi qu'il y a une seul intervalle $I$ pour lequel $u$
est une courbe intégrale maximale et $u(t_0) = u_0$.

Voyons maintenant comment déterminer si une fonction $u$ est une
courbe intégrale maximale.
Une fonction $u\colon I\to\R$ est une courbe intégrale maximale
si et seulement si
$I$ est un intervalle \emph{ouvert non vide} et que,
pour tout compact $K \subseteq \Omega$,
\begin{align*}
  \sup\{t \in I\colon (t, u(t)) \in K\} & \in I\\
  \inf\{t \in I\colon (t, u(t)) \in K\} & \in I.
\end{align*}

On remarque tout de suite l'importance d'imposer que $I$ soit ouvert
car s'il est fermé, les deux inclusions sont trivialement respectées
alors que $u$ ne peut pas être maximale avec un $I$ fermé.

Il faut comprendre ce théorème comme une manière rigoureuse de dire que
$u$ explose aux deux bornes ouvertes de $I$.
En effet, si $I$ explose à droite de $I$,
quel que soit le compact qu'on prend, on coupera toujours $u(t)$ pour un
$t < t_2$, en effet, on tend vers l'infini en $t_2$ et c'est impossible
pour un compact de ne pas intercepter cette asymptote.

Par contre, si on n'explose pas en $t_2$, un compact bien choisi
contiendra $(t_2, \lim_{t \to t_2} u(t))$ et le suprémum sera alors $t_2$
qui n'appartient pas à $I$ car $I$ est ouvert.

\section{Stabilité}
Dans ce chapitre, on s'intéressera uniquement aux
champs de vecteurs $f$ indépendants du temps.
On s'intéresse donc à la stabilité des solutions d'une EDO de la forme
\begin{align*}
  u'(t) & = f(u(t))\\
  u(0) & = x
\end{align*}
lorsqu'on modifie $x$.
C'est à dire que si on passe de $x = u_0$ à $x = u_0 + \delta$,
on va regarder si
la solution va être proche ou très différente de celle avec $u_0$.

On peut définir la stabilité de différentes manières.
On va en étudier deux.

\subsection{Équilibre stable}
Un équilibre est \emph{stable} (au sens de Lyapounov)
si on peut s'assurer que la
solution ne s'écarte pas de plus de $\epsilon>0$
en prenant un $x$ qui n'est pas
à une distance de $u_0$ de plus que $\delta>0$ avec un $\delta$
suffisamment petit.

Il existe une condition suffisante pour que $u_0$ soit un équilibre stable.
Il suffit que $f$ soit \emph{localement lipschitzien} et qu'il existe
$V \in C^1(\Omega;\R)$ tel que pour toute courbe intégrale $u\colon I\to\Rn$ de $f$,
$V \circ u$ soit \emph{décroissante} et
que pour tout $x\in\Omega\setminus\{u_0\}$,
$V(u_0) < V(x)$.

\subsection{Équilibre asymptotiquement stable}
Un équilibre est asymptotiquement stable (au sens de Lyapounov)
s'il est \emph{stable}
et qu'on peut s'assurer qu'en prenant $x$ qui n'est pas à une distance
de $u_0$ de plus de $\delta>0$ avec un $\delta$ suffisamment petit,
la solution tendra vers $u_0$
\[ \lim_{t\to\infty}u(t) = u_0. \]
Il est important de remarquer qu'un équilibre \emph{asymptotiquement stable}
doit être un équilibre stable.
Si on ne l'avait pas demandé, on aurait juste
eu un équilibre \emph{attractif}.

On a à nouveau une condition suffisante
pour que $u_0$ soit un équilibre asymptotiquement stable.
C'est d'ailleurs la même que pour que ce soit un équilibre stable
sauf qu'on exige une décroissance stricte.
Il suffit que $f$ soit \emph{localement lipschitzien} et qu'il existe
$V \in C^1(\Omega;\R)$ tel que pour toute courbe intégrale $u\colon I\to\Rn$
\emph{dans} $\Omega \setminus \{u_0\}$ de $f$,
$V \circ u$ soit \emph{strictement décroissante} et
que pour tout $x \in \Omega \setminus \{u_0\}$,
$V(u_0) < V(x)$.

\paragraph{Décroissance et $u_0$}
Une erreur classique à éviter est d'oublier que $V \circ u$ ne doit
être strictement décroissante que pour $u \neq u_0$.
En effet, si $u = (u_1,u_2)$, on voit que
\[ (V \circ u)' = \fpart{V}{u_1} u_1' + \fpart{V}{u_2} u_2' =
\grad V \cdot f(u). \]
Comme pour un équilibre $u_0$, $f(u_0) = 0$, c'est difficile que
$(V \circ u)'(u_0) < 0$.

\subsection{Analyse de la stabilité avec la jacobienne}
Si $f$ est \emph{localement lipschitzien},
que $f(u_0) = 0$ et que $f$ est dérivable
en $u_0$.
On peut analyser la stabilité de $u_0$ à l'aide de la Jacobienne de $f$
en $u_0$, $Df(u_0)$ (aussi notée $Jf(u_0)$).

Soient $\lambda_i$ les valeurs propres de $Df(u_0)$.
\begin{itemize}
  \item Si $\forall i$ $\Re(\lambda_i) < 0$, $u_0$
    est un équilibre asymptotiquement stable;
  \item Si $\exists i$ $\Re(\lambda_i) > 0$, $u_0$
    est un équilibre instable;
  \item Si $\forall i$ $\Re(\lambda_i) \leq 0$ et que $\forall i$ tel que
    $\Re(\lambda_i) = 0$, $m_{\textnormal{g}}(\lambda_i) =
    m_{\textnormal{a}}(\lambda_i)$,
    $u_0$ est un équilibre stable;
  \item Si $\forall i$ $\Re(\lambda_i) \leq 0$ et que $\exists i$ tel que
    $\Re(\lambda_i) = 0$, où $m_{\textnormal{g}}(\lambda_i) <
    m_{\textnormal{a}}(\lambda_i)$,
    $u_0$ est un équilibre instable.
\end{itemize}

Les deux premiers items sont sûrs mais \emph{pas les deux suivants}.
En effet, comme $f$ est localement lipschitz et continu (car dérivable),
on sait qu'il existe une solution mais pas nécessairement qu'elle est maximale
sur $\R$ or la stabilité recquiert
que $u$ soit défini sur $\intervalco{0}{\infty}$.

\section{Problèmes aux limites}
Un problème aux limites est un problème dans lequel,
contrairement au problème de Cauchy,
on cherche une fonction $u$ qui satisfasse des conditions initiales
à deux points différents $a$ et $b$ et non à un seul point $t_0$.

C'est une EDO de la forme
\begin{align*}
  u'(t) & = A(t)[u(t)] + f(t) & \forall t \in \intervalcc{a}{b}\\
  B_{\textnormal{a}}[u(a)] & = B_{\textnormal{b}}[u(b)]
\end{align*}
où $f\colon \intervalcc{a}{b}\to\Rn$, $A \in C(\intervalcc{a}{b};\Rn)$ et $B_{\textnormal{a}},B_{\textnormal{b}} \in \lin(\Rn,\R^p)$.

On va maintenant voir une condition nécessaire et suffisante pour
l'existence et l'unicité.
\subsection{Existence}
Pour qu'il existe une fonction $u$,
il faut et il suffit d'imposer une condition sur $f$.
C'est à dire que
\[ \int_a^b \la v(s),f(s)\ra \dif s = 0. \]
pour tout $v$ tels que
\begin{align*}
  v'(t) & = -A(t)^*[v(t)] + f(t) && \forall t \in \intervalcc{a}{b}\\
  \la \alpha,v(a)\ra & = \la \beta,v(b)\ra &&
  \forall \alpha,\beta \in \Rn \suchthat B_{\textnormal{a}}[\alpha] = B_{\textnormal{b}}[\beta]
\end{align*}
où $A(t)^*$ est la matrice adjointe de $A$ c'est à dire la matrice telle
que $\la A(t)^*x, y\ra = \la x,A(t)y \ra$ pour tout $x,y \in \Rn$.

\subsection{Unicité}
Pour s'assurer qu'il existe au plus une fonction $u$,
comme notre problème est linéaire,
il faut et il suffit que la seule solution
du problème homogène (donc avec $f = 0$)
soit la solution nulle.

En effet,
on sait comme c'est linéaire que l'ensemble des solutions
est l'ensemble des solutions homogènes dont on ajoute
pour chacune une même solution particulière.
Il y a donc autant de solutions que de solutions homogènes.

\annexe

\section{Valeurs propres généralisées}
\label{ann:vpg}
Un scalaire $\lambda$ est une valeur propre d'une matrice carrée $A \in \Rnn$
si et seulement si $A - \lambda I$ est singulière.
C'est à dire que $\det(A-\lambda I) = 0$ ou encore
$\ker(A - \lambda I) \neq \{0\}$, ce qui signifie encore que
le sous espace vectoriel regroupant les solutions de
\[ (A - \lambda I)x = 0 \]
a une dimension plus grande que 0.
Les solutions de $(A - \lambda I)x = 0$ sont appelés les vecteurs propres
de $\lambda$.

La propriété clef des valeurs propres c'est que si on arrive
à trouver une base de $\Rn$ composée uniquement de vecteurs propres,
on peut exprimer le produit matriciel $Av$ comme une combinaison linéaire
de ces vecteurs propres.
En effet, si la base est $v_1, \dots, v_n$ de valeurs propres respectives
$\lambda_1, \dots, \lambda_n$,
si les coordonnées de $v$ dans la base sont $\alpha_1, \dots, \alpha_n$,
on a
\[ Av = A(\alpha_1v_1 + \dots + \alpha_nv_n) =
\lambda_1\alpha_1v_1 + \dots + \lambda_n\alpha_nv_n. \]

Une autre manière de dire cela est qu'il existe une matrice
diagonale $D$ et une matrice inversible $P$ telles que
\[ A = PDP^{-1}. \]
$P$ est alors la matrice qui transforme $v$ en ses coordonnées dans
la base $v_1, \dots, v_n$, $D$ a sur sa diagonale les valeurs propres
$\lambda_1, \dots, \lambda_n$ et $P$ permet de passer des coordonnées
du résultat dans la base $v_1, \dots, v_n$ au vecteur $Av$.
Les colonnes de $P$ sont donc les vecteurs $v_1, \dots, v_n$.

Cette propriété ne marche pas pour n'importe quelle matrice,
elle nécessite qu'on puisse trouver une base composée de vecteurs propres.
Une matrice pour laquelle ça marche est dite \emph{diagonalisable}.
Cette tâche qui semble assez difficile est facilitée par le fait qu'une
famille de vecteurs propres de valeurs propres différentes est toujours
libre.
De plus, on voit qu'on sait trouver les valeurs propres en trouvant les
racines du polynôme en $\lambda$ formé par $\det(A - \lambda I)$.
Le théorème fondamental de l'algèbre nous assûre malheureusement
pas qu'il y en a $n$ mais juste que la somme des multiplicités
\[ \sum m_{\textnormal{a}}(\lambda_i) = n. \]

Pour chaque valeur propre $\lambda_i$,
si on arrivait à trouver $m_{\textnormal{a}}(\lambda_i)$ vecteurs propres linéairement
indépendants, on serait sauvé.
On a vu que les vecteurs propres sont les vecteurs appartenant à
$\ker(A - \lambda_iI)$.
Le nombre de vecteurs propres linéairement indépendants est donc
$m_{\textnormal{g}}(\lambda_i) \eqdef \newnull(A-\lambda_iI)$.

On sait que si $\lambda_i$ est une valeur propre,
\[ 1 \leq m_{\textnormal{g}}(\lambda_i) \leq m_{\textnormal{a}}(\lambda_i). \]
Ça nous apporte une bonne et une mauvaise nouvelle
\begin{itemize}
  \item La bonne c'est que si $m_{\textnormal{a}}(\lambda_i) = 1$,
    on sait que $m_{\textnormal{g}}(\lambda_i) = 1$ donc si toutes les valeurs propres
    sont différentes, $A$ est diagonalisable.
  \item La mauvaise c'est que comme $m_{\textnormal{g}}(\lambda_i) \leq m_{\textnormal{a}}(\lambda_i)$,
    si $m_{\textnormal{g}}(\lambda_i) < m_{\textnormal{a}}(\lambda_i)$, ça sera impossible qu'une
    autre valeur propre vienne compenser pour faire
    \[ \sum m_{\textnormal{g}}(\lambda_i) = n. \]
    Mais ça permet de dire qu'une matrice est diagonalisable si et seulement
    si $m_{\textnormal{g}}(\lambda_i) = m_{\textnormal{a}}(\lambda_i)$ $\forall i$.
\end{itemize}

\subsection{Les valeurs propres généralisées à la rescousse}
On a donc une problème avec les $\lambda_i$ pour lesquels
$m_{\textnormal{a}}(\lambda_i) > 1$ et que $m_{\textnormal{g}}(\lambda_i) < m_{\textnormal{a}}(\lambda_i)$.
Notre rêve de trouver $P$ inversible et $D$ diagonale
telles que $A = PDP^{-1}$ tombe alors un peu à l'eau mais
on peut toujours s'en sortir pour exprimer $Av$ sans produit matriciel
une fois qu'on a exprimé $v$ dans une base de vecteurs propres.

Seulement, comme $\sum m_{\textnormal{g}}(\lambda_i) < n$, on ne trouvera pas
de base de vecteurs propres, il faut donc aller chercher plus loin
pour les $\lambda_i$ tels que $m_{\textnormal{g}}(\lambda_i) < m_{\textnormal{a}}(\lambda_i)$.
On va en effet allez chercher non plus seulement dans $\ker(A - \lambda I)$
mais aussi dans $\ker(A - \lambda I)^2$ et si ce n'est pas suffisant dans
$\ker(A - \lambda I)^3$ et ainsi de suite.
On dit donc que $\lambda_i$ est une valeur propre \emph{généralisée}
d'ordre $k$ si $k$ est le plus petit naturel tel que
$\newnull(A - \lambda_i I)^k = m_{\textnormal{a}}(\lambda_i)$.
Les vecteurs propres généralisés sont alors les vecteurs de
$\ker(A - \lambda_i I)^k$.

\end{document}
