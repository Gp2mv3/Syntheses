\documentclass[en,license=none]{../../../eplsummary}

\usepackage{enumerate}
\usepackage{diagbox}
\usepackage[normalem]{ulem}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\DeclareMathOperator{\vect}{vec}

\newcommand{\sprod}[2]{\langle #1, #2 \rangle}

\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\tr}{trace}
\DeclareMathOperator{\newint}{int}

\usepackage{algorithm}
\usepackage{algorithmic}

\hypertitle{Optimization models and methods}{7}{INMA}{2471}
{Beno\^it Legat\and M\'elanie Sedda}
{FranÃ§ois Glineur}

\section{Reminder}
\subsection{Convex set}
\begin{mydef}[Convex set]
  A set $X$ is convex if for all $x,y \in X$ and $\lambda \in [0,1]$,
  $\lambda x + (1-\lambda) y \in X$.
\end{mydef}

\begin{myprop}
  Let $X,Y$ be convex set.
  The intersection $X \cap Y$ and the cartesian product $X \times Y$ are convex
  but the union $X \cup Y$ is \emph{not} necessarily convex.
\end{myprop}

\subsection{Convex function}
\begin{mydef}[Epigraph]
  The epigraph of a function is defined as
  \[ \epi(f) = \{\,(x,t) \in \Rn \times \R \mid f(x) \leq t\,\}. \]
\end{mydef}

\begin{mydef}[Convex function]
  A function $f$ is convex iff $\epi(f)$ is convex.
\end{mydef}

\begin{myprop}
  Let $f$ and $g$ be convex functions.
  $\max(f,g)$ is convex but $\min(f,g)$ is \emph{not} necessarily convex.
  \begin{proof}
    \begin{align*}
      \epi(\max(f,g))
      & = \{(x,t) \in \Rn \times \R : \max(f(x),g(x)) \leq t\}\\
      & = \{(x,t) \in \Rn \times \R : f(x) \leq t \land g(x) \leq t\}\\
      & = \{(x,t) \in \Rn \times \R : f(x) \leq t\} \cap \{(x,t) \in \Rn \times \R : g(x) \leq t\}\\
      & = \epi(f) \cap \epi(g)\\
      \epi(\min(f,g))
      & = \{(x,t) \in \Rn \times \R : \min(f(x),g(x)) \leq t\}\\
      & = \{(x,t) \in \Rn \times \R : f(x) \leq t \lor g(x) \leq t\}\\
      & = \{(x,t) \in \Rn \times \R : f(x) \leq t\} \cup \{(x,t) \in \Rn \times \R : g(x) \leq t\}\\
      & = \epi(f) \cup \epi(g).
    \end{align*}
  \end{proof}
\end{myprop}

\begin{myprop}
  If $f$ is convex and $g$ is linear, $f \circ g$ is convex.
\end{myprop}

\section{Model Building}
\subsection{Building linear models}
\subsubsection{Objective function}
\begin{myprop}[Linear objective]
  \label{prop:linobj}
  Every optimization problem can be modeled with a linear objective.
  \begin{proof}
    Let
    \begin{align*}
      \min_{x \in \Rn} f(x)\\
      x & \in X
    \end{align*}
    be a modelisation of the problem.
    It can be reformulated as
    \begin{align*}
      \min_{x \in \Rn} t\\
      f(x) & \leq t\\
      x & \in X.
    \end{align*}
  \end{proof}
\end{myprop}

\begin{myrem}[Convex]
  \label{rem:objconvextolin}
  If the original model was convex, $f$
  is convex and therefore, $f(x) \leq t \equiv \epi(f)$ defines
  a convex set.
  The intersection of convex sets is convex so the
  reformulation is convex too!
\end{myrem}

\begin{myrem}[Piecewise linear]
  If $f$ was piecewise linear \emph{and convex}, the reformulation is linear.
\end{myrem}

\subsubsection{Monotone transformation}
We can apply strictly monotone transformation to equations and/or the objective
(changing the inequality and/or the $\min$ in $\max$ if it is strictly decreasing).
The reformulated model is equivalent but can become convex or even linear !

\begin{myexem}[Geometric optimization]
  The problem
  \begin{align*}
    \min_{r \geq 0} r_1 r_2 r_3 r_4 r_5\\
    r_1r_2r_3 & \geq 1.05\\
    r_2r_3r_4 & \geq 1.10\\
    r_3r_4r_5 & \geq 1.15
  \end{align*}
  can be reformulated (since $\log$ is strictly increasing) as
  \begin{align*}
    \min_{r \geq 0} \log(r_1) + \log(r_2) + \log(r_3) + \log(r_4) + \log(r_5)\\
    \log(r_1) + \log(r_2 + \log(r_3) & \geq \log(1.05)\\
    \log(r_2) + \log(r_3 + \log(r_4) & \geq \log(1.10)\\
    \log(r_3) + \log(r_4 + \log(r_5) & \geq \log(1.15)
  \end{align*}
  which is linear in $x_i \eqdef \log(r_i)$.
\end{myexem}

\subsubsection{D.E.A Data Envelopment Analysis}
Let's analyse the non-convex problem
\begin{align*}
  \min_x \frac{c^T x + d}{f^T x + g}\\
  Ax & \leq b
\end{align*}
under the asumption $f^Tx+g > 0$ for all $x$ such that $Ax \leq b$.

Charnes and Cooper in \cite{charnes1985foundations} describes
how to reformulate this as a convex problem.
We introduce $t > 0$ and $y \in \Rn$
such that $x = y/t$.
The problem becomes
\begin{align*}
  \min_{y \in \Rn,t > 0} \frac{c^T y + dt}{f^T y + gt}\\
  Ay & \leq bt
\end{align*}
which is \emph{homogeneous} since for $\lambda > 0$,
$(y,t)$ and $(\lambda y, \lambda t)$ have the same objective
and $(y,t)$ is feasible iff $(\lambda y, \lambda t)$ is feasible).

We can therefore suppose that $f^T y + g t = 1$
(if not, we take $\lambda = \frac{1}{f^Ty + gt}$ and
$(y,t) \leftarrow (\lambda y, \lambda t)$), we have
\begin{align*}
  \min_x c^T y + dt\\
  Ay & \leq bt\\
  f^T y + g t & = 1.
\end{align*}

\subsection{Building convex model}

\subsubsection{Convexification}
Let's suppose that we have a general (see property~\ref{prop:linobj}) problem
\begin{align*}
  \min_x c^T x\\
  x & \in X
\end{align*}

Its feasible domain is not necessarily convex but we can make it so while
keeping the \emph{same} optimal value.

\begin{mydef}[Convex hull]
  The convex hull $\conv(X)$ of a set $X$ is the smallest convex set containing $X$.
  It can be defined as the set of of all convex combinations of the points of $X$
  \[ \left\{\,\sum_{i=1}^k \lambda_ix_i \mid x_1, \ldots, x_k \in X, \sum \lambda_i = 1, \lambda_i \geq 0\,\right\}. \]
\end{mydef}
From the simple observation
\begin{align*}
  c^T\left(\sum_{i=1}^k \lambda_ix_i\right)
  & = \sum_{i=1}^k \lambda_i c^Tx_i\\
  & \geq \min_i c^Tx_i
\end{align*}
we see that the convexified problem has the same optimal value but its optimal
set is a superset of the optimal set of the original problem.

Sadly this is very hard to do in general even if it is easy in some case.

\subsubsection{Strict convexity}
\begin{mydef}[Strict convexity]
  $f$ is strictly iff
  \[ f(\lambda x + (1-\lambda)y) < \lambda f(x) + (1-\lambda)f(y) \]
  for all $0 < \lambda < 1$ and for all $x,y$.
\end{mydef}

\begin{mytheo}
  The problem
  \begin{align*}
    \min_x f(x)\\
    x & \in X
  \end{align*}
  with $f$ stricly convex and $X$ convex has at most 1 optimal solution.
\end{mytheo}

If $\lap f(x) \succ 0$ then $f$ is strictly convex but it is \emph{not} necessary.
For example $x^4$ is strictly convex but $12x^2$ is 0 for $x=0$ and
is therefore only positive \emph{semi}-definite.

An usual trick to make a convex problem strictly convex is regularization
\begin{mytheo}[Regularization]
  If $f$ is convex then $f + \mu\|x\|^2$ is strictly convex for all $\mu > 0$.
\end{mytheo}

\begin{myexem}
  If a problem has $x^TQx$ as objective with $Q \succeq 0$, it is not strictly convex.
  Since $Q$ is not positive definite we also cannot do Cholesky factorization.

  However, with the objective $x^TQx + \mu \|x\|^2 = x^T(Q + \mu I)x$,
  $Q + \mu I \succ 0$ for $\mu > 0$.
  Hence the optimal solution is unique and we can find an upper triangular $L$ such that $L^TL = Q + \mu I$ and solve the problem for
  $y = Lx$ with the simple objective $y^Ty = \|y\|_2^2$.

  Expressions with $Ax$ will become $AL^{-1}y$ but $AL^{-1}$ is easy to compute since $L$ is triangular.
\end{myexem}

\subsubsection{Convex maximization}
Let's first notice that since a linear function is both convex and concave,
a linear problem is both a convex minimizatoin \emph{and} a convex maximization.
It has therefore the properties of both.

\begin{mydef}[Extreme point]
  Let $X$ be a convex set.
  The point $x$ is said to be extreme if it does is not inside any segment formed
  by two distinct points of $X$.
\end{mydef}

\begin{mytheo}
  If the problem $\max_{x \in X} f(x)$ with $f$ and $X$ convex has an optimal solution,
  one of the optimal solution is an extreme point of $X$.
\end{mytheo}
This is a property that we already knew for linear optimization.
Now we see that it is actually a property of a bigger class of problems.

\subsubsection{Quasiconvexity}
\begin{mydef}[Quasiconvex]
  A function $f$ is quasiconvex if for all $\alpha$,
  its sub-level set
  \[ L_\alpha(f) = \{\,x \in \dom f \mid f(x) \leq \alpha\,\} \]
  is convex for all $\alpha$.
\end{mydef}

If $f$ is convex, $L_\alpha(f)$ is also convex for all $\alpha$
but the converse is not true (e.g. $\sqrt{|x|}$).

If $f$ is convex and $g$ is concave, $\frac{f}{g}$ is quasiconvex on $\{\,x \mid g(x) > 0\,\}$.
We see that $\frac{f}{g} \leq \alpha$ is equivalent to
$f - \alpha g \leq 0$ which is a convex set since $f - \alpha g$ is convex.
Here we see that it only works for $\alpha \geq 0$, since if $\alpha < 0$, $-\alpha g$ is concave.

\paragraph{Solving procedure}
Let's suppose we want to solve
\begin{align*}
  \min f(x)\\
  x & \in X
\end{align*}
where the objective $f$ is quasiconvex and the feasible set $X$ is convex.

We can see in the remark~\ref{rem:objconvextolin} that if we want the feasible set to be convex
when we transform the problem to
\begin{align*}
  \min \alpha\\
  f(x) & \leq \alpha\\
  x & \in X.
\end{align*}
we need $\epi(f) \equiv \{\,(x,\alpha) \mid f(x) \leq \alpha\,\}$ to be convex which by definition means that $f$ is convex.
Here $f$ in only quasi-convex so we need to leave $\alpha$ constant.

We can however find the optimal solution using bisection.
If the convex set
\begin{align*}
  f(x) & \leq \alpha\\
  x & \in X
\end{align*}
is empty, $\alpha$ is a lower bound.
Otherwise it is an upper bound.

Checking whether it is non-empty can be done by solving a convex problem
such as
\begin{align*}
  \min 0\\
  f(x) & \leq \alpha\\
  x & \in X.
\end{align*}

% TODO relation restriction

\section{Alternative et duality}
\subsection{Theorems of the alternative}
The Farkas' lemma are the basis of the duality.
They are proven using decision hyperplan.

Farka's lemma are easier to interpret by comparing them
to the primal and the dual in their standard form
\begin{align*}
  \min c^Tx & & \max b^Ty\\
  Ax & = b & A^Ty & \leq c\\
  x & \geq 0.
\end{align*}
% TODO switch dual primal to be like the rest

The dual has no feasible solution (i.e. is impossible) iff the primal is unbounded.
When $b = 0$, this can be expressed by:
One and only one of the two following systems has a solution
\begin{align*}
  A\lambda & = 0 & A^Ty & \leq c\\
  \lambda & \geq 0 & &\\
  c^T\lambda & < 0.
\end{align*}

The primal has no feasible solution (i.e. is impossible) iff the dual is unbounded.
When $c = 0$, this can be expressed by:
One and only one of the two following systems has a solution
\begin{align*}
  Ax & = b & A^T\lambda & \leq 0\\
  x & \geq 0 & b^T\lambda & > 0.
\end{align*}

We also have the following:
One and only one of the two following systems has a solution
\begin{align*}
  Ax & = b & A^T\lambda & = 0\\
     & & b^T\lambda & \neq 0.
\end{align*}

\subsection{Duality and certificates}
Let's consider the problem primal (P) (it is actually the dual in the standard form)
\begin{align*}
  \max b^Ty\\
  A^Ty & \leq c
\end{align*}

If we want to prove that the optimal objective is $\theta$,
we have to find a proof that
\[ A^Ty \leq c \Rightarrow b^Ty \leq \theta. \]
Let's analyse the LHS
\begin{align*}
  a_1 y & \leq c_1\\
  \vdots \quad  & \quad\quad \vdots\\
  a_m y & \leq c_m
\end{align*}
For $x \geq 0$, this system implies the following system
\begin{align*}
  x_1 a_1 y & \leq x_1 c_1\\
  \vdots \quad  & \quad\quad \vdots\\
  x_m a_m y & \leq x_m c_m
\end{align*}
and also
\begin{align*}
  \sum_{i=1}^m x_m a_m y & \leq \sum_{i=1}^m x_m c_m\\
  x^T A^T y & \leq x^T c.
\end{align*}

We see that $x$ is a certificate for $\theta$ if
\begin{align*}
  A x & = b\\
  c^T x & = \theta\\
  x & \geq 0.
\end{align*}

Of course, finding a certificate just gives us an upper bound.
Finding the certificate for the lowest upper bound is the optimization problem
\begin{align*}
  \min c^T x\\
  A x & = c\\
  x & \geq 0
\end{align*}
which is called the \emph{dual} (D) of (P).

Of course if we find an feasible couple $(y,x)$ for which $b^T y = \theta$ and
$c^T x = \theta$ we are sure that $\theta$ is the optimal value.
In this case $x$ is called the optimality certificate of $y$.

\subsubsection{Weak and strong duality}
\begin{mytheo}[Weak duality]
  For all feasible solution $y$ of the primal and all feasible solution $x$ of the dual,
  \[ b^Ty \leq c^Tx. \]
  \begin{proof}
    This is a particular case of theorem~\ref{theo:weakdualityconic}
    as shown by the \sectionref{coniclin}.
  \end{proof}
\end{mytheo}

\begin{mytheo}[Strong duality]
  Let $y^*$ be an optimal solution for (P).
  Then there exists an (optimal\footnote{The fact that this solution is optimal is a consequence of the weak duality}) solution $x^*$ to the problem (D) such that
  \[ c^Tx^* = b^Ty^*. \]
  \begin{proof}
    See \annexeref{strongdualityproof}.
  \end{proof}
\end{mytheo}

\begin{mycorr}
  The different possibilites are summarized below.
  \begin{center}
    \begin{tabular}{l|c|c|c}
      \diagbox{P}{D} & impossible & finite & unbounded\\
      \hline
      impossible & \checkmark & \color{dkred}{\xmark} & \cmark\\
      \hline
      finite & \color{dkred}{\xmark} & \cmark & \color{dkgreen}{\xmark}\\
      \hline
      unbounded & \cmark & \color{dkgreen}{\xmark} & \color{dkgreen}{\xmark}\\
    \end{tabular}
  \end{center}
  {\color{dkgreen}\xmark} means that it is impossible by weak duality
  and {\color{dkred}\xmark} means that we need strong duality to prove that it is impossible.

  \checkmark and \cmark need neither weak nor strong duality,
  an example is sufficient to show that those cases are possible.

  $\checkmark$ means that it is possible but is unstable
  in the sense that for a problem for which the dual and the primal is
  impossible, modifying a little bit $A$, $b$ or $c$ makes it fall in another category.

  In practice, we are never in this category except if the problem has
  been crafted on purpose.

  We can see by the way that the Farkas' lemma ensures us that we cannot be in this category of our problem is in the standard form.

  In text, it says that
  \begin{itemize}
    \item If (P) is unbounded, (D) is impossible by weak duality (since if D had an feasible solution, it would give a bound to P).
    \item If (D) is unbounded, (P) is impossible by weak duality.
    \item If (P) is finite, (D) is finite by strong duality.
    \item If (D) is finite, (P) is finite by strong duality.
    \item \sout{If (P) is impossible, (D) is unbounded.} (it is only true if we prove that D has at least one feasible solution).
    \item \sout{If (D) is impossible, (P) is unbounded.}
  \end{itemize}
\end{mycorr}

\subsection{Sensibility analysis}
Let
\[
  \phi_P(b) = \max_{A^Ty \leq c} b^Ty
\]

We can see that $\phi_P$ is the maximum over all feasible $y$ of $b^Ty$.
Hence it is the maximum of an infinity of convex functions, so it is convex.
Therefore,
\[ \phi_P(b + \Delta b) \geq (b + \Delta b)^T y^* = \phi_P(b) + (y^*)^T \Delta b. \]
$y^*$ is still feasible since $b$ only affects the objective.
If $y$ is an unique optimal solution for the objective $b^Ty$,
$\phi_P(b + \Delta b) = (b + \Delta b)^T y^*$ for $\Delta b$ small enough.

If
\[ \phi_P(b + \Delta b) > (b + \Delta b)^T y^*, \]
that means that just before (for a $\Delta b$ just smaller),
$y^*$ wasn't anymore the unique optimal solution.

By strong duality,
we have $\phi_D(b) = \phi_P(b)$ so this analysis gives a good sensibility analysis on the independent term $b$ of the constraints of D.

\subsection{Robust optimization}
Let consider a linear problem
\begin{align*}
  \max b^T y\\
  a_i^Ty & \leq c_i & \forall i, a_i \in \mathcal{A}_i = \{\,a \mid C_ia \leq d_i\,\}.
\end{align*}

Each constraint is equivalent to
\[ \max_{a_i \in \mathcal{A}_i} y^Ta_i \leq c_i. \]
The LHS is a linear optimization problem in $a_i$ (not $y$ !).
By strong duality, we know that it is equivalent to
\begin{align*}
  \min d_i^Tx & \leq c_i\\
  C_i^T x & = y\\
  x & \geq 0
\end{align*}
or simply asking that there exists an $x$ such that
\begin{align*}
  d_i^Tx & \leq c_i\\
  C_i^T x & = y\\
  x & \geq 0
\end{align*}
but asking for the existence of such $x$ is done
by replacing $a_i^Ty \leq c_i$ by these 3 constraints in the model.

\section{Conic optimization}
For a cone $K$,
we define the partial order $\succeq_K$ as
\[ y \preceq_K x \iff x \succeq_K y \iff x - y \succeq_K 0 \iff x - y \in K. \]
It is a \emph{partial} order since there can be $x,y$ such that
neither $x \succeq_K y$ nor $x \preceq_K y$.

\begin{mydef}[Convex cone]
  \label{def:convexcone}
  A convex cone is a set $K$ such that
  \begin{itemize}
    \item if $x \in K$ and $\lambda \in \R_+$, $\lambda x \in K$
      (i.e. if $x \preceq_K y$, $\lambda x \preceq_K \lambda y$);
    \item $x, y \in K$, $x + y \in K$
      (i.e. if $x_1 \preceq_K y_1$ and $x_2 \preceq_K y_2$, $x_1 + x_2 \preceq_K y_1 + y_2$).
  \end{itemize}
\end{mydef}

The second condition of definition~\ref{def:convexcone} is a consequence of the first condition (nonnegative scaling) and the convexity.
Indeed if $x,y \in K$, then $x/2 + y/2 \in K$ by convexity and $2(x/2+y/2) \in \K$ by the nonnegative scaling.
Moreover a cone is simply defined as a set for which we have nonnegative scaling.
Therefore an alternative way to define a convex cone is to say that it is a cone that is convex.

It is interesting to compare it with the definition of a vector space.
The difference is that here we impose that $\lambda \geq 0$.
If we allowed $\lambda < 0$, it would mean that if $x \preceq y$, $-x \preceq -y$ and that wouldn't work.

Sadly, all cones cannot be used for optimization.
We also need the cone to be proper.

\begin{mydef}[Proper cone]
  A \emph{proper} cone is a convex cone $K$ that is closed,
  \begin{itemize}
    \item solid ($K$ has a non-empty interior) and
    \item pointed ($K \cap (-K) = \{0\}$).
  \end{itemize}
\end{mydef}

If $K$ is not pointed, there is $x \neq 0$ such that $x \in K$ and $-x \in K$,
we have $(\alpha-\beta)x \in K$ for all $a,b \in \R_+$ so $\gamma x \in K$ for all $\gamma \in \R$.
Hence we ahve a whole line in $K$.
For example, the cone $\{\,(x,y) \in \R^2 \mid x \geq 0\,\}$ is not pointed and we have the whole line $x = 0$ in $K$.

\begin{mydef}[Dual cone]
  The dual of the cone $K$ for the scalar product $\sprod{\cdot}{\cdot}$ is defined as
  \[ K^* \eqdef \{\,\lambda \mid \sprod{x}{\lambda} \geq 0, \forall x \in K\,\}. \]
\end{mydef}

\begin{mytheo}
  If $K$ is a proper cone,
  $K^*$ is also a proper cone and $K^{**} = K$.
\end{mytheo}

Actually, given \emph{any} set $S$, its dual $S^*$ is always a closed convex cone and $S^{**}$ is the closure%
\footnote{A closure of a set $X$ is the union of $X$ and its limit points, that is, the ``smallest'' closed set containing $X$} % FIXME why not say that $S^{**}$ is the smallest closed convex cone containing $S$ ? This property comes from the lecture 1 of MIT 6.256
of the smallest convex cone that contains $S$.
Also, if $S$ is only a closed convex cone, we already have the property $S^{**} = S$.

\begin{myprop}
  \label{prop:cartesiancone}
  If $K_1$ and $K_2$ are proper cones,
  $K_1 \times K_2$ is also a proper cone and $(K_1 \times K_2)^* = (K_1^*) \times (K_1^*)$.
\end{myprop}

\begin{mydef}[Self-dual cone]
  A cone $K$ is self-dual for a scalar product $\sprod{\cdot}{\cdot}$ iff $K^* = K$.
\end{mydef}

We can generalize the standard form for conic optimization
\begin{align*}
  \max b^Ty & & \min \sprod{x}{c}\\
  \sum_{i=1}^n a_iy_i & \preceq_K c & \sprod{x}{a_i} & = b_i \quad \forall i = 1, \ldots, n\\
  y & \in \Rn & x & \in K^*.
\end{align*}
for $b \in \Rn$ and $a_i$ and $c$ in a superset of $K$ (e.g. $\Rn$ for $\Rn_+$ and $\mathbb{L}^n$ and $\Rnn$ for $\mathbb{S}^n_+$).

From the property~\ref{prop:cartesiancone}, we can see that if we have a problem with
conic constraints of $\Rn_+$, $\mathbb{L}^{m_i}$ and $\mathbb{S}^{m_i}_+$, we can set
\[ K = \Rn_+ \times \mathbb{L}^{m_1} \times \cdots \times \mathbb{L}^{m_l} \times \mathbb{S}^{m_1} \times \cdots \times \mathbb{S}^{m_k}. \]
which will a proper self-dual cone.

We can verify that
\begin{itemize}
  \item all conic problems are convex;
  \item all convex problems can be formulated as a convex problem (using the conic hub).
\end{itemize}

\subsection{Alternative and Duality for conic optimization}
We will see that we will have weak duality for conic optimization too.
Therefore, if the dual is unbounded, the primal is impossible.
When $c = 0$, this can be expressed using the 2 following systems.
\begin{align*}
  \sum_{i=1}^n a_iy_i & \preceq_K c & \sprod{\lambda}{a_i} & = 0 \quad \forall i = 1, \ldots, n\\
                      & & \lambda & \in K^*\\
                      & & \sprod{\lambda,b} & < 0.
\end{align*}
if the second has a solution, the first is impossible.
Sadly the reciprocal isn't true.
If the second is impossible, the first can also be impossible.

However, if the seconnd is impossible the first is \emph{almost} possible.

\begin{mydef}
  $Ax \preceq_K b$ is almost possible iff
  $\forall \varepsilon$, there is $b' \in B_\varepsilon(b)$
  such that $Ax \preceq_K b'$ is possible.
\end{mydef}

\begin{mytheo}[Weak duality for conic optimization]
  \label{theo:weakdualityconic}
  If $y$ is an feasible solution for the primal and $x$ an feasible solution for the dual,
  \[ b^Ty \leq \sprod{x}{c}. \]
  \begin{proof}
    \begin{align*}
      \sprod{x}{c} - b^Ty
      & = \sprod{x}{c} - \sum_{i=1}^n \sprod{x}{a_i}y_i\\
      & = \sprod{x}{c - \sum_{i=1}^n a_iy_i}
    \end{align*}
    by the properties of the scalar product.
    Since $y$ is feasible, $c - \sum_{i=1}^n a_iy_i \in K$
    and since $x$ is feasible, $x \in K^*$ so by definition of $K^*$,
    \[ \sprod{x}{c} - b^Ty = \sprod{x}{c - \sum_{i=1}^n a_iy_i} \geq 0. \]
  \end{proof}
\end{mytheo}

\begin{mydef}
  Let's define $\prec_K$ and $\succ_K$ such that
  \[ y \prec_K x \iff x \succ_K y \iff x - y \succ_K 0 \iff x - y \in \newint K. \]
  \begin{itemize}
    \item $y$ is strictly feasible iff $\sum_{i=1}^n a_i y_i \prec_K c$.
    \item $x$ is strictly feasible iff $x$ is feasible and $x \succ_K 0$.
  \end{itemize}
\end{mydef}

\begin{mytheo}[Strong duality for conic optimization]
  If there is a strictly feasible $x$ then the primal has an optimal solution $y^*$ and
  \[ b^Ty^* = \inf_{x\text{ feasible}} \sprod{x}{c}. \]
  We are not sure that the dual has an optimal solution (e.g. $\min_{x > 0} 1/x$).

  If there is a strictly feasible $y$ then the dual has an optimal solution $x^*$ and
  \[ \sprod{x^*}{c} = \sup_{y\text{ feasible}} b^Ty. \]
\end{mytheo}

\begin{mycorr}
  If there is a strictly feasible $x$ and a strictly feasible $y$,
  $\exists x^*, y^*$ optimal solutions of the dual and the primal respectively such that
  \[ b^Ty^* = \sprod{x^*}{c}. \]
\end{mycorr}

\subsection{Cone of linear optimization}
\label{sec:coniclin}
With the scalar product $\sprod{x}{y} = y^Tx$, the proper cone $\Rn_+$ is self-dual.
The standard form becomes
\begin{align*}
  \max b^Ty & & \min c^Tx\\
  \sum_{i=1}^n a_iy_i & \preceq_{\Rn_+} c & a_i^Tx & = b_i \quad \forall i = 1, \ldots, n\\
  y & \in \Rn & x & \in \Rn_+.
\end{align*}
which is the usual linear standard form with $A^T = \begin{pmatrix}a_1 & \cdots & a_n\end{pmatrix}$.

\subsection{Lorentz cone}
\begin{mydef}[Lorentz cone]
  The Lorentz cone $\mathbb{L}^n$ is defined a,
  \[ \mathbb{L}^n \eqdef \Big\{\,(x_0, x_1, \ldots, x_n) \in \R^{n+1} \mid x_0 \geq \|(x_1, \ldots, x_n)\|_2 = \sqrt{x_1^2 + \cdots + x_n^2}\,\Big\}. \]
\end{mydef}

\begin{myexem}[Ellipsoid]
  An ellipsoid is defined as $\{x|(x-c)^TE(x-c) \leq 1\}$ wher $c$ is the center
  and $E \succ 0$.
  Using Cholesky factorization, we have $E = LL^T$ and the condition becomes
  \begin{align*}
    \sqrt{(x-c)^TE(x-c)} & \leq 1\\
    \sqrt{(x-c)^TLL^T(x-c)} & \leq 1\\
    \|L^T(x-c)\|_2 & \leq 1\\
    \begin{pmatrix}
      1\\
      L^T(x-c)
    \end{pmatrix}
    & \in \mathbb{L}^n.
  \end{align*}
\end{myexem}

With the scalar product $\sprod{x}{y} = y^Tx$, the proper cone $\mathbb{L}^n$ is self-dual.
The standard form becomes
\begin{align*}
  \max b^Ty & & \min c^Tx\\
  Ay_i & \preceq_{\mathbb{L}^n} c & Ax & = b_i\\
  y & \in \Rn & x & \in \mathbb{L}^n.
\end{align*}
with $A^T = \begin{pmatrix}a_1 & \cdots & a_n\end{pmatrix}$.

\subsection{Cone of positive definite matrices}
We define $\mathbb{S}^n_+ \subseteq \mathbb{S}^n \subseteq \Rn$
where $\mathbb{S}^n$ is the set of \emph{symmetric} matrices of $\Rnn$
and $\mathbb{S}^n_+$ is the set of positive \emph{semi}definite matrices of $\mathbb{S}^n$.

With the scalar product $\sprod{X}{Y} = \tr(YX) = \sum_{i,j=1}^n X_{ij}Y_{ij} = \vect(Y)^T\vect(X)$\footnote{$Y^TX = YX$ since $Y$ is symmetric.},
the proper cone $\mathbb{S}^n_+$ is self-dual.
The standard form becomes (using capitals for matrices for clarity)
\begin{align*}
  \max b^Ty & & \min \sprod{X}{C}\\
  \sum_{i=1}^n A_iy_i & \preceq_{\mathbb{S}^n_+} C & \sprod{X}{A_i} & = b_i \quad \forall i = 1, \ldots, n\\
  y & \in \Rn & x & \in \mathbb{S}^n_+.
\end{align*}

% TODO prob de Weber (CMS7_2)
% TODO prob de sep de marge maximale (CMS7_2)


% TODO SOS CMS7_5

\section{Interior Point Method (IPM)}
\subsection{Newton's method}
\begin{align*}
  n(x) & = -(\grad^2 f(x))^{-1} \grad f(x)\\
  x_{k+1} & = x_k + n(x_i) & i = 0, 1, 2, \ldots
\end{align*}

We would like to measure the distance from an iterate to the solution.
The problem with the norm $\|\cdot\|_2$ is that it can change
with a linear transformation of the axis.

We will define a new norm $\|\cdot\|_x$ and its dual $\|\cdot\|_x^*$.
The primal norm will be used to measure the distance between iterates
and the dual will be used to measure the norm of the gradient.
\begin{mydef}[Local norm]
  \begin{align*}
    \|z\|_x   & = \sqrt{z^T \grad^2 f(x) z}\\
    \|z\|_x^* & = \sqrt{z^T (\grad^2 f(x))^{-1} z}.
  \end{align*}
\end{mydef}

We define
\begin{align*}
  \delta(x) & \eqdef \|\grad f(x)\|_x^*\\
            & = \sqrt{\grad f(x)^T (\grad^2 f(x))^{-1} \grad f(x)}\\
            & = \sqrt{(-(\grad^2 f(x))^{-1} \grad f(x))^T \grad^2 f(x) (-(\grad^2 f(x))^{-1} \grad f(x)}\\
            & = \|n(x)\|_x
\end{align*}

\begin{mytheo}
  If $\delta(x) < 1$ then
  \begin{itemize}
    \item $f$ has a minimum
    \item $n(x)$ is feasible (i.e. $x + n(x) \in \dom f$
    \item $\delta(x + n(x)) \leq \frac{\delta(x)^2}{1-\delta(x)}$
  \end{itemize}
\end{mytheo}

\begin{mycorr}
  If $\delta(x) < 1/2$, $\delta(x + n(x)) \leq 2\delta^2(x)$ so we have a quadratic convergence.
\end{mycorr}

\subsubsection{Self-concordant functions}
Self-concordant functions are functions for which
the Newton's method works well since
their hessian has a small derivative.
\begin{mydef}[Self-concordant functions]
  Let $f \in C^3$ be a convex function defined on a open, strictly convex domain.
  $f$ is self-concordant iff
  \[ D^3 f(x)[h,h,h] \leq 2 (D^2 f(x)[h,h])^{3/2} \]
  for all $x \in \dom f$ and for all $h$.
\end{mydef}

\begin{myrem}
  Since $f$ is convex, $D^2 f(x)[h,h]$ is positive so we can take its square root.
\end{myrem}

With self-concordant functions, we have a better bound when $\delta < 1$
\begin{mytheo}
  If $f$ is self-concordant and $\delta(x) < 1$ then
  \[ \delta(x + n(x)) \leq \left(\frac{\delta(x)}{1-\delta(x)}\right)^2 \]
\end{mytheo}

\begin{mycorr}
  If $\delta(x) < 1/2$, $\delta(x + n(x)) \leq 4\delta^2(x)$ so we have a quadratic convergence.
\end{mycorr}

\begin{mytheo}
  If $f$ is self-concordant and $\delta(x) \geq 1$,
  $\frac{n(x)}{1 + \delta(x)}$ is feasible and
  \[ f(x + \frac{n(x)}{1 + \delta(x)}) \leq f(x) + \log(1 + \delta(x)) - \delta(x) \stackrel{\delta(x) \geq 1}{\leq} f(x) + \log(2) - 1 \approx f(x) - 0.3. \]
\end{mytheo}

For an initial $x_0$, if we use the step $\frac{n(x)}{1+\delta(x)}$ while $\delta(x_k) > 1$ and $n(x)$ when $\delta(x_k) < 1$,
we will have
\[ \bigoh\left(\underbrace{\frac{f(x_0) - f^*}{0.3}}_{\delta(x) > 1} + \underbrace{\log \log \frac{1}{\epsilon}}_{\delta(x) < 1}\right) \]
iterations.

\subsection{Self-concordant barrier}
The barrier of a set $X$ is a function that is $+\infty$ at the border $\partial X$ of $X$.

The parameter of a self-concordant barrier $F$ is defined as
\[ \nu = \max_{x \in \dom F} \delta^2(x) = \max_{x \in \dom F} \|n(x)\|_x^2 = \max_{x \in \dom F} (\|\grad f(x)\|_x^*)^2. \]

\begin{myprop}
  $F$ is a self-concordant barrier of parameter $\nu$,
  \[ (DF[h])^2 \leq \nu D^2F[h,h]. \]
\end{myprop}

\begin{myprop}
  If $F_1$ and $F_2$ are self-concordant barriers of parameter $\nu_1$ and $\mu_2$, $f+g$ is a self-concordant barrier of parameter $\nu_1 + \nu_2$.
\end{myprop}

\begin{myprop}
  If $F$ is a self-concordant of parameter $\nu$, $y \mapsto F(Ay + b)$ is a self-concordant barrier of parameter $\nu$.
\end{myprop}

Here are self-concordant barriers for the 3 usual proper self-dual cones and others ($Q \succeq 0, p \geq 1$)
\begin{align*}
  A^Ty & \leq c & - \sum_{i=1}^m \log(c_i - a_i^Ty) & & \nu & = m\\
  \|(x_1, \ldots, x_1)\|_2 & \leq x_0 & - \log(x_0^2 - \|(x_1, \ldots, x_n)\|_2^2) & & \nu & = 2\\
  X & \succeq 0 & - \log \det X = - \sum \log \lambda_i(X) & & \nu & = n\\
  x^TQx & \leq \constant^2 & - \log(\constant^2 - x^TQx) & & \nu & = 1\\
  e^x & \leq t & - \log(\log(t) - x) - \log(t) & & \nu & = 2\\
  |x|^p & \leq t & - \log(t^{2/p} - x^2) - 2\log(t) & & \nu & = 4.
\end{align*}
We know that if the expression inside the $\log$ is homogenous of power $k$, then $\nu = k$.
That explains the value of $\nu$ for the barrier of $\mathbb{L}^n$ and $\mathbb{S}^n_+$.
By the way, for these 2 barriers, we can see that there an points in the domain of the $\log$
which are not in the cone.
For $\mathbb{L}^n$, it is points with $x_0 < 0$ for which $-x_0 \geq \|(x_1,\ldots,x_n)\|_1$
and for $\mathbb{S}^n_+$, it is matrices with an even number of odd eigenvalues.
However, if we start with a point in the cone, Newton will never go to these regions since
in order to do that it needs to pass a barrier.

\begin{mytheo}[Universal barrier]
  Let $K \subseteq \Rn$ be a cone.
  \[ \varphi_K(x) = c \log\left[\int_{K^*} e^{-x^Ty} \dif y\right] \]
  is an self-concordant barrier for $K$.

  We do not know $c$ for every cone even if there is a conjecture that $c = 1$.
\end{mytheo}
Computing this barrier in general is not efficent and
we know it is not possible to find a barrier that is efficiently computable for all cone
since some hard problems for which we know that thare is no efficient algorithm can be modelled as convex problem.

It can only be usefull when we can compute the integral analytically.
For $|x|^p \leq t$ it is not analytically computable but we know one other barrier that we have just seen.

\subsection{Barriers for conic optimization}
We can use barrier for conic optimization.
From our problem in the standard form,
we can create a problem for all $\mu > 0$.
Let $F$ be a self-concordant barrier of parameter $\nu$ for a self-dual cone $K$,
the problem for $\mu > 0$ is
\begin{align*}
  \max b^Ty + \mu F(c - \sum_{i=1}^n a_iy_i) & & \min \sprod{x}{c} - \mu F(x)\\
  & & \sprod{x}{a_i} & = b_i \quad \forall i = 1, \ldots, n\\
\end{align*}
or equivalently
\begin{align*}
  \max b^Ty + \mu F(s) & & \min \sprod{x}{c} - \mu F(x)\\
  \sum_{i=1}^n a_iy_i + s & = c & \sprod{x}{a_i} & = b_i \quad \forall i = 1, \ldots, n\\
\end{align*}

For the rest of this section, we will assume that both the primal and the dual have a strictly feasible solution.

For $\mu > 0$, both the primal and the dual are strictly convex so they have an unique solution $x^*(\mu), y^*(\mu)$.
These unique solutions $x^*(\mu)$ and $y^*(\mu)$ are linked by the KKT relation which is
\begin{align*}
  Ax & = b\\
  A^Ty + s & = c\\
  x_is_i & = \mu
\end{align*}
for $K = \Rn_+$ and $\sprod{x}{y} = y^Tx$.
Still in the linear case, it can be shown that
\[ b^Ty^*(\mu) - \sprod{x^*(\mu)}{c} \leq \nu\mu. \]
But thanks to the weak duality, since $x^*(\mu)$ and $y^*(\mu)$ are also feasible for the inital problem, we know that
\begin{align*}
  b^Ty^*    & \leq \sprod{x(\mu)}{c}\\
  b^Ty(\mu) & \leq \sprod{x^*}{c}
\end{align*}
so we know that $b^Ty^*(\mu)$ and $\sprod{x^*(\mu)}{c}$ are at most at a distance $\nu\mu$ from the optimal value of the solution.
But this is actually true in the general case
\[ b^Ty^*(0) - b^Ty^*(\mu) \leq \mu\nu. \]

If we are at a ``distance'' $\delta_\mu(x) \leq \tau < 1$ of $x^*(\mu)$, we have the relation (TODO: do we have the same for $x$ ?)
\[ b^Ty^*(0) - b^Ty^*(\mu) \leq \mu\nu + (\tau+\sqrt{\nu})\frac{\mu\tau}{1-\tau} \]
but if $\nu - \sqrt{\nu} \geq \tau$ (which is true for $\nu \geq 2$), it can be simplified to
\[ b^Ty^*(0) - b^Ty^*(\mu) \leq \frac{\nu\mu}{1 - \tau} \]
which is less precise but a lot simpler.
We can see that for both these expressions, with $\tau = 0$, we get back to $\nu\mu$.

We see here that if we want a precision $\epsilon = b^Ty^* - b^Ty$ and $\delta_\mu(y) < \tau$, we
can take
\[ \mu = \epsilon\frac{1-\tau}{\nu}. \]

\begin{mydef}[Analytic center]
  We define the analytic center as $y^*(\infty)$ for the primal
  and $x^*(\infty)$ for the dual.
\end{mydef}

\subsection{Short step barrier iteration}
For an initial $\mu_0$ and $y_0$, we first find $y_0'$ such that $\delta_{\mu_0}(y_0') < \tau$.
Afterwards, we take $\mu_1 = \mu_0(1-\theta)$ and do a single Newton Step $y_1 = y_0' + n_{\mu_1}(y_0')$.
If $\tau$ and $\theta$ are chosen small enough, we will have $\delta_{\mu_1}(y_1) < \tau$ and we can take $\mu_2 = \mu_1(1-\theta)$, etc...

We show at the \annexeref{chemin} that $\tau=\frac{1}{4}$ and $\theta = \frac{1}{16\sqrt{\nu}}$ are small enough.

\begin{algorithm}
  \caption{Short step barrier iteration for a given precision $\epsilon$ for $b^Ty - b^Ty^*$, an intial feasible solution $y_0$ and initial $\mu_0$}
  \label{algo:shortstep}
  \begin{algorithmic}
    \STATE $\tau = 1/4$
    \STATE $\theta = 1/(16\sqrt{\nu})$
    \WHILE{$\delta_{\mu_0}(y_0) > \tau$}
    \IF{$\delta_{\mu_0}(y_0) > 1$}
    \STATE $y_0 \leftarrow y_0 + \frac{n_{\mu_0}(y_0)}{1+\delta_{\mu_0}(y_0)}$
    \ELSE
    \STATE $y_0 \leftarrow y_0 + n_{\mu_0}(y_0)$
    \ENDIF
    \ENDWHILE
    \STATE $\mu_f = \epsilon(1-\tau)/\nu$
    \WHILE{$\mu_k > \mu_f$}
    \STATE $\mu_{k+1} = \mu_k(1 - \theta)$
    \STATE $y_{k+1} = y_k + n_{\mu_{k+1}}(y_k) = y_k - (\grad^2 f(y_k))^{-1} \grad f(y_k)$
    \STATE $k \leftarrow k+1$
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

Sadly, we can see that $\theta = \frac{1}{16\sqrt{\nu}}$ is very small.
For $\nu = 100$ for example, $1 - \theta = 0.99375$ we we will need a lot of iterations.

The number of iterations is given by
\begin{align*}
  \log_{1-\theta}(\mu_f/\mu_0) & = \frac{\log(\mu_0) - \log(\mu_f)}{-\log(1-\theta)}\\
                               & \approx \frac{\log(\mu_0) - \log(\epsilon) - \log(1-\tau) + \log\sqrt{\nu}}{\theta}\\
                               & = 16\sqrt{\nu}\left((\log(\mu_0) + \log\frac{1}{\epsilon} - \log(1-\tau) + \log\sqrt{\nu}\right)\\
                               & = \bigoh\left(\sqrt{\nu}\left(\log\frac{1}{\epsilon} + \log\sqrt{\nu}\right)\right)\\
                               & \approx \bigoh\left(\sqrt{\nu}\log\frac{1}{\epsilon}\right).
\end{align*}

\subsection{Long step barrier iteration}
For the long step, we take a larger constant $\theta < 1$ and do several iterations for the same $k$.
This time, $\theta$ is constant, the number of iteration is
\begin{align*}
  \bigoh\left(\frac{1}{\theta}\left(\log\frac{1}{\epsilon} + \log\sqrt{\nu}\right)\right) & = \bigoh\left(\log\frac{1}{\epsilon} + \log\sqrt{\nu}\right)\\
  & \approx \bigoh\left(\log\frac{1}{\epsilon}\right)
\end{align*}
but at each iteration, we need to do
\[
  \bigoh(\theta\sqrt{\nu} + \theta\nu) = \bigoh(\nu)
\]
Newton's steps.
Indeed after changing $\mu$, we are at a distance of (see \annexeref{chemin})
\[ \delta_{\mu_{k+1}}(x_k) < \frac{\tau + \sqrt{\nu} \theta}{1-\theta} \]
which for $\tau = 1/4$ and $\theta = 0.9$ is $2.5 + 9\sqrt{\nu} > 1$.
So we will need some careful $n(x)/(1+\delta(x))$ steps and a few quadratic $n(x)$ steps before $\delta_{\mu_{k+1}}(x_{k+1}) < \tau$.

In total, we need
\[
  \bigoh\left(\nu\log\frac{1}{\epsilon}\right)
\]
Newton's steps.

In practice it is more like
\[
  \bigoh(\log\nu).
\]
Newton's steps at each iteration instead of $\bigoh(\nu)$ but we have no proof.

\begin{algorithm}
  \caption{Long step barrier iteration for a given precision $\epsilon$ for $b^Ty - b^Ty^*$, an intial feasible solution $y_0$ and initial $\mu_0$}
  \label{algo:longstep}
  \begin{algorithmic}
    \STATE $\tau = 1/4$
    \STATE $\theta = 0.9$ \COMMENT{Let's do long steps and hope we will only need $\bigoh(\log(\nu))$ Newton's steps}
    \STATE $\mu_f = \epsilon(1-\tau)/\nu$
    \WHILE{$\mu_k > \mu_f$}
    \STATE $\mu_{k+1} = \mu_k(1 - \theta)$
    \STATE $y_{k+1} \leftarrow y_k$
    \STATE $k \leftarrow k+1$
    \WHILE{$\delta_{\mu_k}(y_k) > \tau$}
    \IF{$\delta_{\mu_k}(y_k) > 1$}
    \STATE $y_k \leftarrow y_k + \frac{n_{\mu_k}(y_k)}{1+\delta_{\mu_k}(y_k)}$
    \ELSE
    \STATE $y_k \leftarrow y_k + n_{\mu_k}(y_k)$
    \ENDIF
    \ENDWHILE
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

\section{First order methods}
Let's put ourself in a black-box scenario where the only information we have on $f$ is the answers $(f(x), \grad f(x))$
answered by an oracle for all $x$.

\begin{mydef}
  $f:[0,1]^n \to \R$ is $L$-lipschitz iff
  \[ |f(y) - f(x)| \leq L\|y - x\|. \]
\end{mydef}

\begin{myprop}
  If $f \in C^1$ and $\|\grad f(x)\| \leq L$, then $f$ is $L$-lipschitz.
\end{myprop}

Let $I_p = \{0, 1/p, 2/p, \ldots, (p-1)/p, 1\}$.
The grid method evaluates $f(x)$ for all $(p+1)^n$ points of $I_p^n$
and returns the minimum value $f(\bar{x})$.
In the worst case, the points is in the center of a hypercube of the grid
which is at a distance $\sqrt{n}/2p$ of the closest point of the grid.
Therefore
\[ f(\bar{x}) - f(x^*) \leq L\frac{\sqrt{n}}{2p} \]
so for a precision $\epsilon$, we need
\[ p  = \left\lceil L\frac{\sqrt{n}}{2\epsilon}+1\right\rceil \geq L\frac{\sqrt{n}}{2\epsilon} \]

It is essentially impossible to improve because there exist a resisting oracle that will set zero except on a hypercube of length $1/p$.
so all algorithme with a precision $\epsilon$ needs at least
\[ \left(\frac{L}{2\epsilon}\right)^n \]
evaluations.

\begin{mydef}
  $f \in C_L^{1,1}$ iff $f \in C^1$ and
  \[ |\grad f(y) - \grad f(x)| \leq L\|y - x\|. \]
\end{mydef}

\begin{myprop}
  If $f \in C^2$ and $\|\grad^2 f(x)\| \leq L$, then $f \in C_L^{1,1}$.
\end{myprop}

We can bound such function from above and below using a quadratic functions extrapolated from a point $x_0$ \cite[Lemma~1.2.3]{nesterov2004introductory}.
Those two functions are
\[ f(x_0) + \grad f(x_0)^T (x - x_0) \pm \frac{L}{2} \|x - x_0\|_2^2. \]

Using those bounds, we get the rate of convergence of the gradient's method \cite[p.~30]{nesterov2004introductory}
\[ x_{k+1} = x_k - h_k \grad f(x_k). \]
They tell us that
\[ f(x_{k+1}) - f(x_k) = \bigg(\frac{L}{2} h_k^2 - h_k\bigg) \|\grad f(x_k)\|_2^2 \]
hence for $h_k \in [0; 2/L]$ we have a guarantee that $f(x_{k+1}) \leq f(x_k)$
but the best guarantee we have is for $h_k = 1/L$ for which
\[ f(x_{k+1}) - f(x_k) = -\frac{1}{2L} \|\grad f(x_k)\|_2^2 \]
is guaranteed.
After $N$ iteration, we have
\[ \min_{0 \leq i \leq N} \|\grad f(x_i)\|_2 \leq \sqrt{\frac{2L(f(x_0)-f(x^*))}{N+1}} \]
so to obtain a precision of $\epsilon$ on the norm of the gradient, we need
\[ N \geq \frac{2L(f(x_0)-f(x^*))}{\epsilon^2}-1. \]

\begin{algorithm}
  \caption{Gradient's method for $f \in C^{1,1}_l$ and $x_0$.}
  \label{algo:longstep}
  \begin{algorithmic}
    \FOR{$k = 0, 1, 2, 3, \ldots$}
    \STATE{$x_{k+1} = x_k - \frac{1}{L}\grad f(x_k)$}
    \ENDFOR
  \end{algorithmic}
\end{algorithm}


\begin{mytheo}
  There exists $f:\R\to\R$ $L$-lipschitz such that $\|f'(x_k)\| \to 0$ in
  $\bigoh\left(\frac{1}{(N+1)^{1/2+\delta}}\right)$ for all $\delta > 0$.
\end{mytheo}

Since $f$ is not necessarily convex, we have no guarantee on the convergence to a solution.

\begin{myexem}[]
  This example is exactly \cite[Example~1.2.2]{nesterov2004introductory}.

  For $f(x_1,x_2) = 2x_1^2 + x_2^4 - 2x_2^2$, $\grad f(x_1,x_2) = 4(x_1, x_2^3 - x_2)$
  so if we start with $(1,0)$, $\grad f(x_1, x_2) = (*, 0)$ and $x_2$ will always be 0.
  Sadly $(0,0)$ is a saddle point and the real minima are $(0,1)$ and $(0,-1)$.
\end{myexem}

If $f$ is also convex, the Gradient's method with $h_k = 1/L$ can guarantee
\[ f(x_k) - f(x^*) \leq \frac{L}{2} \frac{\|x_0 - x^*\|_2^2}{k+1}. \]

With the help of a resistant oracle, we can prove that
\[ f(x_k) - f(x^*) \leq \frac{3L}{32} \frac{\|x_0 - x^*\|_2^2}{(k+1)^2} \]
so there is still rooms for improvement for the Gradient method.

The accelerated Gradient~\cite{nesterov1983method} guarantees
\[ f(x_k) - f(x^*) \leq 2L \frac{\|x_0 - x^*\|_2^2}{(k+1)^2}. \]

\begin{algorithm}
  \caption{Accelerated gradient for a convex $f \in C^{1,1}_l$ and $x_0$.}
  \label{algo:longstep}
  \begin{algorithmic}
    \STATE $x_{-1} = x_0$
    \FOR{$k = 0, 1, 2, 3, \ldots$}
    \STATE{$\beta_k = \frac{k-1}{k+2}$}
    \STATE{$y_k = x_k + \beta_k(x_k - x_{k-1})$}
    \STATE{$x_{k+1} = y_k - \frac{1}{L}\grad f(y_k)$}
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

\subsection{Constraint first order methods}
For the problem
\begin{align*}
  \min f(x)\\
  x & \in C
\end{align*}
with $C$ convex, the gradient's method iteration $x_k - \frac{1}{L}\grad f(x_k)$
might go out of $C$.
Hence we will take $x_{k+1}$ as the closest point of $x_k - \frac{1}{L}\grad f(x_k)$ in $C$.
Let's define the projection on $C$
\[ \{P_C[x]\} = \argmin_{y \in C} \|x - y\|_2^2. \]
This problem is strictly convex so the solution is unique.
If $C$ is non-empty and closed, we know this unique solution exists.

In conclusion, at each iteration, we do
\[ x_{k+1} = P_C[x_k - \frac{1}{L}\grad f(x_k)]. \]

For the accelerated gradient, we just modify the update of $x_{k+1}$,
\[ x_{k+1} = P_C[y_k - \frac{1}{L}\grad f(y_k)]. \]

\biblio
%http://www2.isye.gatech.edu/~nemirovs/Lect_IPM.pdf

% TODO cholesky
% TODO cite nemirovski2004interior

\annexe

\section{Preuve dualitÃ© forte}
\label{ann:strongdualityproof}
Soient les problÃ¨mes
\begin{align*}
(P) \max b^T y&\\
A^Ty &\leq c\\
\end{align*}

\begin{align*}
(D) \min c^T x&\\
Ax &= b\\
x &\geq 0\\
\end{align*}

\paragraph{ThÃ©orÃ¨me}
Si (P) admet une solution optimale $y^*$ alors (D) admet une solution
(optimale) $x^*$ telle que $b^T y^* = c^T x^*$.


\paragraph{Preuve\\}
\begin{enumerate}[label=\alph*)]
  \item Par hypothÃ¨se, (P) est admissible donc le systÃ¨me $A^Ty \leq c$ admet
    des solutions. Le thÃ©orÃ¨me de l'alternative implique donc que le
    systÃ¨me
    \[
      \left\{%
        \begin{array}{l}
          Ax=0\\
          c^Tx < 0\\
          x \geq 0
        \end{array}%
      \right.
    \]
    est impossible.\\

  \item Soit $\theta$ un paramÃ¨tre rÃ©el. ConsidÃ©rons le systÃ¨me (I-$\theta$)
    \[
      \left\{%
        \begin{array}{l}
          b^T y \geq \theta \\
          A^T y \leq c
        \end{array}%
      \right.
    \]
    Ce systÃ¨me affirme qu'il existe au primal (P) une solution admissible
    de valeur supÃ©rieure ou Ã©gale Ã  $\theta$, en d'autres termes $val(P) \geq
    \theta$.\\

  \item
    Ce systÃ¨me (I-$\theta$) peut se rÃ©Ã©crire
    \[
      \left\{%
        \begin{array}{l}
          A^T y \leq c\\
          -b^T y \leq -\theta
        \end{array}%
      \right.
    \]
    ou encore
    $$\left( \begin{array}{c}
        A^T\\
        -b^T
    \end{array} \right) y \leq \left( \begin{array}{c}
        c\\
        -\theta
    \end{array} \right),$$
    dont l'alternative est (on note $x$ le vecteur multiplicateur des
    contraintes $A^T y \leq c$ et $\mu$ le multiplicateur scalaire de la derniÃ¨re
    contrainte $-b^T y \leq -\theta$)
    \[
      \left\{%
        \begin{array}{l}
          Ax - b \mu = 0\\
          c^Tx - \theta \mu < 0\\
          x \geq 0\\
          \mu \geq 0
        \end{array}%
      \right.
    \]
    qu'on nommera (II-$\theta$).\\

  \item
    Dans le systÃ¨me (II-$\theta$), on peut prendre sans perte de gÃ©nÃ©ralitÃ© $\mu=1$.
    En effet, si $\mu=0$ est solution, on obtient
    \[
      \left\{%
        \begin{array}{l}
          Ax = 0\\
          c^T<0\\
          x \geq 0,
        \end{array}%
      \right.
    \]
    qui
    est pourtant impossible (voir point a.).
    Si $\mu>0$, on peut multiplier $x$ et $\mu$ par une constante ($1/\mu$ en
    l'occurrence) pour obtenir une autre solution admissible oÃ¹ $\mu=1$.
    Par consÃ©quent, on a en rÃ©alitÃ© une alternative entre le systÃ¨me
    (I-$\theta$) et le systÃ¨me (II'-$\theta$) suivant
    \[
      \left\{%
        \begin{array}{l}
          Ax = b\\
          c^T x < \theta\\
          x \geq 0.
        \end{array}%
      \right.
    \].\\

  \item
    Le systÃ¨me
    \[
      \left\{%
        \begin{array}{l}
          Ax = b\\
          c^T x < \theta \\
          x \geq 0
        \end{array}%
      \right.
    \]
    affirme qu'il existe au dual (D) une solution de valeur infÃ©rieure Ã  $\theta$, soit $val(D) <
    \theta$.\\

  \item
    Nous avons donc une alternative entre les deux propositions
    $val(P) \geq \theta$
    et
    $val(D) < \theta$
    (c'est ici qu'au cours j'avais conclu directement que $val(P)=val(D)$ ;
    l'argument qui suit est plus explicite).
    Si on choisit une valeur de $\theta$ Ã©gale Ã  $b^T y^* + \epsilon$
    (oÃ¹ $\epsilon$ est un paramÃ¨tre strictement positif et $b^T y^*$ est la valeur
    optimale du primal),
    on voit que la proposition $val(P) \geq \theta$ est clairement impossible,
    donc la proposition $val(D) < \theta$ est vraie.
    On a donc $val(D) < b^T y^* + \epsilon$ et ce pour tout $\epsilon>0$. En prenant la
    limite $\epsilon \rightarrow 0$ on voit qu'au final il faut forcÃ©ment que $val(D) \leq b^T
    y^*$.\\

  \item
    Comme par ailleurs on savait que $val(D) \geq b^T y^*$ par dualitÃ© faible,
    on a bien Ã©tabli l'Ã©galitÃ©
    $val(D) = b^T y^*$, et donc l'absence de saut de dualitÃ© entre les
    valeurs optimales du primal et du dual.\\

    \textbf{NB}: On n'a pas formellement Ã©tabli l'existence de la solution optimale
    duale $x^*$, mais seulement le fait que le coÃ»t optimal du dual est bornÃ©
    (et Ã©gal au coÃ»t optimal du primal). Cependant on sait (cf. cours
    d'opti 1) que tout problÃ¨me linÃ©aire atteint sa valeur optimale en au
    moins une solution, ce qui permet bien de conclure. A noter que cette
    propriÃ©tÃ© n'est pas forcÃ©ment valable en optimisation convexe
    non-linÃ©aire (considÃ©rer par exemple le problÃ¨me
    \begin{align*}
      \min_x \frac{1}{x+1}&\\
      x \geq 0).
    \end{align*}
\end{enumerate}

\section{Optimisation de portefeuille}

Soient $n$ actifs dont les rendements moyens sont $r_i$ ($i=1,...,n$) et soit
une somme $S$ Ã  investir. Le gain moyen maximal s'obtient en optimisant
\begin{align*}
\max \sum_i r_i x_i &\\
\sum x_i &= S\\
x &\geq 0,
\end{align*}
oÃ¹ $x_i$ est la somme Ã  investir dans l'actif $i$. La solution optimale de ce problÃ¨me consiste Ã  placer l'entiÃ¨retÃ© de la somme $S$ dans l'actif possÃ©dant le rendement maximal (parmi tous les $r_i$). En cas d'ex-aequo on peut rÃ©partir arbitrairement. \\

L'inconvÃ©nient de cette stratÃ©gie est qu'elle est trÃ¨s risquÃ©e. Une
mesure naturelle du risque est la variance du portefeuille, qui vaut
prÃ©cisÃ©ment $x^T C x$, oÃ¹ $C$ est la matrice de variance-covariance
(estimÃ©e) des diffÃ©rents rendements. Markowitz suggÃ¨re de maximiser une combinaison du gain moyen et de la
variance, selon
\begin{align*}
\max \sum_i r_i - \rho x^T C x &\\
\sum x_i &= S\\
x \geq 0
\end{align*}
(et il suggÃ¨re d'analyser la "frontiÃ¨re" des solutions obtenues en
faisant varier $\rho$ entre $0$ et $+\infty$).\\

Une formulation conique s'obtient facilement en introduisant une
variable $v$ pour la variance
\begin{align*}
\max \sum_i r_i - \rho v &\\
\sum x_i &= S\\
 x &\geq 0\\
x^T C x &\leq v
\end{align*}
et la derniÃ¨re contrainte $x^T C x \leq v $ s'exprime aisÃ©ment Ã  l'aide du
cÃ´ne de Lorentz (cf. ensemble S9 de la sÃ©ance - $C$ est bien semidÃ©finie
positive).

\subsection{Formulation puissance $3/2$}

Pour aller plus loin, voici comment modÃ©liser une puissance $3/2$. Plus
prÃ©cisÃ©ment, on va montrer comment modÃ©liser la contrainte $x^{3/2} \leq y$
pour une variable $x$ supposÃ©e positive (on vÃ©rifie facilement que c'est
un ensemble convexe, car Ã©pigraphe de la fonction convexe $x^{3/2}$).\\

On Ã©crit les deux contraintes
$x^2 \leq y z$ et $z^2 \leq x$
qui sont toutes les deux exprimables Ã  l'aide du cÃ´ne de Lorentz (cf. sÃ©ance).
On voit qu'elles sont Ã©quivalentes Ã  $x^4 \leq y^2 z^2$ et $z^2 \leq x$ qui
reviennent Ã  $x^4 \leq y^2 x$
qui revient bien Ã  $x^3 \leq y^2$ ou $x^{3/2} \leq y$. On peut alors modÃ©liser $||v||^{3/2}$ en posant $||v||\leq x$, ou mÃªme passer
Ã  $(v^T C v)^{3/2}$.\\

A l'aide de cette technique on peut "modÃ©rer" l'influence de la
variance dans la fonction objectif (modulo quelques Ã©tapes de
reformulation que je ne dÃ©taille pas ici).

\section{Preuve que l'algorithme de suivi de chemin fonctionne}
\label{ann:chemin}
Pour rappel, on a le problÃ¨me initial $(P)$
\begin{align*}
\min_x c^T x&\\
x &\in X
\end{align*}
et le problÃ¨me $(P_\mu)$ perturbÃ© par la barriÃ¨re $F(x)$, pour un paramÃ¨tre $\mu>0$
$$\min_x \frac{c^T x}{\mu} + F(x)  =  \min_x f_\mu(x).$$
Le pas de Newton pour le problÃ¨me perturbÃ© s'Ã©crit
$$n_\mu(x) = - (\nabla^2 f_\mu(x))^{-1} \nabla f_\mu(x)$$
et la mesure de proximitÃ© correspondante
$$\delta_\mu(x) = || n_\mu(x) ||_x = || \nabla f_\mu(x) ||^*_x.$$\\

PlaÃ§ons-nous Ã  l'itÃ©ration $k$ et posons
\begin{align*}
x &= x_k; \\
x^+&= x_{k+1};\\
\mu &= \mu_k; \\
\mu^+&= \mu_{k+1} = (1-\theta) \mu.
\end{align*}
On sait qu'on a au dÃ©part $\delta_\mu(x) < \tau$ et on voudrait prouver qu'aprÃ¨s la mise Ã  jour de $\mu$ Ã  $\mu^+$ et le pas de Newton on a $\delta_{\mu^+}(x^+) < \tau$ avec donc $x^+ = x + n_{\mu^+}(x)$. On analyse d'abord la quantitÃ© intermÃ©diaire $\delta_{\mu^+}(x)$ (aprÃ¨s mise Ã  jour de $\mu$, mais avant le pas de Newton). On a
$$\delta_{\mu^+}(x) = \norm{n_{\mu^+}(x)}_x = \norm{\nabla f_{\mu^+}(x)}^*_x = \norm{\frac{c}{\mu^+} + \nabla F(x)}^*_x.$$
Or, on peut Ã©crire
$$\frac{c}{\mu^+} + \nabla F(x) = \frac{\mu}{\mu^+} \left(\frac{c}{\mu} + \nabla F(x)\right) + \left(1-\frac{\mu}{\mu^+}\right) \nabla F(x),$$
d'oÃ¹
$$\delta_{\mu^+}(x) \leq \frac{\mu}{\mu^+} \norm{\frac{c}{\mu} + \nabla F(x)}^*_x + \left |1-\frac{\mu}{\mu^+}\right| \norm{\nabla F(x)}^*_x.$$
Notons que l'hypothÃ¨se que $F$ est une b.a.c. de paramÃ¨tre $\nu$ fournit $\norm{\nabla F(x)}^*_x \leq \sqrt{\nu}$, tandis que $\delta_\mu(x) < \tau$ fournit la borne $\norm{\frac{c}{\mu} + \nabla F(x)}^*_x < \tau$ ce qui permet d'Ã©crire, en utilisant le fait que $\frac{\mu}{\mu^+} = \frac{1}{1-\theta}$ et $ |1-\frac{\mu}{\mu^+}|=\frac{\theta}{1-\theta}$ (attention : la valeur absolue est nÃ©cessaire sur $1-\frac{\mu}{\mu^+}$, le prof l'avait oubliÃ©e au tableau),
$$\delta_{\mu^+(x)} < \frac{\tau + \sqrt{\nu} \theta}{1-\theta}.$$

A ce stade, on utilise les valeurs concrÃ¨tes $\tau=\frac{1}{4}$ et $\theta = \frac{1}{16\sqrt{\nu}}$. On utilise Ã©galement le fait que $\nu \geq 1$ quelle que soit la barriÃ¨re, d'oÃ¹ $1-\theta \geq 1-\frac{1}{16} = \frac{15}{16}$, d'oÃ¹ on tire $$\delta_{\mu^+}(x) < \frac{\frac{1}{4} + \frac{1}{16}}{\frac{15}{16}} = \frac{5/16}{15/16} = \frac{1}{3}.$$ On voit donc que la proximitÃ© au chemin central se dÃ©tÃ©riore un peu $\left(\frac{1}{4} \rightarrow \frac{1}{3}\right)$ mais reste assez bonne avec la nouvelle valeur (plus petite) de $\mu$.

Maintenant, on applique un pas de Newton $n_{\mu^+}(x)$ (pour minimiser $f_{\mu^+}$), et le rÃ©sultat de convergence quadratique nous indique que $$\delta_{\mu^+}(x^+) \leq \left(\frac{\delta_{\mu^+}(x)}{1-\delta_{\mu^+}(x)}\right)^2 < \left(\frac{1/3}{2/3}\right)^2 = \frac{1}{4},$$ ce qui permet de montrer que le nouvel itÃ©rÃ© a bien restaurÃ© sa proximitÃ© initiale ($\tau=\frac{1}{4}$) au par rapport au (nouveau point du) chemin central.

L'algorithme fonctionne donc correctement, et tout itÃ©rÃ© $x_k$ vÃ©rifiera $\delta_{\mu_k}(x_k) < \tau$
\end{document}
