
The paper proposes a new abstraction called resilient
distributed datasets (RDDs) that enables efficient
data reuse in a broad range of applications. RDDs are
fault-tolerant, parallel data structures that let users explicitly
persist intermediate results in memory, control
their partitioning to optimize data placement, and manipulate
them using a rich set of operators.
The main challenge in designing RDDs is defining a
programming interface that can provide fault tolerance
efficiently.

\subsection{RDDs}

Formally, an RDD is a read-only, partitioned collection
of records. RDDs can only be created through deterministic
operations on either (1) data in stable storage or (2)
other RDDs. We call these operations transformations to
differentiate them from other operations on RDDs. Examples
of transformations include map, filter, and join.

RDDs do not need to be materialized at all times. Instead,
an RDD has enough information about how it was
derived from other datasets (its lineage) to compute its
partitions from data in stable storage. This is a powerful
property: in essence, a program cannot reference an
RDD that it cannot reconstruct after a failure.

users can control two other aspects of RDDs:
persistence and partitioning. Users can indicate which
RDDs they will reuse and choose a storage strategy for
them (e.g., in-memory storage). They can also ask that
an RDDâ€™s elements be partitioned across machines based
on a key in each record. This is useful for placement optimizations,
such as ensuring that two datasets that will
be joined together are hash-partitioned in the same way.

\subsection{Spark Programming Interface}

Spark is an implementation of RDDs.

\subsection{Advantages of the RDD Model}

\begin{tabular}{l|p{5cm}|p{5cm}}
\textbf{Aspect} & \textbf{RDDs} & \textbf{Distributed Memory (DSM)} \\
\hline
\hline
Reads & Coarse- or fine-grained & Fine-grained \\ \hline
Writes & Coarse-grained & Fine-grained  \\ \hline
Consistency & Trivial (immutable)  & Up to app / runtime \\ \hline
Fault recovery & Fine-grained and lowoverhead using lineage & Requires checkpoints and program rollback \\ \hline
Straggler mitigation & Possible using backup tasks & Difficult \\ \hline
Work placement & Automatic based on data locality & Up to app (runtimes aim for transparency) \\ \hline
Behavior if not enough RAM & Similar to existing data flow systems & Poor performance (swapping?) \\ \hline
\end{tabular}

\subsection{Applications Not Suitable for RDDs}
RDDs are best suited for batch applications that apply the same operation to
all elements of a dataset. In these cases, RDDs can ef-
ficiently remember each transformation as one step in a
lineage graph and can recover lost partitions without having
to log large amounts of data. RDDs would be less
suitable for applications that make asynchronous finegrained
updates to shared state, such as a storage system
for a web application or an incremental web crawler.

\subsection{Application suitable for RDDs}
Iterative Machine Learning Applications. Page rank with optimized partitioning of the RDDs. In-Memory Analytics. Traffic Modeling. Twitter Spam Classification.


\subsection{Checkpointing}

Although lineage can always be used to recover RDDs
after a failure, such recovery may be time-consuming for
RDDs with long lineage chains. Thus, it can be helpful
to checkpoint some RDDs to stable storage.

\subsection{Memory Management}
Three options : Memory, serealized in memory, hard-drive.

\subsection{Expressing MapReduce with Spark}
MapReduce can be expressed using the
flatMap and groupByKey operations in Spark, or reduceByKey
if there is a combiner.


