

\subsection{Above cloud}

\subsubsection*{The paper discusses three new aspects in cloud computing,
from hardware point of view. List those three points.}

1. The illusion of infinite computing resources available on demand,
thereby eliminating the need for Cloud Computing users to plan far
ahead for provisioning.

2. The elimination of an up-front commitment by Cloud users, thereby
allowing companies to start small and increase hardware resources only
when there is an increase in their needs.

3. The ability to pay for use of computing resources on a short-term
basis as needed (e.g., processors by the hour and storage by the day)
and release them as needed, thereby rewarding conservation by letting
machines and storage go when they are no longer useful.

\subsubsection*{For services with varying demand over time, how does
server utilisation effect the preferability to use cloud or private
servers?}

Use cloud computing service is more flexible than provisioning a data
center. Indeed, provisioning a data center for the peak load it must
sustain a few days per month leads to underutilization at other times,
for example. Instead, Cloud Computing lets an organization pay by the
hour for computing resources, potentially leading to cost savings even
if the hourly rate to rent a machine from a cloud provider is higher
than the rate to own one.

\subsubsection*{For batch analytics, other than the cost what is the key
factor to decide whether the batch processing should be done locally or
on cloud?}

It we be interresting to use the cloud to perform batch analytics in
order to achieve the foal faster. Indeed, organizations that perform
batch analytics can use the ”cost associativity” of cloud computing
to finish computations faster: using 1000 EC2 machines for 1 hour costs
the same as using 1 machine for 1000 hours.


\subsection{GFS}

\subsubsection*{The Google File System (GFS) decouples the flow of data
from the flow of control. This allows clients to send the data in
parallel to all the replicas in order to optimize the usage of network
bandwidth for the data transfer. Is this correct? If yes, write: "YES:"
followed by the reason that this strategy improves network performance.
If no, write: "NO:" followed by a brief description of how the data is
sent to all replicas.}

NO: the client send the data only to all data by a pipelined fashion.

Indeed, (1) after ask replica to the master and (2) receive the replicas
identity, (3) the client send data to all replicas by a pipelined
fashion. After this, when all replicas have receiving the data, (4)
client send a "write request" to the primary replica which (5) forward
the "write request" to all other replicas. (6) The replicas response by
a acknowledgement of the "write request" and when primary receive all
ack, (7) primary replica replies to the client with the reported errors
encountered by the other replicas.


\subsubsection*{A GFS client that intends to append to a file needs to
first interact with the GFS master to retrieve the metadata regarding
the chunk to write to. From the point of view of the workload imposed on
the master, in what conditions would you pick a small chunk size rather
than a large one?}

If we share with large chunk size, the master (1) use small memory to
stored the metadata about file, (2) need small time to interacted with
client about chunk location information and (3) keep longer TCP
connection with the chunkserver in order to retreive the chunk.

But this can be a disadvantages if many client want the same small size
file (Small means file separated in one or two chunk). Indeed, in this
case, the GFS master memory and time resource can be not really use
because of (1) and (2). In addition, because of (3) some chunkserver may
be use a lot because they contains a chunk while some other not at all
because their don't have a chunk. 

\subsection{Dynamo}

\subsubsection*{According to the paper, what is the problem with databases
with ACID properties and why does Dynamo compromises on some of the ACID
properties?}

The problem with ACID properties is to have poor availability. Indeed,
experience at Amazon has shown that data stores that provide ACID
guarantees tend to have poor availability. This has been widely
acknowledged by both the industry and academia.

Dynamo compromise with a weak consistency if this results in high
availability.
Dynamo also work without provide an isolation guarantees and permits
only single key updates.

\subsubsection*{Suppose that in Dynamo we have two version of an object
with their respective vector clocks. Version D1 of the object has vector
clock ([Sx,2], [Sy,5], [Sz,1]) and version D2 of the object has vector
clock ([Sx,3], [Sy,5], [Sz,3]). Is there a causal relation between the
two versions or are the versions from parallel branches of updates? How
will these versions combine?}

VC1:([Sx,2], [Sy,5], [Sz,1]) is the ancestor of VC2:([Sx,3], [Sy,5],
[Sz,3]). 
Indeed, if the counters on the first object’s clock are
less-than-or-equal to all of the nodes in the second clock, then the
first is an ancestor of the second and can be forgotten.
May
So, VC1 <H VC2 (<H : causal order) and in this case VC2 is the ancestor
(updates) of VC1. We don't need to combine, only keep VC2.

\subsubsection*{Name the three data partitioning strategies that the
authors discuss for Dynamo. Which strategy was selected in the end and
why?}

Strat1. T random tokens per node and partition by token value
Strat2. T random tokens per node and equal sized partitions
Strat3. Q/S tokens per node, equal-sized partitions

The strategy 3 is selected because
- he achieves the best load balancing efficiency 
- reduces the size of membership information maintained at each node.
Indeed, storage is not a major issues on the node but in order to gossip
periodically the membership informations it's usefull to have a small
size.
- Faster bootstrapping/recovery because the partition ranges are fixed,
they can be stored in separate files, meaning a partition can be
relocated as a unit by simply transferring the file.
- Ease of archival because  the partition files can be archived
separately.

\subsection{Zookeeper}

\subsubsection*{ZooKeeper provides a coordination service for distributed
applications. The interface exposed to clients is meant to be
sufficiently general and flexible. Based on what core concept do the
clients of ZooKeeper build upon in order to solve specific coordination
problems?}

ZooKeeper provides to its clients the abstraction of a set of data nodes
(called znodes) in order to build some high level abstraction such as
rendezvous, groupmembership or simple log. Theire are organized with a
hierarchical name space which is useful for organized subtrees between
applications and access rights.

Znodes are designed to store information for meta-data coordination
rather than data storage.

In addition, zooKeeper also implements watches in order to allow client
to receive notifications when informations changes whitout polling.


\subsection{Oktopus}

\subsubsection*{The paper proposes 2 virtual network abstractions that allow tenants of cloud datacenters to achieve more predictable performance for network transfers. A hard problem is to share the datacenter network bandwidth according to the allocations of the virtual networks. How does Oktopus ensure that VMs do not exceed their allocated bandwidth?}

Oktopus uses rate-limiting at endhost hypervisors to enforce the bandwidth available at each VM. It means that for each VM on a physical machine, an enforcement module resides in the OS hypervisor.

This work as follow :
1) The enforcement module for a VM measures the traffic rate to other VMs and send periodically these measurements to a tenant VM which is designated as the controller VM.
2) The enforcement module at the controller then calculates the max-min fair share for traffic between the VMs. He communicated back to other tenant VMs where the enforcement module uses per-destination-VM rate limiters to enforce them.

\subsubsection*{Compared to the virtual cluster abstraction, the virtual oversubscribed cluster abstraction provides an advantage for the datacenter operator. What is it? Should tenants that use data-intensive application prefer the virtual cluster in favor of the virtual oversubscribed cluster? Why?}

The VOC is better suited for the datacenter operator because it capitalizes on application structure to reduce the bandwidth needed from the underlying physical infrastructure compared to virtual clusters, there by improving provider flexibility and reducing tenant costs.

Yes, data-intensive application should prefer the VC rather than VOC. VC offer as dense a connectivity and allow each VM to use the full dedicaced link.


\subsection{RDDs}

\subsubsection*{What are the advantages of the immutable property of
RDDs?}

RDDs : coarse-grained "written" and fine-grained "read"
DSM : fine grained "written" and "read"
=> In this case, RDDs must perform bulk writes but allow for more efficient fault tolerance. 

Immutable allow some advantages :
1) RDDs do not need to incur the overhead of checkpointing, as they can be recovered using lineage. 
2) RDDs lets a system mitigate slow nodes by running backup copies of slow tasks as in MapReduce. Indeed, the backup is easily than DSM because the two backup copies don't interfere with other's update.
3) Only the lost partitions of an RDD need to be recomputed upon failure, and they can be recomputed in parallel on different nodes, without having to roll back the whole program, thanks to lineage graph.

At the end, thanks to immutable property RDDs is trivialy consistent.

\subsubsection*{In relation to RDDs, what is a lineage graph and what are the benefits of linages graph?}


Lineage is when an RDD has enough information about how it was derived from other datasets to compute its partitions from data in stable storage. Indeed, this allow to reconstruct it after a failure.

This lineage graph is easier to described with RDDs immutable property.

1) Lineage can always be used to recover RDDs after a failure, such recovery may be time-consuming for RDDs with long lineage chains. Thus, it can be helpful to checkpoint some RDDs to stable storage.


\subsection{Raft}

\subsubsection*{RAFT is based on a replicated append-only log. Can the
log entries be rewritten? If YES, in what circumstances do log entries
get rewritten?}

Yes, he can be rewrite in some cicumstances. Indeed, in raft when their is inconsistencies the leader force the follower to duplicate its own logs. It means that some conflicting entries can be rewrite on the follower's side after a Leader election.

This inconsistence is detecting when there is a new leader election. Indeed, follower will reject the AppendEntries. Leader can force follower to overwrite by maintains a nextindex for each follower and decrement it when a AppendEntries until the AppendEntries eventually match.


\subsubsection*{If a server crashes and then restarts, it might receive
duplicate AppendEntries RPCs. How does the server correctly handle this
condition?}

RPC are idempotent, so the follower ignores those entries when he
receive a request that already includes the log entries.



