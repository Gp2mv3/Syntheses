\documentclass[en]{../../../../../../eplexam}

\hypertitle{Machine Learning}{7}{ELEC}{2870}{2019}{Janvier}{All}
{ELEC students 2018--2019}
{Michel Verleysen and John Lee}

\section{Verleysen}

\begin{itemize}
    \item \textbf{Generalization (5)}: Explain how overfitting, generalization and complexity are linked together. Illustrate with some examples (in 2D or 3D) for both regression and classification.
    \item \textbf{Feature selection}: Explain what feature extraction is. Why is it important to do feature extraction to obtain less but not too few features?
    \item \textbf{Feature selection}: greedy algorithms, explain the advantages and drawbacks of the forward and backward ones, then speak about the compromises between them. Why do greedy algorithms belong to the non optimal algorithms?
    \item \textbf{Feature selection}: Explain and compare filters and wrappers. Give and explain a non linear criterion to evaluate the dependence between two variables and give an example where the correlation does not give meaningful results. 
    \item \textbf{Model selection}: What are cross-validation and bootstrap? Give the differences between them, the advantages and drawbacks of each one. Explain the plugin principle.
    \item \textbf{Nonlinear dimensionality reduction}: What is 
    \begin{itemize}
    \item the curse of dimensionality;
    \item a special phenomenon due to high dimensionality;
    \item distance distortion due to HD and how does it affect machine learning?
    \end{itemize}
    \item \textbf{Distances (8)\footnote{It seems that the question was oriented on the non linear methods that require distances, and the difference with these methods.}}:
    \begin{itemize}
        \item Type of distances that can be used (Euclidean or not)?
        \item Possibility of weighing the distances, types of weights?
        \item Why is it important? How do they influence the result of a non linear dimensionality reduction?
        \item What is the fundamental difference between PCA and MDS?
    \end{itemize}
    \item \textbf{Time series:} Why is it an interpolation? How to construct a prediction?
    \item \textbf{Non linear regression (4.5)}: Explain how, with a regression method, we can achieve an estimation of a time series. Particularly, explain why it consists of an interpolation (and not extrapolation) problem.    
    \item \textbf{Regression}: Explain and compare these three methods according to their structure, error function, adaptation rule, convergence, and give a geometric 2D interpretation when these are used for classification:
    \begin{itemize}
     \item adaline (linear),
     \item generalized adaline (non linear),
     \item and perceptron (with one layer, not MLP).
    \end{itemize}
\end{itemize}

\nosolution

\section{Lee}

\begin{itemize}
    \item \textbf{Deep learning}: Explain the concept of vanishing/exploding gradient.
    \item \textbf{Self-organizing maps}: Explain the convergence problems of the Kohonen cards: butterfly effect, pinch effect and centroids outside distribution. How to avoid them? Give a measurement of the quality of the organisation.
    \item \textbf{Vector quantization}: Explain all the VQ algorithms, with the advantages and the inconvenients (competitive learning, frequency sensitive learning, LVQ1, LVQ2, SLS, SRS, NG).
    \item \textbf{PCA (8)}: Give the goal, on the axes, with the variance/covariance matrix, in dimension reduction, difference with metric multidimensional scaling (MDS).
    \item \textbf{Deep learning}: Explain the activation function ReLU, its properties, advantages and disadvantages with respect to the classical activation functions. Why is it chosen for deep learning, and why did it appear so lately in the field?
    
     \item \textbf{Deep learning}: The universal approximation theorem states that a network with one hidden unit is enough to approximate any continuous function. But deep learning is based on deep networks, why? Explain the advantages and drawbacks of shallow vs. deep networks. 
    \item \textbf{Deep Learning, convolutional networks (5)}:
    \begin{itemize}
        \item What is the principle? What are the possible inputs?
        \item What is the difference with a fully connected classical neural network?
    \end{itemize}
    \item \textbf{ICA(8)}:
        \begin{itemize}
            \item Goal
            \item Principle and differences with PCA
            \item Methods
            \item Possible extensions
        \end{itemize}
    \item \textbf{RBFN}: Give the shape of the interpolation and the error. How do we learn them (the three stages)?
    \item \textbf{RBFN (8)}: Goal, properties, learning, advantages.
\end{itemize}

\nosolution

\end{document}
