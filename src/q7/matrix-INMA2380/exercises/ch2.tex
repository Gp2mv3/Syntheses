\section{Linear system}
\exo{1}
\begin{solution}
  \begin{itemize}
    \item If $a, b \in \mathcal{S}_1 \cap \mathcal{S}_2$,
      it means that $a \in \mathcal{S}_1, a \in \mathcal{S}_2$
      and $b \in \mathcal{S}_1, b \in \mathcal{S}_2$.
      Since $\mathcal{S}_1$ and $\mathcal{S}_2$
      are linear subspaces, $\alpha a + \beta b \in \mathcal{S}_1$
      and $\alpha a + \beta b \in \mathcal{S}_2$.
      Finally, $\alpha a + \beta b \in \mathcal{S}_1 \cap \mathcal{S}_2$.
    \item If $a,b \in \mathcal{S}_1 + \mathcal{S}_2$,
       it means that $a$ can be written as $a_1 + a_2$
       such that $a_1 \in \mathcal{S}_1, a_2 \in \mathcal{S}_2$
       and that $b$ can be written as $b_1 + b_2$
       such that $b_1 \in \mathcal{S}_1, b_2 \in \mathcal{S}_2$.
       Next, $\alpha a + \beta b = \alpha (a_1 + a_2) + \beta (b_1 + b_2)
       =(\alpha a_1 + \beta b_1) + (\alpha a_2 + \beta b_2)$,
       which is the sum of an element of $\mathcal{S}_1$ and
       an element of $\mathcal{S}_2$, and thus $\mathcal{S}_1 + \mathcal{S}_2$
       is a linear subspace.
    \item We need to show that $\alpha a + \beta b \in \Ker A$,
      or equivalently $A(\alpha a + \beta b) = 0$
      if $a, b \in \Ker A$.
      We have that $A(\alpha a + \beta b) = \alpha Aa + \beta Ab = 0$.
    \item Same reasoning as third point.
    \item Same reasoning as third point.
  \end{itemize}
\end{solution}

\exo{1}
\begin{solution}
  \begin{itemize}
    \item If $x \in \Ker A$, then $Ax = 0$
      which also means that $RAx = R0 = 0$ so $x \in \Ker(RA)$.
    \item If $y \in \Ima(AR)$, then there is a $x$
      such that $ARx = y$
      which also means that there is a $x'$ such that
      $Ax' = y$ since we can just take $x' = Rx$.
      So $y \in \Ima A$.
  \end{itemize}
\end{solution}

\exo{2}
\begin{solution}
  Let's suppose by contradiction that there are 2 ways
  to decompose $\mathbf{x}$

  \begin{align}
    \label{eq:2.3.1}
    \mathbf{x} & = \sum_{i=1}^k \alpha_i \mathbf{a}_i\\
    \label{eq:2.3.2}
               & = \sum_{i=1}^k \beta_i \mathbf{a}_i
  \end{align}
  with $\beta_i \neq \alpha_i$ for at least one $i \in \{0, \ldots, k\}$.
  $\eqref{eq:2.3.2}-\eqref{eq:2.3.1}$ gives
  \[
    0 = \sum_{i=1}^k (\beta_i-\alpha_i) \mathbf{a}_i.
  \]
  by hypothesis, there is a $i$ such that $\beta_i-\alpha_i \neq 0$
  which is a contradiction with the linear independence of the $\mathbf{a}_i$.
\end{solution}

\exo{4}
\begin{solution}
  Let's suppose by contradiction that there are 2
  basis $a_1, \ldots, a_n$ and $b_1, \ldots, b_m$ with $n < m$.

  Let's just remember that $a_1, \ldots, a_n$ is spanning
  and $b_1, \ldots, b_m$ are linearly independent.

  We have therefore that
  \[ b_1 = \alpha_1 a_1 + \ldots + \alpha_n a_n\]
  with at least one $\alpha_i \neq 0$ since $b_1 \neq 0$.
  Wlog, let's say that $\alpha_1 \neq 0$.
  Therefore
  \[ a_1 = \frac{1}{\alpha_1}b_1 + \frac{\alpha_2}{\alpha_1} a_2 + \ldots + \frac{\alpha_n}{\alpha_1} a_n\]
  so $(b_1, a_2, \ldots, a_n)$ is spanning.

  We now have
  \[ b_2 = \beta_1 b_1 + \beta_2 a_2 + \ldots + \beta_n a_n\]
  if $\beta_2 = \cdots = \beta_n = 0$, $b_1, b_2$ are not linearly independent.
  Wlog, let's say that $\beta_2 \neq 0$,
  $(b_1,b_2,a_3,\ldots,a_n)$ is therefore spanning.

  Continuing this reasoning, $(b_1,\ldots,b_n)$ is spanning.
  $b_{n+1}$ is therefore a linear combination of $b_1, \ldots, b_n$ which is a contradiction.
\end{solution}

\exo{2}
\begin{solution}
  Since $\mathbb{C}^{m \times n}$ is a vector space and if $X,Y \in \mathbb{C}^{m \times n}$, $\trace(Y^*X) \in \mathbb{C}$ which is a field,
  $\trace(Y^*X)$ could be the scalar product $(X,Y)$.
  Let's check.
  Since

  \begin{align*}
    \trace(X^*X)
    & = \sum_{i = 1}^m \sum_{j = 1}^n \overline{x_{ij}}x_{ij}\\
    & = \sum_{i = 1}^m \sum_{j = 1}^n |x_{ij}|^2,
  \end{align*}
  we clearly have $\trace(X^*X) \geq 0$ for all $X \in \mathbb{C}^{m \times n}$ and
  $\trace(X^*X) = 0 \Longleftrightarrow X = 0$ for all $X \in \mathbb{C}^{m \times n}$.
  We also have
  \begin{align*}
    \trace(Z^*(\alpha X + \beta Y))
    & = \trace(\alpha Z^*X + \beta Z^*Y)\\
    & = \alpha\trace(Z^*X) + \beta\trace(Z^*Y)\\
  \end{align*}
  and
  \begin{align*}
    \trace(Y^*X)
    & = \trace((X^*Y)^*)\\
    & = \overline{\trace(X^*Y)}.
  \end{align*}
  
  We just proved it is a scalar product and we can 
  therefore apply the \emph{Schwarz inequality} which gives
  \begin{align*}
    |\trace(Y^*X)|
    & \leq \sqrt{\trace(X^*X)} \sqrt{\trace(Y^*Y)}\\
    & = \|X\|_F \|Y\|_F.
  \end{align*}
\end{solution}

\exo{1}
Prove that $R$ is the Cholesky factor of the positive definite matrix $A^*A$.
\begin{solution}
  Let us recall that the Cholesky decomposition of a matrix $A$ consist in finding
  a lower triangular matrix $L$ such that $A = L L^*$.
  
  In this case we have
  \begin{align*}
    A^*A
    & = R^*Q^*QR\\
    & = R^*IR\\
    & = R^*R.
  \end{align*}

  Note that $A$ is \emph{squared}
  (since otherwise it wouldn't be defined for it to be positive definite)
  and is \emph{full rank} (if not, it has an zero eigenvalue and cannot be positive definite).
  Therefore $R$ is an upper triangular square matrix with no zero element in this case.
\end{solution}

\exo{2}
\begin{solution}
  \begin{description}
    \item[$\Rightarrow$]
      $\mathcal{X} \perp \mathcal{Y}$ implies that 
      $(\mathbf{x},\mathbf{y}) = 0$, $\forall \mathbf{x} \in \mathcal{X}$ 
      and $\forall \mathbf{y} \in \mathcal{Y}$.
      In particular, it should be true for every element of the base.
      We need to have $(x_i,y_j) = 0$, $\forall i,j$ so in other words
      \[
        \begin{pmatrix}
          y_1 & y_2 & \cdots & y_n
        \end{pmatrix}^*
        \begin{pmatrix}
          x_1 & x_2 & \cdots & x_n
        \end{pmatrix}
        = 0.
      \]
      and thus $Y^* X = 0$.
    \item[$\Leftarrow$]
      Let $\sum a_ix_i$ be an element of $\mathcal{X}$ and $\sum b_iy_i$ be an element of $\mathcal{Y}$.
      We need
      \begin{align*}
        (\sum_i a_ix_i, \sum_i b_iy_i)
        & = \sum_i a_i (x_i, \sum_j b_jy_j)\\
        & = \sum_i a_i \overline{(\sum_j b_jy_j, x_i)}\\
        & = \sum_i \sum_j a_i\overline{b_j} \, \overline{(y_j, x_i)}\\
        & = \sum_i \sum_j a_i\overline{b_j} (x_i, y_j)\\
        & = \sum_i \sum_j a_i \overline{b_j} 0\\
        & = 0.
      \end{align*}
  \end{description}
\end{solution}

\exo{3}
\begin{solution}
  \begin{enumerate}
    \item
      \begin{align*}
        (S^\perp)^\perp
        & = \{x | (x,y) = 0, \forall y \in \mathcal{S}^\perp\}\\
        & = \{x | (x,y) = 0, \forall y \in \{z | (z,w) = 0, \forall w \in \mathcal{S}\}\}.
      \end{align*}
      In other word, $x$ should be perpendicular to every vector which is perpendicular to all vectors of $\mathcal{S}$.
      This is clearly true for $x \in \mathcal{S}$.
      If $x \notin \mathcal{S}$, since it is perpendicular to every vector of $\mathcal{S}^\perp$, $x \notin \mathcal{S}^\perp$,
      this contradicts the lemma~2.8.
    \item
      \begin{align*}
        (\mathcal{S}_1 + \mathcal{S}_2)^\perp
        & = \{x | (x,y) = 0, \forall y \in (\mathcal{S}_1+\mathcal{S}_2)\}\\
        & = \{x | (x,\alpha y_1 + \beta y_2) = 0, \forall \alpha,\beta, \forall y_1 \in \mathcal{S}_1, \forall y_2 \in \mathcal{S}_2\}\\
        & = \{x | ((x, y_1) = 0, \forall y_1 \in \mathcal{S}_1) \land ((x, y_2) = 0, \forall y_2 \in \mathcal{S}_2)\}\\
        & = \{x | (x, y_1) = 0, \forall y_1 \in \mathcal{S}_1\} \cap \{x | (x, y_2) = 0, \forall y_2 \in \mathcal{S}_2\}\\
        & = \mathcal{S}_1^\perp \cap \mathcal{S}_2^\perp.
      \end{align*}
    \item
      $(\mathcal{S}^\perp)^\perp$ actually ensures us that
      2 subspaces are equal iff their orthogonal complements are equal.
      If they are equal, it is obvious that their complement is equal
      and if their complement is equal, the complement of their complement is equal so their are equal.

      We can therefore prove that the complements are equals

      \begin{align*}
        ((\mathcal{S}_1 \cap \mathcal{S}_2)^\perp)^\perp
        & = (\mathcal{S}_1^\perp + \mathcal{S}_2^\perp)^\perp\\
        \mathcal{S}_1 \cap \mathcal{S}_2
        & = (\mathcal{S}_1^\perp + \mathcal{S}_2^\perp)^\perp
      \end{align*}
      Using the previous property, we have

      \begin{align*}
        (\mathcal{S}_1^\perp + \mathcal{S}_2^\perp)^\perp
        & = (\mathcal{S}_1^\perp)^\perp \cap (\mathcal{S}_2^\perp)^\perp\\
        & = \mathcal{S}_1 \cap \mathcal{S}_2.
      \end{align*}
  \end{enumerate}
\end{solution}

\exo{2}
\begin{solution}
  Let $U_{up} = [u_{ij}]_{i,j=1}^{n,n}$.
  We must have
  \[ U_{up}\Lambda = \Lambda U_{up}. \]
  At the elements $i,j$, we have
  \[ u_{ij} \lambda_j = \lambda_i u_{ij} \]
  or
  \[ u_{ij} (\lambda_j - \lambda_i) = 0. \]
  If $i \neq j$, this means that $u_{ij} = 0$ since $\lambda_j - \lambda_i \neq 0$.

  $U_{up}$ must therefore be diagonal.
  However it also needs to be hermitian for $UU_{up}$ to be hermitian.
  Indeed, we need to have

  \begin{align*}
    UU_{up} U_{up}^*U^* & = I\\
    U_{up} U_{up}^* & = U^*U\\
    U_{up} U_{up}^* & = I.
  \end{align*}
  This means that we need to have $1 = u_{ii}\overline{u_{ii}} = |u_{ii}|^2$
  so $u_{ii}$ is on the unit circle.
\end{solution}

\exo{4}
\begin{solution}
  \emph{Hint:}
  Use the property $\trace(AB) = \trace(BA)$.

  Since $H$ is hermitian, there is an unitary $U$ and diagonal $\Lambda$ such that $H = U \Lambda U^*$.
  Let's first analyse the RHS

  \begin{align*}
    \trace(H) & = \trace(U \Lambda U^*)\\
              & = \trace(\Lambda U^*U)\\
              & = \trace(\Lambda)\\
              & = \sum_{i=1}^n \lambda_i.
  \end{align*}
  For the LHS, we first need to analyse $Q$.
  It is unitary so its eigenvalues are on the unit circles therefore $|x^* Q x| \leq 1$ for all $x$ such that $\|x\|=1$.
  Using the triangle inequality and the fact that the $\lambda_i$ are positive

  \begin{align*}
    \trace(HQ)
    & = \trace(U \Lambda U^* Q)\\
    & = \trace(\Lambda U^* Q U)\\
    & = \sum_{i=1}^n \lambda_i u_{:i}^* Q u_{:i}\\
    & \leq \left|\sum_{i=1}^n \lambda_i u_{:i}^* Q u_{:i}\right|\\
    & \leq \sum_{i=1}^n |\lambda_i| \cdot |u_{:i}^* Q u_{:i}|\\
    & = \sum_{i=1}^n \lambda_i |u_{:i}^* Q u_{:i}|\\
    & \leq \sum_{i=1}^n \lambda_i.
  \end{align*}
\end{solution}

\exo{3}
\begin{solution}
  If $A$ is $m \times n$, then $H$ is $m \times r$ and $Q$ is $r \times n$ for some $r$.
  We want $H$ to be positive definite so we need $H$ to be square for it to be defined.
  That means that $Q$ is $m \times n$.

  The problem with the proof when $m \neq n$ is that we cannot say
  $U \Sigma V^* = U \Sigma U^* U V^*$ since the product is not defined because $U$ is $m \times m$
  and $V$ is $n \times n$.
  \begin{itemize}
    \item If $m < n$, we have
      \begin{align*}
        A
        & = U
        \begin{pmatrix}
          \Sigma & 0
        \end{pmatrix}
        \begin{pmatrix}
          V_1 & V_2
        \end{pmatrix}^*\\
        & = U \Sigma V_1^*\\
        & = (U \Sigma U^*) (U V_1^*)
      \end{align*}
      with $V_1^*V_1 = I$ (but not $V_1V_1^* = I$, since $I$ is full rank and $V_1$ is not full line rank)
      since
      \begin{align*}
        I
        & =
        \begin{pmatrix}
          V_1 & V_2
        \end{pmatrix}^*
        \begin{pmatrix}
          V_1 & V_2
        \end{pmatrix}\\
        & =
        \begin{pmatrix}
          V_1^*V_1 & V_1^*V_2\\
          V_2^*V_1 & V_2^*V_2
        \end{pmatrix}
      \end{align*}

      Therefore $QQ^* = (U V_1^*) (U V_1)^* = I$ but $Q^*Q = (U V_1^*)^* (U V_1^*) = V_1V_1^* \neq I$.
    \item If $m > n$, we have
      \begin{align*}
        A
        & =
        \begin{pmatrix}
          U_1 & U_2
        \end{pmatrix}
        \begin{pmatrix}
          \Sigma \\ 0
        \end{pmatrix}
        V^*\\
        & = U_1 \Sigma V^*\\
        & = (U_1 \Sigma U_1^*) (U_1 V^*)
      \end{align*}
      with $U_1^*U_1 = I$ but not $U_1U_1^* = I$ like for the previous point.

      This time it's $Q^*Q = I$ and $QQ^* = I$.
  \end{itemize}
\end{solution}

\exo{3}
\begin{solution}
  \emph{Hint} Use the property $\|A\|_F = \trace(A^TA) = \trace(AA^T)$ and the property $\trace(AB) = \trace(BA)$.

  We have (also using $\trace(A^T) = \trace(A)$).

  \begin{align*}
    \|AQ^T - B\|_F^2
    & = \trace((AQ^T - B)(AQ^T - B)^T)\\
    & = \trace((AQ^T - B)(QA^T - B^T))\\
    & = \trace(AQ^TQA^T) + \trace(BB^T) - \trace(AQ^TB^T) - \trace(BQA^T)\\
    & = \trace(AQ^TQA^T) + \trace(BB^T) - \trace(AQ^TB^T) - \trace(BQA^T)\\
    & = \trace(AA^T) + \trace(BB^T) - \trace(B^TAQ^T) - \trace(QA^TB)\\
    & = \|A\|_F^2 + \|B\|_F^2 - \trace(B^TAQ^T) - \trace(B^TAQ^T)\\
    & = \|A\|_F^2 + \|B\|_F^2 - 2\trace(B^TAQ^T)\\
    & = \|A\|_F^2 + \|B\|_F^2 - 2\trace(\tilde{H}\tilde{Q} Q^T)\\
    & \geq \|A\|_F^2 + \|B\|_F^2 - 2\trace(\tilde{H})
  \end{align*}
  using exercise~2.10 since $\tilde{Q}Q^T$ is unitary and $\tilde{H}$ is hermitian positive semi-definite.
  However, taking $Q = \tilde{Q}$, we have the equality. It is therefore optimal.
\end{solution}

\exo{2}
\begin{solution}
  Let $\mathcal{S}_1$ of dimension $m$ and $\mathcal{S}_2$ of dimension $n < m$.
  We can have $n$ canonical angles using

  \begin{align*}
    S_1^*S_2
    & = U_1
    \begin{pmatrix}
      \Sigma\\0
    \end{pmatrix}
    U_2^*\\
    & =
    \begin{pmatrix}
      U_{1,1} & U_{1,2}
    \end{pmatrix}
    \begin{pmatrix}
      \Sigma\\0
    \end{pmatrix}
    U_2\\
    & = U_{1,1} \Sigma U_2
  \end{align*}
  $\Sigma = \cos(\Theta)$ where the $\theta_i$ are the angles between the $n$ vectors of $S_1U_{1,1}$
  and the $n$ vectors of $S_2U_2$.
\end{solution}

\exo{2}
\begin{solution}
  See syllabus.
\end{solution}

\exo{2}
\begin{solution}
  See syllabus.
\end{solution}

\exo{2}
\begin{solution}
  \begin{itemize}
    \item
      \begin{align*}
        AXAX
        & = (AXA)X\\
        & = AX.
      \end{align*}
    \item
      $AX$ is a projector since (1) is satisfied.

      From the theorem~2.18, $\Ker(AX) = \Ima((AX)^*)^\perp$.
      Therefore we can just show that $\Ima((AX)^*) = \Ima(AX)$ which is obvious
      since (3) is satisfied.
    \item
      \begin{align*}
        XAXA
        & = (XAX)A\\
        & = XA.
      \end{align*}
    \item
      $XA$ is a projector since (1) is satisfied.

      From the theorem~2.18, $\Ker(XA) = \Ima((XA)^*)^\perp$.
      Therefore we can just show that $\Ima((XA)^*) = \Ima(XA)$ which is obvious
      since (4) is satisfied.
  \end{itemize}
\end{solution}

\exo{3}
\begin{solution}
  See syllabus.

  If $P$ is orthogonal, we have
  \begin{align*}
    PP & = P\\
    PPP^* & = PP^*\\
    P & = PP^*
  \end{align*}
  and $P^*P = I$.
\end{solution}

\exo{0}
\nosolution

\exo{0}
\nosolution

\exo{3}
\begin{solution}
  For $s_i \in [\sigma_i - \sigma_{s+1}(A), \sigma_i + \sigma_{s+1}(A)]$ for $i = 1, \ldots, s$,
  and
  \[ b = \sum_{i=1}^s u_i s_i v_i^T, \]
  we have
  \[ A - B = \sum_{i=1}^s u_i(\sigma_i - s_i)v_i^T + \sum_{i=s+1}^r u_i\sigma_iv_i^T. \]
  which also has its maximum singular value equal to $\sigma_{s+1}(A)$ by definition of the $s_i$.
\end{solution}
