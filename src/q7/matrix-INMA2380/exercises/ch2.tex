\section{Linear applications, orthogonalization
and the \texorpdfstring{\(QR\)}{QR} factorization}

\exo{1}
Verify that \(\sset_1 \cap \sset_2, \sset_1 + \sset_2, \Ker(A), \Ima(A)\) and \(A\sset\) are linear subspaces.
\begin{solution}
  \begin{itemize}
    \item If $\mathbf{a}, \mathbf{b} \in \mathcal{S}_1 \cap \mathcal{S}_2$,
      it means that $\mathbf{a} \in \mathcal{S}_1, \mathbf{a} \in \mathcal{S}_2$
      and $\mathbf{b} \in \mathcal{S}_1, \mathbf{b} \in \mathcal{S}_2$.
      Since $\mathcal{S}_1$ and $\mathcal{S}_2$
      are linear subspaces, $\alpha \mathbf{a} + \beta \mathbf{b} \in \mathcal{S}_1$
      and $\alpha \mathbf{a} + \beta \mathbf{b} \in \mathcal{S}_2$.
      Finally, $\alpha \mathbf{a} + \beta \mathbf{b} \in \mathcal{S}_1 \cap \mathcal{S}_2$.
    \item If $\mathbf{a},\mathbf{b} \in \mathcal{S}_1 + \mathcal{S}_2$,
       it means that $\mathbf{a}$ can be written as $\mathbf{a}_1 + \mathbf{a}_2$
       such that $\mathbf{a}_1 \in \mathcal{S}_1, \mathbf{a}_2 \in \mathcal{S}_2$
       and that $\mathbf{b}$ can be written as $\mathbf{b}_1 + \mathbf{b}_2$
       such that $\mathbf{b}_1 \in \mathcal{S}_1, \mathbf{b}_2 \in \mathcal{S}_2$.
       Next, $\alpha \mathbf{a} + \beta \mathbf{b} = \alpha (\mathbf{a}_1 + \mathbf{a}_2) + \beta (\mathbf{b}_1 + \mathbf{b}_2)
       =(\alpha \mathbf{a}_1 + \beta \mathbf{b}_1) + (\alpha \mathbf{a}_2 + \beta \mathbf{b}_2)$,
       which is the sum of an element of $\mathcal{S}_1$ and
       an element of $\mathcal{S}_2$, and thus $\mathcal{S}_1 + \mathcal{S}_2$
       is a linear subspace.
    \item We need to show that $\alpha \mathbf{a} + \beta \mathbf{b} \in \Ker A$,
      or equivalently $A(\alpha \mathbf{a} + \beta \mathbf{b}) = 0$
      if $\mathbf{a}, \mathbf{b} \in \Ker A$.
      We have that $A(\alpha \mathbf{a} + \beta \mathbf{b}) = \alpha A\mathbf{a} + \beta A\mathbf{b} = 0$.
    \item Same reasoning as third point.
    \item Same reasoning as third point.
  \end{itemize}
\end{solution}

\exo{1}
Show that for all matrices \(R\) (with suitable dimensions), we have
\[
\Ker(RA) \supseteq \Ker(A), \quad \Ima(RA) \subseteq \Ima(A).
\]
\begin{solution}
  \begin{itemize}
    \item If $x \in \Ker A$, then $Ax = 0$
      which also means that $RAx = R0 = 0$ so $x \in \Ker(RA)$.
    \item If $y \in \Ima(AR)$, then there is an $x$
      such that $ARx = y$
      which also means that there is an $x'$ such that
      $Ax' = y$, since we can just take $x' = Rx$.
      So $y \in \Ima A$.
  \end{itemize}
\end{solution}

\exo{2}
Show that all vectors \(\mathbf{x} \in \sset = \mathop{\mathrm{span}}\{\mathbf{a}_1, \dots, \mathbf{a}_k\}\) have a unique representation
\[
\mathbf{x} = \sum_{i=1}^k \alpha_i \mathbf{a}_i
\]
if \(\{\mathbf{a}_i \mid i = 1, \dots, k\}\) is a basis of \(\sset\).
\begin{solution}
  Let's suppose by contradiction that there are two ways
  to decompose $\mathbf{x}$.

  \begin{align}
    \label{eq:2.3.1}
    \mathbf{x} & = \sum_{i=1}^k \alpha_i \mathbf{a}_i\\
    \label{eq:2.3.2}
               & = \sum_{i=1}^k \beta_i \mathbf{a}_i
  \end{align}
  with $\beta_i \neq \alpha_i$ for at least one $i \in \{0, \ldots, k\}$.
  However, $\eqref{eq:2.3.2}-\eqref{eq:2.3.1}$ gives
  \[
    0 = \sum_{i=1}^k (\beta_i-\alpha_i) \mathbf{a}_i.
  \]
  By hypothesis, there is an $i$ such that $\beta_i-\alpha_i \neq 0$
  which is in contradiction with the linear independence of the vectors $\mathbf{a}_i$.
\end{solution}

\exo{4}
Show that two bases of a same space \(\sset\) have the same number of elements.
\begin{solution}
  Let's suppose by contradiction that there are two
  bases $\{\mathbf{a}_1, \ldots, \mathbf{a}_n\}$ and $\{\mathbf{b}_1, \ldots, \mathbf{b}_m\}$ for \(\sset\), with $n < m$.

  Remembering that $\mathbf{a}_1, \ldots, \mathbf{a}_n$ is spanning
  and $\mathbf{b}_1, \ldots, \mathbf{b}_m$ are linearly independent, we have that
  \[ \mathbf{b}_1 = \alpha_1 \mathbf{a}_1 + \dots + \alpha_n \mathbf{a}_n,\]
  with at least one $\alpha_i \neq 0$ since $\mathbf{b}_1 \neq 0$.
  Without loss of generality, let's say that $\alpha_1 \neq 0$.
  Therefore,
  \[ \mathbf{a}_1 = \frac{1}{\alpha_1}\mathbf{b}_1 + \frac{\alpha_2}{\alpha_1} \mathbf{a}_2 + \dots + \frac{\alpha_n}{\alpha_1} \mathbf{a}_n,\]
  so $(\mathbf{b}_1, \mathbf{a}_2, \ldots, \mathbf{a}_n)$ is spanning.

  We now have
  \[ \mathbf{b}_2 = \beta_1 \mathbf{b}_1 + \beta_2 \mathbf{a}_2 + \dots + \beta_n \mathbf{a}_n.\]
  If $\beta_2 = \cdots = \beta_n = 0$, $\mathbf{b}_1$ and $\mathbf{b}_2$ are not linearly independent.
  Without loss of generality, let's say that $\beta_2 \neq 0$;
  $(\mathbf{b}_1,\mathbf{b}_2,\mathbf{a}_3,\ldots,\mathbf{a}_n)$ is therefore spanning.

  Continuing this reasoning, $(\mathbf{b}_1,\ldots,\mathbf{b}_n)$ is spanning.
  $\mathbf{b}_{n+1}$ would therefore be a linear combination of $\mathbf{b}_1, \ldots, \mathbf{b}_n$, which is a contradiction.
\end{solution}

\exo{2}
Using the Schwarz inequality, show that
\[
\abs{\trace(Y^*X)} \leqslant \norm{X}_F \norm{Y}_F
\]
where \(X, Y \in \C^{m \times n}\) and \(\norm{\cdot}_F\) is the Frobenius norm.
\begin{solution}
  Since $\mathbb{C}^{m \times n}$ is a vector space and if $X,Y \in \mathbb{C}^{m \times n}$, $\trace(Y^*X) \in \mathbb{C}$ which is a field,
  $\trace(Y^*X)$ could be a scalar product $\langle X,Y \rangle$.
  Since
  \begin{align*}
    \trace(X^*X)
    & = \sum_{i = 1}^n \sum_{j = 1}^m \overline{x_{ji}}x_{ji}\\
    & = \sum_{i = 1}^n \sum_{j = 1}^m \abs{x_{ji}}^2,
  \end{align*}
  we clearly have $\trace(X^*X) \geq 0$ for all $X \in \mathbb{C}^{m \times n}$ and
  $\trace(X^*X) = 0 \iff X = 0$.
  We also have
  \begin{align*}
    \trace(Z^*(\alpha X + \beta Y))
    & = \trace(\alpha Z^*X + \beta Z^*Y)\\
    & = \alpha\trace(Z^*X) + \beta\trace(Z^*Y)
  \end{align*}
  and
  \begin{align*}
    \trace(Y^*X)
    & = \trace((X^*Y)^*)\\
    & = \overline{\trace(X^*Y)}.
  \end{align*}
  
  We thus just proved that the trace can be used to define a scalar product and we can therefore apply the \emph{Schwarz inequality} which gives
  \begin{align*}
    \abs{\trace(Y^*X)}
    & \leq \sqrt{\trace(X^*X)} \sqrt{\trace(Y^*Y)}\\
    & = \norm{X}_F \norm{Y}_F.
  \end{align*}
\end{solution}

\exo{1}
Show that $R$ is the \emph{Cholesky factor} of the positive definite matrix $A^*A$.
\begin{solution}
  Let us recall that the Cholesky decomposition of a matrix $A$ consists in finding
  a lower triangular matrix $L$ such that $A = L L^*$.
  
  In this case we have
  \begin{align*}
    A^*A
    & = R^*Q^*QR\\
    & = R^*IR\\
    & = R^*R.
  \end{align*}

  Note that $A$ is \emph{square}
  (since otherwise it wouldn't be defined for it to be positive definite)
  and has \emph{full rank} (if not, it must have a zero eigenvalue and cannot be positive definite).
  Therefore, $R$ is an upper triangular square matrix with no zero element in this case.
\end{solution}

\exo{2}
Let \(X\) and \(Y\) be two matrices whose columns form bases of the subspaces \(\mathcal{X}\) and \(\mathcal{Y}\) respectively.
Show that \(\mathcal{X} \perp \mathcal{Y}\) if and only if \(Y^*X = 0\).
\begin{solution}
  \begin{description}
    \item[$\implies$]
      $\mathcal{X} \perp \mathcal{Y}$ implies that 
      $\langle \mathbf{x},\mathbf{y} \rangle = 0$, $\forall \mathbf{x} \in \mathcal{X}$ 
      and $\forall \mathbf{y} \in \mathcal{Y}$.
      In particular, it should be true for every element of the basis.
      We need to have $\langle \mathbf{x}_i,\mathbf{y}_j\rangle = 0$, $\forall i,j$.
      In other words
      \[
        \begin{bmatrix}
          \mathbf{y}_1 & \mathbf{y}_2 & \cdots & \mathbf{y}_n
        \end{bmatrix}^*
        \begin{bmatrix}
          \mathbf{x}_1 & \mathbf{x}_2 & \cdots & \mathbf{x}_n
        \end{bmatrix}
        = 0,
      \]
      and thus $Y^* X = 0$.
    \item[$\impliedby$]
      Let $\sum_i a_i\mathbf{x}_i$ be an element of $\mathcal{X}$ and $\sum_i b_i\mathbf{y}_i$ be an element of $\mathcal{Y}$.
      We need
      \begin{align*}
        \left\langle\sum_i a_i\mathbf{x}_i, \sum_i b_i\mathbf{y}_i\right\rangle
        & = \sum_i a_i \left\langle \mathbf{x}_i, \sum_j b_j\mathbf{y}_j\right\rangle\\
        & = \sum_i a_i \overline{\left\langle \sum_j b_j\mathbf{y}_j, \mathbf{x}_i\right\rangle}\\
        & = \sum_i \sum_j a_i\overline{b_j} \, \overline{\langle \mathbf{y}_j, \mathbf{x}_i\rangle}\\
        & = \sum_i \sum_j a_i\overline{b_j} \langle \mathbf{x}_i, \mathbf{y}_j\rangle\\
        & = \sum_i \sum_j a_i \overline{b_j} 0\\
        & = 0.
      \end{align*}
  \end{description}
\end{solution}

\exo{3}
Show that the orthogonal complement satisfies the following properties:
\begin{align*}
(\sset^\perp)^\perp &= \sset,\\
(\sset_1 + \sset_2)^\perp &= \sset_1^\perp \cap \sset_2^\perp,\\
(\sset_1 \cap \sset_2)^\perp &= \sset_1^\perp + \sset_2^\perp.
\end{align*}
\begin{solution}
  \begin{enumerate}
    \item
      \begin{align*}
        (\sset^\perp)^\perp
        & = \{\mathbf{x} \mid \langle \mathbf{x},\mathbf{y}\rangle = 0, \forall \mathbf{y} \in \mathcal{S}^\perp\}\\
        & = \big\{\mathbf{x} \mid \langle \mathbf{x},\mathbf{y}\rangle = 0, \forall \mathbf{y} \in \{\mathbf{z} \mid \langle \mathbf{z},\mathbf{w}\rangle = 0, \forall \mathbf{w} \in \mathcal{S}\}\big\}.
      \end{align*}
      In other words, $\mathbf{x}$ should be perpendicular to every vector which is perpendicular to all vectors of $\mathcal{S}$.
      This is clearly true for $\mathbf{x} \in \mathcal{S}$.
      If $\mathbf{x} \notin \mathcal{S}$, since it is perpendicular to every vector of $\mathcal{S}^\perp$, $\mathbf{x} \notin \mathcal{S}^\perp$, which contradicts Lemma~2.8.
    \item
      \begin{align*}
        (\mathcal{S}_1 + \mathcal{S}_2)^\perp
        & = \{\mathbf{x} \mid \langle\mathbf{x},\mathbf{y}\rangle = 0, \forall \mathbf{y} \in (\mathcal{S}_1+\mathcal{S}_2)\}\\
        & = \{\mathbf{x} \mid \langle \mathbf{x},\alpha \mathbf{y}_1 + \beta \mathbf{y}_2\rangle = 0, \forall \alpha,\beta, \forall \mathbf{y}_1 \in \mathcal{S}_1, \forall \mathbf{y}_2 \in \mathcal{S}_2\}\\
        & = \{\mathbf{x} \mid (\langle\mathbf{x}, \mathbf{y}_1\rangle = 0, \forall \mathbf{y}_1 \in \mathcal{S}_1) \land (\langle\mathbf{x}, \mathbf{y}_2\rangle = 0, \forall \mathbf{y}_2 \in \mathcal{S}_2)\}\\
        & = \{\mathbf{x} \mid \langle\mathbf{x}, \mathbf{y}_1\rangle = 0, \forall \mathbf{y}_1 \in \mathcal{S}_1\} \cap \{\mathbf{x} \mid \langle\mathbf{x}, \mathbf{y}_2\rangle = 0, \forall \mathbf{y}_2 \in \mathcal{S}_2\}\\
        & = \mathcal{S}_1^\perp \cap \mathcal{S}_2^\perp.
      \end{align*}
    \item
      $(\mathcal{S}^\perp)^\perp = \sset$ actually ensures that
      two subspaces are equal if and only if their orthogonal complements are equal.
      If they are equal, it is obvious that their complements are equal
      and if their complements are equal, then the complements of their complements are equal so they are equal.

      We can therefore prove that the complements are equal:
      \begin{align*}
        ((\mathcal{S}_1 \cap \mathcal{S}_2)^\perp)^\perp
        & = (\mathcal{S}_1^\perp + \mathcal{S}_2^\perp)^\perp\\
        \mathcal{S}_1 \cap \mathcal{S}_2
        & = (\mathcal{S}_1^\perp + \mathcal{S}_2^\perp)^\perp.
      \end{align*}
      Using the previous property, we then have
      \begin{align*}
        (\mathcal{S}_1^\perp + \mathcal{S}_2^\perp)^\perp
        & = (\mathcal{S}_1^\perp)^\perp \cap (\mathcal{S}_2^\perp)^\perp\\
        & = \mathcal{S}_1 \cap \mathcal{S}_2.
      \end{align*}
  \end{enumerate}
\end{solution}

\exo{2}
How to construct the complex Givens transformation \(G \in \C^{2 \times 2}\) that transforms an arbitrary vector \(\mathbf{x} \in \C^2\) into \(\begin{bmatrix}\norm{\mathbf{x}} \\ 0\end{bmatrix}\)?

\begin{solution}
Suppose \(G\) has the form
\[
G = \begin{bmatrix} c & s \\ -\bar{s} & \bar{c} \end{bmatrix}.
\]
It is easy to see that \(G^*G = I\), as required, when \(\abs{c}^2 + \abs{s}^2 = 1\).
Next, we observe that
\[
G\mathbf{x} = \begin{bmatrix} c & s \\ -\bar{s} & \bar{c} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \triangleq \begin{bmatrix} \norm{\mathbf{x}} \\ 0 \end{bmatrix}.
\]
The choices
\[
c = \frac{\bar{x}_1}{\sqrt{\abs{x_1}^2 + \abs{x_2}^2}}, \quad s = \frac{\bar{x}_2}{\sqrt{\abs{x_1}^2 + \abs{x_2}^2}}
\]
satisfy this condition, and thus we have a formula for \(G\).
\end{solution}

\exo{3}
How to construct the complex Householder transformation \(H \in \C^{n \times n}\) that transforms an arbitrary vector \(\mathbf{x} \in \C^n\) into \[
H\mathbf{x} = \begin{bmatrix}\pm\norm{\mathbf{x}} \\ 0 \\ \vdots \\ 0\end{bmatrix}
\]
and satisfies \(H^*H = HH^* = I_n\)?
\begin{solution}
We write \(x_1\) as \(r e^{i \theta}\), for \(r, \theta \in \R\).
Taking \(\mathbf{v} = \mathbf{x} \pm e^{i \theta} \norm{\mathbf{x}} \mathbf{e}_1\), and \(H = I_n - 2\frac{\mathbf{v}\mathbf{v}^*}{\mathbf{v}^*\mathbf{v}}\) yields \(H\mathbf{x} = \mp e^{i \theta} \norm{\mathbf{x}} \mathbf{e}_1\).
\end{solution}