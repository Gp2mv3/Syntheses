\section{Matrix' algebra}
\exo{1}
Prove that the addition of matrices is commutative and associative and that its neutral element is the zero matrix: $O = [0]_{i,j=1}^{m,n}$.
\begin{solution}
  Let $\mathcal{R}$ be the ring in which the elements of the matrix are
  \begin{description}
    \item[Commutativity]
      We should have $A+B=B+A$ or
      $[a_{ij} + b_{ij}]_{i,j=1}^{m,n} = [b_{ij} + a_{ij}]_{i,j=1}^{m,n}$
      which is true since $\mathcal{R}$ is commutative.
    \item[Associativity] $A+(B+C)=(A+B)+C$
      We should have $A+(B+C)=(A+B)+C$ or
      $[a_{ij} + (b_{ij} + c_{ij})]_{i,j=1}^{m,n} 
      = [(a_{ij} + b_{ij}) + c_{ij}]_{i,j=1}^{m,n}$
      which is true since $\mathcal{R}$ is associative.
    \item[Neutral element] Let $B$ be the neutral element.
      We have
      $A + B = [a_{ij} + b_{ij}]_{i,j=1}^{m,n}$
      and $A = [a_{ij}]_{i,j=1}^{m,n}$.
      Two matrices are equal if all their elements are equal so $b_{ij}$
      should be the neutral element of $\mathcal{R}$.
  \end{description}
\end{solution}

\exo{1}
\begin{solution}
  Simply develop $\alpha A_{m \times n}=[\alpha a_{ij}]_{i,j=1}^{m,n}$.
\end{solution}

\exo{1}
\begin{solution}
  Develop the products.
\end{solution}

\exo{1}
\begin{solution}
  Develop the products.
\end{solution}

\exo{2}
\begin{solution}
  We have to prove that
  \begin{itemize}
    \item $(\mathcal{R}^{n \times n}, +)$ is a commutative group
    \item $(\mathcal{R}^{n \times n}, \cdot)$ is a mono√Ød
    \item $\cdot$ is distributive over $+$
  \end{itemize}
  which can be shown with the help of the previous exercises.
  \begin{itemize}
    \item If $A$ and $B$ are $\mathcal{R}^{n \times n}$, $A+B \in \mathcal{R}^{n \times n}$
      so $+$ is a composition law in $\mathcal{R}^{n \times n}$.
      We have seen in exercise~1.1 that the addition is associative, and that the neutral element is
      $[0]_{i,j=1}^{n,n}$.
      The symmetric element of $A$ exists and is $[-a_{ij}]_{i,j=1}^{n,n}$ since
      \[ [a_{ij}]_{i,j=1}^{n,n} + [-a_{ij}]_{i,j=1}^{n,n} = [a_{ij}-a_{ij}]_{i,j=1}^{n,n} = [0]_{i,j=1}^{n,n}. \]
      It is also commutative as we have seen in exercise~1.1.
    \item If $A$ and $B$ are $\mathcal{R}^{n \times n}$, $A \cdot B \in \mathcal{R}^{n \times n}$
      so $\cdot$ is a composition law in $\mathcal{R}^{n \times n}$.
      It is associative and has a neutral element as seen in exercise~1.3.
    \item We have seen in exercise~1.3 that $\cdot$ is distributive over $+$.
  \end{itemize}
\end{solution}

\exo{2}
\begin{solution}
  Let $m,n,o,p,q,s \in \mathbb{N}$ such that
  $A \in \mathcal{R}^{m \times n}, B \in \mathcal{R}^{p \times q}, C \in \mathcal{R}^{n \times o}, D \in \mathcal{R}^{q \times s}$.
  We can check that
  $A \otimes B \in \mathcal{R}^{mp \times nq}$ and
  $C \otimes D \in \mathcal{R}^{nq \times os}$
  so $(A \otimes B) (C \otimes D)$ is well defined.
  The two expressions are equal as shown by
  \begin{align*}
    (A \otimes B) (C \otimes D)
    & =
    \begin{pmatrix}
      a_{11}B & \cdots & a_{1n}B\\
      \vdots  & \ddots & \vdots\\
      a_{m1}B & \cdots & a_{mn}B
    \end{pmatrix}
    \begin{pmatrix}
      c_{11}D & \cdots & c_{1o}D\\
      \vdots  & \ddots & \vdots\\
      c_{n1}D & \cdots & c_{no}D
    \end{pmatrix}\\
    & =
    \begin{pmatrix}
      \sum_{i=1}^n a_{1i}c_{i1}BD & \cdots & \sum_{i=1}^n a_{1i}c_{io}BD\\
      \vdots  & \ddots & \vdots\\
      \sum_{i=1}^n a_{mi}c_{i1}BD & \cdots & \sum_{i=1}^n a_{mi}c_{io}BD
    \end{pmatrix}\\
    & =
    \begin{pmatrix}
      \sum_{i=1}^n a_{1i}c_{i1} & \cdots & \sum_{i=1}^n a_{1i}c_{io}\\
      \vdots  & \ddots & \vdots\\
      \sum_{i=1}^n a_{mi}c_{i1} & \cdots & \sum_{i=1}^n a_{mi}c_{io}
    \end{pmatrix}
    \otimes BD\\
    & =
    AC \otimes BD.
  \end{align*}
\end{solution}

\exo{1}
\begin{solution}
  \begin{align*}
    A^pA^q
    & = \underbrace{A \cdots A}_{p} \underbrace{A \cdots A}_{q}\\
    & = \underbrace{A \cdots A}_{p+q}\\
    & = A^{p+q}\\
    & = \underbrace{A \cdots A}_{q+p}\\
    & = \underbrace{A \cdots A}_{q} \underbrace{A \cdots A}_{p}\\
    & = A^qA^p\\
    (A^p)^q
    & = \underbrace{\underbrace{A \cdots A}_{p} \cdots \underbrace{A \cdots A}_{p}}_q\\
    & = \underbrace{A \cdots A}_{pq}\\
    & = A^{pq}
  \end{align*}
\end{solution}

\exo{2}
Let's consider (upper) triangular Toeplitz,
i.e. the (upper) triangular matrices having equal elements
 along the diagonal ($t_{ij} = t_{i+k,j+k}$):
\[
  T =
  \begin{pmatrix}
    t_1 & t_2 & \cdots & t_n\\
    0   & t_1 & \ddots & \vdots\\
    \vdots & \ddots & \ddots & t_2\\
    0 & \cdots & 0 & t_1
  \end{pmatrix}.
\]
Prove that these matrices commute.

\begin{solution}
  Let $T,U$ be upper triangular Toeplitz,
  \begin{align*}
    TU
    & =
    \begin{pmatrix}
      t_1 & t_2 & \cdots & t_n\\
      0   & t_1 & \ddots & \vdots\\
      \vdots & \ddots & \ddots & t_2\\
      0 & \cdots & 0 & t_1
    \end{pmatrix}
    \begin{pmatrix}
      u_1 & u_2 & \cdots & u_n\\
      0   & u_1 & \ddots & \vdots\\
      \vdots & \ddots & \ddots & u_2\\
      0 & \cdots & 0 & u_1
    \end{pmatrix}\\
    & =
    \begin{pmatrix}
      t_1u_1 & t_1u_2 + t_2u_1 & \cdots & t_1u_n + \cdots + t_nu_1\\
      0   & t_1u_1 & \ddots & \vdots\\
      \vdots & \ddots & \ddots & t_1u_2 + t_2u_1\\
      0 & \cdots & 0 & t_1u_1
    \end{pmatrix}\\
    & =
    \begin{pmatrix}
      u_1t_1 & u_1t_2 + u_2t_1 & \cdots & u_1t_n + \cdots + u_nt_1\\
      0   & u_1t_1 & \ddots & \vdots\\
      \vdots & \ddots & \ddots & u_1t_2 + u_2t_1\\
      0 & \cdots & 0 & u_1t_1
    \end{pmatrix}\\
    & = UT.
  \end{align*}
\end{solution}

\exo{3}
Let's consider square circulant matrices:
\[
  C =
  \begin{pmatrix}
    c_1 & c_2 & \cdots & c_n\\
    c_n & c_1 & \ddots & \vdots\\
    \vdots & \ddots & \ddots & c_2\\
    c_2 & \cdots & c_n & c_1
  \end{pmatrix}.
\]
Prove that these matrices commute.

\begin{solution}
  Let $C,D$ be square circulant matrices and let's define
  \[ \sigma : (i,j) \mapsto (j - i) \pmod{n}+1 \]
  such that
  $C = [c_{\sigma(i,j)}]_{i,j=1}^{n,n}$ and
  $D = [d_{\sigma(i,j)}]_{i,j=1}^{n,n}$.
  We can observe that
  \begin{equation}
    \label{eq:cycln}
    \sigma(i,j+n) = \sigma(i+n,j) = \sigma(i,j).
  \end{equation}
  and
  \begin{equation}
    \label{eq:cycldiff}
    \sigma(i,j) = \sigma(i+a,j+a)
  \end{equation}
  for all $a \in \mathbb{N}$.
  We can now observe that
  \begin{align*}
    CD
    & =
    \left[
      \sum_{k=1}^n
      c_{\sigma(i,k)}d_{\sigma(k,j)}
    \right]_{i,j=1}^{n,n}\\
    & =
    \left[
      \sum_{k=1}^{i+j-1}
      d_{\sigma(i,k)}c_{\sigma(k,j)}
      +
      \sum_{k=i+j}^{n}
      d_{\sigma(i,k)}c_{\sigma(k,j)}
    \right]_{i,j=1}^{n,n}\\
    & =
    \left[
      \sum_{k=1}^{i+j-1}
      d_{\sigma(i,i+j-k)}c_{\sigma(i+j-k,j)}
      +
      \sum_{k=i+j}^{n}
      d_{\sigma(i,n+i+j-k)}c_{\sigma(n+i+j-k,j)}
    \right]_{i,j=1}^{n,n}\\
    & \stackrel{\eqref{eq:cycln}}{=}
    \left[
      \sum_{k=1}^{i+j-1}
      d_{\sigma(i,i+j-k)}c_{\sigma(i+j-k,j)}
      +
      \sum_{k=i+j}^{n}
      d_{\sigma(i,i+j-k)}c_{\sigma(i+j-k,j)}
    \right]_{i,j=1}^{n,n}\\
    & =
    \left[
      \sum_{k=1}^n
      c_{\sigma(i,i+j-k)}d_{\sigma(i+j-k,j)}
    \right]_{i,j=1}^{n,n}\\
    & \stackrel{\eqref{eq:cycldiff}}{=}
    \left[
      \sum_{k=1}^n
      c_{\sigma(k,j)}d_{\sigma(i,k)}
    \right]_{i,j=1}^{n,n}\\
    & = DC
  \end{align*}
\end{solution}

\exo{2}
\begin{solution}
  Asking that they commute to every matrix is actually equivalent to asking
  that they commute to every element of the base of $\mathcal{R}^{n \times n}$.
  The canonical base is $e_ie_j^T$ for $i,j \in \{1, \ldots, n\}$ where
  $e_k$ is a column vector with 1 at $k$ and 0 elsewhere.
  We can see that $Ae_ie_j^T = A_{:i}e_j^T$ and $e_ie_j^TA = e_iA_{j:}$.

  If $A_{:i}e_j^T = e_iA_{j:}$, the LHS imposes that only the $j$th column is non-zero
  and the RHS imposes that only the $i$th row is non-zero.
  We therefore need $A_{ki} = 0$ for $k \neq i$ and $A_{jk} = 0$ for $k \neq j$.
  What's left is $e_j^Te_iA_{ii} = A_{jj}e_j^Te_i$.
  Therefore $A_{ii} = A_{jj}$.
  Since this must hold for every $i,j \in \{1, \ldots, n\}$, $A$ must be diagonal
  with equal elements at the diagonal.
\end{solution}

\exo{2}
\begin{solution}
  \begin{description}
    \item[Solution 1]
    Asking that a matrix $A$ commutes to a diagonal matrix is actually equivalent to asking
    that it commutes to every element of the base 
    $e_ie_i^T$ for $i \in \{1, \ldots, n\}$ where
    $e_k$ is a column vector with 1 at $k$ and 0 elsewhere.
    We can see that $Ae_ie_i^T = A_{:i}e_i^T$ and $e_ie_i^TA = e_iA_{i:}$.
  
    If $A_{:i}e_i^T = e_iA_{i:}$, the LHS imposes that only the $i$th column is non-zero
    and the RHS imposes that only the $i$th line is non-zero.
    We therefore need $A_{ki} = 0$ for $k \neq i$ and $A_{ik} = 0$ for $k \neq i$.
    Since this must hold for every $i \in \{1, \ldots, n\}$, $A$ must be diagonal.
   
    \item[Solution 2]
    Let $B$ a square matrix of dimension $n$ commuting with $A = \diag\{a_1,\dots,a_n\}$,
    we have that
    \[
      B A =
      \begin{pmatrix}
        b_{11} a_1 & \dots & b_{1n} a_n \\
        \vdots & \ddots & \vdots \\
        b_{n1} a_1 & \dots & b_{nn} a_n \\
      \end{pmatrix}
      =
      \begin{pmatrix}
        b_{11} a_1 & \dots & b_{1n} a_1 \\
        \vdots & \ddots & \vdots \\
        b_{n1} a_n & \dots & b_{nn} a_n \\
      \end{pmatrix}
      = A B
    \]
    We thus need $b_{ij} a_j = b_{ij} a_i$ for every $i \neq j$
    and since $a_i \neq a_j$ for $i \neq j$, $B$ has to be diagonal itself.
  \end{description}
\end{solution}

\exo{1}
\begin{solution}
  Using the properties $(AB)^T = B^TA^T$ and $(AB)^* = B^*A^*$,
  \begin{align*}
    (AA^T)^T & = (A^T)^TA^T\\
             & = AA^T\\
    (A^TA)^T & = A^T(A^T)^T\\
             & = A^TA\\
    (AA^*)^* & = (A^*)^*A^*\\
             & = AA^*\\
    (A^*A)^* & = A^*(A^*)^*\\
             & = A^*A.
  \end{align*}
\end{solution}

\exo{1}
\begin{solution}
  We have
  \begin{align*}
    (A + A^T)^T & = A^T + A\\
                & = A + A^T\\
    (A - A^T)^T & = A^T - A\\
                & = -(A - A^T)\\
    (A + A^*)^* & = A^* + A\\
                & = A + A^*\\
    (A - A^*)^* & = A^* - A\\
                & = -(A - A^*).
  \end{align*}
\end{solution}

\exo{2}
\begin{solution}
  We simply notice that
  \begin{align*}
    A & = \frac{A + A^T}{2} + \frac{A - A^T}{2}\\
      & = \frac{A + A^*}{2} + \frac{A - A^*}{2}.
  \end{align*}
\end{solution}

\exo{1}
\begin{solution}
  It is shown by
  \begin{align*}
    (A \otimes B)^*
    & =
    \begin{pmatrix}
      a_{11}B & \cdots & a_{1n}B\\
      \vdots  & \ddots & \vdots\\
      a_{m1}B & \cdots & a_{mn}B
    \end{pmatrix}^*\\
    & =
    \begin{pmatrix}
      (a_{11}B)^* & \cdots & (a_{m1}B)^*\\
      \vdots  & \ddots & \vdots\\
      (a_{1n}B)^* & \cdots & (a_{mn})B^*
    \end{pmatrix}\\
    & =
    \begin{pmatrix}
      a_{11}^*B^* & \cdots & a_{m1}^*B^*\\
      \vdots  & \ddots & \vdots\\
      a_{1n}^*B^* & \cdots & a_{mn}^*B^*
    \end{pmatrix}\\
    & =
    A^* \otimes B^*.
  \end{align*}
\end{solution}

\exo{2}
\begin{solution}
  If $U_1$ and $U_2$ are unitary,
  using exercise~1.6 and exercise~1.15,
  \begin{align*}
    (U_1 \otimes U_2) (U_1 \otimes U_2)^*
    & = (U_1 \otimes U_2) (U_1^* \otimes U_2^*)\\
    & = U_1U_1^* \otimes U_2U_2^*\\
    & = I \otimes I\\
    & = I
  \end{align*}
  which implies that $(U_1 \otimes U_2)^*$
  is the inverse of $U_1 \otimes U_2$
  (also its left inverse since it is a square matrix).
\end{solution}

\exo{4}

\begin{solution}

  Let's prove the equivalent property

  \[
    \det\begin{pmatrix}
      A^T & -I_n\\
      0 & B^T
    \end{pmatrix} =
    \det\begin{pmatrix}
      A^T & -I_n\\
      B^TA^T & 0
    \end{pmatrix}
  \]

  to work on the columns since the properties are expressed
  on the columns.

  Let $C$ be the matrix of the LHS and
  $D$ be the matrix of the RHS.
  We can see that

  \begin{align*}
    d_{i:} &= c_{i:} & i & = 1, \ldots, n\\
    d_{i:} &= c_{i:} + \sum_{k=1}^n b_{k(i-n)} c_{k:} & i & = n+1, \ldots, 2n
  \end{align*}

  since

  \begin{align*}
    \begin{pmatrix}
      B^TA^T & 0
    \end{pmatrix}
    & =
    \begin{pmatrix}
      0 & B^T
    \end{pmatrix}
    +
    \begin{pmatrix}
      B^TA^T & -B^T
    \end{pmatrix}\\
    & =
    \begin{pmatrix}
      0 & B^T
    \end{pmatrix}
    +
    B^T
    \begin{pmatrix}
      A^T & -I_n
    \end{pmatrix}.
  \end{align*}

  This gives us what we want using properties 3. and 8.

\end{solution}

\exo{3}
\begin{solution}
  We know that for $C \in \mathcal{R}^{2n}$
  \[ \det(C) = \sum_{\mathbf{j}} (-1)^{t(\mathbf{j})} c_{1j_1} \cdots c_{(2n)j_{2n}}. \]
  For
  \[
    C = \begin{pmatrix}
      A & 0\\
      -I_n & B
    \end{pmatrix},
  \]
  we see that if $j_i > n$ for $i \leq n$, $c_{ij_i} = 0$.
  Therefore the first $n$ elements of $\mathbf{j}$ must be a permutation of $\{1, \ldots, n\}$
  for the product $c_{1j_1} \cdots c_{(2n)j_{2n}}$ to be non-zero.
  The last $n$ elements of $\mathcal{j}$ are therefore a permutation of $\{n+1, \ldots, 2n\}$.
  Consequently, we have (defining $J$ as the set of such permutations)
  \begin{align*}
    \det(C)
    & = \sum_{\mathbf{j} \in J} (-1)^{t(\mathbf{j})} c_{1j_1} \cdots c_{nj_n} c_{(n+1)j_{n+1}} \cdots c_{(2n)j_{2n}}\\
    & = \sum_{\mathbf{j} \in J} (-1)^{t(j_1, \ldots, j_n)} a_{1j_1} \cdots a_{nj_n} (-1)^{t(j_{n+1}, \ldots, j_{2n})} b_{1(j_{n+1}-n)} \cdots b_{n(j_{2n}-n)}\\
    & = \left(\sum_{\mathbf{j}} (-1)^{t(\mathbf{j})} a_{1j_1} \cdots a_{nj_n}\right)
    \left(\sum_{\mathbf{j}} (-1)^{t(\mathbf{j})} b_{1(j_1)} \cdots b_{n(j_n)}\right)\\
    & = \det(A) \det(B).
  \end{align*}

  For
  \[
    D = \det\begin{pmatrix}
      A & AB\\
      -I_n & 0
    \end{pmatrix},
  \]
  we see using the same reasoning that we can restrict ourself to
  $\mathbf{j} = \mathbf{j}_1 \times \mathbf{j}_2$
  where $\mathbf{j}_1$ is a permutation of $\{n+1, \ldots, 2n\}$
  and $\mathbf{j}_2$ is a permutation of $\{1, \ldots, n\}$.
  This time, we have $t(\mathbf{j}) = t(\mathbf{j_1}) + t(\mathbf{j_2}) + n^2$ so
  (defining $J$ as expected, noting $AB = [x_{ij}]_{i,j=1}^{n,n}$)
  \begin{align*}
    \det(D)
    & = \sum_{\mathbf{j} \in J} (-1)^{t(\mathbf{j})} d_{1j_1} \cdots d_{nj_n} d_{(n+1)j_{n+1}} \cdots d_{(2n)j_{2n}}\\
    & = \sum_{\mathbf{j} \in J} (-1)^{t(\mathbf{j}_1)} x_{1(j_1-n)} \cdots x_{n(j_n-n)} (-1)^{t(\mathbf{j}_2)+n^2} (-\delta_{1j_{n+1})}) \cdots (-\delta_{nj_{2n})})\\
    & = \left(\sum_{\mathbf{j}} (-1)^{t(\mathbf{j})} x_{1j_1} \cdots x_{nj_n}\right)
    (-1)^{n^2}\left(\sum_{\mathbf{j}} (-1)^{t(\mathbf{j})} (-\delta_{1(j_1)}) \cdots (-\delta_{n(j_n)})\right)\\
    & = \det(AB) (-1)^{n^2}\det(-I_n)\\
    & = \det(AB) (-1)^{n^2+n}\\
    & = \det(AB) (-1)^{n(n+1)}\\
    & = \det(AB)
  \end{align*}
  since either $n$ or $n+1$ is even.
\end{solution}

\exo{2}
\begin{solution}
  We first notice that
  \begin{align*}
    \det\begin{pmatrix}
      \lambda & -1 &\\
      & \ddots & \ddots\\
      & & \lambda & -1\\
      & & & \lambda
    \end{pmatrix}^{n \times n}
    & =
    \lambda
    \det\begin{pmatrix}
      \lambda & -1 &\\
              & \ddots & \ddots\\
              & & \lambda & -1\\
              & & & \lambda
    \end{pmatrix}^{n-1 \times n-1} \\
    & = \lambda^n
  \end{align*}
  where $n$ is the dimension of the original matrix.
  Using the cofactor technique on the last column recursively,
  \begin{align*}
    \det(\lambda I_n - C) & = (\lambda + a_{n-1})\lambda^{n-1}
    + \det(\lambda I_{n-1} - C_{n-1} - \lambda e_{n-1}e_{n-1}^T)\\
    & =
    \lambda^n + a_{n-1}\lambda^{n-1}
    + a_{n-2} \lambda^{n-1}
    + \det(\lambda I_{n-2} - C_{n-2} - \lambda e_{n-2}e_{n-2}^T)\\
    & = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots + a_0.
  \end{align*}
\end{solution}

\exo{2}
\begin{solution}
  Using the cofactor technique on the last column then on the last line,
  \begin{align*}
    \det(J_n)
    & = a_n\det(J_{n-1}) - b_n
    \det
    \begin{pmatrix}
     J_{n-2}  &   & 0 \\
              &   & b_{n-1} \\
     0        & 0 & c_n
    \end{pmatrix}\\
    & = a_n\det(J_{n-1}) - b_n c_n \det(J_{n-2}).
  \end{align*}
\end{solution}

\exo{4}
\begin{solution}
  Let's prove it by induction.
  \begin{itemize}
    \item If $n = 1$, we have
      \[
        \det
        \begin{pmatrix}
          1
        \end{pmatrix}
        = 1
      \]
      and for $n = 2$, we have
      \[
        \det
        \begin{pmatrix}
          1 & 1\\
          x_1 & x_2
        \end{pmatrix}
        = (x_2 - x_1)
      \]
    \item
      For $i = 1, \ldots, n-1$, replacing the line $i+1$
      by the line $i+1$ subtracted by $x_n$ times the line $i$
      does not change the determinant since
      its an elementary line transformation (property 9.).
      We have therefore
      \begin{align*}
        \det
        \begin{pmatrix}
          1 & 1 & \cdots & 1\\
          x_1 & x_2 & \cdots & x_n\\
          x_1^2 & x_2^2 & \cdots & x_n^2\\
          \vdots & \vdots & \ddots & \vdots\\
          x_1^{n-1} & x_2^{n-1} & \cdots & x_n^{n-1}
        \end{pmatrix}
        & =
        \det
        \begin{pmatrix}
          1 & 1 & \cdots & 1\\
          x_1-x_n & x_2-x_n & \cdots & 0\\
          x_1^2-x_1x_n & x_2^2 - x_2x_n & \cdots & 0\\
          \vdots & \vdots & \ddots & \vdots\\
          x_1^{n-1}-x_1^{n-2}x_n & x_2^{n-1}-x_2^{n-2}x_n & \cdots & 0
        \end{pmatrix}\\
        & =
        \det
        \begin{pmatrix}
          1 & 1 & \cdots & 1\\
          (x_1-x_n) & x_2-x_n & \cdots & 0\\
          (x_1-x_n)x_1 & (x_2-x_n)x_2 & \cdots & 0\\
          \vdots & \vdots & \ddots & \vdots\\
          (x_1-x_n)x_1^{n-2} & (x_2-x_n)x_2^{n-2} & \cdots & 0
        \end{pmatrix}\\
        & =
        (-1)^{n+1}
        \det
        \begin{pmatrix}
          (x_1-x_n) & x_2-x_n & \cdots\\% & x_{n-1}-x_n\\
          (x_1-x_n)x_1 & (x_2-x_n)x_2 & \cdots\\% & (x_{n-1}-x_n)x_{n-1}\\
          \vdots & \vdots & \ddots\\% & \vdots\\
          (x_1-x_n)x_1^{n-2} & (x_2-x_n)x_2^{n-2} & \cdots\\% & (x_{n-1}-x_n)x_{n-1}^{n-2}
        \end{pmatrix}\\
        & =
        (-1)^{n-1} \prod_{j < n} (x_j - x_n)
        \det
        \begin{pmatrix}
          1 & 1 & \cdots & 1\\
          x_1 & x_2 & \cdots & x_{n-1}\\
          \vdots & \vdots & \ddots & \vdots\\
          x_1^{n-2} & x_2^{n-2} & \cdots & x_{n-1}^{n-2}
        \end{pmatrix}\\
        & =
        \left(\prod_{j < i = n} (x_i - x_j)\right)
        \left(\prod_{j < i < n} (x_i - x_j)\right).
      \end{align*}
  \end{itemize}
\end{solution}

\exo{3}
\begin{solution}
  \begin{itemize}
    \item
      Since $A_{12}$ must be square,
      $A_{11}$ and $A_{22}$ must have the same dimension.
      Let $p \times p$ be their dimension.
      Let $\mathbf{i}_p = (1, \ldots, p)$.
      If $\mathbf{j}_p \neq (1, \ldots, p)$,
      there will be a $k \in \{1, \ldots, p\}$ such that
      $k$ is in $\mathbf{j}_p^c$.
      Therefore,
      \(
      A^c
      \begin{pmatrix}
        \mathbf{i}_p\\
        \mathbf{j}_p
      \end{pmatrix}
      \)
      will have a zero column and a determinant of 0.

      Therefore, the sum over all $\mathbf{j}_c$
      is equal to
      \begin{align*}
        A
        \begin{pmatrix}
          (1, \ldots, p)\\
          (1, \ldots, p)
        \end{pmatrix}
        A^c
        \begin{pmatrix}
          (1, \ldots, p)\\
          (1, \ldots, p)
        \end{pmatrix}
        & = A
        \begin{pmatrix}
          (1, \ldots, p)\\
          (1, \ldots, p)
        \end{pmatrix}
        (-1)^{2\frac{p(p+1)}{2}}
        A
        \begin{pmatrix}
          (p+1, \ldots, 2p)\\
          (p+1, \ldots, 2p)
        \end{pmatrix}\\
        & = (-1)^{p(p+1)}\det(A_{11}) \det(A_{22})\\
        & = \det(A_{11}) \det(A_{22})
      \end{align*}
      since either $p$ or $p+1$ is even.
    \item
      Let $\mathbf{i}_p = (1, \ldots, p)$.
      If $\mathbf{j}_p \neq (n-p+1, \ldots, n)$,
      there will be a $k \in \{n-p+1, \ldots, n\}$ such that
      $k$ is in $\mathbf{j}_p^c$.
      Therefore,
      \(
      A^c
      \begin{pmatrix}
        \mathbf{i}_p\\
        \mathbf{j}_p
      \end{pmatrix}
      \)
      will have a zero column and a determinant of 0.

      Therefore, the sum over all $\mathbf{j}_c$
      is equal to
      \begin{align*}
        \det(A) & =
        A
        \begin{pmatrix}
          (1, \ldots, p)\\
          (n-p+1, \ldots, n)
        \end{pmatrix}
        A^c
        \begin{pmatrix}
          (1, \ldots, p)\\
          (n-p+1, \ldots, n)
        \end{pmatrix}\\
        & = A
        \begin{pmatrix}
          (1, \ldots, p)\\
          (n-p+1, \ldots, n)
        \end{pmatrix}
        (-1)^{\frac{p(1+p)}{2} + \frac{p(2n-p+1)}{2}}
        A
        \begin{pmatrix}
          (n-p+1, \ldots, n)\\
          (1, \ldots, p)
        \end{pmatrix}\\
        & = (-1)^{\frac{p + p^2 + 2np - p^2 + p}{2}}\det(A_{12}) \det(A_{21})\\
        & = (-1)^{(n+1)p}\det(A_{11}) \det(A_{22}).
      \end{align*}
  \end{itemize}
\end{solution}

\subsection{}
\nosolution

\exo{1}
\begin{solution}
  For the set of \emph{invertible square} matrices $\mathcal{M}$,
  to be a \emph{multiplicative group}, it has to verify the four following properties,
  which we will demonstrate.
  \begin{enumerate}
    \item \emph{Closure}. If $A,B \in \mathcal{M}$ then $AB \in \mathcal{M}$.
    Using the Cauchy-Binet formula, when $A$ and $B$ have $n \times n$ dimensions
    \[
      \det(AB) = \sum_{\mathbf{j}_n}
      A
      \begin{pmatrix}
        \mathbf{n} \\
        \mathbf{j}_n \\
      \end{pmatrix}
      B
      \begin{pmatrix}
        \mathbf{j}_n \\
        \mathbf{n} \\
      \end{pmatrix}
      =\det\left([a_{i_k,j_l}]_{k,l=1}^{n}\right) \det\left([b_{i_k,j_l}]_{k,l=1}^{n}\right) 
      =\det(A) \det(B)
    \]
    because the only set $\mathbf{j}_n$ possible is $\mathbf{j}_n = \{1,\dots,n\}$.
    We also know that $\det(A), \det(B) \neq 0$ because $A, B \in \mathcal{M}$,
    therefore $\det(AB) \neq 0$ and thus $AB \in \mathcal{M}$.
    \item \emph{Associativity}. See exercise~1.3.
    \item \emph{Identity element}. See exercise~1.3.
    \item \emph{Invertibility}. Using theorem~1.3 (A non singular matrix $A$ defined on a 
    field $\mathcal{F}$ has an inverse.),
    we know that if $A \in \mathcal{M}$,
    there exists a matrix $A^{-1} = (\det A)^{-1} \text{adj}A$ 
    such that $A A^{-1} = A^{-1} A = I$.
  \end{enumerate}
  We conclude that invertible square matrices form a multiplicative group.
\end{solution}

\exo{1}
\begin{solution}
  \begin{itemize}
    \item $\adj(A^T) = [A_{ji}^T]_{i,j=1}^n = [A_{ij}]_{i,j=1}^n = (\adj A)^T$
    \item $\adj I = [I_{ji}]_{i,j=1}^n = [1_{ii}]_{i=1}^n = I$
    \item Here we need to verify that the inverse of $A^T$ is $(A^{-1})^T$.
    More precisely
    \[
      \left\{ \begin{array}{l}
         (A^T)(A^{-1})^T = (A^{-1}A)^T = I^T = I \\
         (A^{-1})^T (A^T) = (A A^{-1})^T = I^T = I \\
      \end{array}\right.
    \]
    \item We know that $\det(A^{-1}) \det(A) = \det(A^{-1}A) = \det(I) = 1$
    and thus that $\det(A^{-1}) = (\det(A))^{-1}$.
    \item $\adj(A^*) = (\adj A)^*$ : same as the first point except we take the
    complex conjugate.
    \item Using determinant and inverse matrix properties
    \begin{align*}
      \adj(kA) &= (kA)^{-1} \det(kA) \\
      &= k^{-1} A^{-1} k^n \det(A) \\
      &= k^{n-1} \adj(A)
    \end{align*}
    \item Here we need to verify that the inverse of $A^*$ is $(A^{-1})^*$.
    More precisely
    \[
      \left\{ \begin{array}{l}
         (A^*)(A^{-1})^* = (A^{-1}A)^* = I^* = I \\
         (A^{-1})^* (A^*) = (A A^{-1})^* = I^* = I \\
      \end{array}\right.
    \]
    \item Here we need to verify that the inverse of $AB$ is $B^{-1}A^{-1}$.
    More precisely
    \[
      \left\{ \begin{array}{l}
         (AB) (B^{-1}A^{-1}) = A I A^{-1} = A A^{-1} = I \\
         (B^{-1}A^{-1}) (AB) = B^{-1} I B = B^{-1} B = I \\
      \end{array}\right.
    \]
  \end{itemize}
\end{solution}

\subsection{}
\nosolution
