\section{Matrix algebras}
\exo{1}
Show that addition of matrices is commutative, associative and that the neutral element is the zero matrix: $0_{m \times n} = [0]_{i,j=1}^{m,n}$.
\begin{solution}
  Let $\mathcal{R}$ be the ring to which the elements of the matrix belong.
  \begin{description}
    \item[Commutativity]
      We should have $A+B=B+A$ or
      $[a_{ij} + b_{ij}]_{i,j=1}^{m,n} = [b_{ij} + a_{ij}]_{i,j=1}^{m,n}$,
      which is true since addition is commutative in $\mathcal{R}$ by virtue of it being a ring (strictly speaking, a commutative group would be sufficient).
    \item[Associativity] $A+(B+C)=(A+B)+C$
      We should have $A+(B+C)=(A+B)+C$ or
      $[a_{ij} + (b_{ij} + c_{ij})]_{i,j=1}^{m,n} 
      = [(a_{ij} + b_{ij}) + c_{ij}]_{i,j=1}^{m,n}$
      which is true since addition is commutative in $\mathcal{R}$ by virtue of it being a ring (strictly speaking, a semigroup would be sufficient).
    \item[Neutral element] Let $B$ be the neutral element.
      We have
      $A + B = [a_{ij} + b_{ij}]_{i,j=1}^{m,n}$
      and $A = [a_{ij}]_{i,j=1}^{m,n}$.
      Two matrices are equal if all their elements are equal, so $b_{ij}$
      should be the neutral element of $\mathcal{R}$, which exists by virtue of it being a ring (strictly speaking, a monoid would be sufficient).
  \end{description}
\end{solution}

\exo{1}
Verify that, for all matrices \(A\) and \(B\) belonging to \(M\), the set of matrices of fixed dimensions \(m \times n\), and for all scalars \(\alpha, \beta\) belonging to a field \(\mathcal{F}\) or a ring \(\mathcal{R}\), it holds that
\begin{align*}
0A &= 0,\\
1A &= A,\\
(\alpha + \beta)A &= \alpha A + \beta A,\\
\alpha(A + B) &= \alpha A + \alpha B,\\
\alpha(\beta A) &= (\alpha \beta) A.
\end{align*}

\begin{solution}
\begin{itemize}
	\item By the definition, \(0A = [0 a_{ij}]_{i,j = 1}^{m, n}\), and thus \(0A = [0]_{i, j = 1}^{m, n} = 0_{m, n}\).
	\item By the definition, \(1A = [1 a_{ij}]_{i,j = 1}^{m, n}\), and thus \(1A = [a_{ij}]_{i, j = 1}^{m, n} = A\).
	\item By the definition, \((\alpha + \beta)A = [(\alpha + \beta) a_{ij}]_{i,j = 1}^{m, n} = [\alpha a_{ij} + \beta a_{ij}]_{i, j = 1}^{m, n} = \alpha A + \beta A\).
	\item By the definition, \(\alpha (A + B) = [\alpha (a_{ij} + b_{ij})]_{i,j = 1}^{m, n} = [\alpha a_{ij} + \alpha b_{ij}]_{i, j = 1}^{m, n} = \alpha A + \alpha B\).
	\item By the definition, \(\alpha (\beta A) = [\alpha (\beta a_{ij})]_{i,j = 1}^{m, n} = \alpha [\beta a_{ij}]_{i, j = 1}^{m, n} = [(\alpha \beta) a_{ij}]_{i, j = 1}^{m, n} = (\alpha \beta) A\).
\end{itemize}

\end{solution}

\exo{1}
Show that the following properties hold true:
\begin{align*}
AI &= IA = A, \tag{neutral element}\\
A(B + C) &= AB + AC, \tag{distributivity}\\
(B + C) D &= BD + CD, \tag{distributivity}\\
A(BC) &= (AB)C. \tag{associativity}
\end{align*}

\begin{solution}
\begin{itemize}
	\item We use \(\delta_{ij}\), the Kronecker delta, which is \(1\) when \(i = j\) and \(0\) otherwise.
	\begin{align*}
	AI &= \left[\sum_k a_{ik} \delta_{kj}\right]_{i, j = 1}^{m, n}\\
	&= [a_{ij}]_{i, j = 1}^{m, n}\\
	&= \left[\sum_k \delta_{ik} a_{kj}\right]_{i, j = 1}^{m, n}\\
	&= IA.
	\end{align*}
	\item \begin{align*}
	A (B + C) &= \left[\sum_k a_{ik} (b_{kj} + c_{kj})\right]_{i, j = 1}^{m, n}\\
	&= \left[\sum_k \left(a_{ik} b_{kj} + a_{ik} c_{kj}\right)\right]_{i, j = 1}^{m, n}\\
	&= \left[\sum_k a_{ik} b_{kj} + \sum_k a_{ik} c_{kj}\right]_{i, j = 1}^{m, n}\\
	&= AB + AC.
	\end{align*}
	\item  \begin{align*}
	(B + C) D &= \left[\sum_k (b_{ik} + c_{ik}) d_{kj}\right]_{i, j = 1}^{m, n}\\
	&= \left[\sum_k \left(b_{ik} d_{kj} + c_{ik} d_{kj}\right)\right]_{i, j = 1}^{m, n}\\
	&= \left[\sum_k b_{ik} d_{kj} + \sum_k c_{ik} d_{kj}\right]_{i, j = 1}^{m, n}\\
	&= BD + CD.
	\end{align*}
	\item \begin{align*}
	A(BC) &= \left[\sum_\ell a_{i\ell} \left(\sum_k b_{\ell k} c_{k j}\right)\right]_{i, j = 1}^{m, n}\\
	&= \left[\sum_\ell \sum_k a_{i\ell} (b_{\ell k} c_{kj})\right]_{i, j = 1}^{m, n}\\
	&= \left[\sum_\ell \sum_k (a_{i\ell} b_{\ell k}) c_{kj}\right]_{i, j = 1}^{m, n}\\
	&= \left[\sum_\ell \left(\sum_k a_{i\ell} b_{\ell k}\right) c_{kj}\right]_{i, j = 1}^{m, n}\\
	&= (AB)C.
	\end{align*}
\end{itemize}
\end{solution}

\exo{1}
Verify that the matrices
\[
A = \begin{bmatrix} 2 & 1 \\ 0 & 0 \\ 1 & 0 \end{bmatrix},\quad B = \begin{bmatrix} 0 & 1 & 2 \\ -1 & 0 & 0 \end{bmatrix}
\]
and
\[
A = \begin{bmatrix} 2 & 1 \\ 0 & 0 \end{bmatrix},\quad B = \begin{bmatrix} 1 & 0 \\ -1 & 0 \end{bmatrix}
\]
do not satisfy \(AB = BA\).

\begin{solution}
For the first pair, \(AB\) is a \(3 \times 3\) matrix, whereas \(BA\) is a \(2 \times 2\) matrix.
They can hence not be equal.

For the second pair, computation yields
\[
AB = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \neq BA = \begin{bmatrix} 2 & 1 \\ -2 & -1 \end{bmatrix}.
\]
\end{solution}

\exo{2}
Show that the set of square matrices of dimension \(n\) forms a ring.

\begin{solution}
  We have to prove that
  \begin{itemize}
    \item $(\mathcal{R}^{n \times n}, +)$ is a commutative group;
    \item $(\mathcal{R}^{n \times n}, \cdot)$ is a monoid;
    \item $\cdot$ is distributive over $+$,
  \end{itemize}
  which can be shown with the help of the previous exercises.
  \begin{itemize}
    \item If $A$ and $B$ are in $\mathcal{R}^{n \times n}$, $A+B \in \mathcal{R}^{n \times n}$,
      so $+$ is a composition law in $\mathcal{R}^{n \times n}$.
      We have seen in Exercise~1.1 that addition of matrices is associative, and that the neutral element is $[0]_{i,j=1}^{n}$.
      The symmetric element of $A$ exists and is $[-a_{ij}]_{i,j=1}^{n}$ since
      \[ [a_{ij}]_{i,j=1}^{n} + [-a_{ij}]_{i,j=1}^{n} = [a_{ij}-a_{ij}]_{i,j=1}^{n} = [0]_{i,j=1}^{n}. \]
      It is also commutative as we have seen in Exercise~1.1.
    \item If $A$ and $B$ are in $\mathcal{R}^{n \times n}$, $A \cdot B \in \mathcal{R}^{n \times n}$,
      so $\cdot$ is a composition law in $\mathcal{R}^{n \times n}$.
      It is associative and has a neutral element as seen in Exercise~1.3.
    \item We have seen in Exercise~1.3 that $\cdot$ is distributive over $+$.
  \end{itemize}
\end{solution}

\exo{2}
Show that if \(AC\) and \(BD\) are well defined, then
\[
(A \otimes B)(C \otimes D) = (AC) \otimes (BD).
\]

\begin{solution}
  Let $m,n,o,p,q,s \in \mathbb{N}$ such that
  $A \in \mathcal{R}^{m \times n}, B \in \mathcal{R}^{p \times q}, C \in \mathcal{R}^{n \times o}, D \in \mathcal{R}^{q \times s}$.
  We can check that
  $A \otimes B \in \mathcal{R}^{mp \times nq}$ and
  $C \otimes D \in \mathcal{R}^{nq \times os}$
  so $(A \otimes B) (C \otimes D)$ is well defined.
  The two expressions are equal as shown by
  \begin{align*}
    (A \otimes B) (C \otimes D)
    & =
    \begin{bmatrix}
      a_{11}B & \cdots & a_{1n}B\\
      \vdots  & \ddots & \vdots\\
      a_{m1}B & \cdots & a_{mn}B
    \end{bmatrix}
    \begin{bmatrix}
      c_{11}D & \cdots & c_{1o}D\\
      \vdots  & \ddots & \vdots\\
      c_{n1}D & \cdots & c_{no}D
    \end{bmatrix}\\
    & =
    \begin{bmatrix}
      \sum_{i=1}^n a_{1i}c_{i1}BD & \cdots & \sum_{i=1}^n a_{1i}c_{io}BD\\
      \vdots  & \ddots & \vdots\\
      \sum_{i=1}^n a_{mi}c_{i1}BD & \cdots & \sum_{i=1}^n a_{mi}c_{io}BD
    \end{bmatrix}\\
    & =
    \begin{bmatrix}
      \sum_{i=1}^n a_{1i}c_{i1} & \cdots & \sum_{i=1}^n a_{1i}c_{io}\\
      \vdots  & \ddots & \vdots\\
      \sum_{i=1}^n a_{mi}c_{i1} & \cdots & \sum_{i=1}^n a_{mi}c_{io}
    \end{bmatrix}
    \otimes BD\\
    & =
    AC \otimes BD.
  \end{align*}
\end{solution}

\exo{1}
Show that for the matrix powers the classical laws of exponentiation hold true: that is, for all nonnegative integers \(p, q\), and letting \(A^0 \coloneqq I\), we have
\[
A^pA^q = A^{p+q} = A^qA^p \quad \textnormal{and} \quad (A^{p})^q = A^{pq}.
\]

\begin{solution}
  \begin{align*}
    A^pA^q
    & = \underbrace{A \cdots A}_{p} \underbrace{A \cdots A}_{q}\\
    & = \underbrace{A \cdots A}_{p+q}\\
    & = A^{p+q}\\
    & = \underbrace{A \cdots A}_{q+p}\\
    & = \underbrace{A \cdots A}_{q} \underbrace{A \cdots A}_{p}\\
    & = A^qA^p.\\
    (A^p)^q
    & = \underbrace{\underbrace{A \cdots A}_{p} \cdots \underbrace{A \cdots A}_{p}}_q\\
    & = \underbrace{A \cdots A}_{pq}\\
    & = A^{pq}.
  \end{align*}
\end{solution}

\exo{2}
Consider the set of upper triangular Toeplitz matrices, i.e., the upper triangular matrices with equal elements
 along the diagonals ($t_{ij} = t_{i+k,j+k}$):
\[
  T =
  \begin{bmatrix}
    t_1 & t_2 & \cdots & t_n\\
    0   & t_1 & \ddots & \vdots\\
    \vdots & \ddots & \ddots & t_2\\
    0 & \cdots & 0 & t_1
  \end{bmatrix}.
\]
Show that these matrices commute.

\begin{solution}
  Let $T,U$ be upper triangular Toeplitz matrices, as defined in the exercise statement.
  The proof is then trivial:
  \begin{align*}
    TU
    & =
    \begin{bmatrix}
      t_1 & t_2 & \cdots & t_n\\
      0   & t_1 & \ddots & \vdots\\
      \vdots & \ddots & \ddots & t_2\\
      0 & \cdots & 0 & t_1
    \end{bmatrix}
    \begin{bmatrix}
      u_1 & u_2 & \cdots & u_n\\
      0   & u_1 & \ddots & \vdots\\
      \vdots & \ddots & \ddots & u_2\\
      0 & \cdots & 0 & u_1
    \end{bmatrix}\\
    & =
    \begin{bmatrix}
      t_1u_1 & t_1u_2 + t_2u_1 & \cdots & t_1u_n + \cdots + t_nu_1\\
      0   & t_1u_1 & \ddots & \vdots\\
      \vdots & \ddots & \ddots & t_1u_2 + t_2u_1\\
      0 & \cdots & 0 & t_1u_1
    \end{bmatrix}\\
    & =
    \begin{bmatrix}
      u_1t_1 & u_1t_2 + u_2t_1 & \cdots & u_1t_n + \cdots + u_nt_1\\
      0   & u_1t_1 & \ddots & \vdots\\
      \vdots & \ddots & \ddots & u_1t_2 + u_2t_1\\
      0 & \cdots & 0 & u_1t_1
    \end{bmatrix}\\
    & = UT.
  \end{align*}
\end{solution}

\exo{3}
Consider the set of square, circulant matrices:
\[
  C =
  \begin{bmatrix}
    c_1 & c_2 & \cdots & c_n\\
    c_n & c_1 & \ddots & \vdots\\
    \vdots & \ddots & \ddots & c_2\\
    c_2 & \cdots & c_n & c_1
  \end{bmatrix}.
\]
Show that these matrices commute.

\begin{solution}
  Let $C,D$ be square circulant matrices and define
  \[ \sigma \colon (i,j) \mapsto \big((j - i) \bmod{n}\big)+1 \]
  such that
  $C = [c_{\sigma(i,j)}]_{i,j=1}^{n}$ and
  $D = [d_{\sigma(i,j)}]_{i,j=1}^{n}$.
  We can observe that
  \begin{equation}
    \label{eq:cycln}
    \sigma(i,j+n) = \sigma(i+n,j) = \sigma(i,j),
  \end{equation}
  and
  \begin{equation}
    \label{eq:cycldiff}
    \sigma(i,j) = \sigma(i+a,j+a),
  \end{equation}
  for all $a \in \mathbb{N}$.
  We can now observe that
  \begin{align*}
    CD
    & =
    \left[
      \sum_{k=1}^n
      c_{\sigma(i,k)}d_{\sigma(k,j)}
    \right]_{i,j=1}^{n}\\
    & =
    \left[
      \sum_{k=1}^{i+j-1}
      c_{\sigma(i,k)}d_{\sigma(k,j)}
      +
      \sum_{k=i+j}^{n}
      c_{\sigma(i,k)}d_{\sigma(k,j)}
    \right]_{i,j=1}^{n}\\
    & =
    \left[
      \sum_{k=1}^{i+j-1}
      c_{\sigma(i,i+j-k)}d_{\sigma(i+j-k,j)}
      +
      \sum_{k=i+j}^{n}
      c_{\sigma(i,n+i+j-k)}d_{\sigma(n+i+j-k,j)}
    \right]_{i,j=1}^{n}\\
    & \stackrel{\eqref{eq:cycln}}{=}
    \left[
      \sum_{k=1}^{i+j-1}
      c_{\sigma(i,i+j-k)}d_{\sigma(i+j-k,j)}
      +
      \sum_{k=i+j}^{n}
      c_{\sigma(i,i+j-k)}d_{\sigma(i+j-k,j)}
    \right]_{i,j=1}^{n}\\
    & =
    \left[
      \sum_{k=1}^n
      c_{\sigma(i,i+j-k)}d_{\sigma(i+j-k,j)}
    \right]_{i,j=1}^{n}\\
    & \stackrel{\eqref{eq:cycldiff}}{=}
    \left[
      \sum_{k=1}^n
      c_{\sigma(k,j)}d_{\sigma(i,k)}
    \right]_{i,j=1}^{n}\\
    &=
    \left[
    \sum_{k=1}^n
    d_{\sigma(i,k)}c_{\sigma(k,j)}
    \right]_{i,j=1}^{n}\\
    & = DC.
  \end{align*}
\end{solution}

\exo{2}
Show that a square matrix of dimension \(n\) commuting with all the other matrices of the same dimension is necessarily a ``scalar'' matrix, i.e., it has the form \(cI\).

\begin{solution}
  Asking that a matrix commutes with every matrix is actually equivalent to asking
  that it commutes with every element of the base of $\mathcal{R}^{n \times n}$.
  The canonical base is $e_ie_j^\top$ for $i,j \in \{1, \dots, n\}$ where
  $e_k$ is a column vector with \(1\) at position $k$ and \(0\) elsewhere.
  We can see that $Ae_ie_j^\top = A_{:i}e_j^\top$ and $e_ie_j^\top A = e_iA_{j:}$.

  If $A_{:i}e_j^\top = e_iA_{j:}$, the LHS imposes that only the $j$-th column is nonzero
  and the RHS imposes that only the $i$-th row is nonzero.
  We therefore need $A_{ki} = 0$ for $k \neq i$ and $A_{jk} = 0$ for $k \neq j$.
  What's left is $e_j^\top e_iA_{ii} = A_{jj}e_j^\top e_i$.
  Therefore $A_{ii} = A_{jj}$.
  Since this must hold for every $i,j \in \{1, \dots, n\}$, $A$ must be diagonal
  with equal elements on its diagonal, that is, a scalar multiple of the identity matrix: \(A = cI\).
\end{solution}

\exo{2}
Show that a square matrix of dimension \(n\) commuting with a diagonal matrix \(\diag\{a_1, \dots, a_n\}\), where \(a_i \neq a_j\) for all \(i \neq j\), is also diagonal.

\begin{solution}
  \begin{description}
    \item[Solution 1]
    Asking that a matrix $A$ commutes with a diagonal matrix is actually equivalent to asking
    that it commutes with every element of the base 
    $e_ie_i^\top$ for $i \in \{1, \ldots, n\}$ where
    $e_k$ is a column vector with \(1\) at position $k$ and \(0\) elsewhere.
    We can see that $Ae_ie_i^\top = A_{:i}e_i^\top$ and $e_ie_i^\top A = e_iA_{i:}$.
  
    If $A_{:i}e_i^\top = e_iA_{i:}$, the LHS imposes that only the $i$-th column is nonzero
    and the RHS imposes that only the $i$-th row is nonzero.
    We therefore need $A_{ki} = 0$ for $k \neq i$ and $A_{ik} = 0$ for $k \neq i$.
    Since this must hold for every $i \in \{1, \ldots, n\}$, $A$ must be diagonal.
   
    \item[Solution 2]
    Let $B$ be a square matrix of dimension $n$ commuting with $A = \diag\{a_1,\dots,a_n\}$.
    We then have that
    \[
      B A =
      \begin{bmatrix}
        b_{11} a_1 & \dots & b_{1n} a_n \\
        \vdots & \ddots & \vdots \\
        b_{n1} a_1 & \dots & b_{nn} a_n \\
      \end{bmatrix}
      =
      \begin{bmatrix}
        b_{11} a_1 & \dots & b_{1n} a_1 \\
        \vdots & \ddots & \vdots \\
        b_{n1} a_n & \dots & b_{nn} a_n \\
      \end{bmatrix}
      = A B.
    \]
    We thus need $b_{ij} a_j = b_{ij} a_i$ for every $i \neq j$
    and since $a_i \neq a_j$ for $i \neq j$, $B$ has to be diagonal itself.
  \end{description}
\end{solution}

\exo{1}
Show that for every matrix \(A \in \C^{m \times n}\), the matrices \(AA^\top\) and \(A^\top A\) are symmetric, and the matrices \(AA^*\) and \(A^*A\) are Hermitian.

\begin{solution}
  Using the properties $(AB)^\top = B^\top A^\top$ and $(AB)^* = B^*A^*$,
  \begin{align*}
    (AA^\top)^\top & = (A^\top)^\top A^\top\\
             & = AA^\top,\\
    (A^\top A)^\top & = A^\top(A^\top)^\top\\
             & = A^\top A,\\
    (AA^*)^* & = (A^*)^*A^*\\
             & = AA^*,\\
    (A^*A)^* & = A^*(A^*)^*\\
             & = A^*A.
  \end{align*}
\end{solution}

\exo{1}
Show that for a square matrix \(A \in \C^{n \times n}\), the matrix \(A + A^\top\) is symmetric, the matrix \(A + A^*\) is Hermitian, the matrix \(A - A^\top\) is antisymmetric and the matrix \(A - A^*\) is anti-Hermitian.

\begin{solution}
  We have
  \begin{align*}
    (A + A^\top)^\top & = A^\top + A\\
                & = A + A^\top,\\
    (A - A^\top)^\top & = A^\top - A\\
                & = -(A - A^\top),\\
    (A + A^*)^* & = A^* + A\\
                & = A + A^*,\\
    (A - A^*)^* & = A^* - A\\
                & = -(A - A^*).
  \end{align*}
\end{solution}

\exo{2}
Show that every complex matrix can be written as the sum of a symmetric matrix and an antisymmetric matrix, and as the sum of a Hermitian matrix and an anti-Hermitian matrix.

\begin{solution}
  We simply notice that
  \begin{align*}
    A & = \frac{A + A^\top}{2} + \frac{A - A^\top}{2}\\
      & = \frac{A + A^*}{2} + \frac{A - A^*}{2}.
  \end{align*}
\end{solution}

\exo{1}
Show that the Kronecker product satisfies
\[
(A \otimes B)^* = A^* \otimes B^*.
\]

\begin{solution}
  It is shown by
  \begin{align*}
    (A \otimes B)^* =
    \begin{bmatrix}
      a_{11}B & \cdots & a_{1n}B\\
      \vdots  & \ddots & \vdots\\
      a_{m1}B & \cdots & a_{mn}B
    \end{bmatrix}^* =
    \begin{bmatrix}
      (a_{11}B)^* & \cdots & (a_{m1}B)^*\\
      \vdots  & \ddots & \vdots\\
      (a_{1n}B)^* & \cdots & (a_{mn}B)^*
    \end{bmatrix} =
    \begin{bmatrix}
      a_{11}^*B^* & \cdots & a_{m1}^*B^*\\
      \vdots  & \ddots & \vdots\\
      a_{1n}^*B^* & \cdots & a_{mn}^*B^*
    \end{bmatrix},
  \end{align*}
  where the last expression is equal to \(A^* \otimes B^*\).
\end{solution}

\exo{2}
Show with the help of the previous exercise that if \(U_1\) and \(U_2\) are unitary matrices, then \(U_1 \otimes U_2\) is unitary as well.

\begin{solution}
  If $U_1$ and $U_2$ are unitary, then using Exercise~1.6 and Exercise~1.15 we know that
  \begin{align*}
    (U_1 \otimes U_2) (U_1 \otimes U_2)^*
    & = (U_1 \otimes U_2) (U_1^* \otimes U_2^*)\\
    & = U_1U_1^* \otimes U_2U_2^*\\
    & = I \otimes I\\
    & = I,
  \end{align*}
  which implies that $(U_1 \otimes U_2)^*$
  is the inverse of $U_1 \otimes U_2$
  (it is also its left inverse since it is a square matrix).
\end{solution}

\exo{4}
Show that for arbitrary matrices \(A_{m \times n}\) and \(B_{n \times m}\), we have
\[
\det\begin{bmatrix}
A & 0\\
-I_n & B
\end{bmatrix} =
\det\begin{bmatrix}
A & AB\\
-I_n & 0
\end{bmatrix}.
\]

\begin{solution}
  Let's prove the equivalent property
  \[
    \det\begin{bmatrix}
      A^\top & -I_n\\
      0 & B^\top
    \end{bmatrix} =
    \det\begin{bmatrix}
      A^\top & -I_n\\
      B^\top A^\top & 0
    \end{bmatrix}
  \]
  to work on the columns, since the properties are expressed
  on the columns.

  Let $C$ be the matrix of the LHS and
  $D$ be the matrix of the RHS.
  We can see that

  \begin{align*}
    d_{i:} &= c_{i:} & i & = 1, \dots, n,\\
    d_{i:} &= c_{i:} + \sum_{k=1}^n b_{k(i-n)} c_{k:} & i & = n+1, \dots, 2n,
  \end{align*}

  since

  \begin{align*}
    \begin{bmatrix}
      B^\top A^\top & 0
    \end{bmatrix}
    & =
    \begin{bmatrix}
      0 & B^\top
    \end{bmatrix}
    +
    \begin{bmatrix}
      B^\top A^\top & -B^\top
    \end{bmatrix} =
    \begin{bmatrix}
      0 & B^\top
    \end{bmatrix}
    +
    B^\top
    \begin{bmatrix}
      A^\top & -I_n
    \end{bmatrix}.
  \end{align*}

  This gives us what we want using Properties~3 and~8 from Proposition~1.2.
\end{solution}

\exo{3}
Using the previous exercise, show that for two arbitrary square matrices \(A_{n \times n}\) and \(B_{n \times n}\), it holds that
\[
\det(A) \det(B) = \det(AB).
\]

\begin{solution}
  We know that for $C \in \mathcal{R}^{2n \times 2n}$
  \[ \det(C) = \sum_{\mathbf{j}} (-1)^{t(\mathbf{j})} c_{1j_1} \cdots c_{(2n)j_{2n}}. \]
  For
  \[
    C = \begin{bmatrix}
      A & 0\\
      -I_n & B
    \end{bmatrix},
  \]
  we see that if $j_i > n$ for $i \leq n$, $c_{ij_i} = 0$.
  Therefore the first $n$ elements of $\mathbf{j}$ must be a permutation of $\{1, \ldots, n\}$
  for the product $c_{1j_1} \cdots c_{(2n)j_{2n}}$ to be nonzero.
  The last $n$ elements of $\mathbf{j}$ are therefore a permutation of $\{n+1, \ldots, 2n\}$.
  Consequently, we have (defining $J$ as the set of such permutations)
  \begin{align*}
    \det(C)
    & = \sum_{\mathbf{j} \in J} (-1)^{t(\mathbf{j})} c_{1j_1} \cdots c_{nj_n} c_{(n+1)j_{n+1}} \cdots c_{(2n)j_{2n}}\\
    & = \sum_{\mathbf{j} \in J} (-1)^{t(j_1, \ldots, j_n)} a_{1j_1} \cdots a_{nj_n} (-1)^{t(j_{n+1}, \ldots, j_{2n})} b_{1(j_{n+1}-n)} \cdots b_{n(j_{2n}-n)}\\
    & = \left(\sum_{\mathbf{j}} (-1)^{t(\mathbf{j})} a_{1j_1} \cdots a_{nj_n}\right)
    \left(\sum_{\mathbf{j}} (-1)^{t(\mathbf{j})} b_{1(j_1)} \cdots b_{n(j_n)}\right)\\
    & = \det(A) \det(B).
  \end{align*}

  For
  \[
    D = \begin{bmatrix}
      A & AB\\
      -I_n & 0
    \end{bmatrix},
  \]
  we see using the same reasoning that we can restrict ourself to
  $\mathbf{j} = \mathbf{j}_1 \times \mathbf{j}_2$
  where $\mathbf{j}_1$ is a permutation of $\{n+1, \ldots, 2n\}$
  and $\mathbf{j}_2$ is a permutation of $\{1, \ldots, n\}$.
  This time, we have $t(\mathbf{j}) = t(\mathbf{j_1}) + t(\mathbf{j_2}) + n^2$ so
  (defining $J$ as expected, noting $AB = [x_{ij}]_{i,j=1}^{n}$ and with \(\delta_{ij}\) the Kronecker delta function)
  \begin{align*}
    \det(D)
    & = \sum_{\mathbf{j} \in J} (-1)^{t(\mathbf{j})} d_{1j_1} \cdots d_{nj_n} d_{(n+1)j_{n+1}} \cdots d_{(2n)j_{2n}}\\
    & = \sum_{\mathbf{j} \in J} (-1)^{t(\mathbf{j}_1)} x_{1(j_1-n)} \cdots x_{n(j_n-n)} (-1)^{t(\mathbf{j}_2)+n^2} (-\delta_{1(j_{n+1})}) \cdots (-\delta_{n(j_{2n})})\\
    & = \left(\sum_{\mathbf{j}} (-1)^{t(\mathbf{j})} x_{1j_1} \cdots x_{nj_n}\right)
    (-1)^{n^2}\left(\sum_{\mathbf{j}} (-1)^{t(\mathbf{j})} (-\delta_{1(j_1)}) \cdots (-\delta_{n(j_n)})\right)\\
    & = \det(AB) (-1)^{n^2}\det(-I_n)\\
    & = \det(AB) (-1)^{n^2+n}\\
    & = \det(AB) (-1)^{n(n+1)}\\
    & = \det(AB),
  \end{align*}
  since either $n$ or $n+1$ is even.
  Since we know \(\det C = \det D\), we thus conclude that \(\det A \det B = \det(AB)\).
\end{solution}

\exo{2}
Show that we have
\begin{align*}
\det(\lambda I_n - C) &= \det \begin{bmatrix}
\lambda & -1 & & & 0\\
& \lambda & \ddots & & \\
&& \ddots & \ddots &\\
0 &&& \lambda & -1\\
a_0 & a_1 & \cdots & a_{n-2} & \lambda + a_{n-1}
\end{bmatrix}\\
&= a_0 + a_1 \lambda + \dots + a_{n-1}\lambda^{n-1} + \lambda^n,
\end{align*}
where the matrix \(C_{n \times n}\) above is called the \emph{companion matrix} of the polynomial.

\begin{solution}
  We first notice that
  \begin{align*}
    \det\begin{bmatrix}
      \lambda & -1 &\\
      & \ddots & \ddots\\
      & & \lambda & -1\\
      & & & \lambda
    \end{bmatrix}^{n \times n}
    & =
    \lambda
    \det\begin{bmatrix}
      \lambda & -1 &\\
              & \ddots & \ddots\\
              & & \lambda & -1\\
              & & & \lambda
    \end{bmatrix}^{(n-1) \times (n-1)} \\
    & = \lambda^n,
  \end{align*}
  where $n$ is the dimension of the original matrix.
  Using the cofactor technique on the last column recursively,
  \begin{align*}
    \det(\lambda I_n - C) & = (\lambda + a_{n-1})\lambda^{n-1}
    + \det(\lambda I_{n-1} - C_{n-1} - \lambda e_{n-1}e_{n-1}^\top)\\
    & =
    \lambda^n + a_{n-1}\lambda^{n-1}
    + a_{n-2} \lambda^{n-1}
    + \det(\lambda I_{n-2} - C_{n-2} - \lambda e_{n-2}e_{n-2}^\top)\\
    & = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots + a_0.
  \end{align*}
\end{solution}

\exo{2}
Show that the determinant of a tridiagonal matrix (sometimes referred to as a \emph{Jacobi matrix})
\[
J_n = \begin{bmatrix}
a_1 & b_2 & & & 0\\
c_2 & a_2 & \ddots & & \\
& \ddots & \ddots & \ddots &\\
&& \ddots & \ddots & b_n\\
0 &&& c_n & a_n\\
\end{bmatrix}
\]
satisfies the following recurrence relation:
\[
\det(J_n) = a_n \det(J_{n-1}) - b_n c_n \det(J_{n-2}).
\]

\begin{solution}
  Using the cofactor technique on the last column then on the last line,
  \begin{align*}
    \det(J_n)
    & = a_n\det(J_{n-1}) - b_n
    \det
    \begin{bmatrix}
     J_{n-2}  &   & 0 \\
              &   & b_{n-1} \\
     0        & 0 & c_n
    \end{bmatrix}\\
    & = a_n\det(J_{n-1}) - b_n c_n \det(J_{n-2}).
  \end{align*}
\end{solution}

\exo{4}
Verify that, for a \emph{Vandermonde matrix}, the following identity holds true:
\[
\det
\begin{bmatrix}
	1 & 1 & \cdots & 1\\
	x_1 & x_2 & \cdots & x_n\\
	x_1^2 & x_2^2 & \cdots & x_n^2\\
	\vdots & \vdots &  & \vdots\\
	x_1^{n-1} & x_2^{n-1} & \cdots & x_n^{n-1}
\end{bmatrix} = \prod_{j < i} (x_i - x_j).
\]

\begin{solution}
  Let's prove it by induction.
  \begin{itemize}
    \item If $n = 1$, we have
      \[
        \det
        \begin{bmatrix}
          1
        \end{bmatrix}
        = 1,
      \]
      and for $n = 2$, we have
      \[
        \det
        \begin{bmatrix}
          1 & 1\\
          x_1 & x_2
        \end{bmatrix}
        = (x_2 - x_1).
      \]
    \item
      For $i = 1, \ldots, n-1$, replacing line $i+1$
      by line $i+1$ from which we subtract $x_n$ times line $i$
      does not change the determinant since
      it is an elementary line transformation (Property~9 of Proposition~1.2).
      We therefore have
      \begin{align*}
        \det
        \begin{bmatrix}
          1 & 1 & \cdots & 1\\
          x_1 & x_2 & \cdots & x_n\\
          x_1^2 & x_2^2 & \cdots & x_n^2\\
          \vdots & \vdots & \ddots & \vdots\\
          x_1^{n-1} & x_2^{n-1} & \cdots & x_n^{n-1}
        \end{bmatrix}
        & =
        \det
        \begin{bmatrix}
          1 & 1 & \cdots & 1\\
          x_1-x_n & x_2-x_n & \cdots & 0\\
          x_1^2-x_1x_n & x_2^2 - x_2x_n & \cdots & 0\\
          \vdots & \vdots & \ddots & \vdots\\
          x_1^{n-1}-x_1^{n-2}x_n & x_2^{n-1}-x_2^{n-2}x_n & \cdots & 0
        \end{bmatrix}\\
        & =
        \det
        \begin{bmatrix}
          1 & 1 & \cdots & 1\\
          (x_1-x_n) & x_2-x_n & \cdots & 0\\
          (x_1-x_n)x_1 & (x_2-x_n)x_2 & \cdots & 0\\
          \vdots & \vdots & \ddots & \vdots\\
          (x_1-x_n)x_1^{n-2} & (x_2-x_n)x_2^{n-2} & \cdots & 0
        \end{bmatrix}\\
        & =
        (-1)^{n+1}
        \det
        \begin{bmatrix}
          (x_1-x_n) & x_2-x_n & \cdots\\% & x_{n-1}-x_n\\
          (x_1-x_n)x_1 & (x_2-x_n)x_2 & \cdots\\% & (x_{n-1}-x_n)x_{n-1}\\
          \vdots & \vdots & \ddots\\% & \vdots\\
          (x_1-x_n)x_1^{n-2} & (x_2-x_n)x_2^{n-2} & \cdots\\% & (x_{n-1}-x_n)x_{n-1}^{n-2}
        \end{bmatrix}\\
        & =
        (-1)^{n-1} \prod_{j < n} (x_j - x_n)
        \det
        \begin{bmatrix}
          1 & 1 & \cdots & 1\\
          x_1 & x_2 & \cdots & x_{n-1}\\
          \vdots & \vdots & \ddots & \vdots\\
          x_1^{n-2} & x_2^{n-2} & \cdots & x_{n-1}^{n-2}
        \end{bmatrix}\\
        & =
        \left(\prod_{j < i = n} (x_i - x_j)\right)
        \left(\prod_{j < i < n} (x_i - x_j)\right)\\
        &= \prod_{j < i} (x_i - x_j).
      \end{align*}
  \end{itemize}
\end{solution}

\exo{3}
\begin{solution}
  \begin{itemize}
    \item
      Since $A_{12}$ must be square,
      $A_{11}$ and $A_{22}$ must have the same dimension.
      Let $p \times p$ be their dimension.
      Let $\mathbf{i}_p = (1, \ldots, p)$.
      If $\mathbf{j}_p \neq (1, \ldots, p)$,
      there will be a $k \in \{1, \ldots, p\}$ such that
      $k$ is in $\mathbf{j}_p^c$.
      Therefore,
      \(
      A^c
      \begin{pmatrix}
        \mathbf{i}_p\\
        \mathbf{j}_p
      \end{pmatrix}
      \)
      will have a zero column and a determinant of 0.

      Therefore, the sum over all $\mathbf{j}_c$
      is equal to
      \begin{align*}
        A
        \begin{pmatrix}
          (1, \ldots, p)\\
          (1, \ldots, p)
        \end{pmatrix}
        A^c
        \begin{pmatrix}
          (1, \ldots, p)\\
          (1, \ldots, p)
        \end{pmatrix}
        & = A
        \begin{pmatrix}
          (1, \ldots, p)\\
          (1, \ldots, p)
        \end{pmatrix}
        (-1)^{2\frac{p(p+1)}{2}}
        A
        \begin{pmatrix}
          (p+1, \ldots, 2p)\\
          (p+1, \ldots, 2p)
        \end{pmatrix}\\
        & = (-1)^{p(p+1)}\det(A_{11}) \det(A_{22})\\
        & = \det(A_{11}) \det(A_{22})
      \end{align*}
      since either $p$ or $p+1$ is even.
    \item
      Let $\mathbf{i}_p = (1, \ldots, p)$.
      If $\mathbf{j}_p \neq (n-p+1, \ldots, n)$,
      there will be a $k \in \{n-p+1, \ldots, n\}$ such that
      $k$ is in $\mathbf{j}_p^c$.
      Therefore,
      \(
      A^c
      \begin{pmatrix}
        \mathbf{i}_p\\
        \mathbf{j}_p
      \end{pmatrix}
      \)
      will have a zero column and a determinant of 0.

      Therefore, the sum over all $\mathbf{j}_c$
      is equal to
      \begin{align*}
        \det(A) & =
        A
        \begin{pmatrix}
          (1, \ldots, p)\\
          (n-p+1, \ldots, n)
        \end{pmatrix}
        A^c
        \begin{pmatrix}
          (1, \ldots, p)\\
          (n-p+1, \ldots, n)
        \end{pmatrix}\\
        & = A
        \begin{pmatrix}
          (1, \ldots, p)\\
          (n-p+1, \ldots, n)
        \end{pmatrix}
        (-1)^{\frac{p(1+p)}{2} + \frac{p(2n-p+1)}{2}}
        A
        \begin{pmatrix}
          (n-p+1, \ldots, n)\\
          (1, \ldots, p)
        \end{pmatrix}\\
        & = (-1)^{\frac{p + p^2 + 2np - p^2 + p}{2}}\det(A_{12}) \det(A_{21})\\
        & = (-1)^{(n+1)p}\det(A_{11}) \det(A_{22}).
      \end{align*}
  \end{itemize}
\end{solution}

\subsection{}
\nosolution

\exo{1}
Show that the set of square invertible matrices forms a multiplicative group.
\begin{solution}
  For the set of \emph{square invertible} matrices $\mathcal{M}$,
  to be a \emph{multiplicative group}, it has to verify the four following properties,
  which we will demonstrate.
  \begin{enumerate}
    \item \emph{Closure}. If $A,B \in \mathcal{M}$ then $AB \in \mathcal{M}$.
    Using the Cauchy--Binet formula, when $A$ and $B$ are $n \times n$ matrices
    \[
      \det(AB) = \sum_{\mathbf{j}_n}
      A
      \begin{pmatrix}
        \mathbf{n} \\
        \mathbf{j}_n \\
      \end{pmatrix}
      B
      \begin{pmatrix}
        \mathbf{j}_n \\
        \mathbf{n} \\
      \end{pmatrix}
      =\det\left([a_{i_k,j_l}]_{k,l=1}^{n}\right) \det\left([b_{i_k,j_l}]_{k,l=1}^{n}\right) 
      =\det(A) \det(B),
    \]
    because the only possible tuple $\mathbf{j}_n$ is $\mathbf{j}_n = (1,\dots,n)$.
    We also know that $\det(A), \det(B) \neq 0$ because $A, B \in \mathcal{M}$,
    therefore $\det(AB) \neq 0$ and thus $AB \in \mathcal{M}$.
    \item \emph{Associativity}. See Exercise~1.3.
    \item \emph{Identity element}. See Exercise~1.3.
    \item \emph{Invertibility}. Using Theorem~1.3 (``a non-singular matrix $A$ defined on a 
    field $\mathcal{F}$ has an inverse''),
    we know that if $A \in \mathcal{M}$,
    there exists a matrix $A^{-1} = (\det A)^{-1} \adj A$ 
    such that $A A^{-1} = A^{-1} A = I$.
  \end{enumerate}
  We conclude that square invertible matrices form a multiplicative group.
\end{solution}

\exo{1}
Show that the following properties hold true.

\begin{align*}
\adj(A^\top) &= \adj(A)^\top, & \adj(A^*) &= \adj(A)^*,\\
\adj(I) &= I, & \adj(kA) &= k^{n-1} \adj(A),\\
(A^\top)^{-1} &= (A^{-1})^\top, & (A^*)^{-1} &= (A^{-1})^*,\\
\det(A^{-1}) &= \det(A)^{-1}, & (AB)^{-1} &= B^{-1}A^{-1}.
\end{align*}

\begin{solution}
  \begin{itemize}
    \item $\adj(A^\top) = [A_{ji}^\top]_{i,j=1}^n = [A_{ij}]_{i,j=1}^n = (\adj A)^\top$.
    \item $\adj I = [I_{ji}]_{i,j=1}^n = [1_{ii}]_{i=1}^n = I$.
    \item Here we need to verify that the inverse of $A^\top$ is $(A^{-1})^\top$.
    More precisely
    \[
      \left\{ \begin{array}{l}
         (A^\top)(A^{-1})^\top = (A^{-1}A)^\top = I^\top = I,\\
         (A^{-1})^\top (A^\top) = (A A^{-1})^\top = I^\top = I.\\
      \end{array}\right.
    \]
    \item We know that $\det(A^{-1}) \det(A) = \det(A^{-1}A) = \det(I) = 1$
    and thus that $\det(A^{-1}) = (\det(A))^{-1}$.
    \item $\adj(A^*) = (\adj A)^*$: same as the first point except we take the
    complex conjugate instead of the transpose.
    \item Using determinant and inverse matrix properties,
    \begin{align*}
      \adj(kA) &= (kA)^{-1} \det(kA) \\
      &= k^{-1} A^{-1} k^n \det(A) \\
      &= k^{n-1} \adj(A).
    \end{align*}
    \item Here we need to verify that the inverse of $A^*$ is $(A^{-1})^*$.
    More precisely,
    \[
      \left\{ \begin{array}{l}
         (A^*)(A^{-1})^* = (A^{-1}A)^* = I^* = I,\\
         (A^{-1})^* (A^*) = (A A^{-1})^* = I^* = I.\\
      \end{array}\right.
    \]
    \item Here we need to verify that the inverse of $AB$ is $B^{-1}A^{-1}$.
    More precisely,
    \[
      \left\{ \begin{array}{l}
         (AB) (B^{-1}A^{-1}) = A I A^{-1} = A A^{-1} = I, \\
         (B^{-1}A^{-1}) (AB) = B^{-1} I B = B^{-1} B = I. \\
      \end{array}\right.
    \]
  \end{itemize}
\end{solution}

\subsection{}
The LDU decomposition of a matrix \(A\) (whose columns and rows can be permuted) is given by:
\[
P_1 A P_2 = LDU,
\]
where \(P_1\) and \(P_2\) are permutation matrices, \(L\) and \(U\) are triangular (lower and upper respectively) matrices with ones on the diagonal and \(D\) is a diagonal matrix.
Such a decomposition is the result of Gaussian elimination with complete pivoting.
Compare it to the canonical form (1.14).

\nosolution
