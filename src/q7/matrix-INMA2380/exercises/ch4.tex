\section{Eigenvalues, eigenvectors and similarity transformations}

\exo{1}
Show that the one-dimensional invariant subspaces of a matrix are those generated by its eigenvectors.

\begin{solution}
	If $A \Ima(x) \subseteq \Ima(\mathbf{x})$, since $\Ima(\mathbf{x})$ and $A\Ima(\mathbf{x})$
	are vector subspaces and $\Ima(\mathbf{x})$ is of dimension \(1\),
	that means that either $A\Ima(\mathbf{x}) = \Ima(\mathbf{x})$ or $A\Ima(\mathbf{x}) = \{0\}$.
	In the first case, it is an eigenvector of a nonzero eigenvalue.
	In the second case, it is an eigenvector of \(0\).
\end{solution}

\exo{2}
Show that if \(A \in \R^{n \times n}\) satisfies \(A^\top A = AA^\top\) (i.e., \(A\) is normal), then its real Schur form is block-diagonal with blocks
\[
A_{ii} = \alpha_i \quad \textnormal{or} \quad A_{jj} = \begin{bmatrix} \alpha_j & \beta_j \\ -\beta_j & \alpha_j \end{bmatrix}
\]
for each real eigenvalue \(\alpha_i\) and each complex eigenvalue \(\alpha_j \pm j \beta_j\).

\begin{solution}
	Let's first show that if $A$ is normal,
	it is still normal after a unitary transformation.
	\begin{align*}
	(U^\top AU)^\top (U^\top AU)
	& = U^\top A^\top AU\\
	& = U^\top AA^\top U\\
	& = U^\top AUU^\top A^\top U\\
	& = U^\top AU(U^\top AU)^\top.
	\end{align*}
	That means that the Schur form of $A$ is normal too.
	
	We can now use the same argument as in the proof of Theorem~4.4 since $A_S^* = A_S^\top$
	and apply it for all $n_1+n_2 = n$ such that $n_1$ is at the end of a block.
	This shows that the real Schur form of the matrix is block-diagonal, and combining it with results on what the blocks are in the lecture notes, we arrive at a solution.
\end{solution}

\exo{2}
Show that if \(A \in \R^{n \times n}\) is anti-symmetric (i.e., \(A = -A^\top\)), then the real Schur form is block-diagonal with blocks
\[
A_{ii} = 0 \quad \textnormal{or} \quad A_{jj} = \begin{bmatrix} 0 & \beta_j \\ -\beta_j & 0 \end{bmatrix}.
\]

\begin{solution}
	Let's first show that if $A$ is anti-symmetric,
	it is still anti-symmetric after a unitary transformation.
	\begin{align*}
	(U^\top AU)^\top
	& = U^\top A^\top U\\
	& = U^\top (-A)U\\
	& = -U^\top AU.
	\end{align*}
	That means that the Schur form of $A$ is anti-symmetric too.
	Since it is anti-symmetric, $A^\top A = -A^2 = AA^\top$, so it is also normal.
	
	By Exercise~4.2, we know that the Schur form is block-diagonal.
	But since $A_S = -A_S^\top$,
	we must have $\alpha_j = -\alpha_j$, which means that $\alpha_j = 0$
	(note that it imposes no restrictions on $\beta_j$).
	The same reasoning also implies $A_{ii} = 0$.
\end{solution}

\exo{1}
The orthogonal polynomials defined by the recurrence
\[
\left\{\begin{array}{rcl}
p_0(\lambda) & = & 1,\\
p_1(\lambda) & = & \lambda - \alpha_1,\\
p_i(\lambda) & = & (\lambda - \alpha_i) p_{i-1}(\lambda) - \beta_i^2 p_{i-2}(\lambda), \quad i = 2, \dots, n,
\end{array}\right.
\]
are the characteristic polynomials of the matrices
\[
T_i = \begin{bmatrix}
\alpha_1 & \beta_2 & & \\
\beta_2 & \alpha_2 & \ddots &\\
& \ddots & \ddots & \beta_i\\
& & \beta_i & \alpha_i \\
\end{bmatrix}.
\]
Show that the roots of two consecutive polynomials are interlacing.
The interweaving is strict if the \(\beta_j\)'s are nonzero.

\begin{solution}
	We simply use the corollary since we remove the last line and column of \(T_i\) to obtain \(T_{i-1}\).
	
	If the $\beta_j$ are nonzero, we can see that it is strict since
	if $p_i(\lambda') = p_{i-1}(\lambda') = 0$ for some hypothetical root \(\lambda'\), we have
	\[ 0 = -\beta_i^2 p_{i-2}(\lambda') \]
	so $p_{i-2}(\lambda') = 0$.
	By induction, we have $p_0(\lambda') = 0$ which is in contradiction with the base case, hence \(\lambda'\) cannot be a root of two consecutive polynomials.
\end{solution}

\exo{1}
If \(\hat{M}_{m \times n}\) is the matrix \(M_{m \times n}\) in which we have replaced a row \(\mathbf{m}_{i:}\) with a row of zeros, then the singular values \(\{\sigma_1, \dots, \sigma_n\}\) and \(\hat{\sigma}_1, \dots, \hat{\sigma}_n\) of \(M\) and \(\hat{M}\) satisfy
\[
\sigma_k^2 - \norm{\mathbf{m}_{i:}}_2^2 \leq \hat{\sigma}_k^2 \leq \sigma_k^2 + \norm{\mathbf{m}_{i:}}_2^2.
\]

\begin{solution}
	We remember that the singular values of a matrix \(A\) are the square roots of the eigenvalues of the matrix \(A^\top A\).
	We also note that \(\hat{M}^\top \hat{M} = M^\top M - \mathbf{m}_{i:}^T\mathbf{m}_{i:}\).
	Applying the result derived from Corollary~4.32 in the lecture notes, with \(A = M\), \(\Delta = -\mathbf{m}_{i:}^\top \mathbf{m}_{i:}\) and \(\hat{A} = \hat{M}\) we then find that
	\[
	\sigma_k^2 - \norm{\mathbf{m}_{i:}}_2^2 \leq \hat{\sigma}_k^2 \leq \sigma_k^2 + \norm{\mathbf{m}_{i:}}_2^2.
	\]
\end{solution}

\exo{1}
Show that
\[
e^{J_k(\lambda_0)t} = e^{(\lambda_0I_kt + J_k(0)t)} = e^{\lambda_0t} e^{J_k(0)t}.
\]

\begin{solution}
	We start by noticing that \(J_k(\lambda_0) = \lambda_0 I + J_k(0)\).
	This explains the first equality.
	The second equality is then a direct consequence of Proposition~4.39, as multiples of the identity matrix commute with any matrix (including \(J_k(0)\)).
\end{solution}

\exo{1}
From the Taylor expansion, show that
\[
\fdif{}{t} e^{At} = A e^{At} = e^{At} A.
\]

\begin{solution}
	\begin{align*}
	\fdif{}{t}e^{At}
	& = \fdif{I}{t} + \sum_{i=1}^\infty \frac{A^i}{i!} \fdif{t^i}{t}\\
	& = \sum_{i=1}^\infty \frac{iA^it^{i-1}}{i!}\\
	& = A\sum_{i=1}^\infty \frac{(At)^{i-1}}{(i-1)!}\\
	& = A \sum_{i=0}^\infty \frac{(At)^i}{i!} = A e^{At}\\
	& = \sum_{i=0}^\infty \frac{A^{i+1} t^i}{i!}\\
	& = \sum_{i=0}^\infty \frac{(At)^i}{i!} A = e^{At} A.
	\end{align*}
\end{solution}

\exo{2}
Show that, in the general case, we have
\[
\mathbf{x}(t) = e^{At}\mathbf{x}(0) + \int_0^t e^{A(t - \tau)} \mathbf{f}(\tau) \dif \tau.
\]
\begin{solution}
We can compute the derivative of \(\mathbf{x}(t)\) as follows
\begin{align*}
\dot{\mathbf{x}}(t) &= Ae^{At}\mathbf{x}(0) + \mathbf{f}(t) + \int_0^t A e^{A(t - \tau)} \mathbf{f}(\tau) \dif \tau\\
&= A \left(e^{At}\mathbf{x}(0)  + \int_0^t e^{A(t - \tau)} \mathbf{f}(\tau) \dif \tau\right) + \mathbf{f}(t)\\
&= A \mathbf{x}(t) + \mathbf{f}(t),
\end{align*}
which shows (thanks to the Leibniz integral rule) that our general solution satisfies the differential equation given in (4.22) in the lecture notes.
\end{solution}