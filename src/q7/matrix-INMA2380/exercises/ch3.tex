\section{Eigenvalues, eigenvectors and similitude}
\exo{1}
\begin{solution}
  If $A \Ima(x) \subseteq \Ima(x)$, since $\Ima(x)$ and $A\Ima(x)$
  are vector subspaces and $\Ima(x)$ if of dimension 1,
  that means that either $A\Ima(x) = \Ima(x)$ or $A\Ima(x) = \{0\}$.
  In the first case, it is an eigenvector of a nonzero eigenvalue.
  In the second case, it is an eigenvector of 0.
\end{solution}

\exo{2}
\begin{solution}
  Let's first show that if $A$ is normal,
  it is still normal after an unitary transformation.

  \begin{align*}
    (U^TAU)^T(U^TAU)
    & = U^TA^TAU\\
    & = U^TAA^TU\\
    & = U^TAUU^TA^TU\\
    & = U^TAU(U^TAU)^T.
  \end{align*}
  That means that the Schur form of $A$ is normal too.

  We can now use the same argument than the theorem~3.3 since $A_s^* = A_s^T$
  and apply it for all $n_1+n_2 = n$ such that $n_1$ is a the end of a block.
\end{solution}

\exo{2}
\begin{solution}
  Let's first show that if $A$ is anti-symetric,
  it is still anti-symetric after an unitary transformation.

  \begin{align*}
    (U^TAU)^T
    & = U^TA^TU\\
    & = U^T(-A)U\\
    & = -U^TAU.
  \end{align*}
  That means that the Schur form of $A$ is anti-symetric too.
  Since it is anti-symetric, $A^TA = -A^2 = AA^T$ so it is also normal.

  By the exercise~3.2, we know that the Schur form is block diagonal.
  But since $A_s = -A_s^T$,
  we must have $\alpha_j = -\alpha_j$ which means that $\alpha_j = 0$.
  Same for $A_{ii}$.
\end{solution}

\exo{1}
\begin{solution}
  We simply use the corrolary since we remove the last line and column.

  If the $\beta_i$ are nonzero, we can see that it is strict since
  if $p_i(\lambda) = p_{i-1}(\lambda) = 0$, we have
  \[ 0 = -\beta_i^2 p_{i-2}(\lambda) \]
  so $p_{i-2}(\lambda) = 0$.
  By induction, we have $p_0(\lambda) = 0$ which is absurd.
\end{solution}

\exo{2}
\begin{solution}
  See syllabus.
  You should obtain
  \[ \sigma_k^2 - \|m_{i:}\|_2^2 \leq \hat{\sigma}_k^2 \leq \sigma_k^2 + \|m_{i:}\|_2^2 \]
  and not
  \[ \sigma_k^2 - \|m_{i:}\|_2^2 \leq \hat{\sigma}_k^2 \leq \sigma_k^2 \]
  like written in the syllabus.
\end{solution}

\exo{3}
\begin{solution}
  See syllabus.
\end{solution}

\exo{2}
\begin{solution}
  See syllabus.

  \begin{align*}
    \sum_{i=0}^\infty \frac{(A+B)^i}{i!}
    & = \sum_{i=0}^\infty \frac{\sum_{k=0}^i \frac{i!}{k!(i-k)!} A^{i-k}B^k}{i!}\\
    & = \sum_{i=0}^\infty \sum_{k=0}^i \frac{A^{i-k}}{(i-k)!} \frac{B^k}{k!}\\
    & =
    \left(\sum_{i=0}^\infty \frac{A^i}{i!}\right)
    \left(\sum_{i=0}^\infty \frac{B^i}{i!}\right).
  \end{align*}
  because each term $A^aB^b$ is present with the term $\frac{1}{a!b!}$.
\end{solution}

\exo{1}
\begin{solution}
  We have

  \begin{align*}
    \begin{pmatrix}
      \lambda_0 & 1      &        & \\
                & \ddots & \ddots & \\
                &        & \ddots & 1\\
                &        &        & \lambda_0\\
    \end{pmatrix}
    & =
    \lambda_0 I + N
  \end{align*}
  for
  \[
    N =
    \begin{pmatrix}
      & 1 &        & \\
      &   & \ddots & \\
      &   &        & 1\\
      &   &        & \\
    \end{pmatrix}.
  \]
  The rest is a simple consequence of the exercise~3.7.

  It is important to note for the next page that $N^n = 0$.
  This is indeed a consequence of the fact that $N = J - \lambda_0I$
  where $J$ is a Jordan block of $\lambda_0$ of size $n$.
  $J$ is therefore a matrix with only one eigenvalue $\lambda_0$
  of algebraic multiplicity $n$ but geometric multiplicity $1$.
  Hence the whole set $\mathbb{C}^n$ is an invariant subspace of $N$
  which means that $(J - \lambda_0I)^n = 0$.

  We can see for example for $n = 4$ that

  \begin{align*}
    N & =
    \begin{pmatrix}
      0 & 1 & 0 & 0\\
      0 & 0 & 1 & 0\\
      0 & 0 & 0 & 1\\
      0 & 0 & 0 & 0
    \end{pmatrix}\\
    N^2 & =
    \begin{pmatrix}
      0 & 0 & 1 & 0\\
      0 & 0 & 0 & 1\\
      0 & 0 & 0 & 0\\
      0 & 0 & 0 & 0
    \end{pmatrix}\\
    N^3 & =
    \begin{pmatrix}
      0 & 0 & 0 & 1\\
      0 & 0 & 0 & 0\\
      0 & 0 & 0 & 0\\
      0 & 0 & 0 & 0
    \end{pmatrix}\\
    N^4 & =
    \begin{pmatrix}
      0 & 0 & 0 & 0\\
      0 & 0 & 0 & 0\\
      0 & 0 & 0 & 0\\
      0 & 0 & 0 & 0
    \end{pmatrix}
  \end{align*}
  We can see that $e_1$ is an eigenvector of $\lambda_0$ but
  $e_2$, $e_3$, $e_4$ are not.
  \begin{itemize}
    \item $Je_2 = \lambda_0 e_2 + e_1$
      so $(J - \lambda_0 I)e_2 = e_1$.
    \item $(J - \lambda_0 I)e_3 = e_2$ and $(J - \lambda_0 I)^2e_3 = e_1$.
    \item $(J - \lambda_0 I)e_4 = e_3$, $(J - \lambda_0 I)^2e_3 = e_2$
      and $(J - \lambda_0 I)^3e_4 = e_1$.
  \end{itemize}

  Note that $J^4 \mathbb{C}^n = \mathbb{C}^n$ if $\lambda_0 \neq 0$.
  It is not to be mistaken from $(J - \lambda_0 I)^4 \mathbb{C}^n = \{0\}$.
\end{solution}

\exo{1}
\begin{solution}
  \begin{align*}
    \fdif{}{t}e^{At}
    & = \fdif{I}{t} + \sum_{i=1}^\infty \frac{A^i}{i!} \fdif{t^i}{t}\\
    & = \sum_{i=1}^\infty \frac{iA^it^{i-1}}{i!}\\
    & = A\sum_{i=1}^\infty \frac{(At)^{i-1}}{(i-1)!}\\
    & = A\sum_{i=0}^\infty \frac{A^i}{i!}\\
    & = \sum_{i=0}^\infty \frac{A^{i+1}}{i!}\\
    & = \left(\sum_{i=0}^\infty \frac{A^i}{i!}\right)A.
  \end{align*}
\end{solution}

\exo{1}
\begin{solution}
  See syllabus.
\end{solution}
