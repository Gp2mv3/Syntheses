\documentclass[fr,license=none]{../../../eplsummary}

\hypertitle{Théorie des matrices}{7}{INMA}{2380}
{Yannick Tivisse}
{Paul van Dooren}

\section{Algèbre de matrices}
\subsection{Somme et produits}
\begin{itemize}
\item Règles basiques, pipi de chat. Attention aux définions d'anneaux, champ, ...
\item Produit de Hadamard $A\odot B$
\item Produit de Kronecker $A\otimes B$
\item Propriétés basiques, matrices particulières (Triangulaire, Toeplitz,...)
\end{itemize}
\subsection{Transposée et transposée conjuguée}
\begin{itemize}
\item Matrices (anti-)hermitiennes, (anti-)symetriques
\item Matrices unitaires, orthogonales, normales
\end{itemize}
\subsection{Déterminant, inverse et rang d'une matrice}
\begin{itemize}
\item Quasi-diagonales
\item Propriétés des transformations élémentaires
\item Mineurs, cofacteurs
\item Généralisation du point précédent aux ordres supérieurs
\item Matrice de permutation, mise-à-échelle, élimination\\
\end{itemize}

\paragraph{Théorème 1.4}
Toute matrice $A_{m\times n}$ dont les éléments font partie d'un champ peuvent être réduite par transformations inversibles(et même élémentaires) de ses lignes et colonnes à une forme du type:
$$RAQ = \begin{bmatrix}
I_r &  \\
 & 0_{(m-r)\times (n-r)}
\end{bmatrix} $$

\paragraph{Théorème 1.5}
Le rang d'une matrice $A$ est égal à l'ordre du plus grand de ses mineurs non nuls.

\paragraph{Théorème 1.6}
Complément de Schur. Soit $A_{n\times n}$ une sous matrice inversible de $M_{(n+p)\times (n+m)} = \begin{bmatrix}
A & B \\
C & D
\end{bmatrix}$. Alors $\text{rang} \ M = n + \ \text{rang}(D-CA^{-1}B)$. La matrice $D-CA^{-1}B$ est appelé le complément de Schur.

\section{Systèmes linéaires}
\subsection{Espaces vectoriels et systèmes linéaires}
\paragraph{Théorème 2.1} Si $A_{n\times n}$ est inversible, alors le système $Ax = y$ possède une solution $x$ unique $\forall y \in \mathcal{F}^n$, et $\text{Ker} A = \{0\}$, $\text{Im} A = \mathcal{F}^n$

\paragraph{Lemme 2.2} Si $\mathcal{S}_k \subset \mathcal{S}_l$ sont des sous-espaces de dimension $k$ et $l$ telles que $k < l$, alors une base $\{a_1,\dots,a_k\}$ de $\mathcal{S}_k$ peut toujours être complétée pour former une base $\{a_1,\dots,a_l\}$ de $\mathcal{S}_l$

\paragraph{Théorème 2.3} Soit $R_{n\times n}$ une matrice inversible et $\mathcal{S} \subset \mathcal{F}^n$ un sous-espace linéaire. On a alors \\
(a) dim $R\mathcal{S}$ = dim $\mathcal{S}$\\
(b) Ker $RA$ = Ker $A$\\
(c) Im $AR^{-1}$ = Im $A$\\
(d) Im $RA$ = $R$ Im $A$\\
(e) Ker $AR^{-1}$ = $R$Ker $A$

\paragraph{Théorème 2.4} Si $A_{m\times n} = R \begin{bmatrix}
I_r & 0 \\
0 & 0
\end{bmatrix} Q^{-1}$ pour certaines matrices $R$ et $Q$ inversibles, alors Im($A$) = span$\{r_{:1},\dots,r_{:n}\}$ et Ker($A$) = span$\{q_{:,r+1},\dots,q_{:n}\}$

\paragraph{Corollaire} Le rang d'une matrice $A_{m\times n}$ est égal à la dimension de son image Im($A$)

\paragraph{Théorème 2.5} Inégalité de Schwarz. $|(x,y)| \leq ||x||\cdot ||y||$

\paragraph{Théorème 2.6} Soit $\{x_1,\dots,x_r\}$ une base quelconque de $\mathcal{S} \subset \mathcal{V}$. Alors, les vecteurs $\{y_1,\dots,y_r\}$ définis de la façon suivante: $y_1 = x_1$,
$$ y_p = x_p - \sum_{j = 1}^{p-1} \frac{(y_j,x_p)}{y_j,y_j} y_j$$
forment une base orthogonale de $\mathcal{S}$.

\paragraph{Théorème 2.7} Factorisation QR. Toute matrice $A_{n\times r}$ de rang colonne plein, possède une factorisation $A_{n\times r} = Q_{n\times r} R_{r\times r}$ telle que $Q^* q = I_r$ et $R$ est triangulaire supérieure avec une diagonale positive.

\paragraph{Lemme 2.8} Si $\mathcal{S}$ est un sous-espace de dimension $r= \text{dim} \mathcal{S}$, alors $\mathcal{S}^\perp$ est un sous-espace de dimension $n-r$.

\paragraph{Lemme 2.9} Soit $U_1$ une isométrie partielle ($U_1^* U_1 = I_r$), alors il existe toujours une matrice $U_2$ telle que $U = [U_1|U_2]$ est une matrice unitaire.

\subsection{Diagonalisation par transformation unitaires}


\paragraph{Définition 2.2} Valeurs propres et vecteurs propres associés

\paragraph{Théorème 2.10} Toute matrice hermitienne peut-être diagonalisée par une transformation de similitude unitaire: $U^*AU = \text{diag}\{\lambda_1,\dots,\lambda_n\}$

\paragraph{Théorème 2.11} Les valeurs propres d'une matrice hermitienne $A$ sont invariantes sous similitudes unitaires . Les classes d'équivalences définies par ce groupe de transformations possèdent uen forme canonique unique qui est la matrice diagonale $\Lambda$ des valeurs propres ordonnées de $A$.

\paragraph{Théorème 2.12} Singular Value Decomposition (SVD). Pour toute matrice $A_{m\times n}$ complexe, il existe des transformations unitaires $U^*U = I_m, V^*V = I_n$ telles que $A = U \Sigma V^*$, avec des valeurs singulières $\sigma_i$ réelles, positives et ordonnées.

\paragraph{Théorème 2.13} Les valeurs singulières d'une matrice arbitraire $A_{m\times n}$ sont invariantes sous transformations unitaires à gauche et à droite. Les classes d'équivalences définies par ce groupe de tranformations possèdent une forme canonique unique qui est la matrice quasi-diagonale $\Sigma$ des valeurs singulières ordonnées de $A$.

\subsection{Application de la SVD}
\subsubsection{Décomposition polaire}

\paragraph{Théorème 2.14} Toute matrice carrée $A_{n\times n}$ possède une décomposition polaire $A = HQ$ où $H = H^* \geq 0$ et $Q^*Q =I_n$

\subsubsection{Angles canoniques}

\paragraph{Théorème 2.15} Pour deux sous-espaces $\mathcal{S}_i$, il existe toujours des bases orthonormées $\hat{\mathcal{S}}_i$ satisfaisant $$\hat{\mathcal{S}}^*_1\hat{\mathcal{S}}_2 = \begin{bmatrix}
\Sigma_r & 0 \\
0 & 0
\end{bmatrix}$$

\paragraph{Lemme 2.16} Etant donné deux base $X$ et $Y$ de dimension $k$, on peut les transformer au moyen de transformations inversibles $T_x$ et $T_y$ de telle sorte que $\hat{X} = XT_x$ et $\hat{Y} = YT_y$ satisfont
$$\begin{bmatrix}
\hat{X}^* \\
\hat{Y}^*
\end{bmatrix}
\begin{bmatrix}
\hat{X} & \hat{Y}
\end{bmatrix}
=
\begin{bmatrix}
I_k & \Sigma \\
\Sigma & I_k
\end{bmatrix} $$, où $\Sigma$ est diagonale réelle avec diagonale décroissante.

\subsubsection{Analyse en composantes principales}
Pas matière :)

\subsubsection{Normes unitairement invariantes}
Pas de théorème ou définition,mais quelques exemples importants (norme de frobenius,...)

\subsubsection{Systèmes duaux}

\paragraph{Théorème 2.18}On a les relations suivantes:\\
Ker ($A$) = Im$\left(\tilde{A}\right)^\perp$\\
Ker $\left(\tilde{A}\right)$ = Im $(A)^\perp$\\
Im $(A)$ = Ker$\left(\tilde{A}\right)^\perp$\\
Im $\left(\tilde{A}\right)$ = Ker$(A)^\perp$

\paragraph{Lemme 2.19} Deux relations:\\
Im $A = \mathcal{Y} \Leftrightarrow r = m\Leftrightarrow \exists A^r:AA^r = I_m \Leftrightarrow$ il existe une solution de $Ax=y$ pour tout y (surjectivité)\\
Ker $A = \{0\} \Leftrightarrow r=n \Leftrightarrow \exists A^l : AA^l = I_n \Leftrightarrow $ Le système Ax=y possède une solution unique (injectivité)

\paragraph{Définition 2.3} Si deux espaces possèdent une intersection nulle, alors leur somme est appelée directe et notée $\otimes$. Notée $\otimes^\perp$ si les espaces sont en plus orthogonaux entre eux.

\paragraph{Lemme 2.20} Tout vecteur $x \in \mathcal{X}_1 \otimes \mathcal{X}_2$ possède une décomposition unique $x = x_1 + x_2$ avec $x_i \in \mathcal{X}_i$

\paragraph{Théorème 2.21} Dans le système de coordonées (2.39),(2.40), la matrice $A$ prend la forme $A = \begin{bmatrix}
A_{11} & 0 \\
0 & 0
\end{bmatrix} $ où $A_{11}$ est bijectif.

\subsubsection{Projecteurs et inverses généralisés}

\paragraph{Théorème 2.22} L'inverse de Moore-Penrose $A^I$ d'une matrice $A$ est unique et donnée par:
$$ A^I = V \begin{bmatrix}
\Sigma_r^{-1} & 0 \\
0 & 0
\end{bmatrix} U^*$$

\subsection{Algorithmes constructifs}

\subsubsection{Transformations de Givens}
Voir l'algo
\subsubsection{Transformations de Householder}
Voir l'algo
\subsubsection{Factorisation QR}

\paragraph{Théorème 2.23} Toute matrice $A_{m\times n}$ peut-être transformée, par une transformation unitaire à gauche, de telle sorte que
$$ Q^*_l A = \begin{bmatrix}
A_1 \\
0
\end{bmatrix} $$ où $A_1 \in \mathcal{C}^{r\times n}$ possède des lignes linéairement indépendantes

\paragraph{Théorème 2.24}  Toute matrice $A_{m\times n}$ peut-être transformée, par une transformation unitaire à droite, de telle sorte que
$$ AQ_r = \begin{bmatrix}
A_1 0
\end{bmatrix} $$ où $A_1 \in \mathcal{C}^{m\times r}$ possède des colonnes linéairement indépendantes

\paragraph{Théorème 2.25} Toute matrice $A_{m\times n}$ peut être transformée, par transformations unitaires à gauche et à droite, de telle sorte que
$$ U^* A V = \begin{bmatrix}
A_{11} & 0 \\
0 & 0
\end{bmatrix} $$ où $A_{11}$ est une matrice carrée de plein rang.

\subsubsection{Complexité et aspects numériques}
Voir algos MatLab

\subsubsection{Problèmes de moindres carrés}

\paragraph{Théorème 2.26} Pour une matrice $A_{m\times n}$ de rang $r$, la solution du système d'équations $Ax=y$ possède les propriétés suivantes:\\
- $m=n=r : x = A^{-1}y$ est unique\\
- $m=r<n:x=A^ry$ est une solution pour toute matrice $A^r$ tq $AA^r = I_m$. De plus , $x=A^I y$ est la solution de norme minimale.\\
- $m>r=n : x = A^l y$ est une solution si et seulement si $y \in \text{Im} A$ pour toute matrice $A^l$ telle que $A^l A = I_n$. De plus, $x = A^Iy$ est la solution aux moindres carrés.\\
- $m>r<n: x = A^I y$ est la solution aux moindres carrés et de norme minimale.

\subsubsection{Application type et régulariqation}

\paragraph{Lemme 2.27}
$$\sigma_i \left( \begin{bmatrix}
A \\
\delta I
\end{bmatrix} \right) = \sqrt{\sigma_i^2(A) + \delta^2} \geq \max\{\sigma_i (A),\delta\}$$

\subsubsection{Moindres carrés récursifs}
\paragraph{Updating}
\paragraph{Windowing}

\subsection{Problèmes variationnels}

\paragraph{Théorème 2.28} Le quotient de Rayleigh est réel et satisfait $\lambda_{\text{min}} (H) \leq R(x) \leq \lambda_{\text{max}} (H)$

\paragraph{Corollaire} $\lambda_n = \min_{x\neq 0} R(x)$; $\lambda_1 = \max_{x\neq 0} R(x)$

\paragraph{Théorème 2.29} Le quotient de Rayleigh $R(x)$ possède comme valeurs stationnaires les valeurs propres $\lambda_i$ de $H$. Les arguments correspondants sont les vecteurs propres $x_i$.

\paragraph{Lemme 2.30} Soit $\mathcal{S}_j$ un espace de dimension $j$ de $\mathcal{X}$. $\min_{0\neq x \in \mathcal{S}_j} R(x) \leq \lambda_j$; $\max_{0\neq x \in \mathcal{S}_j} R(x) \geq \lambda_{n-j+1}$

\paragraph{Théorème 2.31} Courant-Fisher. Pour toute matrice hermitienne $H$, le quotient de Rayleigh $R(x) = \frac{(Hx,x)}{(x,x)}$ satisfait les équations suivantes.
$$\lambda_j = \max_{\mathcal{S}_j} \min_{0\neq x \in \mathcal{S}_j} R(x)$$
$$\lambda_{n-j+1} = \min_{\mathcal{S}_j} \max_{0\neq x \in \mathcal{S}_j} R(x)$$ où $\mathcal{S}_j$ sont des espaces de dimension $j$.

\paragraph{Théorème 2.32} Pour une matrice quelconque $A_{m\times n}$, les valeurs singulières peuvent êtres définies par
$$\sigma_j(A) = \max_{\mathcal{S}_j} \min_{0\neq x \in \mathcal{S}_j} \frac{||Ax||_2}{||x||_2}$$
$$\sigma_{n-j+1}(A) = \min_{\mathcal{S}_j} \max_{0\neq x \in \mathcal{S}_j} \frac{||Ax||_2}{||x||_2}$$où $\mathcal{S}_j$ sont des espaces de dimension $j$.

\paragraph{Théorème 2.33}Soit $A_{m\times n}$ une matrice de rang $r$. Alors, la meilleure approximation de $A$ par une matrice $B$ de rang $s<r$ satisfait
$$ \min_{\text{rang} B \leq s} ||A - B||_2 = \sigma_{s+1} (A)$$

\paragraph{Théorème 2.34} Eckart Young. Soit $A_{m\times n}$ une matrice de rang $r$. Alors la meilleure approximation de $A$ par une matrice $B$ de rang $s<r$ satisfait l'égalité suivante:
$$ \min_{\text{rang} B \leq s} ||A-B||_F^2 = \sigma_{s+1}^2 + \dots + \sigma_r^2$$

\section{Valeurs propres, vecteurs propres et similitudes}

\subsection{Similitudes}

\paragraph{Lemme 3.1} Les valeurs propres d'une matrices sont invariantes sous similitudes.

\paragraph{Théorème 3.2} Schur. Toute matrice $A \in \mathbb{C}^{n\times n}$ peut être triangularisée par similitudes unitaires où la forme résultante $A_s$ possèdes les valeurs propres de $A$ sur la diagonale.

\paragraph{Definition 3.1} Une matrice normale est une matrice qui satisfait $AA^* = A^* A$.

\paragraph{Théorème 3.3} Une matrice est normale si et seulement si elle est diagonalisable sous similitude unitaire

\subsection{Espaces Invariants}

\paragraph{Definition 3.2} $\mathcal{X}$ est un sous-espace invariant sous l'opérateur $A$ si $A\mathcal{X} \subset \mathcal{X}$

\paragraph{Théorème 3.4} Soit $\mathcal{X}$ un sous-espace, $X$ une base de $\mathcal{X}$, et $X_c$ une complétion de $X$ telle que $T=[X|X_c]$ soit non singulière. Alors les trois énoncés sont équivalents:\\
1. $A\mathcal{X} \subset \mathcal{X}$\\
2. $AX = XA_{11}$\\
3.$ T^{-1}AT = \begin{bmatrix}
A_{11} & A_{12} \\
0 & A_{22}
\end{bmatrix} $

\paragraph{Lemme 3.5} Une matrice réelle $A \in \mathbb{R}^{n\times n}$ possède toujours un espace invariant $\mathcal{X}$ réel de dimension 1 ou 2.

\paragraph{Théorème 3.6} Schur réel. Toute matrice \textbf{réelle} $A\in \mathbb{R}^{n\times n}$ peut être quasi-triangularisée par similitude orthogonale \textbf{réelle}, avec, sur la diagonale, des blocs de dimension 1x1 ou 2x2.

\subsection{Forme de Jordan}
\paragraph{Théorème 3.8} Toute matrice $A \in \mathbb{C}^{n\times n}$ possède une forme de Schur partitionnée en blocs où chaque $A_{ii}$ ne possède qu'une valeur propre (multiple) distincte des autres.

\paragraph{Théorème 3.9} Weyr. Toute matrice $A \in \mathbb{C}^{n\times n}$ qui ne possède qu'une valeur propre $\lambda_0$ (multiple) peut être transformée par similitude \textbf{unitaire} en une forme
$$ U^*AU = \begin{bmatrix}
\lambda_0 I_{\rho_1} & A_{12} & \dots & A_{1j} \\
 & \lambda_0 I_{\rho_2} & \ddots & \vdots \\
 &  & \ddots & A_{j-1,j} \\
 &  &  & \lambda_0 I_{\rho_j}
\end{bmatrix} $$où les blocs $A_{i,i+1}$ au dessus de la diagonales sont de rang colonne $\rho_{i+1}$ plein.

\paragraph{Lemme 3.10} Sylvester. Si $M = \begin{bmatrix}
A & C \\
0 & B
\end{bmatrix} ; A\in \mathbb{C}^{n_1\times n_1} ; B \in \mathbb{C}^{n_2\times n_2}$, et si $A$ et $B$ n'ont pas de valeurs propres communes, alors il existe une matrice $X$ telle que $AX-XB+C =0$

\paragraph{Corollaire} Deux matrices $A$ et $B$ sont similaires si et seulement si elles ont la même forme de Jordan.

\subsection{Dérivées de valeurs propres}

Pas dans la matière :)

\subsection{Calcul de valeurs propres}
\paragraph{Algorithme 3.1} Méthode des puissances

\paragraph{Théorème 3.15} Si $A \in \mathbb{C}^{n\times n}$ possède des valeurs propres distinctes et de modules différents, alors l'algorithme 3.1 produit un vecteur $q_{(k)}$ qui converge (à une phase près) vers $x_1$, pour autant que le vecteur $q_{(0)}$ ait une composante non nulle dans la direction de $x_1$.

\paragraph{Algorithme 3.2} Méthode des puissances version matricielle.

\paragraph{Théorème 3.16} Si $A \in \mathbb{C}^{n\times n}$ possède des valeurs propres groupées en deux groupes distincts, alors l'algo 3.2 produit une base $Q_{(k)}$ qui converge à une transformation unitaire près vers $X_1$, la base orthonormale de l'espace invariant de $\lambda_1,\dots, \lambda_p$, pour autant que la base $Q_{(0)}$ ait une intersection de dimension $p$ avec $X_1$.

\paragraph{Algorithme 3.3} QR de Francis.

\paragraph{Théorème 3.17} L'algorithme QR appliqué à une matrice possédant des valeurs propres distinctes et de modules différents converge vers une forme triangulaire supérieure
$$ \lim_{k\rightarrow\infty} A_k = \begin{bmatrix}
\lambda_1 & \dots & \times \\
  & \ddots & \vdots \\
0 &   & \lambda_n
\end{bmatrix} = A_s$$ avec, sur la diagonale, des valeurs propres ordonnées de façon décroissante.

\paragraph{Lemme 3.18} Hessenberg. On peut toujours construire une transformation unitaire $Q_0$ telle que $Q_0^* A Q_0$ ait la forme de Hessenberg.

\paragraph{Lemme 3.19} La factorisation QR d'une matrice Hessenberg $A_H$ s'effectue au moyen de $n-1$ transformations de Givens. De plus, le produit RQ des facteurs est à nouveau une forme de Hessenberg.

\paragraph{Algorithme 3.4} Shifts.

\subsection{Estimation de valeurs propres}

\paragraph{Définition 3.3} Le champ des valeurs d'une matrice $A\in \mathbb{C}^{n\times n}$ est l'ensemble
$$\mathcal{F} = \left\lbrace \frac{x^* A x}{x^*x}, x \neq 0 \right\rbrace$$

\paragraph{Théorème 3.20} Hausdorff Toeplitz. $\mathcal{F}(A)$ est un ensemble convexe et compact de $\mathbb{C}$ et contient les valeurs propres de $A$.

\paragraph{Lemme 3.21} Le champ des valeurs d'une matrice est invariant sous similitudes unitaires: $\mathcal{F}(A) = \mathcal{F}(U^*AU)$

\paragraph{Corollaire} Si $A$ est normale, alors$\mathcal{F}(A)$ est la combinaison convexe des valeurs propres de $A$.

\paragraph{Corollaire} Si $A$ est hermitienne, alors $\mathcal{F}(A)$ est l'intervalle de l'axe réel $[\lambda_{\text{min}}(A), \lambda_{\text{max}} (A)]$

\paragraph{Théorème 3.22} Soit $\alpha + j \beta$ un point complexe de $\mathcal{F}(A)$. Alors
$$ \lambda_{\text{min}} \left(\frac{A + A^*}{2} \right) \leq \alpha \leq \lambda_{\text{max}} \left( \frac{A+A^*}{2} \right)$$
$$ \lambda_{\text{min}} \left(\frac{A - A^*}{2j} \right) \leq \beta \leq \lambda_{\text{max}} \left( \frac{A-A^*}{2j} \right)$$
Ces intervalles définissent le plus petit rectangle du plan complexe contenant $\mathcal{F}(A)$.

\paragraph{Corollaire} Bendixon. Les valeurs propres d'une matrice $A$ se trouvent dans le rectangle défini au théorème 3.22.

\paragraph{Théorème 3.23} Soit $A \in \mathbb{C}^{n\times n}$. Alors les valeurs propres de $A$ de trouvent dans l'union des disques de Gersgorin.

\subsection{Estimation des valeurs propres d'une matrice hermitienne}

\paragraph{Définition 3.4} Les valeurs propres contraintes de $A$ sont définies comme étant les valeurs propres de $\hat{A} = Q^*AQ$ où $Q$ est une base de l'espace des vecteurs $x$ relative à la caractérisation des quotients de Rayleigh contraints correspondants.

\paragraph{Théorème 3.24} Soit $\{\lambda_1,\dots,\lambda_n\}$ et $\{\mu_1,\dots,\mu_{n-r}\}$ les valeurs propres, ordonnées de façon décroissante, des matrices hermitiennes $A$ et $\hat{A}$. Alors	$ \lambda_{i+r} \leq \mu_i \leq \lambda_i$, $i = 1,\dots,n-r$.

\paragraph{Corollaire}Soit $\hat{A}$ la matrice hermitienne $A$ à laquelle on a supprimé une ligne et la colonne correspondante. Alors les valeurs propres ordonnées $\{\lambda_1,\dots,\lambda_n\}$ et $\{\mu_1,\dots,\mu_{n-1}\}$ de $A$ et $\hat{A}$ s'entrelacent.

\paragraph{Théorème 3.25} Soit $\hat{A} = A + \Delta$ une matrice hermitienne perturbée. Si les valeurs propres ordonnées de $A$, $\Delta$ et $\hat{A}$ sont respectivement $\{\lambda_1,\dots,\lambda_n\}$, $\{\delta_1,\dots,\delta_n\}$ et $\{\mu_1,\dots,\mu_n\}$, alors $\mu_{r+s-1} \leq \lambda_r + \delta_s$, $r+s-1 \leq n$

\paragraph{Corollaire} Si $\Delta$ est une petite perturbation de la matrice $A$, on a $ \lambda_k  \delta_n \leq \mu_k \leq \lambda_k + \delta_1$

\subsection{Fonctions de matrices}

\paragraph{Théorème 3.26} Si $A$ possède la décomposition de Jordan $A = TJT^{-1}$, alors $p(A) = Tp(J)T^{-1}$ et donc $\lambda_i(p(A)) = p(\lambda_i(A))$

\paragraph{Lemme 3.27} La $d^e$ puissance d'un bloc de Jordan $J_k(\lambda_0)$ donne pour $d \geq k-1$: Voir p.87.

\paragraph{Théorème 3.28} Pour toute fonction $f(\lambda)$ définie en les valeurs propres $\lambda_i$ de $A_{n\times n}$ et telle que ses dérivées $f^{(k)}(\lambda_i), k<n$, le soient aussi, on a que: voir p.87.

\paragraph{Théorème 3.29} Caley-Hamilton. Le polynôme caractéristique de $A$ satisfait $\chi(A) = 0$ et le polynôme de degré minimal qui satisfait cette relation est le polynôme minimal $m(\lambda)$ : $m(A) =0$.

\section{Inertie et stabilité de matrices}

\subsection{Congruence et Inertie}

\paragraph{Définition 4.1} Soit $H=H^*$, alors  $\hat{H} = THT^*$ est congruent avec $H$ si $T$ est inversible.

\paragraph{Définition 4.2} L'inertie d'une matrice hermitienne $H$ est le triplet In$(H) =\{\pi_H, \nu_h, \delta_h\}$. La multiplicité des valeurs propres est prise en compte.

\paragraph{Théorème 4.1} Toute matrice hermitienne est congruente à une matrice diagonale définie par son inertie: $THT^* = diag\{I_{\pi_H},-I_{\nu_H}, 0_{\delta_H}\}$

\paragraph{Corollaire} Si $A=A^*$ et $B=B^*$ ont la même inertie, alors elles sont congruentes.

\paragraph{Théorème 4.2} Si $A$ et $B$ sont des matrice hermitiennes congruentes, alors elles ont la même inertie.

\paragraph{Corollaire} La forme diagonale $diag\{I_{\pi_H},-I_{\nu_H}, 0_{\delta_H}\}$ est une forme canonique d'une matrice hermitienne sous congruences, et son inertie est l'unique invariant sous ces transformations.

\paragraph{Définition 4.3} Une matrice hermitienne est dite définie positive si $x^*Hx > 0 \ \forall x \neq 0$

\paragraph{Algorithme 4.1} Construction d'une décomposition $H = L\text{diag}\{D_{11},\dots,D_{kk}\}L^*$ de type Cholesky.

\paragraph{Algorithme 4.2} Récurrence calculant les déterminants des mineurs principaux d'une forme de Hessenberg d'une matrice hermitienne (Forme tridiagonale).

\paragraph{Théorème 4.3} Le nombre de changement de signes de la séquence $\{d_0,\dots,d_n\}$ donne le nombre de valeurs propres de $H$ strictement inférieures à 0.

\subsection{Stabilité des systèmes dynamiques}

\paragraph{Théorème 4.4} Si $A$ satisfait $A^*P + PA = -Q$ avec $P>0$ et $Q>0$, alors Re$\lambda_i(A)<0$ pour toutes les valeurs propres de $A$.

\paragraph{Théorème 4.5} Si $A$ satisfait à l'équation $P-A^*PA = Q$ avec $P>0$ et $Q>0$, alors $|\lambda_i(A)|<1$ pour toute valeur propre de $A$.

\subsection{Robustesse de systèmes dynamiques}

\paragraph{Lemme 4.6} La perturbation $\Delta$ de norme minimale telle que $A+\Delta$ ait une valeur propre en $\lambda^*$ est de norme $\text{min} ||\Delta||_2 = \sigma_{\text{min}} (A-\lambda^*I)$

\paragraph{Théorème 4.7} Si $A$ est stable (Re$\lambda_i < 0$) et $||\Delta||_2$ est bornée par $ ||\Delta||_2 < \text{min}_{\lambda^* = j\omega} \sigma_{\text{min}} (A-\lambda^*I)$, alors $A+\Delta$ est stable.

\paragraph{Si $A$ est stable ($|\lambda_i| < 1$} et $||\Delta||_2$ est borné par $||\Delta||_2 < \text{min}_{\lambda^* = e^{j\theta}} \sigma_{\text{min}} (A-\lambda^*I)$, alors $A+\Delta$ reste stable.

\section{Matrices polynomiales}

\paragraph{Définition 5.1} Une matrice unimodulaire est une matrice polynomiale dont le déterminant est une constante non nulle.

\paragraph{Théorème 5.1} Pour tous polynômes $a(\lambda)$ et $b(\lambda)$, il existe une transformation unimodulaire $U(\lambda)$ telle que $\begin{bmatrix}
a(\lambda) \\
b(\lambda)
\end{bmatrix}=U(\lambda)\begin{bmatrix}
d(\lambda) \\
0
\end{bmatrix}$, où $d(\lambda) = \text{pgcd}(a(\lambda),b(\lambda))$

\paragraph{Corollaire} Il existe une transformation unimodulaire $Q(\lambda)$ telle que $Q(\lambda)\begin{bmatrix}
p_1(\lambda) \\
\vdots \\
\vdots \\
p_n(\lambda)
\end{bmatrix} = \begin{bmatrix}
d(\lambda) \\
0 \\
\vdots \\
0
\end{bmatrix} $

\paragraph{Théorème 5.2} Hermitte. Toute matrice polynomiale peut être transformée en une forme quasi triangulaire $M(\lambda)P(\lambda)N(\lambda)$, où $M$ et $N$ sont des matrices unimodulaires, et où $N$ est en plus une matrice de permutation.

\paragraph{Algorithme 5.1} Preuve du théorème de Smit.

\paragraph{Théorème 5.3} Toute matrice polynomiale peut être réduite par transformations unimodulaires $M(\lambda)$ et $N(\lambda)$ en une forme quasi-diagonale, où $e_i(\lambda)$ divise $e_{i+1}(\lambda), i=1,\dots,r$

\paragraph{Définition 5.2} La rang normal d'une matrice polynomiale $P(\lambda)$ est l'ordre du plus grand de ses mineurs non nuls

\section{Matrices positives}

\paragraph{Définition 6.1} Une matrice $A$ non négative est dite irréductible s'il n'existe pas de permutation symétrique telle que $PAP^T = \begin{bmatrix}
A_{11} & A_{12} \\
0 & A_{22}
\end{bmatrix}$

\paragraph{Théorème 6.1} Si $A \geq 0$ est irréductible, alors $(I+A)^{n-1}>0$

\paragraph{Théorème 6.2} Soit $A \geq 0$ irréductible.  Alors $r$ est une valeur propre de $A$ et chaque vecteur $x$ extrémal est un vecteur positif et un vecteur propre de $A$

\paragraph{Théorème 6.3} Perron-Frobenius. Soit $A\geq 0$, une matrice non négative irréductible. Alors $r$ est le rayon spectral de $A$. De plus, $r$ est une valeur propre simple et le vecteur $x$ correspondant est positif.

\paragraph{Théorème 6.4} Si $S$ est une matrice stochastique irréductible, alors $\rho(S) = 1$ et cette valeur propre est de multiplicité 1.

\end{document}
