\documentclass[en,license=none]{../../../eplsummary}

\usepackage{../../../eplcode}

\usepackage{graphicx}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
%\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{framed}
\usepackage{multicol}
\usepackage{todonotes}

\graphicspath{{images/}}

\hypertitle{Concurrent system theorical part}{7}{INGI}{2143}
{Nicolas Houtain\and Gorby Nicolas Ndonda Kabasele\and Florian Thuin}{Charles Pecheur}

\section{Question 1: LTS and Sequential processes}

\textit{Define labelled transition systems  and explain their underlying
assumptions. Using small examples, describe how sequential processes are
modelled in FSP (sequences of actions, termination, loops, choices, non-
determinism). Explain how data values are handled in FSP.}

\paragraph{}

\textbf{LTS} are finite state machines with labels on transitions that
define  a process,  each  state  of the  automata  represents the  state of
the  process (value of  implicit/explicit variables,\ldots).

\begin{tabular}{p{3cm}m{9cm}}
    \textsc{Assumptions} =
    & \begin{enumerate}
         \item Actions are atomic (do not interrupt each other).
         \item States are opaque (no info on the value, variable, register, memory,\ldots)
         \item Transitions are transparent.
     \end{enumerate}
    \\
\end{tabular}


Executions of the process are finite or infinite, and this is a sequence
of  successive transitions  from the  \textit{initial state}.  (There is
different transitions for the same action)

As the process  executes, it executes statement (atomic action like
read/write  in the  register). Each  action  in the  process triggers  a
transition from the current to the corresponding state in the LTS.\@
FSP is the language used to model Finite State Process. Every Finite
State Machine can be modeled with FSP.\@

\paragraph{Example} for the FSP

\lstinputlisting{code/sequential_process.lts}

For different data values, different actions are done (in.0,in.1,\ldots). The range of the data values is finite. With
syntaxic sugar, we use indexing to represent implicit choices between indexes.


\section{Question 2: Concurrent process}

\textit{Explain  the model  of concurrency  used in  labelled transition
systems.  State  the  underlying   assumptions.  Using  small  examples,
describe  how  concurrent interacting  processes  are  modelled in  FSP.
Discuss how direction of interaction and data transfer is modelled.}

\paragraph{}

The model of concurrency used by LTS is \textbf{temporal ordering of events}. Two
actions are concurrent  if there are no defined order in which they can
occur,  ($a\to b$  or $b\to  a$). It  doesn't describe  the time  needed
for  an event  to  finish  and  how  long  is  the  delay  between  each
event. (Concurrency  can be described  as choice because  of scheduling)

\begin{tabular}{p{3cm}m{9cm}}
    \textsc{Assumption} =
    & \begin{enumerate}
         \item Actions are atomic, Transitions occur instantaneously
         \item Processes execute at arbitrary speeds
         \item Actions are  interleaved (does not model the  case where $a$
               and $b$ occur simultanously)
     \end{enumerate}
    \\
\end{tabular}

\paragraph{Interleaved} is not a problem because of \textit{no partial overlap}
(assumption\#1) and when they occur at the same time then they can also occur consecutively
(assumption\#2).

$ \rightarrow $ Occur at the same time does not affect our model of concurrent executions


\paragraph{FSP}
One can compose concurrent processes with \textit{Disjoint Parallel Composition} ($||$).
LTS generates this by combining LTS from sub-processes.


\paragraph{ } Concurrency can be described as \textit{choice} by scheduling


\paragraph{Example} for the FSP

\lstinputlisting{code/concurrent_process.lts}

The concurrent model is a cartesian product between the set of actions of
each process.

\begin{itemize}
    \item \textit{Disjoint} actions are interleaved
    \item \textit{Shared} actions are synchronized
\end{itemize}
In  our  example,  it  means that  to  do  \textit{ready},  the
concurrent process must have done a \textit{make} first.


\paragraph{Discuss data transfer}
For different data values, different actions are done (in.0,in.1,\ldots). The range of the data values is finite. With
syntaxic sugar, we use indexing to represent the implicit choice between indexes.


\section{Question 3: Hiding, relabelling, priority}


\textit{Using  small  examples,  describe the  relabelling,  hiding  and
priority operators of FSP. Explain the  semantics of hiding, its relation
to  abstraction, and  how  this abstraction  helps  in verifying  large,
complex models.}

\lstinputlisting[caption={Example FSP}, captionpos=below]{code/example_fsp.lts}

\begin{description}

    \item[Relabelling] allow proccesses  to \textit{share} common actions.
    It's used  to make  the different processes synchronize on the action
    relabelled (here, you can explain client-server).
    $$ \mathrm{PROCESS} / \mathrm{\{newname} / \mathrm{\{oldname}_{1}, \mathrm{oldname}_{2} \mathrm{\} \}} $$

     \item[Hiding] (like an abstraction) allows process to make an action
    invisible  from the outside, the process becomes like an  interface
    (here, you can explain the two buffers). If an action is hidden, it is replaced by an
    internal action \verb#tau#.
    $$ \mathrm{PROCESS \textbackslash \{action}_{1}, \mathrm{action}_2\textrm{\} (hidden action) } $$
    $$ \mathrm{PROCESS @ \{action}_{1}\mathrm{, action}_{2}\mathrm{ \} }\textrm{ (show only action)} $$

    \item[Priority] allows processes to know what it should prioritize during the execution.
    (\verb#P<<A# gives $A$ higher priority than $P$).

        If $A$ has an higher priority, whenever an action $A$ is enabled other
    actions are disabled including \verb#tau# (if the process has to
    do a choice, it will take $A$)

\end{description}

\paragraph{Abstraction}
Hiding is an abstraction because the process becomes like a black box where internal actions are hidden from
the outside.
\subparagraph{tau} is never a part of the alphabet, and never synchronized.
(you can minimize to eliminate it)

\subsection{Hiding semantic}
Hiding $B \subset Act$
\begin{itemize}
    \item $\alpha(E \backslash B) = (\alpha(E) - B)$
    \item $end(E \backslash B) = end(E)$
\end{itemize}
$$ \frac{E -a \to E'}{E\backslash B -\tau \to E'\backslash B} a\in B \quad \textrm{ and } \quad  \frac{E -a \to E'}{E\backslash B -a \to E'\backslash B} a\notin B $$


\section{Question 4: Semantic traces}

\textit{State the three key elements that constitute a formal semantics.
Define  those elements  for  the  trace semantics  of  FSP. Discuss  the
limitations of using finite traces as a semantics for FSP}

\paragraph{}
The three key elements are the following:
\begin{enumerate}
\item semantic domain \textbf{Dom}, represents the meaning of FSP programs (Trace or LTS)
\item semantic map \textbf{sem} from an $Exp \to Dom$
\item semantic relations $\leftrightarrow$ that defines relation over Dom (ex: equivalence, preorder, \ldots). Gives a semantic to every program
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{semantic.png}
\end{figure}

\begin{description}
    \item[$\mathbf{Act}$]: universal set of \textbf{observable actions} (infinite)
    \item[$\mathbf{tau}$]: $\tau \notin Act$. All actions = $Act \cup \{\tau\}$
    \item[$\mathbf{Exp}$] is the set of all FSP (basic) process expressions.
    \item[$\mathbf{Exp^+}$] is the set of all FSP (full) process definition
    \item[$\mathbf{T = Act^*}$] is the set of all traces
    \item[$\mathbf{2^T}$] is the set of all sets of traces
    \item[$\mathbf{\alpha(E)}$] is the alphabet of processes
\end{description}

\paragraph{Trace  semantic}  is \textcolor{red}{TR}:  Exp  $\rightarrow$
2$^T$. A trace is a finite sequence observable actions.

\todo[inline]{Recursion and fixpoint : feel free to contribute}

\paragraph{}
Trace semantics is limited for non-determinism and deadlock:

\lstinputlisting{code/semantic_traces.lts}

\verb#P1# and \verb#P2# have the same trace, while they are different as \verb#P1# is
deterministic and \verb#P2# is not. Same result for \verb#P3# and \verb#P4#, they have the
same trace but they are different as \verb#P4# may deadlock after \verb#a#. With trace
semantics, we can't distinguish between the two.
\section{Question 5: Semantics LTS}

\textit{State the three key elements that constitute a formal semantics.
Define  those  elements  for  the  LTS semantics  of  FSP.  Explain  the
principles of  structured operational semantic rules.  State and explain
the semantic rules for the choice and parallel operators.}

\paragraph{}
The three key elements are the following:
\begin{enumerate}
\item semantic domain \textbf{Dom}, represent the meaning of FSP programs (Trace or LTS)
\item semantic map \textbf{sem} from an $Exp \to Dom$
\item semantic relations $\leftrightarrow$ that defines relation over Dom (ex: equivalence, preorder, \ldots). Gives a semantics to every program
\end{enumerate}

\paragraph{Semantic map} for LTS is \textcolor{red}{lts}: $Exp\to<S,A,\Delta,q>$ where for process expression $E$:

\begin{itemize}
\item $q$ is the initial state ($=E$)
\item $A$ is the action set ($\color{red} \alpha(E) \color{black} \cup {\tau}$)
\item $\Delta$  is the transition relation  as a set of  \textcolor{red}{inference rules}
(set of premisses that leads to a conclusion)
\item Uses auxiliary map \textcolor{red}{end(E)} for sequential composition
\item S  is a  set of states that contains  all the  states that  can be
reached from q through $\Delta$
\end{itemize}

To do the mapping  of an FSP process $E$, we  take this process expression
as initial state of our LTS (q=E).  A = alphabet(E) + tau. Delta defined
by set of  inference rules. Use auxiliary map end(E)  for composition. S
contains all states reachable from q through Delta.

\paragraph{Operational semantics}
Define transitions E -a $\to$ E' then derive $\Delta$

\subsection{An  inference rule}  is a  set  of rules  with the  following
form:
$$\frac{\varphi_1,...,\varphi_n}{\varphi}\quad c, \quad \textrm{  which  means  } \mathbf{IF} \quad
\varphi_1,...,\varphi_n \quad [\mathbf{WHERE} \quad c] \quad \mathbf{THEN} \quad \varphi$$

In the LTS, $\varphi$ are the transitions $E - a\to E'$ and the premisses
$\varphi_1,...,\varphi_n$ are the transition of the subprocesses of $E$.

\subsection{stuctural operational semantics (SOS)}
Structured operational semantics rules define transition relations by induction of the syntax of language.

\subsection{Semantic rules}

\subsubsection{Choice}
The semantic of the \textbf{choice operator} is as follow:
\begin{itemize}
    \item $\alpha(E_1|E_2) = \alpha(E_1)\bigcup\alpha(E_2)$
    \item $end(E_1|E_2) = end(E_1) \vee end(E_2)$ = false because $E_1,E_2$ are guarder processes a->E
\end{itemize}

It is  based on the  two inference rules
$$\frac{E_1-a->E'}{E_1|E_2 -a-> E'} \quad \textrm{ and } \quad \frac{E_2-a->E'}{E_1|E_2 -a-> E'}$$

which state that if one of
the process can reach E', then the whole system can do it too.

\subsubsection{Parallel operators}

The semantic of the \textbf{parallel operator} is as follow:
\begin{itemize}
\item $\alpha(E_1|[A]|E_2) = \alpha(E_1)\bigcup\alpha(E_2)$
\item $end(E_1|[A]E_2) = end(E_1) \wedge end(E_2)$
\end{itemize}
It is based on three rules,
$$\frac{E_1-a->E'_1}{E_1|[A]|E_2->E'_1|[A]|E_2}a\notin A \quad \textrm{ and } \quad
\frac{E_2-a->E'_2}{E_1|[A]|E_2->E_1|[A]|E'_2}a\notin A$$
$$\textrm{ and } \frac{E_1-a->E'_1 \quad E_2-a->E'_2}{E_1|[A]|E_2->E'_1|[A]|E'_2}a\in A$$

where A = $\alpha(E_1)\bigcap\alpha(E_2)$

\section{Question 6: Equivalences LTS}

\textit{Explain  the notions  of equivalence  and preorder  relations on
concurrent models.  Define strong and weak  equivalences. Illustrate then
difference  between  trace and  weak  equivalence.  Give an  example  of
preorder relation based on LTS and discuss its usefulness.}

\paragraph{}  The equivalence allows to tell if two FSP  processes are
equal.

\begin{figure}[!ht]
    \centering
    \begin{tabular}{c|lcr}
         $=_{FSP}$ & same FSP  \\
         $=_{LTS}$ & same LTS (isomorphic) \\
         $\sim$ & strong equivalence &:& \textit{There exists a strong bisimulation $R(E_1, E_2)$} \\
         $\approx_c$ & weak congruence &:& \textit{ }  \\
         $\approx$ & weak equivalence &:& \textit{there exists a weak bisimulation $R(E_1, E_2)$}\\
         $=_{tr}$ & same traces \\
    \end{tabular}
    \caption{Different types of equivalence, listed by order of restricivity}
\end{figure}

\subsection{Trace equivalence}

Given E, E'$\in$ Exp $$ E=_{tr}E'  \textrm{ \textcolor{red}{IFF} }tr(E) = tr(E')$$
$=_{tr}$ is an \textbf{equivalence} relation which means it's reflexive, symmetric, transitive.

\subsection{Trace preorder}
Given $E_s,E_i \in EXP$ $$E_i \le_{tr} E_s \textrm{ \textcolor{red}{IFF} } tr(E_i)\subseteq tr(E_s)$$
$\le_{tr}$ is a reflexive and transitive. $E_i$ (implementation) is a refinement of $E_s$ (specification)

\paragraph{Note:}
\begin{itemize}
    \item $=_{tr}$ ignores non determinism
    \item $\le_{tr}$ preserves safety properties
    \item $\le_{tr}$ says nothing of liveness properties ($ STOP \le_{tr} E_s$)
\end{itemize}

\subsection{Simulation}
A process \verb#P1# \textbf{simulates} a process \verb#P2# \textcolor{red}{IFF} it  covers all the possible
behaviours of \verb#P2#, i.e:
\begin{enumerate}
    \item Any transition of \verb#P2#, there is a corresponding transition that \verb#P1# can perform
    \item The states reached by \verb#P1# cover all the possible behaviours of the
        corresponding state reached by \verb#P2#
\end{enumerate}

\textit{If P2 and P1 can simulate each other, they are in bisimulation.}


\paragraph{Pre-order}
Same idea than trace pre-order

\paragraph{Equivalence}
$E_1$ is (strong) simulation equivalent to
$$E_2 (E_1\asymp E_2) \quad \textrm{IFF} \quad E_1\prec E_2 \quad and E_2\prec E_1 $$


\subparagraph{ } It means that they can simulate each other but it's not
the same as  bisimulation equivalence. One $R(E_{1},E_{2})$ such that $R$ and
$R^{1}$ are both simulations (in  opposition of strong simulation where all
states of $E_{1}$ must simulate all states of $E_{2}$)

\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{sim_equi.png}
    \caption{Simulation equivalence}
\end{figure}

\paragraph{Ready}
Ready simulation is a strong simulation with
$ if E_{2} -a \to$ then $E_{1} -a \to$

\begin{figure}[h]
    \centering
    \includegraphics[width=6cm]{ready.png}
    \caption{Ready simulation}
\end{figure}

\subsubsection{Strong simulation}
Given $R \subset Exp \times Exp$, for every $(E_{1}, E_{2}) \in R$
$$ \textrm{ \textcolor{red}{IF} } E_{1} -a \to E'_{1} \textrm{ \textcolor{red}{THEN} } E_{2} -a \to E'_{2} \textrm{ \textcolor{red}{AND} } (E'_{1}, E'{2}) \in R$$

\paragraph{     }     $E_2$    \textbf{strongly     simulates}     $E_2$
\textcolor{red}{IF}   each  $E'_2$   strongly   simulates  each   $E'_1$
\textcolor{red}{AND} $(E_1, E_2) \in R$

\paragraph{ } There are strong  bisimulation when $E_1$ and $E_2$ are
both strong simulations.


\subsubsection{Weak transitions}
\begin{itemize}
    \item $E -a_1 \cdots a_n \to E'$ \textcolor{red}{IFF} there exists $E_0, E_1, \cdots, E_n$
        such that $E = E_0 -a_1 \to E_1-a_2 \to \cdots -a_n \to E_n = E'$

    \item $E = a \to E'$ \textcolor{red}{IFF} $E -\tau* a\tau* \to E'$ (for a$\in$Act)

    \item $E = \epsilon \to E'$ \textcolor{red}{IFF} $E -\tau* \to E'$
\end{itemize}

\subsubsection{Weak simulation}

Weak simulation is  the same idea but it's less  restrictive as it ignores
the hidden action ($\tau$).

Given $R \subset Exp \times Exp$, for every $(E_1, E_2) \in R$, $t \in Act \cup \{\epsilon\}$
$$ \textrm{ \textcolor{red}{IF} } E_1 =t \to E'_1 \textrm{ \textcolor{red}{THEN} } E_2 =t \to E'_2 \textrm{ \textcolor{red}{AND} } (E'_1, E'2) \in R$$

\paragraph{Note about t} :
\begin{enumerate}
    \item $t = a \in Act$ : t = $\tau* a \tau*$
    \item $t = \epsilon$  : t = $\tau*$
\end{enumerate}

\subsection{Congruence}
A congruence $\zeta$ is an equivalence relation such that \textcolor{red}{IF}
$E \zeta E'$ \textcolor{red}{THEN} $C[E] \zeta C[E']$ for any context $C[\bullet]$


\textit{(Equals can be substituted for equals)}.

\subsection{Other}

\paragraph{Internal choice}
Non deterministic choice and deterministic choice are
NOT weakly equivalent, but are trace equivalent.


The difference between weak equivalence and trace equivalence come from non-determinism.

\paragraph{Divergence}
is a loop of internal actions. (Weak equivalence ignores divergences)


\subsection{Example}
The  example shows that \verb#P1#,\verb#P2#,\verb#P3# are not strongly equivalent
because the internal action are considered.

\lstinputlisting{code/internal_actions.lts}

For  two  FSP  processes (=LTS states) $E_{1},E_{2}$,  $E_{1}$  is  strongly
equivalent to  $E_{2}$ iff there exists a strong bisimulation R such that
$R(E_{1},)$



\section{Question 7: Minimization}

\textit{Define  minimization modulo  an  equivalence.  Give the  general
principles of the coarsest partition algorithm. Discuss how minimization
can be performed in a compositional way.}

\subsection{Minimization modulo equivalence}

For a given   equivalence  relation eq  (ex:weak  bisimulation  equivalence
$\approx$) and a given LTS, $P$:
$$ \textrm{ There is  a smallest LTS P'  such that P'  eq P } \min(P) =  \min{P'\quad | \quad P' \textrm{ eq
P} }$$

It is possible to construct $\min(P)$, called minimizing $P$ (modulo  eq). It can be
done by merging states and by removing some $tau$ transitions.

\subsection{Coarsest partition algorithm}

The goal of  the coarset partition algorithm is to  group all equivalent
states (according to eq). It constructs  equivalence classes and then builds
a partition of the state space. The algorithm works as follow:

\begin{enumerate}
    \item group all states (one single equivalence class)
    \item while some class $X$ does not respect the equivalence relation
    \begin{enumerate}
        \item split non-equivalent states of X into different classes $X_{1},\ldots,X_{n}$
    \end{enumerate}
\end{enumerate}

\begin{figure}[!ht]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=7cm]{strong_mini.png}
        &
        \includegraphics[width=7cm]{weak_mini.png}
        \\
    \end{tabular}
    \caption{Example of strong and weak minimization}
\end{figure}

\textbf{Tau closure} is the principle used to do a minimization based on
weak equivalence, the principle is as follow:

\begin{itemize}
\item add $E\color{red}{-a} \to E'$ for every $E-\tau a\tau* \to E'$
\item add $E\color{blue}{-\tau \to} E'$ for every $E-\tau* \to E'$
\item Then apply strong equivalence.
\end{itemize}

\subsection{Compositional Minimization}

\begin{tabular}{m{11cm}m{4cm}}
Given a composite FSP $E$ with component $E_{1},\ldots,E_{n}$. & \\
\begin{enumerate}
\item Generate LTS $P_i=lts(E_i)$
\item Minimize LTS $P'_i=\min(P_i)$
\item For each composite FSP $E_k=(E_i||E_j)/R\backslash A$
\begin{enumerate}
\item Generate LTS $P_k = (P'_i||P'_j)/R\backslash A$
\item Minimize $P'_k=\min(P_k)$
\end{enumerate}
\item Analyze $P'_n$ where $E_n=E$
\end{enumerate}
&
\includegraphics[width=2cm]{minimization.png}
\\
\end{tabular}

$\rightarrow$ \textcolor{red}{Gain}: $|P'|<|P|<|lts(E)|$

It means  that it is more efficient to do minimizing in hierarchical way
and not  only at  the end because tau closure are really costly. There is
no usefulness to minimize different parts of the system if there is no
synchronization.

(remember ABSTRACT in the project + example) slide 56 chap 5).

\lstinputlisting{code/compositional_minimization.lts}


\section{Question 8: Synchronization models}

\textit{Using   small    examples,   illustrate   how    the   following
synchronization mechanisms can  be modeled in FSP  : rendez-vous, shared
variables,  locks,  semaphores,  buffers.  Explain the  notion  of  data
independence.}

\subsection{Synchronization mechanism}
\lstinputlisting{code/synchronization_mechanism.lts}

\subsection{Mechanisms}

\subsubsection{Rendez-vous}
Actions occur in all participating processes (\textit{synchronously}),
so some processes are blocking to wait that other processes are ready to
perform the action.

\textit{Unless it has the choice to perform an alternative action}

\paragraph{Synchronous message passing}
Like sender/receiver.

\subparagraph{Determined data value}
$P[x:Data] = ( c[x] \to P1[x])$

\subparagraph{Undetermined data value}
$Q = (c[x:Data] \to Q1[x])$

\subsubsection{Shared variables}
is modeled in FSP by an explicit separate process.

\subsubsection{Lock}
is modeled in FSP by an explicit separate process.
(Example pg 75)

\subsubsection{Monitor}

A   monitor   is   a   construct   for   concurrent   programming   with
\textit{encapsulated  data}, \textit{mutual  exclusion}, \textit{monitor
procadure calls} (that can block until a given condition is satisfied).

\paragraph{In FSP}:
\begin{itemize}
    \item Procedure calls are represented as labelled actions
    \item Condition synchronization by enabled/disabled action
    \item Data/states encapsulation is built-in
\end{itemize}

\subsubsection{Semaphores}
Like a monitor but the condition is a non-negative integer
variable.

\subsubsection{Buffer}

\subsection{Data independence}

If the medium which carries the data changes it or interpretes it, the
number of states can become quite large.  But if it's not affected by the
precise value being transmitted the system is \textbf{data-independent}
(\textit{The data can be abstracted away often resulting in simpler models})

\paragraph{Example:} Bounded buffer becomes a bounded counter

\section{Question 9: Timed systems}

\textit{Explain how timed systems can be modeled in FSP and what are the
underlying assumptions. Give a small example. Define the notions of time
consistency and maximal  progress and discuss how  they occur concretely
in FSP models}

\subsection{Time in FSP}
We can use clock as a mean of synchronization to model timed system.

\begin{tabular}{p{3cm}m{9cm}}
    \textsc{Assumption} =
    & \begin{enumerate}
          \item Program execution is negligible with respect to external events. (action take zero time to be executed)
          \item No assumption on duration, relative speed (only ordering of event)
          \item Instantaneous processing
     \end{enumerate}
    \\
\end{tabular}


Even if real time is continuous, we use a \textit{discrete model} of time in FSP.
We  use the  \textbf{ticks}  of a  clock  and the  time  is measured  by
counting the ticks.
\textit{The time between two ticks is a fixed duration d.}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=8cm]{timing.png}
    \caption{timing uncertainty}
\end{figure}

$\rightarrow$ The accuracy  of  time  measurement can  be
increased  by reducing  the value  of  d but  the  price to  pay is  the
increase of the number of state.

\paragraph{Timed systems in FPS}
\begin{enumerate}
    \item Choose one action to represent clock transition (\textit{tick})
    \item All time-dependent processes synchronize on \textit{tick}
    \item Time-indepedent processes do not use \textit{tick}
\end{enumerate}


\subparagraph{Deadlock}
is when time is blocked, i.e \textit{tick} cannot occur

\paragraph{Example}

\lstinputlisting{code/deadlock.lts}

\subsection{Time consistency}
is a \textbf{global model property} and is not a requirement on individual processes

\begin{itemize}
    \item \textbf{A time-stop} is a deadlock in a timed model
    \item If no time-stop can occur in a timed model, then it's \textbf{time consistent}.
\end{itemize}

\lstinputlisting{code/time_consistency.lts}

The deadlock occur with Tc=2 and Tc=3, because after the first item is produced and accepted by the
consumer, the producer tries to produces another item after two clock ticks while the consumer must accept
a third to accept the item.

\subsection{Maximal Progress}

\begin{itemize}
 \item Action occur as soon as all participants are ready to perfom them.
 \item In FSP all actions that can occur will occur before the next tick

     ($\rightarrow$ it can be done by giving a low priority to the tick action).
\end{itemize}

\paragraph{Infinite executions}
With the low priority at \textit{tick} it's possible to never make tick.

Resolve with \textbf{fairness}

\section{Question 10: Properties safety}

\textit{Define  and compare  deadlock, safety  and liveness  properties.
Discuss how deadlocks can appear in concurrent systems, and how they can
be remedied.  Using a small  example, explain how safety  properties are
modeled in FSP. Explain the  general technique used for verifying safety
properties.}

\subsection{Definition and comparaison}
\label{property_definition}

\begin{description}
    \item[deadlock:] A  deadlock state  is  a state  with no  outgoing
        transition.
        \begin{itemize}
            \item state  $q$ sat \textbf{deadlock} \textcolor{red}{IFF} $q'$
            there exists no $a$, such that $q-a\to q'$
        \end{itemize}

    \item[Safety properties:] \textcolor{red}{bad}  things don't  happen (violate  if a
        \textit{finite} path where bad thing happen)

    \item[Liveness  propeties:]  \textcolor{red}{good} things  do happen  (violate if  a
        \textit{infinite} path where no good thing happens)
\end{description}


Safety properties are more a protection on the  model while liveness
properties are more of an assurance on what the model do. \textbf{Both of
them  are  path properties}.  What  we  mean  by  bad things,  it's  the
condition on the succession of states and action along the path.


\subsection{Deadlock in concurrent system}

There can be deadlock in concurrent if the different parts of the system
share  ressources.  There can  be  deadlock  \textcolor{red}{IFF} the  following  four
conditions holds:

\begin{enumerate}

    \item \textbf{Serially reusable  resources:}  the processes  involded share
    resources that they use under mutual exclusion

    \item \textbf{Incremental  acquisition:}  processes  hold  on to  resources
    already  allocated  to  them  while waiting  to  acquire  additional
    resources

    \item \textbf{No pre-emption:} once acquired by a process, resources cannot
    be pre-empted (forcibly withdrawn) but  are only released voluntarily

    \item \textbf{Wait-for cycle:}  a  circular chain (or  cycle) of  processes
    exists such that each process holds a resource which its successor in
    the cycle is waiting to acquire.

\end{enumerate}

\paragraph{They can be remedied by}:
\begin{enumerate}
    \item using different structures (semaphore, lock,\ldots) to protect the resource
    \item using timeout to avoid deadlock
    \item break the cycle (like philosophers)
\end{enumerate}

\subsection{Example}

Example of a safety property

\lstinputlisting{code/property_polite.lts}

The property state that two successive enter or knock can not happen.

\subsection{Verifying}

To verify a safety propety, a \textbf{breadth-first search} is used. The
following situations are reported by the verification:

\begin{itemize}
    \item deadlock states
    \item error states (=ERROR, state that violates the property)
    \item (not END states)
\end{itemize}

\section{Question 11: Properties liveness}

\textit{Define  and compare  deadlock, safety  and liveness  properties.
Explain why  the technique used  for safety verification cannot  be used
for liveness verification. Explain the notion of fairness. Using a small
example,  explain  the  what  FSP's progress  properties  are,  and  the
technique used for verifying those properties}

\subsection{Definition and comparaison}
See~\ref{property_definition} page~\pageref{property_definition}

\subsection{Verification technique}

The verification  technique used  for safety  property (reachability) is
not enough to verify liveness properties as \textbf{the violation trace
are infinite}. Indeed, for a finite prefix, the good thing may not have
happened yet.

\subsection{Fairness}

Fairness is a property which assures that for a set of actions $A$,
each  action  will always  eventually  be  executed. It's  the  opposite
of  \textbf{starvation}.  It  is  often  based  on  \textbf{Fair  choice
Assumption}, which states that if a choice over a  set of transitions is
executed  infinitely often,  then every  transition in  the set  will be
executed infinitely often.

\subsection{Verification on progress}

A progress state  that in infinite execution, assuming  fair choice, and
action will  occur infinitely  often, for the  example below,  the first
progress is  respected but not the  second one because, there  is a case
when no tails can be done.

\lstinputlisting{code/progress_verification.lts}

The  verification  is  done  by  using  the  \textbf{Strongly  Connected
component (SCC)} and terminal set of a LTS P.

\begin{description}

    \item \textcolor{green}{The SCC of P} is a maximal set of states $S' \subseteq S$ such
    that for all $q$, $q'$ in $S'$, $q'$ is reachable from $q$.

\item \textcolor{red}{A terminal set of $P$} is  a SCC $S'\subseteq S$ such that there
    is no $q$ in $S'$, $q''$ outside $S'$ such that $q'$ is reachable from $q$.
\end{description}

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{terminal.png}
    \caption{SCC and terminal set}
\end{figure}

Given a model (LTS) and a set of progress properties $A_i$ with $A_i \subset Act$:
The verification is done as follow:

\begin{enumerate}
    \item Search for all terminal set of states $S_T$
    \item Check that at least one action from every progress property $A_i$ occurs in each terminal set $S_T$
    \item if some $A_i$ does not occur in some $S_t$, report:
    \begin{itemize}
        \item the shortest path to $S_T$
        \item the action occurring in $S_T$
    \end{itemize}
\end{enumerate}

 As the model is finite, all infinite execution end up inside a SCC, and
 because of fair  choice the SCC must be terminal  and all transition in
 the terminal set are executed infinetely often.

\section{Question 12: Temporal logic principle}

\textit{Define the notion  of fluent and explain  its usefulness. Define
the notion  of (linear) temporal  logic. Explain the until  operator and
formalize  its semantics.  Give and  explain an  example of  a branching
temporal logic (CTL) property. }

\subsection{Fluent and Usefulness}

With temporal  logic we can  defined property that are  interpreted over
execution  paths  (ex: After A  becomes  true, Until  A becomes true,\ldots).

\textbf{Temporal Logic} use fluent. A  fluent is a condition that varies
over the time (\textit{abstract state whose value depends on actions}).

In the case  of FSP, propositional fluent changes according to sets of
initiating and terminating actions (if an action in the initiating sets
occurs, the fluent becomes true and if an action in the terminating set
occurs, fluent becomes false).

\paragraph{ }

\begin{figure}[!ht]
    \begin{tabular}{m{8cm}m{6cm}}

    A \textbf{fluent} has:

    \begin{itemize}
        \item initiating actions $\{s_{1},\ldots,s_{m}\}$
        \item terminating actions $\{e_{1},\ldots,e_{n}\}$
        \item initial value $B$ (which is $0$ by default)
        \item Initiating and terminating actions must be disjoint.
    \end{itemize}

    &
    \includegraphics[width=6cm]{fluent.png}
    \\
    \end{tabular}
    \caption{Propositional fluents changes with sets of
    initiating and terminating actions}
\end{figure}

\subsection{Linear Temporal Logic}
A temporal logic is defined in two steps:

\begin{itemize}
    \item From traces (sequences of actions) to fluents (boolean variables)
    \item From fluents to temporal formula (assert)
\end{itemize}

The linear temporal logic is interpreted  over \textbf{infinite traces}
which means there are \textit{no terminal states} (Without: END,  STOP, ERROR).

\paragraph{ } If the model has terminate state, we can make them
non-terminal by looping on themselves.
(Example: Action \textit{done} on the terminal state.)

\subsection{The until operator}

$\varphi_1 U \varphi_2$ is true in state q \textcolor{red}{IFF} $\varphi_2$ is true in some future
state $q'$ \textcolor{red}{AND} $\varphi_1$ is true in all states from $q$ to $q'$ not included.

\begin{figure}[h]
    \centering
    \includegraphics[width=6cm]{until.png}
    \caption{Until operator}
\end{figure}


In the example below, it means that we can't have enter before knock and
knock will happen.

\lstinputlisting{code/assert_polite.lts}

\subsubsection{Semantics}

\begin{itemize}
    \item Interpretation over fluent $W_i$ = ($f_i, f_{i+1}, \cdots$) the $i^{th}$ suffix of $w$.
    \item Each $f_i \in 2^{\phi}$
\end{itemize}

$ W_i \textrm{ sat } \varphi \cup \varphi' \textrm{ \color{red} IFF \color{black} there is} \quad j \geq i
\textrm{ such that} \quad w_j \textrm{ sat } \varphi' \textrm{ and for all } i\leq k < j \textrm{ we have } w_k \textrm{ sat }  \varphi$


\subsection{Branching temporal Logic}

FTL is  limited as  it can only  distinguish trace-based  properties, we
can't for example distinguish property on non-determinism (slide 37 cha
8.a).

Instead of considering trace, \textbf{Branching temporal logic} consider
execution tree.  There are  two types  of formula:  path and  state. The
quantifier are the following:

\begin{itemize}
    \item $\mathbf{AX \varphi}\equiv$ in all next states, $\varphi$ holds
    \item $\mathbf{EX \varphi}\equiv$ in some next states, $\varphi$ holds
    \item $\mathbf{E(\varphi U\varphi'} \varphi) \equiv$ there exists a path where $\varphi$ holds until $\varphi'$ holds
    \item \ldots
\end{itemize}

\lstinputlisting{code/canchoose.lts}

This  property state  that we  can  choose between  b  and c.  As P1  is
deterministic, it  satisfies the  property but not  P2 because  a choice
cannot be done.

\section{Question 13: Temporal logic operator}

\textit{Using  small  examples,  explain  the  five  temporal  operators
of  FLTL (eventually,  globally,  strong and  weak  until, next).  Using
equations, show how [] and $<>$ derive from U and W and how U and W relate
to  each other.  State  and  explain the  expansion  law  for the  until
operator. }

\subsection{Example for operator}
\begin{description}
    \item[Globally:] []$\varphi$ is true in some states \textcolor{red}{IFF} $\varphi$ is true in that state and all future states.
    \item[Eventually:] $<>$ $\varphi$ is true in some states \textcolor{red}{IFF} $\varphi$ is true in that state or in some future states.
    \item[Strong until:] $\varphi_1 U \varphi_2$ is true in state q \textcolor{red}{IFF} $\varphi_2$ is true in some future state q'
    \textbf{and} $\varphi_1$ is true in all states from q to q' not included.
\item[Weak until (unless):]  $\varphi_1 W \varphi_2$ is true in state q \textcolor{red}{IFF} $\varphi_1$ is true in all states from q as
    long as $\varphi_2$ is false.
\item[Next:] X $\varphi$ is true in some state \textcolor{red}{IFF} $\varphi$ is true in the next state
\end{description}

\lstinputlisting{code/temporal_operators.lts}

\subsection{Relation between operators}

$<>$ and [] are particular cases of U and  W. At the same time U and W are
related. The different relations are the following.

\begin{eqnarray*}
<> \varphi &=& \quad  true \quad U \varphi \\
\textrm{[ ] } \varphi &=& \varphi W \quad false \\
\varphi W \varphi' &=& \varphi U \varphi \quad || \quad \textrm{[ ] } \varphi \\
\varphi U \varphi' &=& \varphi W \varphi' \quad\&\& \quad <> \varphi
\end{eqnarray*}


\paragraph{Can be reduced to}
U and [] or W and $<>$

\subsubsection{Law duality U-W and []-$<>$}

\begin{eqnarray*}
! (\varphi U \varphi') &=& ! \varphi ' W (!\varphi \quad \&\& \quad !\varphi ') \\
! (\varphi W \varphi') &=& ! \varphi ' U (!\varphi \quad \&\& \quad !\varphi ') \\
\\
! [] \varphi &=& <> \quad! \varphi \\
! <> \varphi &=& [] \quad! \varphi
\end{eqnarray*}

\paragraph{ } So duality can be reduced by using!

\subsection{Expansion of Law}

\textbf{Expansion  law}  unrolls  operators  on  step,  for  the  until
operator, the expansion is as follow

$$\varphi U\varphi' = \varphi'\quad || \quad (\varphi \quad \&\& \quad X (\varphi U \varphi'))$$

\section{Question 14: Model-checking principle}

\textit{Explain the general  principles of automata-based model-checking
of  linear temporal  logics. Discuss  the cases  of safety  and liveness
properties. Define the notion of  Büchi automaton. Explain what results
are produced.}

\paragraph{Naïve idea}
\begin{enumerate}
    \item Turn a LTL property $\varphi$ into a property automaton $B_{\varphi}$
        so $B_{\varphi}$ reaches ERROR when $\varphi$ is violated
    \item Build ($B || B_{\varphi}$)
    \item Apply reachability algorithm to search for ERROR
\end{enumerate}

Don't work because we need to use \textit{infinite traces} for liveness properties.
(reachability is only on finite trace)

\subsection{General principle}
We have three sets for the language:

\begin{itemize}

    \item $L = {(\alpha B)}^{\omega}$ where B is a model  and $\alpha B$ it
    the  alphabet.  It  represents  the universal  language  of  B,  all
    possible infinite traces. ($\omega$ is denumerable infinity)

    \item $L(B)  =\{ \sigma \in  L | B =  \sigma=>\}$, the language of $B$
        (\textit{set of execution traces}). Attention, there is the assumption that $B$
    has no finite terminating execution.

    \item $L(\varphi) =  \{\sigma \in L|\sigma \textrm{ sat }\varphi\}$  the language of
    $\varphi$ (\textit{set of satisfiying traces}).

\end{itemize}

\subsubsection{Verification}

\paragraph{The goal} is to verify that $B$ sat $\varphi$:
\begin{enumerate}
    \item \textcolor{red}{IFF} for all (infinite full) traces $\sigma$ of $B$, $\sigma$ sat $\varphi$
    \item \textcolor{red}{IFF} $L(B) \subset L(\varphi)$
\end{enumerate}

It's not easy to verify, but there is another way.

\paragraph{Solution } To  see if $B$ satisfies the property $\varphi$ we're going to check the emptiness of $L(B) \bigcap L(!\varphi)$.


\subsubsection{Synchronization}
Generally, synchronization produces the intersection of traces on the shared alphabet:

Given two LTS $B_1, B_2$ with $A = \alpha(B_1) \cap \alpha(B_2)$:
$$ L(( B_1 \quad || \quad B_2) @ A) =  L(B_1 @ A) \cap L(B_2 @ A) $$
($@$ show only this action)

\paragraph{Particular:} if $\alpha(B_1) = \alpha(B_2)$ then $L(B_1 || B_2) =  L(B_1) \cap L(B_2)$
\subsection{Automata-Based Model Checking}
To check the emptiness of $L(B) \cap L(!\varphi)$, we will do as follow:

\begin{enumerate}
    \item Build an automaton $B_{!\varphi}$ with same alphabet as B which accepts $\sigma$ such that $\sigma$  sat !$\varphi$

        (\textit{Need Büchi automata to solve infinite trace problem})
\item Compose $B || B_{! \varphi}$
    \item Search for a trace $\sigma$ accepted by $(B||B_{!\varphi})$

    (\textit{Need SCC and terminal sets to solve the problem of reachability with infinite trace})
\end{enumerate}

If $\sigma$ can be found, it means that there is trace of B that does not
satisfy the  property $\varphi$, it's  therefore an error trace and so B
does not satisfy $\varphi$


\subsection{Model Checking for Safety and Liveness property}
Given a (LTS) model $B$ and a property $\varphi$,
verify that $B$ sat $\varphi$.

\begin{itemize}
    \item To check if a model is \textbf{deadlock free} we check the \textit{reachability} of deadlock states
    \item To check a \textbf{safety property} we check the \textit{reachability} of ERROR state
    \item To check a \textbf{progress property} we check the possible actions in \textit{terminal sets}
    \item To check \textbf{LTL properties} we check the \textit{reachability of accepting state repeativly.}
\end{itemize}

\paragraph{Safety} you only need to use reachability algorithm to have the result

\paragraph{Liveness} is more complicated because the search is done on
infinite traces, reachability is not enough, we use \textit{Strongly connected
component} or \textit{terminal sets}.

\subsection{Büchi automaton}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{buchi.png}
    \caption{Automaton example}
\end{figure}

Büchi automaton are  automaton with states marked  as accepting states.
This kind of  automaton accepts infinite traces that  visit an accepting
state infinitely often and is generally non-deterministic.

The verification  is done on  the following relation, accepting  trace =
error trace.

A Büchi automaton B is a quintuplet $B = <S,\Sigma,\Delta,q,F>$ where

\begin{itemize}
    \item S is a finite set of states.
    \item $\Sigma$ is a finite set of symbols.
    \item $\Delta \subseteq Sx\Sigma xS$ is a labelled transition relation.
    \item $q\in S$ is an initial state.
    \item $F\subseteq S$ is a set of accepting state.
\end{itemize}

\textbf{B  accepts an  infinite  trace $\sigma  = (a_0,a_1,a_2,...)  \in
\Sigma^\omega$  \textcolor{red}{IFF} there exists an infinite execution over $\sigma$ that
visits  F infinitely  often.}


\paragraph{LTL to büchi}
It  is possible  to translate  LTL formula
$\phi$ on alphabet A into Büchi automaton $B_{!\phi}$ over $\Sigma=2^A$
such that  $B_{!\phi}$ accepts $\sigma$ \textcolor{red}{IFF} $\sigma$ sat  $!\varphi$.

\subsubsection{Example}

\begin{figure}[!ht]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=8cm]{buchi_ex1.png}&
        \includegraphics[width=8cm]{buchi_ex2.png}\\
    \end{tabular}
    \caption{Example of buchi automata}
\end{figure}


\paragraph{Result}
The accepting trace is a error trace of the property $\varphi$.

\section{Question 15: Model-checking algorithm}

\textit{Discuss  the  algorithms  needed to  verify  deadlocks,  safety,
progress  and general  LTL properties.  Give the  general principles  of
these algorithms. Explain the double DFS algorithm and its complexity}

\subsection{Algorithms and principle}

\begin{itemize}
    \item To check if a model is \textbf{deadlock free} we check the \textit{reachability} of deadlock states
    \item To check a \textbf{safety property} we check the \textit{reachability} of ERROR state
    \item To check a \textbf{progress propety} we check the possible actions in \textit{terminal sets}
    \item To check \textbf{LTL propeties} we check the \textit{reachability of accepting state repeativly.}
\end{itemize}

\subsection{General principle for reachability algorithm:}
Given a LTS $<S,A,\Delta,q_0>$, returns a state $q$ satisfying $c(q)$ and
reachable from $q_0$, otherwise fails

\lstinputlisting{code/reachability_algorithm.lts}

\begin{itemize}
    \item Keep track of already visited states (closed) for \textbf{efficiently}
    \item Strategy choose the next state

    \item   Depending   on   how   the  fringe   is   handled,   it's   a
    \textcolor{red}{DFS}  (fringe as  stack)  or a  \textcolor{red}{BFS}
    (fringe as a queue).
\end{itemize}


\subsubsection{Verification}
\paragraph{For deadlock:} $c(q) \equiv q$ has no  outgoing transitions.

\paragraph{For safety properties:} $c(q) \equiv q$ is (or contains) \textsc{error}.

\paragraph{ }\textbf{Successful} reachability = \textbf{failed} verification

\subsection{General principle for SCC algorithm:}
The input is a LTS B and the output is the set of SCCs of B

\begin{enumerate}
    \item Perform a depth-first search from $q_0$
    \item Each state q has two label
    \begin{itemize}
        \item index[q] = sequential index assigned in DFS order
        \item lowlink[q] = index of state reachable from q and $\le$ index[q]
    \end{itemize}
    \item State are accumulated on a SCC stack during the search
    \item if index[q] = lowlink[k] after exploring q
    \begin{itemize}
        \item q is the root of an SCC in the DFS search tree
        \item The states of the SCC are on the SCC stack up to q
        \item The path to SCC is the DFS stack
        \item Terminal SCC is not backtracking from earlier SCC
    \end{itemize}
\end{enumerate}

\subsubsection{Example}

\todo[inline]{Feel free to contribute: Add or explain example beginning at
slide 25 chap 8b.}

\subsubsection{Verification}

\paragraph{Progress properties}
The  progress property  are checked  on the  SCC.
\begin{enumerate}
    \item Search for SCCs
    \item Check that each terminal SCC contains at least one action of
          each progress property.
      \item If not, report path to SCC and alphabet of SCC
  \end{enumerate}

\paragraph{LTL properties}

To verify that B sat $\varphi$, check that L(B) $\cap$ L($! \varphi$) = $\emptyset$.

\begin{enumerate}
    \item Build büchi automaton $B_{!\varphi}$
    \item Compose ($ B || B_{!\varphi}$)
    \item Search (infinite) accepting trace in $(B||B_{!\varphi})$.
\end{enumerate}

Hard because seach for \textit{infinite traces} but $B || B_{!\varphi}$ is \textit{finite}

The  goal to achieve  that is  to find a  lasso shaped
trace with an accepting  state on the loop.
($\sigma = \sigma_1 \sigma_2^{\omega} $ where $\sigma_1, \sigma_2$ are finite)

We can do that by using the SCC algorithm or the double DFS.

\subsection{Double DFS algorithm}
Given a Buchi automaton $B = <S,\Sigma,\Delta,q_0,F>$, return true \textcolor{red}{IFF} B has an accepting trace.

\lstinputlisting{code/double_dfs_algorithm.lts}

\subsubsection{Example}

\todo[inline]{Feel free to contribute: add or explain example beginning at
slide 39 chap 8b}

\section{Question 16: Model-checking optimization}

\textit{Discuss the  complexity of  model checking  for LTL.  Define the
notion of  independent transitions, and  explain how  it can be  used to
optimize model-checking. Explain the  principles of bitstate hashing and
discuss its impact on model-checking.}

\subsection{Complexity}
\begin{itemize}
    \item $| \quad B||B_{!\varphi} \quad |  =  O(|B|\times|B_{!\varphi}|$) where  $|B|$  is the number of transitions in $B$.
\end{itemize}

The complexity is \textbf{linear} with respect to the sizes of
the LTS and automaton but it's \textbf{exponential} in the size of the FSP model
(\textcolor{red}{state-space} explosion)

\begin{itemize}
    \item Indeed, $|B!\varphi|$ =  $O(2^{|\varphi|})$ (each value in the  formula can be
true  or false)  $\Rightarrow$  exponential in  the  size of  the  LTL formula.
\end{itemize}

\subsubsection{In pratice}
The size  of  the model  is the  big  factor, because  formula
remains relatively small.

\subsection{Fighting state-space explosion}
There are different solutions:

\begin{enumerate}
    \item Avoid exploring redundant executions (\textbf{partial-order reduction})
    \item Merge similar states (We don't see this part)
    \item Partial model-checking (\textbf{bitstate hashing} and guided search not see
        in this part)
\end{enumerate}

\subsection{Independent Transition and Optimization}

To reduce the state-space explosion, we try to avoid exploring redundant
execution  by partial  order reduction,  it's in  this context  that the
notion of  independent transition is important.

\textbf{Partial order reduction} consists of three things:

\begin{enumerate}
    \item{Independency:} Two transitions $q-a\rightarrow q'$,$q-b\rightarrow q''$ are independent \textcolor{red}{IFF} there is a state $q'''$ such that $q''-a\rightarrow q'''$ and $q'-b\rightarrow q'''$

       (\textit{Hard to determine without exploring full graph})
    \item{Invisibility:} A transition $q-a\rightarrow q'$ is invisible with respect to a property $\varphi$ \textcolor{red}{IFF} it does not affect the  value of $\varphi$

        (\textit{Action don't change de fluent value})

    \item{Ample set:} A set $A$ of enabled actions such that
    \begin{itemize}
        \item $A$ is not empty unless there is no enabled action.
        \item Any execution of action other than $A$ from $q$ contains only actions that are independent from actions in $A$
        \item Action in $A$ are invisible with respect to the verified property unless $A$ contains all enabled actions
        \item On any cycle, an enabled action is not ignored indefinitely
    \end{itemize}
\end{enumerate}

\begin{figure}[!ht]
    \centering
    \begin{tabular}{cc}
        \includegraphics[height=3cm]{independant.png}
        &
        \includegraphics[height=3cm]{ample.png}
        \\
        Independency & Ample set \\
    \end{tabular}
    \caption{Partial order reduction}
\end{figure}


\subsubsection{Fonctionnement}

Partial  Order Reduction  consists  in exploring  only  ample sets,  they
are  used  to  reduce  the   number  of  interleavings  explored  during
model-checking.

\subsection{Bitstate hashing}

Let \textbf{h} be  the size of a hashtable and  \textbf{r} the number of
states stored in the hashtable.

If keep $r<<h$,  most states have a unique  hash code (\textcolor{red}{no
collision}). With Bistate Hashing, instead of keeping the full state in
the hashtable,  we only have  very large hashtable that  contains single
bits.

When we want to  add a state q, we simply set $h(q)$ to 1.

The  problem  is  that  if  there are collisions  some  states  can  be
\textbf{missed} but the probability is low if $r << h$.

\paragraph{ }
With \textbf{bloom filters} where  we set k
bits per states (in  different cells), the number of collisions is reduced
but the table fills k times more quickly.

\section{Question 17: Petri nets }

\textit{Using a  small example,  explain the  structure and  dynamics of
Petri  nets. Formalize  their  semantics in  mathematical terms.  Define
reachability and  coverability of  states. Explain the  construction and
usage of the coverability tree. }

\subsection{Structure of PN}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=10cm]{petri.png}
    \caption{Petri-net example}
\end{figure}

A petri net is a bipartite graph. It's composed of:
\begin{itemize}
    \item Place $\bigcirc$ which represents a condition
    \item Transition $\arrowvert$ which represents an event
    \item Arc $\to$ that relates event to transition.
        \begin{enumerate}
            \item place-transition arcs = input = required
            \item transition-place arcs = output = resulting
        \end{enumerate}
\end{itemize}

The structure of a petri net graph is as follow $$N = <P,T,A,w,x>$$ where

\begin{itemize}
    \item $P$ is a finite set of places
    \item $T$ is a finite set of transitions
    \item $A \subseteq(PxT)\bigcup(TxP)$ is a finite set of arcs
    \item $w:A\to N_0$ is the weight function on the arcs
    \item $x:P\to N$ is a marking or state (the number of tokens in each place)
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{weight.png}
    \caption{Arc weight}
\end{figure}

When a  transition is  triggered, the  number of  tokens in  the incoming
places  of  the transition  are  moved  to  the  outgoing place  of  the
transition according to the weight of the arc (incoming and outgoing).

A  transition can  only be triggered  if there  are enough  token in  the
incoming place compared to the weight of the arc.

\subsection{Semantics of PN}
\label{petri}
\paragraph{Place, transition and marking} are represented as vector:

\begin{itemize}
    \item $P = [p_{1},p_{2},\ldots,p_{n}]$
    \item $T = [t_{1},t_{2},\ldots,t_{n}]$
    \item $x = [x(p_{1}),x(p_{2}),\ldots,x(p_{n})]$
\end{itemize}

\paragraph{ } For a  transition t $\in$ T,
\begin{eqnarray*}
    \bullet t = I(t)  &=& {p \in P  |(p,t) \in A} \textrm{ are the  input places of t }\\
    t\bullet=O(t) &=& {p\in P|(t,p)\in A}\textrm{ are the  output places of t }
\end{eqnarray*}

(same thing can be done for the places).

\paragraph{ }  Given a  petri net $N=(P,T,A,W,x)$, \textbf{the transition
function} $f:N^{n}\times T\to N^N$ is a partial fucntion defined as
\begin{itemize}
    \item $f(x,t)$ is defined \textcolor{red}{IFF} $\quad \forall p \in \bullet  t,x(p) \geq w(p,t)$
    \item if $f(x,t)$ is  defined, \textcolor{red}{THEN} f(x,t) = x' where  $ \quad x'(p) =  x(p)-w(p,t)+w(t,p)$
\end{itemize}

\subparagraph{Attention},  it means  that  the number  of  token is  not
always constant.

\subsection{State equation}
$$ x' = x + u A$$


\subsection{Reachability}

\subsubsection{Reachable state space}
For a \textit{sequence of transition} $\sigma \in T*$:
\begin{eqnarray*}
    f(x, \epsilon) &=& x\\
    f(x, \sigma t) &=& f(f(x, \sigma), t)
\end{eqnarray*}

\paragraph{ }
\begin{description}
    \item[Reachable state space $R(n) \subseteq N$] for a given Petri net N = (P, T, A, w, x):
        $$R(N) = {x' \in N^n  \quad| \quad \exists \sigma \in T*,f(x, \sigma=x')}$$
        (it means all the states marking reachable from x through transition in T)

    \item[Reachability:] given a trajectory equation, a state x' is reachable from x
        \textcolor{red}{IF} $x = \sigma\Rightarrow x'$ with $\sigma$ a sequence of transitions.

        If this is \textbf{true} then there is a firing count vector v such a $$\textrm{vA = x' – x}$$
    Implication in one way only but, if no solution then x' not reachable.
\end{description}

\subsection{Coverability}

\begin{description}
    \item[State coverage:] A state x covers a state y $(x \geq y)$ \textcolor{red}{IFF}
        $$ x(p) \geq y(p) \quad \forall p \in P$$

        The coverage state space grows \textit{monotonically} with the initial marking

    \item[Coverability:] A state x is coverable in Petri net N \textcolor{red}{IFF}
        $$ x\leq y \quad \textrm{for some} \quad y \in R(N) $$
\end{description}

Coveraibility is an approximation of reachability.


\paragraph{ } For a transition $ t \in T, let pre(t) = [w(p_1, t), \cdots, w(p_n, t)]$
t \textbf{can be fired} in a Petri net \textcolor{red}{IFF} pre(t) is \textbf{coverable} in N


\subsubsection{Construction coverability tree}

\paragraph{Principle}: Group all nodes  [1 0 \textcolor{red}{k}] for all
k under a signe node [1 0 $\color{red}\omega$]

(If we have [1 0 0] and [1 0 1]  on the same path, we will have [1 0 2],
[1 0 3], \ldots)


\begin{figure}[!ht]
    \centering
    \includegraphics[width=4cm]{cove_tree.png}
    \caption{Example}
\end{figure}

\paragraph{Algorithm}

\begin{enumerate}
    \item Add $x_0$ to the new nodes
    \item WHILE there is a new node x

        \begin{enumerate}
            \item IF there is y = x on the path to x THEN x is a \textit{duplicate node}
            \item ELSE IF x has no enabled transitions THEN x is a \textit{terminal node}
            \item ELSE FOR each $x -t \to x'$

                \begin{enumerate}
                    \item add $x -t \to x'$ to the tree and add x' to the new nodes

                    \item IF $x(p) = \omega$ THEN set $x'(p) = \omega$
                    \item IF there is $y < x'$ on the path to $x$
                        THEN set $x'(p) = \omega$ for all p such that $x'(p) > y(p)$
                \end{enumerate}
        \end{enumerate}
\end{enumerate}

\paragraph{Property}
\begin{itemize}
    \item Coverability tree of any petri net is \textbf{finite}
    \item Coverability tree \textbf{covers all states} in $R(N)$
\end{itemize}

\paragraph{For a coverability tree for a petri net N given}:

\begin{itemize}
    \item N is \textit{bounded} \textcolor{red}{IFF} no $\omega$ in the tree. (coverability tree = reachability tree)
    \item y \textit{coverable} in N \textcolor{red}{IFF} there is somme x $\geq$ y in the tree.

        (if $x(p) = \omega$ for some p, then the path to x contains loops)
    \item $\gamma$ is \textit{conserved} by N \textcolor{red}{IFF} $x \omega^T = C$ for all x in the tree.

        ($\gamma(p) = 0$ fo all p such that $x(p) = \omega$ for some x)
\end{itemize}



\section{Question 18: Petri nets linear algebra}

\textit{Using a  small example,  explain the  structure and  dynamics of
Petri  nets. Develop  the matrix-based  representation of  Petri nets:
incidence matrix,  trajectory equation,  firing count vectors.  Show how
this can be used to compute conservation and periodicity properties}

\subsection{Structure of PN}
See~\ref{petri} page~\pageref{petri}

\subsection{Matrix-Based Representation}

\begin{description}
    \item[Incidence matrix] $A=a_{ji}$ of size m x n such that $a_{ji} = w(t_j,p_i)-w(p_i,t_j)$

    \item[Trajectory Equation]: For a sequence of transitions
        $$\sigma=t^1...t^k\in T* \quad \textrm{such that} \quad x=\sigma \Rightarrow x'$$

        we have \textbf{x' = x+vA} where v is the firing vector (number of time that a transition is fired over $\sigma$)

    \item[Firing count vector] If x = $\sigma \Rightarrow x'$ then
        $$ x' = x + v A$$
\end{description}


\subsection{Conservation}
Puts lower-bound on \textit{combination} of places.

\begin{description}
    \item [Place-invariant]: vector of weights for the markings of places.
    \item [Weighted marking]: weighted initial marking regardless of firing count.
\end{description}

\paragraph{ }
Consider a \textbf{weighting vector} $\gamma = [ \gamma(p_1), \cdots, \gamma(p_n)]$,
a Marked Petri net N is \textbf{conservative} w.r.t $\gamma$ \textcolor{red}{IFF}
$$ \sum p\in P \quad \gamma(p) x(p) = x \gamma^T \quad \textrm{is a constant } C$$

\subparagraph{$\gamma$ is a \textbf{place-invariant} of N},
it's a property of graph independent of the initial marking.


\subsection{Periodicity}
Given firing count vector restores initial marking.
Transition invariant = firing count vector.

\paragraph{ }
A Marked Petri net N is \textbf{periodic} w.r.t v \textcolor{red}{IFF}
$$ f(x, \sigma) = x \quad \textrm{for all } \sigma \textrm{ whose firing count vector is v }$$

\subparagraph{v is a \textbf{transition-invariant} of N},
it's a property of graph independent of the initial marking.

\subsection{Compute}
Use the incidence matrix to compute.

\end{document}
