\documentclass[en]{../../../../../../eplexam}
\usepackage{parskip}

\hypertitle{Discrete mathematics II - Algorithms and complexity}{7}{INMA}{2111}{2015}{June}{All}
{Adrien Banse}
{Jean-Charles Delvenne}

\section{Question 1}

Many interesting graph problems are NP-complete. For example HAMILTONIAN-CYCLE is the prolem of checking whether in a given undirected graph there exists a Hamiltonian cycle, a cycle visiting all nodes once and only once. In other words, the output on an input graph $G$ is 'yes' iff $G$ admits a Hamiltonian cycle. CLIQUE is the problem, given an undirected graph $G$ and an integer $k$, of checking whether $G$ admits a clique of at least $k$ nodes. COLOURING is the problem, given an undirected graph $G$ and an integer $k$, of checking whether $G$ admits a proper node colouring (i.e., such that neighbours always have different colours) of at most $k$ colours. All those problems and many more are NP-complete. \\Prove that SUBGRAPH-ISOMORPHISM is also NP-complete. It is defined as the problem of checking whether a given undirected graph $G_1$ is isomorphic to a subgraph of $G_2$, i.e. there is an injective mapping of nodes and edges of $G_1$ to nodes and edges of $G_2$ such that neighbours in $G_1$ are mapped to neighbours of $G_2$.\\In your proof, you can use NP-completeness of the three problems above. It is important that the structure of the proof appears clearly.

\begin{solution}

First, let us prove that SUBGRAPH-ISOMORPHISM belongs to NP. 

Let us assume we have access to a certificate $\sigma = \sigma_V \times \sigma_E$ for $(G_1, G_2)$ where $\sigma_V : (V_1  \to V_2)$ and $\sigma_E : (E_1  \to E_2)$. 
Graph $G_i$ has nodes in $V_i$ and edges in $E_i$.
Then we described the following deterministic machine $M((G_1, G_2), \sigma)$ as follows : for each edge in $e = (u, v) \in E_1$ with $u, v \in V_1$, we check if $\sigma_E(e) = (\sigma_V(u), \sigma_V(v))$. 
If all checks are satisfied, then the machine answers 'yes' if and only if the certificate is correct, and it runs in $\mathcal{O}(|E_1|)$, which is linear.

Now let us prove that we can reduce CLIQUE to SUBGRAPH-ISOMORPHISM.

To find if there is a clique of at least $k$ nodes in a graph $G$, we just have to check if $K_k$, the complete graph with $k$ nodes, is isomorphic to a subgraph of $G$. Since $K_k$ is a subgraph of $K_{k+1}$ for any $k \geq 1$, the fact that $K_k$ is isomorphic to a subgraph of $G$ implies that there is a clique of at least $k$ nodes. 

Since every problem in NP can be reduced to CLIQUE and CLIQUE can be reduced to SUBGRAPH-ISOMORPHISM, both problem are equivalent, which implies that SUBGRAPH-ISOMORPHISM belongs to NP-complete.

\end{solution}

\section{Question 2}

\nosolution

\section{Question 3}

In the course we showed that a multiplication of two $n$-digits numbers $x$ and $y$ can be achieved by splitting each number into two smaller numbers of approximately $n/2$ digits, and expressing the product $x \times y$ from a linear combination of three smaller products. Proceeding through recursive splittings in a divide-and-conquer fashion, one gets $\mathcal{O}(n^{\log_2 3})$ complexity for the multiplication. In this question we split each number $x$ and $y$ into three smaller numbers, say $x = x_2x_1x_0$ and $y = y_2y_1y_0$ (as written in their decimal expansion), of approximately $n/3$ digits.
\begin{enumerate}

\item Express $x \times y$ as a linear combination of nine smaller products. Proceeding to recursive splittings, what is the resulting complexity?
\item Consider that $x \times y$ can be expressed as a linear combination of five smaller products. Hint: consider products of the form $(\alpha x_2 + \beta x_1 + \gamma x_0) (\alpha y_2 + \beta y_1 + \gamma y_0)$, for five well-chosen triplets $(\alpha, \beta, \gamma)$; you need not find an explicit expression of $x \times y$ in terms of those five products, only show that it exists. Proceeding to recursive splittings, what is resulting complexity? Is it better or worse than $\mathcal{O}(n^{\log_2 3})$?
\item We now consider splitting each factor into $k$ parts of $n/k$ digits. Show in this context that $x \times y$ can be expressed as a linear combination of $2k-1$ smaller products. Prove that for any $\epsilon > 0$ there is a multiplication algorithm in $\mathcal{O}(n^{1 + \epsilon})$.

\end{enumerate}

A general fact that is always good to know is that the square matrix $\left( t_i^{j-1} \right)_{1 \leq i, j \leq n}$ has determinant $\prod_{i < j} t_j - t_i$ (Vandermonde's determinant), thus is invertible for $t_i \neq t_j$.

\begin{solution}
\begin{enumerate}
% 3.1
\item
First we show the nine smaller products :
\begin{equation}
\begin{aligned}
	x \times y 	&= x_2x_1x_0 \times y_2y_1y_0  \\
			&= (x_2 10^{2\lfloor n/2 \rfloor} + x_1 10^{\lfloor n/2 \rfloor} + x_0) \times (y_2 10^{2\lfloor n/2 \rfloor} + y_1 10^{\lfloor n/2 \rfloor} + y_0)  \\
			&= (x_2 \times y_2) 10^{4\lfloor n/2 \rfloor} + (x_2 \times y_1 + x_1 \times y_2) 10^{3\lfloor n/2 \rfloor} + (x_2 \times y_0 + x_1 \times y_1 + x_0 \times y_2) 10^{2\lfloor n/2 \rfloor} \\
			& \quad \quad \quad + (x_1 \times y_0 + x_0 \times y_1) 10^{\lfloor n/2 \rfloor} + (x_0 \times y_0).
\end{aligned}
\end{equation}

Now we focus on complexity. Note $T(n)$ the complexity of the algorithm described above. Assume $n = 3^k$, it gives
\begin{equation}
	T(n) = 9T\left( \frac{n}{3} \right) = 9T\left(3^{k-1}\right) = \dots = 9^k T(1).
\end{equation}
Since $T(1) = \mathcal{O}(1)$, it exists $c \in \mathbb{R}$ such that $T(n) = c9^k$. Noticing $9^k = (3^2)^k = (3^k)^2 = n^2$, it gives $T(n) = \mathcal{O}(n^2)$.

% 3.2
\item
Let us note that we can write $x$, $y$ and $p := x \times y$ in a polynomial representation $x(z)$, $y(z)$ and $p(z) = x(z)y(z)$ of respectively degree $2$, $2$ and $4$ as it is shown above. Indeed: 
\begin{equation}
\begin{aligned}
	x(z) = x_2 z^2 + x_1 z + x_0 \\
	y(z) = y_2 z^2 + y_1 z + y_0 \\
	p(z) = p_4 z^4 + p_3 z^3 + p_2 z^2 + p_1 z + p_0
\end{aligned}
\end{equation}

The problem can be reduced to the problem of finding the polynomial $p$.

In a general way, we know that a polynomial $p(z)$ of degree $d$ is entirely determined by $d+1$ evaluations of the polynomial $p(\mathbf{a})$ where $\mathbf{a} = (a_1, \dots, a_d) \in \mathbb{R}^{d+1}$ is a vector of $d+1$ distinct points, since its coefficients (vector $\mathbf{p} \in \mathbb{R}^{d+1}$) is the solution of the linear system
\begin{equation}
	V(\mathbf{a}) \mathbf{p} = p(\mathbf{a}).
\label{vandermonde}
\end{equation}
Indeed, since $V(\mathbf{a})$ is invertible for such $\mathbf{a}$ (as explained in the statement), the solution is unique and is $V(\mathbf{a})^{-1} p(\mathbf{a})$. 

Therefore we propose the following algorithm: we take 5 distinct points $\{ a_1, \dots, a_5 \}$ as $p(z)$ has degree 4, and we compute $\{ x(a_1), \dots, x(a_5) \}$ and  $\{ y(a_1), \dots, y(a_5) \}$. We can now compute 
\begin{equation}
	p(a_i) = x(a_i) \times y(a_i) = (a_i^2 x_2 + a_i x_1 + x_0) \times (a_i^2 y_2 + a_i y_1 + y_0), \quad \forall i = 1, \dots, 5,
\end{equation} 
and thanks to the solution $V(\mathbf{a})^{-1} p(\mathbf{a})$, we know $p(10)$, which is the result. Note that if we choose $\mathbf{a}$ in advance, we can also solve \ref{vandermonde} only once in advance. To make the link with the statements, we have $\alpha_i = a_i^2$, $\beta_i = a_i$ and $\gamma_i = 1$.

Now we focus on complexity. Note $T(n)$ the complexity of the algorithm described above. Assume $n = 3^k$, it gives
\begin{equation}
	T(n) = 5T\left( \frac{n}{3} \right) = 5T\left(3^{k-1}\right) = \dots = 5^k T(1).
\end{equation}
Since $T(1) = \mathcal{O}(1)$, it exists $c \in \mathbb{R}$ such that $T(n) = c5^k$. Noticing $5^k = 3^{\log_3(5^k)} = 3^{k\log_3 5} = (3^{k})^{\log_3 5} = n^{\log_3 5}$, it gives $T(n) = \mathcal{O}\left(n^{\log_3 5}\right)$.

Since $\log_3 5 < \log_2 3$, it is an improvement asymptotically.

% 3.3
\item
Since the product polynomial has degree $2(k - 1)$, it follows from the question 3.2 that we need $2k - 1$ points in order to interpolate it. Following the same reasoning as above, it proves that we can express $p(10)$ with $2k-1$ smaller products.

In the same way as above, we note $T(n)$ the complexity of the algorithm described above. Assume $n = k^p$, it gives
\begin{equation}
	T(n) = (2k-1)T\left( \frac{n}{k} \right) = (2k-1)T\left(k^{p-1}\right) = \dots = (2k-1)^p T(1).
\end{equation}
Since $T(1) = \mathcal{O}(1)$, it exists $c \in \mathbb{R}$ such that $T(n) = c(2k-1)^p$. Now we note that
\begin{equation}
	(2k-1)^p = k^{\log_k (2k-1)^p} = k^{p \log_k (2k-1)} = n^{\log_k(2k-1)}
\end{equation}

Thus, the claim of the statement is: for any $\epsilon > 0$, there exists $k$ such that the function $f(k) = \log_k(2k-1) = 1 + \epsilon$. Since $f(k) = 1$ for $k = 1$, and that we only consider values $k > 1$ (since $k =1$ is $n=1$, and we assume $T(1) = \mathcal{O}(1)$), then, by continuity and since $f$ is increasing, the claim is true.

\end{enumerate}
\end{solution}

\end{document}




























