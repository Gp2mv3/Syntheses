\documentclass[en]{../../../../../../eplexam}

\usepackage{parskip}
\usepackage{algpseudocode}

\hypertitle{Discrete mathematics II - Algorithms and complexity}{7}{INMA}{2111}{2016}{June}{All}
{Adrien Banse}
{Jean-Charles Delvenne}

\section{Question 1}

Guillaume, 6 years old, and Henri, 4 years old, collect Paganini stickers for the Euro Cup of Football. There are no less than 325 stickers, to be placed in a beautiful Collector's Book (common to the two brothers). One gets a sticker for every Euro spent in any CrossRoads supermarket.
\begin{enumerate}
\item How much does Guillaume's mother expect to spend in CrossRoads supermarket in order to complete the Collector's Book ? Call $\mu$ this value. NB: you are expected to prove it, without resorting to a ready made formula from the course.
\item What is the standard deviation (i.e. square root of variance) around the excepted value ?
\item Prove that for any nonnegative random variable $X$ of mean $\mu$, one has $\text{Proba}[ X > k\mu ] < 1/k$, for any real $k > 0$. This is called Markov's inequality. You can assume that $X$ is valued in the integers if it simplifies notations.
\item Guillaume's mother will spend 3 000 Eur on groceries before the end of June, at which point stickers are no longer issued. What is a bound on the probability that the Collector's Book isn't complete by the end of June? Use Markov's inequality here.
\item Prove that for any real random variable with mean $\mu$ and standard deviation $\sigma$, $\text{Proba}[|X - \mu| > k\sigma] < 1/k^2$ (Chebyshev's inequality), by using Markov's inequality on the random variable $Y = (X-\mu)^2$.
\item Answer again to item 4 using Chebyshev's inequality. Is it a tighter or loser inequality than Markov's, in this instance?
\item Thanks to a clever shopping strategy, only 5 stickers are now left to be found. How many Euros are still expected to be spent at CrossRoads before completion?
\end{enumerate}

\begin{solution}
\begin{enumerate}

% 1.1
\item
In order to have all the stickers, the only possible event is to get any sticker, then get any sticker except the first one, then get any sticker except the first two ones, etc.. Let $M$ be the random variable of number of trials before having all the stickers, it follows that 
\begin{equation}
	M = M_{325} + \dots + M_1 
\end{equation}
where $M_i \sim \text{Geom}\left(\frac{i}{325}\right)$ is the the number of trial to get any sticker except $325 - i$ sticker from the 325 existing ones. It follows that
\begin{equation}
\begin{aligned}
	\mathbb{E}(M) = \mu 	&= \mathbb{E} (M_{325}) + \dots +\mathbb{E} (M_1) \\
						&= \sum_{i = 1}^{325} \frac{325}{i} \\
						&= 325 \sum_{i = 1}^{325} \frac{1}{i} \\
						&\approx 325 \ln 325.
\end{aligned}
\end{equation}
We can evaluate $\ln 325$ by saying $\ln 325 \approx \ln 300 \approx 4.6 + 1.1 = 5.7$. It gives $\mu \approx 1852.5$.

% 1.2
\item
Following the same notations, we have
\begin{equation}
\begin{aligned}
	\mathbb{V}(M) 	&= \sum_{i = 1}^{325} \mathbb{V}(M_i) \\
				&= \sum_{i = 1}^{325} \frac{1 - \frac{i}{325}}{\left(\frac{i}{325}\right)^2} \\
				&= 325^2 \sum_{i = 1}^{325} \frac{1}{i^2} - 325 \sum_{i = 1}^{325} \frac{1}{i} \\
				&\approx 325^2 \frac{\pi^2}{6} - \mu.
\end{aligned}
\end{equation}
So calling the standard deviation $\sigma$, it gives $\sigma \approx \sqrt{325^2 \frac{\pi^2}{6} - \mu}$. Since we can say that $325^2 \frac{\pi^2}{6} \gg \mu$, we claim that $\sigma \approx \sqrt{325^2 \frac{\pi^2}{6}} = 325 \sqrt{\frac{\pi}{6}} \approx 422.5$.

% 1.3
\item
For any real $k > 0$, assuming $f$ is the probability density function of $X$, since $X$ is nonnegative:
\begin{equation}
\begin{aligned}
	\text{Proba}[X > k\mu] 	&= \int_{k\mu}^{\infty} f(x) \text{d}x  \\
						&= \frac{1}{k\mu}  \int_{k\mu}^{\infty} k\mu f(x) \text{d}x \\
						&< \frac{1}{k\mu}  \int_{k\mu}^{\infty} x f(x) \text{d}x \\
						&\leq  \frac{1}{k\mu}  \int_{0}^{\infty} x f(x) \text{d}x \\
						&= \frac{1}{k}.
\end{aligned}
\end{equation}

% 1.4
\item
Using Markov's inequality, it gives $\text{Proba}[M > k\mu] < 1/k$ with $k\mu = 3000$, so the bound is $\text{Proba}[M > 3000] < \mu / 3000$. We claim that $\mu \approx 6 \times 325$, which gives that $\mu / 3000 \approx 0.65$.

% 1.5
\item
We note $\mathbb{E}(Y) = \mathbb{E}((X-\mu)^2) = \sigma^2$. Knowing that, by using Markov's inequality, it gives directly that, for any real $k > 0$: 
\begin{equation}
\begin{aligned}
			&\text{Proba}[Y > k^2\mathbb{E}(Y)] < \frac{1}{k^2} \\
	\iff 		&\text{Proba}[(X-\mu)^2 > k^2\sigma^2] < \frac{1}{k^2} \\
	\iff 		&\text{Proba}[|X-\mu| > k\sigma] < \frac{1}{k^2}.
\end{aligned}
\end{equation}

% 1.6
\item
Using Chebyshev's inequality, the event that we seek to measure is $|X-\mu| > 3000 - \mu$. So $k\sigma = 3000 - \mu$, which gives that $k = \frac{3000 - \mu}{\sigma}$. The bound is thus $\text{Proba}[|X - \mu| > 3000 - \mu] < \left(\frac{\sigma}{3000 - \mu} \right)^2$. 

Noticing that $k > 2$, the probability of such event to happen is at most $1/4$, which is already way better bound than the one obtained in item 4.

% 1.7
\item
Following the same idea as above, noting $\tilde{M}$ the random variable of remaining Euro to spend, and $\tilde{\mu}$ its expected value:  
\begin{equation}
	\mathbb{E}(\tilde{M}) = \tilde{\mu} = \mathbb{E}(M_5) + \dots + \mathbb{E}(M_1) = 325 \left(\frac{1}{5} + \frac{1}{4} + \frac{1}{3} + \frac{1}{2} + 1\right)
\end{equation}
Since $\frac{1}{5} + \frac{1}{4} + \frac{1}{3} + \frac{1}{2} + 1 = 137/60 \approx 138/60 = 2.3$, we claim that $\tilde{\mu} \approx 747.5$ (overestimated).

\end{enumerate}
\end{solution}

\section{Question 2}
Nothing is more fun for an Elephant's Child than going down to great grey-green greasy Limpopo River. To avoid their prominent noses being teased by mischievous crocodiles the Elephant's Children go down riding hippos. The wily hippos have started a profitable business called 'Hip! Hip! Hip! Hippos!'.

The are $n$ embankments for the elephants, and going downstream from embankments $i$ to $j$ costs $c_{ij}$ melons (the greeny-crackly kind). Elephants never go upstream, it's no fun. 

The elephants, always short on melons (the greeny-crackly kind especially), hire an engineer from UCL (as their reputation reaches the deep savannah and even beyond) to find the minimum cost of the whole ride from 1 to $n$, given the $c_{ij}$ matrix.

\begin{enumerate}
\item Propose a divide-and-conquer strategy to solve the problem.
\item Analyse the time and space (memory) complexity of that strategy, as a function of $n$.
\item Propose a dynamic programming strategy.
\item Analyse the time and space (memory) complexity of that strategy, as a function of $n$.
\item An Elephant's Child with satiable curiosity proposes the following greedy strategy: from embankment 1, pick $j$ as next stop, that minimises $c_{1j}/(j-1)$; then find $k>j$ that minimises $c_{jk}/(k-j)$, etc until reaching $n$. Prove that this strategy is not always optimal.
\end{enumerate}

\begin{solution}
\begin{enumerate}

% 2.1
\item
We propose the following divide-and-conquer strategy. For each instance that goes from 1 to $n$, we compute the solution for the instance that goes from 1 to $\lceil n/2 \rceil$ (we call it the above instance) and the one for the instance that goes from $\lceil n/2 \rceil + 1$ to $n$ (the below instance). Then we merge the solutions in the following way. 

We compute the cheapest gap between the above and the below instances, let us say node $a$ to $b$. We know that the merged solution is the solution of the above instance where we removed all the path after $a$, the gap, and the solution of the below distance where we removed all the path before $b$. 

% 2.2
\item 
Noting the time complexity as $T(n)$, and assuming that we can write $n = 2^k$, it satisfies the equation $T(n) = 2T\left(\frac{n}{2}\right) + \Theta(n^2)$. Indeed, the merging part requires $\Theta(n^2)$ operations since it has to do $n^2/4$ comparaisons in order to find the minimal gap. It yields
\begin{equation}
\begin{aligned}
	T(n) 	&= 2T\left(\frac{n}{2}\right) + \Theta\left(n^2\right) \\
		&= 2T\left( 2^{k-1} \right) + \Theta\left(2^{2k}\right) \\
		&= 4T\left( 2^{k-2} \right) + \Theta\left(2^{2k} + 2^{2k}\right) \\
		&= \dots \\
		&= 2^k T(1) + \Theta\left( k 2^{2k}  \right) \\
		&= \Theta\left( k 2^{2k}  \right)
\end{aligned}
\end{equation}
which is $\Theta\left(n^2 \log_2 n  \right)$.

Concerning the space complexity, it requires $\Theta(n^2)$ memory slots since the heaviest object is the matrix $(c_{ij})_{1 \leq i < j \leq n}$ (in addition to the path, which is $\Theta(n)$). 

% 2.3
\item
Note that we consider that for embankment $i$ that does not go to embankement $j$, we set $c_{ij} = + \infty$.

We know that if the path $1 = p_1, p_2, \dots, p_{n-1}, p_n = n$ has minimal cost, then it is also the case for the sub-path $1 = p_1, \dots, p_{n-1}$. So we propose the following strategy.

We construct a $n \times n$ array $A$ as described below, in which the rows are source nodes and columns are destination nodes. Note that for this particular problem, we will only need the upper triangle of the array.

We begin by filling the $n$-th column with $c_{in}$ while saving the minimum along these values. Then we construct the rest of the array in the following way. For $i < j$, 
\begin{equation}
	A_{ij} = c_{ij} + \min_{k = 1, \dots, n} A_{k, j+1}.
\end{equation}
By doing so, during the execution, for each $i < j$, $A_{ij}$ is the minimal cost to go from $i$ to $n$ by taking edge $ij$. The answer is thus $\min_{i = 1, \dots, n} A_{1i}$ in the end.

% 2.4
\item
Each column requires $\Theta(n)$ computations. Computing the minimum for each columns also takes $\Theta(n)$ computations (it can also be done while filling it). Time complexity of filling the array takes is thus $\Theta(n^2)$. Finally, computing the answer is computing a minimum for the first row, and it also takes $\Theta(n)$ computations, so the total time complexity of the algorithm is $\Theta(n^2)$. 

The space complexity is the same as previously, i.e. $\Theta(n^2)$ since we only need to store the additional array $A$, which is a $n \times n$ array. 

% 2.5
\item
Consider the problem defined by
\begin{equation}
C = 
\begin{pmatrix}
 & 1 & 5 \\
 &   & 8 \\
 &   &
\end{pmatrix}
\end{equation}
The solution found with the algorithm described by the curious Elephant's Child is 9, while there is a solution from that costs 5 (directly from 1 to 3).

\end{enumerate}
\end{solution}

\section{Question 3}
Among all halting Matlab functions (i.e., Matlab functions that enventually halt) taking no input and written with $n$ characters or less, take the one that has the maximum halting time. We call this Matlab function the $n$th Busy Beaver (or 'Castor Affairé'), running in time $BB_n$. Note that $n \mapsto BB_n$ is non-decreasing and defined for all $n$ (as the empty Matlab function stops immediately).
\begin{enumerate}
\item Prove that $BB_n$ is not computable. 
\item Prove that $BB_n$ grows faster than any computable function defined everywhere on the integers. 
\item The $n$th Max Memory Busy Beaver is the halting inputless Matlab function using maximum memory and written with $n$ characters or less. Call $MMBB_n$ the memory used. This is again well defined for all integers. Prove that $MMBB_n \in \mathcal{O}(BB_n)$.
\item Prove that $MMBB_n$ is not computable. 
\item Prove that $MMBB_n$ grows faster than any computable function defined everywhere on the integers. 
\end{enumerate}

\begin{solution}
\begin{enumerate}

% 3.1
\item
Assume $BB_n$ is computable, then there is a Turing machine that computes $BB_n$. Assume we have acces to a Matlab function $f$ that has $n$ characters. We can construct a Matlab program that solves the halting problem in the following way. 

The program computes $BB_n$ and saves the result. It then executes each instruction of $f$, and counts them. If the number of instructions is above $BB_n$, then the function will never halt. Otherwise it halts. 

Since the halting problem is not decidable, $BB_n$ cannot be computable.

% 3.2
\item 
Assume $g$ is a computable function such that $g(n) > BB_n$ for arbitrarily large $n$. Then, in the same way as above, there is a Matlab program that solves the halting problem for any Matlab function $f$ that has $n$ characters in the following way. 

The program computes $g(n)$ and saves the result. It then executes each instruction of $f$, and counts them. Either the number of instructions will reach $BB_n$, and thus reach $g(n)$ since it nevers halts by definition of $BB_n$, or it halts before that the counter reaches $BB_n$. 

Since the halting problem is not decidable, $g$ cannot be greater than $BB_n$ for arbitrarily large $n$.

% 3.3
\item
In $N$ step of time one can only write $N$ new symbols in memory. It follows that $MMBB_n \in \mathcal{O}(BB_n)$.

% 3.4
\item
This is the same proof as for item 1 but by counting the symbols in memory. 

% 3.5
\item
This is the same proof as for item 2 but by counting the symbols in memory. 

\end{enumerate}
\end{solution}

\end{document}
































